==========================================
Experiment 1: Fine-tuning GPT-2 on German
Job ID: 2146661
Start time: Mon Dec 29 02:50:27 PM CET 2025
==========================================
Mon Dec 29 14:50:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40                     On  |   00000000:25:00.0 Off |                  Off |
| N/A   31C    P8             35W /  300W |       1MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2025-12-29__14-50-33_ab1b864305b4b8b1
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 124,439,808 parameters): weight_decay = 0.01
=> all (148 modules with 124,439,808 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2025-12-29 14:50:36.535642.



======================== Training Report ========================
Training target: 
	num_target_tokens: 629686272
	num_target_steps: 76866 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 1
CUDA environment settings: 
	local_rank: 0
	world_size: 1
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (629691904) does not match the number of target tokens (629686272). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (76866) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (76866) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (76866) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2025-12-29 14:50:36.535985.
2025-12-29T14:50:58 | step: 100 | train samples/s: 80.2 | train mfu (16-bit): -1.0 | lr mean: 6.85799568600487e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.4 | train loss last: 4.44 | consumed tokens: 819200.0 | grad norm avg: 2.95 | grad norm last: 2.87 | 
2025-12-29T14:51:19 | step: 200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.2125125067541376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.24 | train loss last: 4.56 | consumed tokens: 1638400.0 | grad norm avg: 2.72 | grad norm last: 2.6 | 
2025-12-29T14:51:39 | step: 300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.9931494534830563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.17 | train loss last: 3.84 | consumed tokens: 2457600.0 | grad norm avg: 2.78 | grad norm last: 2.91 | 
2025-12-29T14:52:00 | step: 400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.89878407784272e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.04 | train loss last: 4.5 | consumed tokens: 3276800.0 | grad norm avg: 2.86 | grad norm last: 3.09 | 
2025-12-29T14:52:20 | step: 500 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.779846156248823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.95 | train loss last: 3.89 | consumed tokens: 4096000.0 | grad norm avg: 2.82 | grad norm last: 2.91 | 
2025-12-29T14:52:41 | step: 600 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.490823994274251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.91 | train loss last: 3.56 | consumed tokens: 4915200.0 | grad norm avg: 2.63 | grad norm last: 2.45 | 
2025-12-29T14:53:01 | step: 700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9142960051540285e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.83 | train loss last: 3.88 | consumed tokens: 5734400.0 | grad norm avg: 2.47 | grad norm last: 2.43 | 
2025-12-29T14:53:22 | step: 800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.9999976909020916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.73 | train loss last: 3.7 | consumed tokens: 6553600.0 | grad norm avg: 2.28 | grad norm last: 2.4 | 
2025-12-29T14:53:42 | step: 900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9999627663055435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.42 | consumed tokens: 7372800.0 | grad norm avg: 2.1 | grad norm last: 1.9 | 
2025-12-29T14:54:02 | step: 1000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.9998852773569524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.55 | consumed tokens: 8192000.0 | grad norm avg: 1.98 | grad norm last: 1.77 | 
2025-12-29T14:54:23 | step: 1100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.999764860258438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.67 | consumed tokens: 9011200.0 | grad norm avg: 1.86 | grad norm last: 1.74 | 
2025-12-29T14:54:43 | step: 1200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.999602242605761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.3 | consumed tokens: 9830400.0 | grad norm avg: 1.78 | grad norm last: 1.88 | 
2025-12-29T14:55:04 | step: 1300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.999397060601041e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.58 | consumed tokens: 10649600.0 | grad norm avg: 1.72 | grad norm last: 1.68 | 
2025-12-29T14:55:24 | step: 1400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.999148950446397e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.45 | consumed tokens: 11468800.0 | grad norm avg: 1.66 | grad norm last: 1.67 | 
2025-12-29T14:55:45 | step: 1500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.998858639737591e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.7 | consumed tokens: 12288000.0 | grad norm avg: 1.58 | grad norm last: 1.54 | 
2025-12-29T14:56:05 | step: 1600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.998525764676742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.58 | consumed tokens: 13107200.0 | grad norm avg: 1.55 | grad norm last: 1.41 | 
2025-12-29T14:56:26 | step: 1700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9981503252638504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.16 | consumed tokens: 13926400.0 | grad norm avg: 1.48 | grad norm last: 1.52 | 
2025-12-29T14:56:46 | step: 1800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9977323214989156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.41 | consumed tokens: 14745600.0 | grad norm avg: 1.45 | grad norm last: 1.39 | 
2025-12-29T14:57:06 | step: 1900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.997271753381938e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.72 | consumed tokens: 15564800.0 | grad norm avg: 1.4 | grad norm last: 1.37 | 
2025-12-29T14:57:27 | step: 2000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.996768620912917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.36 | consumed tokens: 16384000.0 | grad norm avg: 1.38 | grad norm last: 1.3 | 
2025-12-29T14:57:47 | step: 2100 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.996222924091853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.48 | consumed tokens: 17203200.0 | grad norm avg: 1.33 | grad norm last: 1.33 | 
2025-12-29T14:58:08 | step: 2200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.9956346629187465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.16 | consumed tokens: 18022400.0 | grad norm avg: 1.3 | grad norm last: 1.48 | 
2025-12-29T14:58:28 | step: 2300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9950042011914775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.03 | consumed tokens: 18841600.0 | grad norm avg: 1.3 | grad norm last: 1.21 | 
2025-12-29T14:58:49 | step: 2400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9943311751121655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.2 | consumed tokens: 19660800.0 | grad norm avg: 1.26 | grad norm last: 1.35 | 
2025-12-29T14:59:09 | step: 2500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9936155846808106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.83 | consumed tokens: 20480000.0 | grad norm avg: 1.24 | grad norm last: 1.16 | 
2025-12-29T14:59:29 | step: 2600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9928577936952934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.69 | consumed tokens: 21299200.0 | grad norm avg: 1.23 | grad norm last: 1.14 | 
2025-12-29T14:59:50 | step: 2700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9920570745598525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.06 | consumed tokens: 22118400.0 | grad norm avg: 1.23 | grad norm last: 1.21 | 
2025-12-29T15:00:10 | step: 2800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.99121451866813e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.23 | consumed tokens: 22937600.0 | grad norm avg: 1.2 | grad norm last: 1.08 | 
2025-12-29T15:00:31 | step: 2900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.990329034626484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.2 | consumed tokens: 23756800.0 | grad norm avg: 1.2 | grad norm last: 1.13 | 
2025-12-29T15:00:51 | step: 3000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9894013500306755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.55 | consumed tokens: 24576000.0 | grad norm avg: 1.18 | grad norm last: 1.21 | 
2025-12-29T15:01:12 | step: 3100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.988431464880705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.73 | consumed tokens: 25395200.0 | grad norm avg: 1.17 | grad norm last: 1.17 | 
2025-12-29T15:01:32 | step: 3200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.987419015378691e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.95 | consumed tokens: 26214400.0 | grad norm avg: 1.17 | grad norm last: 1.26 | 
2025-12-29T15:01:53 | step: 3300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9863640015246347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.88 | consumed tokens: 27033600.0 | grad norm avg: 1.17 | grad norm last: 1.16 | 
2025-12-29T15:02:13 | step: 3400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.985266787116416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.42 | consumed tokens: 27852800.0 | grad norm avg: 1.14 | grad norm last: 1.17 | 
2025-12-29T15:02:33 | step: 3500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.984127372154035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.88 | consumed tokens: 28672000.0 | grad norm avg: 1.14 | grad norm last: 1.08 | 
2025-12-29T15:02:54 | step: 3600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.982945756637491e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.75 | consumed tokens: 29491200.0 | grad norm avg: 1.13 | grad norm last: 1.2 | 
2025-12-29T15:03:14 | step: 3700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.981721576768905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.88 | consumed tokens: 30310400.0 | grad norm avg: 1.11 | grad norm last: 1.1 | 
2025-12-29T15:03:35 | step: 3800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.980455560144037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.25 | consumed tokens: 31129600.0 | grad norm avg: 1.12 | grad norm last: 1.09 | 
2025-12-29T15:03:55 | step: 3900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.979146979167126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.83 | consumed tokens: 31948800.0 | grad norm avg: 1.11 | grad norm last: 1.08 | 
2025-12-29T15:04:16 | step: 4000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.977796197636053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.53 | consumed tokens: 32768000.0 | grad norm avg: 1.1 | grad norm last: 1.09 | 
2025-12-29T15:04:36 | step: 4100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9764032155508175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 33587200.0 | grad norm avg: 1.1 | grad norm last: 1.11 | 
2025-12-29T15:04:57 | step: 4200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.9749683967093006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.17 | consumed tokens: 34406400.0 | grad norm avg: 1.09 | grad norm last: 1.06 | 
2025-12-29T15:05:17 | step: 4300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9734910135157406e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.02 | consumed tokens: 35225600.0 | grad norm avg: 1.09 | grad norm last: 1.13 | 
2025-12-29T15:05:38 | step: 4400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.971971793565899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.33 | consumed tokens: 36044800.0 | grad norm avg: 1.08 | grad norm last: 1.08 | 
2025-12-29T15:05:58 | step: 4500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9704103730618954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 36864000.0 | grad norm avg: 1.09 | grad norm last: 1.08 | 
2025-12-29T15:06:18 | step: 4600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9688067520037293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.05 | consumed tokens: 37683200.0 | grad norm avg: 1.1 | grad norm last: 1.01 | 
2025-12-29T15:06:39 | step: 4700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.967161294189282e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.5 | consumed tokens: 38502400.0 | grad norm avg: 1.08 | grad norm last: 1.1 | 
2025-12-29T15:06:59 | step: 4800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.9654739996185526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.0 | consumed tokens: 39321600.0 | grad norm avg: 1.09 | grad norm last: 1.08 | 
2025-12-29T15:07:20 | step: 4900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.963744504493661e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.44 | consumed tokens: 40140800.0 | grad norm avg: 1.09 | grad norm last: 1.06 | 
2025-12-29T15:07:40 | step: 5000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.9619728088146076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.33 | consumed tokens: 40960000.0 | grad norm avg: 1.08 | grad norm last: 1.16 | 
2025-12-29T15:08:02 | step: 5100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.960159640177153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.17 | consumed tokens: 41779200.0 | grad norm avg: 1.08 | grad norm last: 1.02 | 
2025-12-29T15:08:23 | step: 5200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.958304270985536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.59 | consumed tokens: 42598400.0 | grad norm avg: 1.07 | grad norm last: 0.99 | 
2025-12-29T15:08:43 | step: 5300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9564074288355187e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.39 | consumed tokens: 43417600.0 | grad norm avg: 1.08 | grad norm last: 1.15 | 
2025-12-29T15:09:04 | step: 5400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.954468386131339e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.45 | consumed tokens: 44236800.0 | grad norm avg: 1.08 | grad norm last: 1.11 | 
2025-12-29T15:09:24 | step: 5500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.952487506670877e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.16 | consumed tokens: 45056000.0 | grad norm avg: 1.07 | grad norm last: 1.06 | 
2025-12-29T15:09:45 | step: 5600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.950465154252015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 45875200.0 | grad norm avg: 1.08 | grad norm last: 1.03 | 
2025-12-29T15:10:05 | step: 5700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.948400965076871e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.62 | consumed tokens: 46694400.0 | grad norm avg: 1.08 | grad norm last: 1.16 | 
2025-12-29T15:10:26 | step: 5800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9462953029433265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.52 | consumed tokens: 47513600.0 | grad norm avg: 1.08 | grad norm last: 1.02 | 
2025-12-29T15:10:46 | step: 5900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9441474402556196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.62 | consumed tokens: 48332800.0 | grad norm avg: 1.07 | grad norm last: 1.13 | 
2025-12-29T15:11:07 | step: 6000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9419584684073925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.86 | consumed tokens: 49152000.0 | grad norm avg: 1.06 | grad norm last: 0.98 | 
2025-12-29T15:11:27 | step: 6100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.939727659802884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.55 | consumed tokens: 49971200.0 | grad norm avg: 1.08 | grad norm last: 1.06 | 
2025-12-29T15:11:48 | step: 6200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9374553782399744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.05 | train loss last: 3.62 | consumed tokens: 50790400.0 | grad norm avg: 1.09 | grad norm last: 1.11 | 
2025-12-29T15:12:08 | step: 6300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.935141623718664e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.58 | consumed tokens: 51609600.0 | grad norm avg: 1.07 | grad norm last: 1.09 | 
2025-12-29T15:12:29 | step: 6400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.932786396238953e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.3 | consumed tokens: 52428800.0 | grad norm avg: 1.09 | grad norm last: 1.07 | 
2025-12-29T15:12:49 | step: 6500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.9303900595987216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.23 | consumed tokens: 53248000.0 | grad norm avg: 1.08 | grad norm last: 1.05 | 
2025-12-29T15:13:10 | step: 6600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.927951886202209e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.33 | consumed tokens: 54067200.0 | grad norm avg: 1.08 | grad norm last: 1.1 | 
2025-12-29T15:13:31 | step: 6700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.925472603645176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.56 | consumed tokens: 54886400.0 | grad norm avg: 1.08 | grad norm last: 1.08 | 
2025-12-29T15:13:51 | step: 6800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.922951848129742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.81 | consumed tokens: 55705600.0 | grad norm avg: 1.08 | grad norm last: 1.01 | 
2025-12-29T15:14:12 | step: 6900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.920389983453788e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.77 | consumed tokens: 56524800.0 | grad norm avg: 1.07 | grad norm last: 1.03 | 
2025-12-29T15:14:32 | step: 7000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.917787009617314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.08 | consumed tokens: 57344000.0 | grad norm avg: 1.09 | grad norm last: 1.1 | 
2025-12-29T15:14:53 | step: 7100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.915142562822439e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.23 | consumed tokens: 58163200.0 | grad norm avg: 1.06 | grad norm last: 1.2 | 
2025-12-29T15:15:13 | step: 7200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.912457006867044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 2.66 | consumed tokens: 58982400.0 | grad norm avg: 1.08 | grad norm last: 1.04 | 
2025-12-29T15:15:34 | step: 7300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.909730705549009e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.33 | consumed tokens: 59801600.0 | grad norm avg: 1.08 | grad norm last: 1.28 | 
2025-12-29T15:15:54 | step: 7400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9069632950704545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.94 | consumed tokens: 60620800.0 | grad norm avg: 1.07 | grad norm last: 1.01 | 
2025-12-29T15:16:15 | step: 7500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.90415477543138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.02 | train loss last: 2.81 | consumed tokens: 61440000.0 | grad norm avg: 1.08 | grad norm last: 1.11 | 
2025-12-29T15:16:35 | step: 7600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.901305146631785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.02 | train loss last: 2.95 | consumed tokens: 62259200.0 | grad norm avg: 1.09 | grad norm last: 1.07 | 
2025-12-29T15:16:56 | step: 7700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.898415136267431e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 2.88 | consumed tokens: 63078400.0 | grad norm avg: 1.1 | grad norm last: 0.99 | 
2025-12-29T15:17:16 | step: 7800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.895484016742557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.27 | consumed tokens: 63897600.0 | grad norm avg: 1.09 | grad norm last: 1.12 | 
2025-12-29T15:17:37 | step: 7900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.8925117880571634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 3.64 | consumed tokens: 64716800.0 | grad norm avg: 1.08 | grad norm last: 1.11 | 
2025-12-29T15:17:57 | step: 8000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.889499177807011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.02 | train loss last: 2.62 | consumed tokens: 65536000.0 | grad norm avg: 1.08 | grad norm last: 0.99 | 
2025-12-29T15:18:18 | step: 8100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.8864458221942186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 2.77 | consumed tokens: 66355200.0 | grad norm avg: 1.08 | grad norm last: 1.19 | 
2025-12-29T15:18:38 | step: 8200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.883352085016668e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.02 | train loss last: 3.03 | consumed tokens: 67174400.0 | grad norm avg: 1.08 | grad norm last: 1.07 | 
2025-12-29T15:18:59 | step: 8300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.8802176024764776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.01 | train loss last: 3.44 | consumed tokens: 67993600.0 | grad norm avg: 1.09 | grad norm last: 1.08 | 
2025-12-29T15:19:19 | step: 8400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.877042374573648e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.0 | train loss last: 3.52 | consumed tokens: 68812800.0 | grad norm avg: 1.1 | grad norm last: 1.11 | 
2025-12-29T15:19:40 | step: 8500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.8738267651060596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.03 | consumed tokens: 69632000.0 | grad norm avg: 1.08 | grad norm last: 1.18 | 
2025-12-29T15:20:00 | step: 8600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.8705707740737125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 3.39 | consumed tokens: 70451200.0 | grad norm avg: 1.11 | grad norm last: 1.04 | 
2025-12-29T15:20:21 | step: 8700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.867274401476607e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.02 | train loss last: 2.86 | consumed tokens: 71270400.0 | grad norm avg: 1.09 | grad norm last: 1.25 | 
2025-12-29T15:20:41 | step: 8800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.863938011112623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 3.12 | consumed tokens: 72089600.0 | grad norm avg: 1.1 | grad norm last: 1.07 | 
2025-12-29T15:21:02 | step: 8900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.860560875386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.99 | train loss last: 2.89 | consumed tokens: 72908800.0 | grad norm avg: 1.11 | grad norm last: 1.11 | 
2025-12-29T15:21:22 | step: 9000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.857144085690379e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 2.8 | consumed tokens: 73728000.0 | grad norm avg: 1.1 | grad norm last: 1.07 | 
2025-12-29T15:21:43 | step: 9100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.85368691443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.99 | train loss last: 2.98 | consumed tokens: 74547200.0 | grad norm avg: 1.09 | grad norm last: 1.05 | 
2025-12-29T15:22:03 | step: 9200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.850189361604862e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.0 | train loss last: 2.56 | consumed tokens: 75366400.0 | grad norm avg: 1.12 | grad norm last: 1.17 | 
2025-12-29T15:22:24 | step: 9300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8466521548107266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.98 | train loss last: 2.81 | consumed tokens: 76185600.0 | grad norm avg: 1.11 | grad norm last: 1.13 | 
2025-12-29T15:22:45 | step: 9400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.8430749302497134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.01 | train loss last: 3.0 | consumed tokens: 77004800.0 | grad norm avg: 1.11 | grad norm last: 1.07 | 
2025-12-29T15:23:05 | step: 9500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.839458051719703e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.0 | train loss last: 3.17 | consumed tokens: 77824000.0 | grad norm avg: 1.11 | grad norm last: 1.08 | 
2025-12-29T15:23:25 | step: 9600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.8358007916249335e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.99 | train loss last: 3.06 | consumed tokens: 78643200.0 | grad norm avg: 1.09 | grad norm last: 1.13 | 
2025-12-29T15:23:46 | step: 9700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.8321042413590476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.99 | train loss last: 2.86 | consumed tokens: 79462400.0 | grad norm avg: 1.11 | grad norm last: 1.08 | 
2025-12-29T15:24:07 | step: 9800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.828367673326284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.99 | train loss last: 2.98 | consumed tokens: 80281600.0 | grad norm avg: 1.11 | grad norm last: 1.16 | 
2025-12-29T15:24:27 | step: 9900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.824591815122403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.97 | train loss last: 3.14 | consumed tokens: 81100800.0 | grad norm avg: 1.11 | grad norm last: 0.99 | 
2025-12-29T15:24:48 | step: 10000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.820775939151645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.96 | train loss last: 2.84 | consumed tokens: 81920000.0 | grad norm avg: 1.11 | grad norm last: 1.14 | 
2025-12-29T15:25:10 | step: 10100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.81692113680765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.97 | train loss last: 3.47 | consumed tokens: 82739200.0 | grad norm avg: 1.11 | grad norm last: 1.07 | 
2025-12-29T15:25:30 | step: 10200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.813026316696778e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.97 | train loss last: 2.69 | consumed tokens: 83558400.0 | grad norm avg: 1.1 | grad norm last: 1.07 | 
2025-12-29T15:25:51 | step: 10300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.809092206414789e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.99 | train loss last: 2.72 | consumed tokens: 84377600.0 | grad norm avg: 1.11 | grad norm last: 1.12 | 
2025-12-29T15:26:11 | step: 10400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.805119169759564e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.96 | train loss last: 3.12 | consumed tokens: 85196800.0 | grad norm avg: 1.13 | grad norm last: 1.13 | 
2025-12-29T15:26:32 | step: 10500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.801106479135342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.96 | train loss last: 3.53 | consumed tokens: 86016000.0 | grad norm avg: 1.11 | grad norm last: 1.08 | 
2025-12-29T15:26:52 | step: 10600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.797054862137884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.95 | train loss last: 3.14 | consumed tokens: 86835200.0 | grad norm avg: 1.11 | grad norm last: 1.02 | 
2025-12-29T15:27:13 | step: 10700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.792963954969309e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.97 | train loss last: 2.86 | consumed tokens: 87654400.0 | grad norm avg: 1.12 | grad norm last: 1.14 | 
2025-12-29T15:27:34 | step: 10800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.788834121427499e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.98 | train loss last: 2.83 | consumed tokens: 88473600.0 | grad norm avg: 1.11 | grad norm last: 1.11 | 
2025-12-29T15:27:54 | step: 10900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.7846653615124524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 2.91 | consumed tokens: 89292800.0 | grad norm avg: 1.11 | grad norm last: 1.13 | 
2025-12-29T15:28:15 | step: 11000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.78045767522417e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.95 | train loss last: 2.75 | consumed tokens: 90112000.0 | grad norm avg: 1.11 | grad norm last: 1.06 | 
2025-12-29T15:28:35 | step: 11100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.776211062562652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.98 | train loss last: 3.14 | consumed tokens: 90931200.0 | grad norm avg: 1.14 | grad norm last: 1.15 | 
2025-12-29T15:28:56 | step: 11200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.7719258873257786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.94 | train loss last: 2.97 | consumed tokens: 91750400.0 | grad norm avg: 1.14 | grad norm last: 1.16 | 
2025-12-29T15:29:16 | step: 11300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.7676017857156694e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.96 | train loss last: 2.94 | consumed tokens: 92569600.0 | grad norm avg: 1.13 | grad norm last: 1.06 | 
2025-12-29T15:29:37 | step: 11400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.763239121530205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 3.06 | consumed tokens: 93388800.0 | grad norm avg: 1.12 | grad norm last: 1.04 | 
2025-12-29T15:29:57 | step: 11500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.758838258567266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.94 | train loss last: 3.27 | consumed tokens: 94208000.0 | grad norm avg: 1.13 | grad norm last: 1.15 | 
2025-12-29T15:30:18 | step: 11600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.7543984692310914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.95 | train loss last: 2.75 | consumed tokens: 95027200.0 | grad norm avg: 1.14 | grad norm last: 1.21 | 
2025-12-29T15:30:38 | step: 11700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.749920481117442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.97 | train loss last: 3.36 | consumed tokens: 95846400.0 | grad norm avg: 1.14 | grad norm last: 1.2 | 
2025-12-29T15:30:59 | step: 11800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.7454042942263186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.95 | train loss last: 2.45 | consumed tokens: 96665600.0 | grad norm avg: 1.13 | grad norm last: 1.09 | 
2025-12-29T15:31:19 | step: 11900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.7408499085577205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.95 | train loss last: 3.06 | consumed tokens: 97484800.0 | grad norm avg: 1.12 | grad norm last: 1.17 | 
2025-12-29T15:31:40 | step: 12000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.736257324111648e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.96 | train loss last: 3.11 | consumed tokens: 98304000.0 | grad norm avg: 1.14 | grad norm last: 1.13 | 
2025-12-29T15:32:00 | step: 12100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.731626540888101e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 2.31 | consumed tokens: 99123200.0 | grad norm avg: 1.13 | grad norm last: 1.06 | 
2025-12-29T15:32:21 | step: 12200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.72695792268496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.95 | train loss last: 2.86 | consumed tokens: 99942400.0 | grad norm avg: 1.15 | grad norm last: 1.19 | 
2025-12-29T15:32:41 | step: 12300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.722251105704345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 3.41 | consumed tokens: 100761600.0 | grad norm avg: 1.14 | grad norm last: 1.06 | 
2025-12-29T15:33:02 | step: 12400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.7175068175420165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.94 | train loss last: 2.88 | consumed tokens: 101580800.0 | grad norm avg: 1.15 | grad norm last: 1.14 | 
2025-12-29T15:33:22 | step: 12500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.7127246944000944e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 3.0 | consumed tokens: 102400000.0 | grad norm avg: 1.12 | grad norm last: 1.1 | 
2025-12-29T15:33:43 | step: 12600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.7079047362785786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 2.5 | consumed tokens: 103219200.0 | grad norm avg: 1.14 | grad norm last: 1.15 | 
2025-12-29T15:34:03 | step: 12700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.70304730697535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 3.06 | consumed tokens: 104038400.0 | grad norm avg: 1.14 | grad norm last: 1.15 | 
2025-12-29T15:34:24 | step: 12800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.698152406490408e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.91 | train loss last: 3.0 | consumed tokens: 104857600.0 | grad norm avg: 1.13 | grad norm last: 1.1 | 
2025-12-29T15:34:44 | step: 12900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.693220034823753e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 2.78 | consumed tokens: 105676800.0 | grad norm avg: 1.13 | grad norm last: 1.05 | 
2025-12-29T15:35:05 | step: 13000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.688250191975385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.94 | train loss last: 3.61 | consumed tokens: 106496000.0 | grad norm avg: 1.16 | grad norm last: 1.13 | 
2025-12-29T15:35:25 | step: 13100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.6832432417431846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 2.77 | consumed tokens: 107315200.0 | grad norm avg: 1.13 | grad norm last: 1.17 | 
2025-12-29T15:35:46 | step: 13200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.678199184127152e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 3.17 | consumed tokens: 108134400.0 | grad norm avg: 1.14 | grad norm last: 1.07 | 
2025-12-29T15:36:07 | step: 13300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.673118019127287e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.9 | train loss last: 3.31 | consumed tokens: 108953600.0 | grad norm avg: 1.14 | grad norm last: 1.11 | 
2025-12-29T15:36:27 | step: 13400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.6679997467435896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.9 | train loss last: 3.47 | consumed tokens: 109772800.0 | grad norm avg: 1.14 | grad norm last: 1.19 | 
2025-12-29T15:36:48 | step: 13500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.662844730773941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.9 | train loss last: 3.05 | consumed tokens: 110592000.0 | grad norm avg: 1.17 | grad norm last: 1.12 | 
2025-12-29T15:37:08 | step: 13600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.6576526074204594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.91 | train loss last: 3.28 | consumed tokens: 111411200.0 | grad norm avg: 1.14 | grad norm last: 1.06 | 
2025-12-29T15:37:29 | step: 13700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.652424104278907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 2.97 | consumed tokens: 112230400.0 | grad norm avg: 1.15 | grad norm last: 1.03 | 
2025-12-29T15:37:49 | step: 13800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.6471588575514033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.89 | train loss last: 3.3 | consumed tokens: 113049600.0 | grad norm avg: 1.15 | grad norm last: 1.16 | 
2025-12-29T15:38:10 | step: 13900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.641856867237948e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 2.97 | consumed tokens: 113868800.0 | grad norm avg: 1.15 | grad norm last: 1.1 | 
2025-12-29T15:38:30 | step: 14000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.6365184971364215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 3.27 | consumed tokens: 114688000.0 | grad norm avg: 1.14 | grad norm last: 1.08 | 
2025-12-29T15:38:51 | step: 14100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.631143747246824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.92 | train loss last: 3.34 | consumed tokens: 115507200.0 | grad norm avg: 1.15 | grad norm last: 1.06 | 
2025-12-29T15:39:11 | step: 14200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.625732981367037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.94 | train loss last: 3.0 | consumed tokens: 116326400.0 | grad norm avg: 1.16 | grad norm last: 1.22 | 
2025-12-29T15:39:32 | step: 14300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.620285835699178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 2.91 | consumed tokens: 117145600.0 | grad norm avg: 1.16 | grad norm last: 1.09 | 
2025-12-29T15:39:52 | step: 14400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.614802310243249e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.91 | train loss last: 2.55 | consumed tokens: 117964800.0 | grad norm avg: 1.16 | grad norm last: 1.3 | 
2025-12-29T15:40:13 | step: 14500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.60928313259501e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.93 | train loss last: 2.92 | consumed tokens: 118784000.0 | grad norm avg: 1.16 | grad norm last: 1.18 | 
2025-12-29T15:40:33 | step: 14600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.603727938956581e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.9 | train loss last: 2.97 | consumed tokens: 119603200.0 | grad norm avg: 1.14 | grad norm last: 1.21 | 
2025-12-29T15:40:54 | step: 14700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5981370931258425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 2.44 | consumed tokens: 120422400.0 | grad norm avg: 1.14 | grad norm last: 1.14 | 
2025-12-29T15:41:14 | step: 14800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.592510231304914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 2.83 | consumed tokens: 121241600.0 | grad norm avg: 1.16 | grad norm last: 1.24 | 
2025-12-29T15:41:35 | step: 14900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.586848081089556e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.89 | train loss last: 2.95 | consumed tokens: 122060800.0 | grad norm avg: 1.15 | grad norm last: 1.08 | 
2025-12-29T15:41:55 | step: 15000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.581150278681889e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.9 | train loss last: 3.22 | consumed tokens: 122880000.0 | grad norm avg: 1.15 | grad norm last: 1.05 | 
2025-12-29T15:42:17 | step: 15100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.5754168240819126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 3.09 | consumed tokens: 123699200.0 | grad norm avg: 1.16 | grad norm last: 1.12 | 
2025-12-29T15:42:38 | step: 15200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.569648444885388e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 2.66 | consumed tokens: 124518400.0 | grad norm avg: 1.15 | grad norm last: 1.13 | 
2025-12-29T15:42:58 | step: 15300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.563844413496554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 2.95 | consumed tokens: 125337600.0 | grad norm avg: 1.16 | grad norm last: 1.04 | 
2025-12-29T15:43:19 | step: 15400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.558005457511172e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.89 | train loss last: 2.92 | consumed tokens: 126156800.0 | grad norm avg: 1.15 | grad norm last: 1.16 | 
2025-12-29T15:43:40 | step: 15500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5521315769292414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 3.16 | consumed tokens: 126976000.0 | grad norm avg: 1.16 | grad norm last: 1.21 | 
2025-12-29T15:44:00 | step: 15600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5462224079528823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 3.23 | consumed tokens: 127795200.0 | grad norm avg: 1.17 | grad norm last: 1.18 | 
2025-12-29T15:44:21 | step: 15700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.540278678177856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 2.77 | consumed tokens: 128614400.0 | grad norm avg: 1.16 | grad norm last: 1.19 | 
2025-12-29T15:44:41 | step: 15800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.534300387604162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 2.7 | consumed tokens: 129433600.0 | grad norm avg: 1.16 | grad norm last: 1.15 | 
2025-12-29T15:45:02 | step: 15900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.52828717243392e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.89 | train loss last: 3.08 | consumed tokens: 130252800.0 | grad norm avg: 1.18 | grad norm last: 1.27 | 
2025-12-29T15:45:22 | step: 16000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.522239396465011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.9 | train loss last: 2.36 | consumed tokens: 131072000.0 | grad norm avg: 1.15 | grad norm last: 1.13 | 
2025-12-29T15:45:43 | step: 16100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.516157423495315e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 2.75 | consumed tokens: 131891200.0 | grad norm avg: 1.17 | grad norm last: 1.14 | 
2025-12-29T15:46:03 | step: 16200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5100412535248324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 3.17 | consumed tokens: 132710400.0 | grad norm avg: 1.16 | grad norm last: 1.36 | 
2025-12-29T15:46:24 | step: 16300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.5038905227556825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 3.12 | consumed tokens: 133529600.0 | grad norm avg: 1.16 | grad norm last: 1.15 | 
2025-12-29T15:46:44 | step: 16400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.497705594985746e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 2.78 | consumed tokens: 134348800.0 | grad norm avg: 1.18 | grad norm last: 1.28 | 
2025-12-29T15:47:05 | step: 16500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.491487197810784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 2.69 | consumed tokens: 135168000.0 | grad norm avg: 1.18 | grad norm last: 1.36 | 
2025-12-29T15:47:25 | step: 16600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.485234239837155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 2.8 | consumed tokens: 135987200.0 | grad norm avg: 1.16 | grad norm last: 1.11 | 
2025-12-29T15:47:46 | step: 16700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.478948176256381e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 3.06 | consumed tokens: 136806400.0 | grad norm avg: 1.16 | grad norm last: 1.21 | 
2025-12-29T15:48:06 | step: 16800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.4726279156748205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.88 | train loss last: 2.72 | consumed tokens: 137625600.0 | grad norm avg: 1.16 | grad norm last: 1.11 | 
2025-12-29T15:48:27 | step: 16900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.466274185688235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 2.8 | consumed tokens: 138444800.0 | grad norm avg: 1.16 | grad norm last: 1.07 | 
2025-12-29T15:48:48 | step: 17000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.459886986296624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 2.91 | consumed tokens: 139264000.0 | grad norm avg: 1.17 | grad norm last: 1.25 | 
2025-12-29T15:49:08 | step: 17100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.4534666812978685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 3.41 | consumed tokens: 140083200.0 | grad norm avg: 1.15 | grad norm last: 1.13 | 
2025-12-29T15:49:29 | step: 17200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.447012906894088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 2.94 | consumed tokens: 140902400.0 | grad norm avg: 1.19 | grad norm last: 1.18 | 
2025-12-29T15:49:49 | step: 17300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.4405260268831626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 2.89 | consumed tokens: 141721600.0 | grad norm avg: 1.17 | grad norm last: 1.17 | 
2025-12-29T15:50:10 | step: 17400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.434006041265093e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 3.23 | consumed tokens: 142540800.0 | grad norm avg: 1.17 | grad norm last: 1.38 | 
2025-12-29T15:50:30 | step: 17500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.4274529500398785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 3.88 | consumed tokens: 143360000.0 | grad norm avg: 1.17 | grad norm last: 1.12 | 
2025-12-29T15:50:51 | step: 17600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.420867480803281e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 2.73 | consumed tokens: 144179200.0 | grad norm avg: 1.18 | grad norm last: 1.14 | 
2025-12-29T15:51:11 | step: 17700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.41424926975742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.36 | consumed tokens: 144998400.0 | grad norm avg: 1.18 | grad norm last: 1.18 | 
2025-12-29T15:51:32 | step: 17800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.407598316902295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.86 | train loss last: 2.88 | consumed tokens: 145817600.0 | grad norm avg: 1.16 | grad norm last: 1.2 | 
2025-12-29T15:51:52 | step: 17900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.400914622237906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 3.28 | consumed tokens: 146636800.0 | grad norm avg: 1.18 | grad norm last: 1.08 | 
2025-12-29T15:52:13 | step: 18000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.3941989133600146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 3.67 | consumed tokens: 147456000.0 | grad norm avg: 1.16 | grad norm last: 1.17 | 
2025-12-29T15:52:33 | step: 18100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.38745082647074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.84 | consumed tokens: 148275200.0 | grad norm avg: 1.16 | grad norm last: 1.16 | 
2025-12-29T15:52:54 | step: 18200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.380670725367963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 2.77 | consumed tokens: 149094400.0 | grad norm avg: 1.17 | grad norm last: 1.21 | 
2025-12-29T15:53:14 | step: 18300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.373858610051684e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.8 | consumed tokens: 149913600.0 | grad norm avg: 1.16 | grad norm last: 1.23 | 
2025-12-29T15:53:35 | step: 18400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.3670144805219024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.81 | consumed tokens: 150732800.0 | grad norm avg: 1.17 | grad norm last: 1.2 | 
2025-12-29T15:53:56 | step: 18500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.360138700576499e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.58 | consumed tokens: 151552000.0 | grad norm avg: 1.17 | grad norm last: 1.2 | 
2025-12-29T15:54:16 | step: 18600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.353231270215474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 2.62 | consumed tokens: 152371200.0 | grad norm avg: 1.18 | grad norm last: 1.23 | 
2025-12-29T15:54:37 | step: 18700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.346292189438827e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.87 | train loss last: 2.98 | consumed tokens: 153190400.0 | grad norm avg: 1.17 | grad norm last: 1.12 | 
2025-12-29T15:54:57 | step: 18800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.3393218220444396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.59 | consumed tokens: 154009600.0 | grad norm avg: 1.17 | grad norm last: 1.17 | 
2025-12-29T15:55:18 | step: 18900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.332320168032311e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.02 | consumed tokens: 154828800.0 | grad norm avg: 1.17 | grad norm last: 1.12 | 
2025-12-29T15:55:38 | step: 19000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.325287227402441e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 3.31 | consumed tokens: 155648000.0 | grad norm avg: 1.18 | grad norm last: 1.17 | 
2025-12-29T15:55:59 | step: 19100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.318223363952711e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 3.03 | consumed tokens: 156467200.0 | grad norm avg: 1.18 | grad norm last: 1.02 | 
2025-12-29T15:56:19 | step: 19200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.31112821388524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 2.38 | consumed tokens: 157286400.0 | grad norm avg: 1.17 | grad norm last: 1.1 | 
2025-12-29T15:56:40 | step: 19300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.30400250479579e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.81 | consumed tokens: 158105600.0 | grad norm avg: 1.18 | grad norm last: 1.21 | 
2025-12-29T15:57:00 | step: 19400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.296845872886479e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 3.14 | consumed tokens: 158924800.0 | grad norm avg: 1.18 | grad norm last: 1.14 | 
2025-12-29T15:57:21 | step: 19500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.289659045753069e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 3.09 | consumed tokens: 159744000.0 | grad norm avg: 1.19 | grad norm last: 1.06 | 
2025-12-29T15:57:42 | step: 19600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.282441295799799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 3.48 | consumed tokens: 160563200.0 | grad norm avg: 1.17 | grad norm last: 1.17 | 
2025-12-29T15:58:02 | step: 19700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.275193714420311e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.45 | consumed tokens: 161382400.0 | grad norm avg: 1.19 | grad norm last: 1.14 | 
2025-12-29T15:58:23 | step: 19800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.2679155740188435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.62 | consumed tokens: 162201600.0 | grad norm avg: 1.19 | grad norm last: 1.15 | 
2025-12-29T15:58:43 | step: 19900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.260607238393277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 1.97 | consumed tokens: 163020800.0 | grad norm avg: 1.2 | grad norm last: 1.2 | 
2025-12-29T15:59:04 | step: 20000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.253269071341492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.5 | consumed tokens: 163840000.0 | grad norm avg: 1.2 | grad norm last: 1.17 | 
2025-12-29T15:59:26 | step: 20100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.2459010728634894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.66 | consumed tokens: 164659200.0 | grad norm avg: 1.2 | grad norm last: 1.15 | 
2025-12-29T15:59:46 | step: 20200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.238503606757149e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.97 | consumed tokens: 165478400.0 | grad norm avg: 1.19 | grad norm last: 1.24 | 
2025-12-29T16:00:07 | step: 20300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.2310763092245907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.85 | train loss last: 3.06 | consumed tokens: 166297600.0 | grad norm avg: 1.2 | grad norm last: 1.21 | 
2025-12-29T16:00:28 | step: 20400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.223619544063695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.73 | consumed tokens: 167116800.0 | grad norm avg: 1.2 | grad norm last: 1.12 | 
2025-12-29T16:00:48 | step: 20500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.2161333112744614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.86 | consumed tokens: 167936000.0 | grad norm avg: 1.19 | grad norm last: 1.22 | 
2025-12-29T16:01:09 | step: 20600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.208618338452652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.72 | consumed tokens: 168755200.0 | grad norm avg: 1.19 | grad norm last: 1.27 | 
2025-12-29T16:01:29 | step: 20700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.201073898002505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.98 | consumed tokens: 169574400.0 | grad norm avg: 1.19 | grad norm last: 1.19 | 
2025-12-29T16:01:50 | step: 20800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.1935007175197825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 2.39 | consumed tokens: 170393600.0 | grad norm avg: 1.19 | grad norm last: 1.14 | 
2025-12-29T16:02:10 | step: 20900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.185898433206603e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.94 | consumed tokens: 171212800.0 | grad norm avg: 1.19 | grad norm last: 1.24 | 
2025-12-29T16:02:31 | step: 21000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.178267772658728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.69 | consumed tokens: 172032000.0 | grad norm avg: 1.18 | grad norm last: 1.33 | 
2025-12-29T16:02:52 | step: 21100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.170608372078277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.72 | consumed tokens: 172851200.0 | grad norm avg: 1.19 | grad norm last: 1.08 | 
2025-12-29T16:03:12 | step: 21200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.162920595263131e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.64 | consumed tokens: 173670400.0 | grad norm avg: 1.19 | grad norm last: 1.21 | 
2025-12-29T16:03:33 | step: 21300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.1552044422132894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.92 | consumed tokens: 174489600.0 | grad norm avg: 1.19 | grad norm last: 1.2 | 
2025-12-29T16:03:53 | step: 21400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.147460276726633e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.67 | consumed tokens: 175308800.0 | grad norm avg: 1.2 | grad norm last: 1.15 | 
2025-12-29T16:04:14 | step: 21500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.139687735005282e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.8 | consumed tokens: 176128000.0 | grad norm avg: 1.21 | grad norm last: 1.18 | 
2025-12-29T16:04:34 | step: 21600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.1318875446449965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 3.08 | consumed tokens: 176947200.0 | grad norm avg: 1.2 | grad norm last: 1.15 | 
2025-12-29T16:04:55 | step: 21700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.124059705645777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.72 | consumed tokens: 177766400.0 | grad norm avg: 1.21 | grad norm last: 1.23 | 
2025-12-29T16:05:15 | step: 21800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.1162038542097434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 3.09 | consumed tokens: 178585600.0 | grad norm avg: 1.19 | grad norm last: 1.14 | 
2025-12-29T16:05:36 | step: 21900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.108321081730537e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.94 | consumed tokens: 179404800.0 | grad norm avg: 1.23 | grad norm last: 1.12 | 
2025-12-29T16:05:56 | step: 22000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.100410296814516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 2.84 | consumed tokens: 180224000.0 | grad norm avg: 1.19 | grad norm last: 1.11 | 
2025-12-29T16:06:17 | step: 22100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.092472590855323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.78 | consumed tokens: 181043200.0 | grad norm avg: 1.2 | grad norm last: 1.21 | 
2025-12-29T16:06:38 | step: 22200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.084507963852957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.88 | consumed tokens: 181862400.0 | grad norm avg: 1.21 | grad norm last: 1.22 | 
2025-12-29T16:06:58 | step: 22300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.076516052009538e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 2.86 | consumed tokens: 182681600.0 | grad norm avg: 1.22 | grad norm last: 1.31 | 
2025-12-29T16:07:19 | step: 22400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.068497582920827e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 2.72 | consumed tokens: 183500800.0 | grad norm avg: 1.2 | grad norm last: 1.17 | 
2025-12-29T16:07:39 | step: 22500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.0604521927889436e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 2.97 | consumed tokens: 184320000.0 | grad norm avg: 1.21 | grad norm last: 1.24 | 
2025-12-29T16:08:00 | step: 22600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.0523802454117686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.55 | consumed tokens: 185139200.0 | grad norm avg: 1.19 | grad norm last: 1.19 | 
2025-12-29T16:08:20 | step: 22700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.0442821045871824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.55 | consumed tokens: 185958400.0 | grad norm avg: 1.23 | grad norm last: 1.39 | 
2025-12-29T16:08:41 | step: 22800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.0361574065173045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.7 | consumed tokens: 186777600.0 | grad norm avg: 1.22 | grad norm last: 1.24 | 
2025-12-29T16:09:01 | step: 22900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.028006878797896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.41 | consumed tokens: 187596800.0 | grad norm avg: 1.21 | grad norm last: 1.25 | 
2025-12-29T16:09:22 | step: 23000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.019830157631077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.84 | train loss last: 2.47 | consumed tokens: 188416000.0 | grad norm avg: 1.2 | grad norm last: 1.11 | 
2025-12-29T16:09:42 | step: 23100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.011627606814727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.7 | consumed tokens: 189235200.0 | grad norm avg: 1.22 | grad norm last: 1.15 | 
2025-12-29T16:10:03 | step: 23200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.003399226348847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.73 | consumed tokens: 190054400.0 | grad norm avg: 1.22 | grad norm last: 1.2 | 
2025-12-29T16:10:23 | step: 23300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.9951453800313175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.55 | consumed tokens: 190873600.0 | grad norm avg: 1.22 | grad norm last: 1.34 | 
2025-12-29T16:10:44 | step: 23400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.986866067862138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.89 | consumed tokens: 191692800.0 | grad norm avg: 1.21 | grad norm last: 1.29 | 
2025-12-29T16:11:04 | step: 23500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.97856165363919e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 2.92 | consumed tokens: 192512000.0 | grad norm avg: 1.21 | grad norm last: 1.19 | 
2025-12-29T16:11:25 | step: 23600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.970231773564592e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.47 | consumed tokens: 193331200.0 | grad norm avg: 1.22 | grad norm last: 1.23 | 
2025-12-29T16:11:46 | step: 23700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.961877155234106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.42 | consumed tokens: 194150400.0 | grad norm avg: 1.22 | grad norm last: 1.23 | 
2025-12-29T16:12:06 | step: 23800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.953497434849851e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.47 | consumed tokens: 194969600.0 | grad norm avg: 1.21 | grad norm last: 1.54 | 
2025-12-29T16:12:27 | step: 23900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.9450929762097076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 3.02 | consumed tokens: 195788800.0 | grad norm avg: 1.23 | grad norm last: 1.2 | 
2025-12-29T16:12:47 | step: 24000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.936664143111557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.75 | consumed tokens: 196608000.0 | grad norm avg: 1.22 | grad norm last: 1.2 | 
2025-12-29T16:13:08 | step: 24100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.9282109355553985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 3.69 | consumed tokens: 197427200.0 | grad norm avg: 1.23 | grad norm last: 1.28 | 
2025-12-29T16:13:28 | step: 24200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.919732989743352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.82 | train loss last: 2.36 | consumed tokens: 198246400.0 | grad norm avg: 1.22 | grad norm last: 1.16 | 
2025-12-29T16:13:49 | step: 24300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.911231397069059e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.83 | train loss last: 3.19 | consumed tokens: 199065600.0 | grad norm avg: 1.22 | grad norm last: 1.15 | 
2025-12-29T16:14:09 | step: 24400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.902705429936759e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.33 | consumed tokens: 199884800.0 | grad norm avg: 1.22 | grad norm last: 1.16 | 
2025-12-29T16:14:30 | step: 24500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.894155815942213e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.83 | consumed tokens: 200704000.0 | grad norm avg: 1.23 | grad norm last: 1.09 | 
2025-12-29T16:14:50 | step: 24600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.88558219128754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.59 | consumed tokens: 201523200.0 | grad norm avg: 1.22 | grad norm last: 1.11 | 
2025-12-29T16:15:11 | step: 24700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.8769852835685015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 3.05 | consumed tokens: 202342400.0 | grad norm avg: 1.23 | grad norm last: 1.27 | 
2025-12-29T16:15:31 | step: 24800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.868364728987217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.33 | consumed tokens: 203161600.0 | grad norm avg: 1.22 | grad norm last: 1.21 | 
2025-12-29T16:15:52 | step: 24900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.859721255139448e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.92 | consumed tokens: 203980800.0 | grad norm avg: 1.23 | grad norm last: 1.31 | 
2025-12-29T16:16:12 | step: 25000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.8510541344294325e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.58 | consumed tokens: 204800000.0 | grad norm avg: 1.25 | grad norm last: 1.34 | 
2025-12-29T16:16:34 | step: 25100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.842364458250813e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.86 | consumed tokens: 205619200.0 | grad norm avg: 1.22 | grad norm last: 1.19 | 
2025-12-29T16:16:55 | step: 25200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.833651862805709e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.78 | consumed tokens: 206438400.0 | grad norm avg: 1.21 | grad norm last: 1.12 | 
2025-12-29T16:17:15 | step: 25300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.8249163480941206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.33 | consumed tokens: 207257600.0 | grad norm avg: 1.24 | grad norm last: 1.27 | 
2025-12-29T16:17:36 | step: 25400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.816158641711809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.64 | consumed tokens: 208076800.0 | grad norm avg: 1.22 | grad norm last: 1.29 | 
2025-12-29T16:17:56 | step: 25500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.807378379860893e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 3.09 | consumed tokens: 208896000.0 | grad norm avg: 1.24 | grad norm last: 1.16 | 
2025-12-29T16:18:17 | step: 25600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.798575562541373e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.83 | consumed tokens: 209715200.0 | grad norm avg: 1.21 | grad norm last: 1.12 | 
2025-12-29T16:18:37 | step: 25700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.789750917349011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.56 | consumed tokens: 210534400.0 | grad norm avg: 1.24 | grad norm last: 1.3 | 
2025-12-29T16:18:58 | step: 25800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.780904444283806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.58 | consumed tokens: 211353600.0 | grad norm avg: 1.24 | grad norm last: 1.19 | 
2025-12-29T16:19:18 | step: 25900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.772036143345758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.62 | consumed tokens: 212172800.0 | grad norm avg: 1.24 | grad norm last: 1.19 | 
2025-12-29T16:19:39 | step: 26000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.763146378332749e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 3.59 | consumed tokens: 212992000.0 | grad norm avg: 1.23 | grad norm last: 1.25 | 
2025-12-29T16:20:00 | step: 26100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.754234785446897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 3.19 | consumed tokens: 213811200.0 | grad norm avg: 1.24 | grad norm last: 1.14 | 
2025-12-29T16:20:20 | step: 26200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.745302092283964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.64 | consumed tokens: 214630400.0 | grad norm avg: 1.24 | grad norm last: 1.16 | 
2025-12-29T16:20:41 | step: 26300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.736347935046069e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 3.14 | consumed tokens: 215449600.0 | grad norm avg: 1.25 | grad norm last: 1.23 | 
2025-12-29T16:21:01 | step: 26400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.727373041328974e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.81 | train loss last: 2.59 | consumed tokens: 216268800.0 | grad norm avg: 1.24 | grad norm last: 1.23 | 
2025-12-29T16:21:22 | step: 26500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.718377047334798e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.92 | consumed tokens: 217088000.0 | grad norm avg: 1.24 | grad norm last: 1.21 | 
2025-12-29T16:21:42 | step: 26600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.709360316861421e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.8 | consumed tokens: 217907200.0 | grad norm avg: 1.25 | grad norm last: 1.15 | 
2025-12-29T16:22:03 | step: 26700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.7003232137067243e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 3.0 | consumed tokens: 218726400.0 | grad norm avg: 1.23 | grad norm last: 1.35 | 
2025-12-29T16:22:23 | step: 26800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.691265737870708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 3.28 | consumed tokens: 219545600.0 | grad norm avg: 1.24 | grad norm last: 1.19 | 
2025-12-29T16:22:44 | step: 26900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.682187889353372e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.44 | consumed tokens: 220364800.0 | grad norm avg: 1.24 | grad norm last: 1.17 | 
2025-12-29T16:23:04 | step: 27000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.673090031952597e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 3.09 | consumed tokens: 221184000.0 | grad norm avg: 1.24 | grad norm last: 1.22 | 
2025-12-29T16:23:25 | step: 27100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.663972165668383e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.52 | consumed tokens: 222003200.0 | grad norm avg: 1.24 | grad norm last: 1.22 | 
2025-12-29T16:23:45 | step: 27200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.65483429050073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.78 | consumed tokens: 222822400.0 | grad norm avg: 1.24 | grad norm last: 1.18 | 
2025-12-29T16:24:06 | step: 27300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.645676770247519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.72 | consumed tokens: 223641600.0 | grad norm avg: 1.24 | grad norm last: 1.12 | 
2025-12-29T16:24:26 | step: 27400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.63649996870663e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.69 | consumed tokens: 224460800.0 | grad norm avg: 1.25 | grad norm last: 1.33 | 
2025-12-29T16:24:47 | step: 27500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.627303885878064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.8 | train loss last: 2.31 | consumed tokens: 225280000.0 | grad norm avg: 1.23 | grad norm last: 1.21 | 
2025-12-29T16:25:08 | step: 27600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.61808852176182e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.91 | consumed tokens: 226099200.0 | grad norm avg: 1.24 | grad norm last: 1.22 | 
2025-12-29T16:25:28 | step: 27700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.608854240155779e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.88 | consumed tokens: 226918400.0 | grad norm avg: 1.26 | grad norm last: 1.26 | 
2025-12-29T16:25:49 | step: 27800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.599601041059941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 3.03 | consumed tokens: 227737600.0 | grad norm avg: 1.25 | grad norm last: 1.34 | 
2025-12-29T16:26:09 | step: 27900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.590329288272187e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.36 | consumed tokens: 228556800.0 | grad norm avg: 1.24 | grad norm last: 1.27 | 
2025-12-29T16:26:30 | step: 28000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.581038617994636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 3.0 | consumed tokens: 229376000.0 | grad norm avg: 1.25 | grad norm last: 1.22 | 
2025-12-29T16:26:50 | step: 28100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.57172975782305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.34 | consumed tokens: 230195200.0 | grad norm avg: 1.25 | grad norm last: 1.13 | 
2025-12-29T16:27:11 | step: 28200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.562403071555309e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.91 | consumed tokens: 231014400.0 | grad norm avg: 1.25 | grad norm last: 1.16 | 
2025-12-29T16:27:31 | step: 28300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.553057831595652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 3.0 | consumed tokens: 231833600.0 | grad norm avg: 1.27 | grad norm last: 1.32 | 
2025-12-29T16:27:52 | step: 28400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.54369476553984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 3.02 | consumed tokens: 232652800.0 | grad norm avg: 1.25 | grad norm last: 1.21 | 
2025-12-29T16:28:12 | step: 28500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.534313873387873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 3.06 | consumed tokens: 233472000.0 | grad norm avg: 1.24 | grad norm last: 1.21 | 
2025-12-29T16:28:33 | step: 28600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.5249155189376324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.79 | train loss last: 2.8 | consumed tokens: 234291200.0 | grad norm avg: 1.25 | grad norm last: 1.32 | 
2025-12-29T16:28:53 | step: 28700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.5155000659869984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 3.06 | consumed tokens: 235110400.0 | grad norm avg: 1.26 | grad norm last: 1.22 | 
2025-12-29T16:29:14 | step: 28800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.5060667869402096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 2.98 | consumed tokens: 235929600.0 | grad norm avg: 1.25 | grad norm last: 1.27 | 
2025-12-29T16:29:34 | step: 28900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.496616773190908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.78 | train loss last: 3.3 | consumed tokens: 236748800.0 | grad norm avg: 1.25 | grad norm last: 1.24 | 
2025-12-29T16:29:55 | step: 29000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.4871496609412134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.83 | consumed tokens: 237568000.0 | grad norm avg: 1.26 | grad norm last: 1.32 | 
2025-12-29T16:30:15 | step: 29100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.477665813989006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.53 | consumed tokens: 238387200.0 | grad norm avg: 1.25 | grad norm last: 1.27 | 
2025-12-29T16:30:36 | step: 29200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.468165232334286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.77 | train loss last: 2.91 | consumed tokens: 239206400.0 | grad norm avg: 1.27 | grad norm last: 1.4 | 
2025-12-29T16:30:56 | step: 29300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.458648279774934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.52 | consumed tokens: 240025600.0 | grad norm avg: 1.28 | grad norm last: 1.19 | 
2025-12-29T16:31:17 | step: 29400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.44911495631095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.66 | consumed tokens: 240844800.0 | grad norm avg: 1.25 | grad norm last: 1.12 | 
2025-12-29T16:31:37 | step: 29500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.439565625740215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.75 | consumed tokens: 241664000.0 | grad norm avg: 1.26 | grad norm last: 1.29 | 
2025-12-29T16:31:58 | step: 29600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.430000288062729e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.7 | consumed tokens: 242483200.0 | grad norm avg: 1.27 | grad norm last: 1.16 | 
2025-12-29T16:32:18 | step: 29700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.420419307076372e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.77 | consumed tokens: 243302400.0 | grad norm avg: 1.26 | grad norm last: 1.27 | 
2025-12-29T16:32:39 | step: 29800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.410822318983264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.52 | consumed tokens: 244121600.0 | grad norm avg: 1.26 | grad norm last: 1.33 | 
2025-12-29T16:32:59 | step: 29900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.4012100513791665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.42 | consumed tokens: 244940800.0 | grad norm avg: 1.27 | grad norm last: 1.11 | 
2025-12-29T16:33:20 | step: 30000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.391582504264079e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.78 | consumed tokens: 245760000.0 | grad norm avg: 1.25 | grad norm last: 1.27 | 
2025-12-29T16:33:42 | step: 30100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.381939677638002e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.59 | consumed tokens: 246579200.0 | grad norm avg: 1.26 | grad norm last: 1.25 | 
2025-12-29T16:34:03 | step: 30200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.372282299096696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 3.12 | consumed tokens: 247398400.0 | grad norm avg: 1.27 | grad norm last: 1.37 | 
2025-12-29T16:34:23 | step: 30300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.3626096410444006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 3.19 | consumed tokens: 248217600.0 | grad norm avg: 1.24 | grad norm last: 1.31 | 
2025-12-29T16:34:44 | step: 30400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.352922431076877e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.56 | consumed tokens: 249036800.0 | grad norm avg: 1.27 | grad norm last: 1.33 | 
2025-12-29T16:35:04 | step: 30500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.3432206691941246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.78 | consumed tokens: 249856000.0 | grad norm avg: 1.24 | grad norm last: 1.27 | 
2025-12-29T16:35:25 | step: 30600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.333504719194025e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.7 | consumed tokens: 250675200.0 | grad norm avg: 1.26 | grad norm last: 1.34 | 
2025-12-29T16:35:45 | step: 30700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.3237742172786966e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 3.27 | consumed tokens: 251494400.0 | grad norm avg: 1.26 | grad norm last: 1.34 | 
2025-12-29T16:36:06 | step: 30800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.314030254841782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.27 | consumed tokens: 252313600.0 | grad norm avg: 1.28 | grad norm last: 1.27 | 
2025-12-29T16:36:26 | step: 30900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.30427210428752e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.98 | consumed tokens: 253132800.0 | grad norm avg: 1.26 | grad norm last: 1.29 | 
2025-12-29T16:36:47 | step: 31000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.294500493211672e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.84 | consumed tokens: 253952000.0 | grad norm avg: 1.27 | grad norm last: 1.28 | 
2025-12-29T16:37:07 | step: 31100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.284715421614237e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.78 | consumed tokens: 254771200.0 | grad norm avg: 1.26 | grad norm last: 1.27 | 
2025-12-29T16:37:28 | step: 31200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.274916889495216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.39 | consumed tokens: 255590400.0 | grad norm avg: 1.27 | grad norm last: 1.16 | 
2025-12-29T16:37:49 | step: 31300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.265104896854609e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 3.06 | consumed tokens: 256409600.0 | grad norm avg: 1.26 | grad norm last: 1.37 | 
2025-12-29T16:38:09 | step: 31400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.2552801712881774e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.47 | consumed tokens: 257228800.0 | grad norm avg: 1.26 | grad norm last: 1.26 | 
2025-12-29T16:38:30 | step: 31500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.245442712795921e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.55 | consumed tokens: 258048000.0 | grad norm avg: 1.27 | grad norm last: 1.3 | 
2025-12-29T16:38:50 | step: 31600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.235592521377839e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.78 | consumed tokens: 258867200.0 | grad norm avg: 1.26 | grad norm last: 1.28 | 
2025-12-29T16:39:11 | step: 31700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.2257299608318135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.97 | consumed tokens: 259686400.0 | grad norm avg: 1.26 | grad norm last: 1.29 | 
2025-12-29T16:39:31 | step: 31800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.215855031157844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.45 | consumed tokens: 260505600.0 | grad norm avg: 1.27 | grad norm last: 1.3 | 
2025-12-29T16:39:52 | step: 31900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.20596773235593e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.83 | consumed tokens: 261324800.0 | grad norm avg: 1.26 | grad norm last: 1.6 | 
2025-12-29T16:40:12 | step: 32000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.1960687920218334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.59 | consumed tokens: 262144000.0 | grad norm avg: 1.26 | grad norm last: 1.49 | 
2025-12-29T16:40:33 | step: 32100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.186157482559793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.76 | train loss last: 2.33 | consumed tokens: 262963200.0 | grad norm avg: 1.28 | grad norm last: 1.28 | 
2025-12-29T16:40:54 | step: 32200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.17623489536345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 3.11 | consumed tokens: 263782400.0 | grad norm avg: 1.27 | grad norm last: 1.31 | 
2025-12-29T16:41:14 | step: 32300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.1663010304328054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.56 | consumed tokens: 264601600.0 | grad norm avg: 1.27 | grad norm last: 1.24 | 
2025-12-29T16:41:35 | step: 32400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.156355523969978e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.75 | consumed tokens: 265420800.0 | grad norm avg: 1.26 | grad norm last: 1.16 | 
2025-12-29T16:41:55 | step: 32500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.1463991035707295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.92 | consumed tokens: 266240000.0 | grad norm avg: 1.27 | grad norm last: 1.3 | 
2025-12-29T16:42:16 | step: 32600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.136431405437179e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.8 | consumed tokens: 267059200.0 | grad norm avg: 1.27 | grad norm last: 1.23 | 
2025-12-29T16:42:36 | step: 32700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.126453157165088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.81 | consumed tokens: 267878400.0 | grad norm avg: 1.27 | grad norm last: 1.22 | 
2025-12-29T16:42:57 | step: 32800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.116463994956575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.61 | consumed tokens: 268697600.0 | grad norm avg: 1.28 | grad norm last: 1.23 | 
2025-12-29T16:43:17 | step: 32900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.106464646407403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.38 | consumed tokens: 269516800.0 | grad norm avg: 1.26 | grad norm last: 1.29 | 
2025-12-29T16:43:38 | step: 33000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.09645474771969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.81 | consumed tokens: 270336000.0 | grad norm avg: 1.26 | grad norm last: 1.28 | 
2025-12-29T16:43:58 | step: 33100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.086435026489198e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.11 | consumed tokens: 271155200.0 | grad norm avg: 1.26 | grad norm last: 1.38 | 
2025-12-29T16:44:19 | step: 33200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.0764051189180464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.45 | consumed tokens: 271974400.0 | grad norm avg: 1.27 | grad norm last: 1.35 | 
2025-12-29T16:44:39 | step: 33300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.066365752601996e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.75 | consumed tokens: 272793600.0 | grad norm avg: 1.26 | grad norm last: 1.27 | 
2025-12-29T16:45:00 | step: 33400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.0563165637431666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.64 | consumed tokens: 273612800.0 | grad norm avg: 1.24 | grad norm last: 1.24 | 
2025-12-29T16:45:20 | step: 33500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.0462577342404984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.3 | consumed tokens: 274432000.0 | grad norm avg: 1.28 | grad norm last: 1.35 | 
2025-12-29T16:45:41 | step: 33600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.0361898097908124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.66 | consumed tokens: 275251200.0 | grad norm avg: 1.26 | grad norm last: 1.19 | 
2025-12-29T16:46:02 | step: 33700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.0261127903941087e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.09 | consumed tokens: 276070400.0 | grad norm avg: 1.26 | grad norm last: 1.18 | 
2025-12-29T16:46:22 | step: 33800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.0160268579493277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.47 | consumed tokens: 276889600.0 | grad norm avg: 1.27 | grad norm last: 1.45 | 
2025-12-29T16:46:43 | step: 33900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.0059321943554096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.66 | consumed tokens: 277708800.0 | grad norm avg: 1.26 | grad norm last: 1.26 | 
2025-12-29T16:47:03 | step: 34000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.995828981511295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.92 | consumed tokens: 278528000.0 | grad norm avg: 1.27 | grad norm last: 1.39 | 
2025-12-29T16:47:24 | step: 34100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.9857174013159238e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.69 | consumed tokens: 279347200.0 | grad norm avg: 1.25 | grad norm last: 1.37 | 
2025-12-29T16:47:44 | step: 34200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.9755974537692964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.5 | consumed tokens: 280166400.0 | grad norm avg: 1.27 | grad norm last: 1.25 | 
2025-12-29T16:48:05 | step: 34300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.9654695026692934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 3.16 | consumed tokens: 280985600.0 | grad norm avg: 1.28 | grad norm last: 1.13 | 
2025-12-29T16:48:25 | step: 34400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.955333548015915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.97 | consumed tokens: 281804800.0 | grad norm avg: 1.28 | grad norm last: 1.26 | 
2025-12-29T16:48:46 | step: 34500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.9451899536070414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.95 | consumed tokens: 282624000.0 | grad norm avg: 1.28 | grad norm last: 1.3 | 
2025-12-29T16:49:06 | step: 34600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.9350389013416134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.53 | consumed tokens: 283443200.0 | grad norm avg: 1.29 | grad norm last: 1.29 | 
2025-12-29T16:49:27 | step: 34700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.924880391219631e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.59 | consumed tokens: 284262400.0 | grad norm avg: 1.27 | grad norm last: 1.23 | 
2025-12-29T16:49:48 | step: 34800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.914714605140034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.94 | consumed tokens: 285081600.0 | grad norm avg: 1.28 | grad norm last: 1.19 | 
2025-12-29T16:50:08 | step: 34900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.904541906900704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.02 | consumed tokens: 285900800.0 | grad norm avg: 1.27 | grad norm last: 1.2 | 
2025-12-29T16:50:29 | step: 35000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8943624784005806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.08 | consumed tokens: 286720000.0 | grad norm avg: 1.27 | grad norm last: 1.23 | 
2025-12-29T16:50:51 | step: 35100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8841761377407238e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.83 | consumed tokens: 287539200.0 | grad norm avg: 1.3 | grad norm last: 1.39 | 
2025-12-29T16:51:11 | step: 35200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.8739834306179546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.56 | consumed tokens: 288358400.0 | grad norm avg: 1.28 | grad norm last: 1.22 | 
2025-12-29T16:51:32 | step: 35300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.863784357032273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.77 | consumed tokens: 289177600.0 | grad norm avg: 1.28 | grad norm last: 1.33 | 
2025-12-29T16:51:52 | step: 35400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.8535790988826193e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.55 | consumed tokens: 289996800.0 | grad norm avg: 1.26 | grad norm last: 1.3 | 
2025-12-29T16:52:13 | step: 35500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.843367838067934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.67 | consumed tokens: 290816000.0 | grad norm avg: 1.28 | grad norm last: 1.21 | 
2025-12-29T16:52:34 | step: 35600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8331507564871572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.47 | consumed tokens: 291635200.0 | grad norm avg: 1.27 | grad norm last: 1.25 | 
2025-12-29T16:52:54 | step: 35700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8229280360392295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.47 | consumed tokens: 292454400.0 | grad norm avg: 1.28 | grad norm last: 1.19 | 
2025-12-29T16:53:14 | step: 35800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.8127000405220315e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.41 | consumed tokens: 293273600.0 | grad norm avg: 1.29 | grad norm last: 1.32 | 
2025-12-29T16:53:35 | step: 35900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.802466588036623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.58 | consumed tokens: 294092800.0 | grad norm avg: 1.3 | grad norm last: 1.14 | 
2025-12-29T16:53:56 | step: 36000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.7922280423808843e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.44 | consumed tokens: 294912000.0 | grad norm avg: 1.28 | grad norm last: 1.31 | 
2025-12-29T16:54:16 | step: 36100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.781984585453756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.84 | consumed tokens: 295731200.0 | grad norm avg: 1.31 | grad norm last: 1.23 | 
2025-12-29T16:54:37 | step: 36200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.7717362172552384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.75 | train loss last: 2.5 | consumed tokens: 296550400.0 | grad norm avg: 1.28 | grad norm last: 1.2 | 
2025-12-29T16:54:57 | step: 36300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.7614834834821522e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.8 | consumed tokens: 297369600.0 | grad norm avg: 1.3 | grad norm last: 1.09 | 
2025-12-29T16:55:18 | step: 36400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.751226202235557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.06 | consumed tokens: 298188800.0 | grad norm avg: 1.28 | grad norm last: 1.18 | 
2025-12-29T16:55:38 | step: 36500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.740964737313334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 3.27 | consumed tokens: 299008000.0 | grad norm avg: 1.28 | grad norm last: 1.27 | 
2025-12-29T16:55:59 | step: 36600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.730699270614423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.14 | consumed tokens: 299827200.0 | grad norm avg: 1.3 | grad norm last: 1.27 | 
2025-12-29T16:56:19 | step: 36700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.720429802138824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.28 | consumed tokens: 300646400.0 | grad norm avg: 1.28 | grad norm last: 1.22 | 
2025-12-29T16:56:40 | step: 36800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.710156695684418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.19 | consumed tokens: 301465600.0 | grad norm avg: 1.28 | grad norm last: 1.21 | 
2025-12-29T16:57:00 | step: 36900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.699879951251205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.47 | consumed tokens: 302284800.0 | grad norm avg: 1.29 | grad norm last: 1.24 | 
2025-12-29T16:57:21 | step: 37000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.6895999326370656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.34 | consumed tokens: 303104000.0 | grad norm avg: 1.28 | grad norm last: 1.26 | 
2025-12-29T16:57:42 | step: 37100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.679316639842e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.74 | train loss last: 2.7 | consumed tokens: 303923200.0 | grad norm avg: 1.29 | grad norm last: 1.42 | 
2025-12-29T16:58:02 | step: 37200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.6690304366638884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.8 | consumed tokens: 304742400.0 | grad norm avg: 1.3 | grad norm last: 1.25 | 
2025-12-29T16:58:23 | step: 37300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6587413231027313e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.7 | consumed tokens: 305561600.0 | grad norm avg: 1.26 | grad norm last: 1.22 | 
2025-12-29T16:58:43 | step: 37400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.648449662956409e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.73 | consumed tokens: 306380800.0 | grad norm avg: 1.28 | grad norm last: 1.21 | 
2025-12-29T16:59:04 | step: 37500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6381552743259817e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.8 | consumed tokens: 307200000.0 | grad norm avg: 1.29 | grad norm last: 1.29 | 
2025-12-29T16:59:24 | step: 37600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.62785870290827e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.23 | consumed tokens: 308019200.0 | grad norm avg: 1.29 | grad norm last: 1.3 | 
2025-12-29T16:59:45 | step: 37700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.6175601306022145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.95 | consumed tokens: 308838400.0 | grad norm avg: 1.3 | grad norm last: 1.29 | 
2025-12-29T17:00:05 | step: 37800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6072593755088747e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.44 | consumed tokens: 309657600.0 | grad norm avg: 1.3 | grad norm last: 1.26 | 
2025-12-29T17:00:26 | step: 37900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.5969569833250716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.19 | consumed tokens: 310476800.0 | grad norm avg: 1.27 | grad norm last: 1.27 | 
2025-12-29T17:00:46 | step: 38000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.5866529540508054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.05 | consumed tokens: 311296000.0 | grad norm avg: 1.3 | grad norm last: 1.22 | 
2025-12-29T17:01:07 | step: 38100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.5763474695850164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.53 | consumed tokens: 312115200.0 | grad norm avg: 1.31 | grad norm last: 1.16 | 
2025-12-29T17:01:27 | step: 38200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.566040711826645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.91 | consumed tokens: 312934400.0 | grad norm avg: 1.31 | grad norm last: 1.21 | 
2025-12-29T17:01:48 | step: 38300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.5557328626746312e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.45 | consumed tokens: 313753600.0 | grad norm avg: 1.3 | grad norm last: 1.43 | 
2025-12-29T17:02:09 | step: 38400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.545424285926856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.53 | consumed tokens: 314572800.0 | grad norm avg: 1.3 | grad norm last: 1.19 | 
2025-12-29T17:02:29 | step: 38500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.5351147996843792e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.05 | consumed tokens: 315392000.0 | grad norm avg: 1.28 | grad norm last: 1.34 | 
2025-12-29T17:02:50 | step: 38600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.5248047677450813e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.73 | consumed tokens: 316211200.0 | grad norm avg: 1.3 | grad norm last: 1.34 | 
2025-12-29T17:03:10 | step: 38700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.5144943720079027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.53 | consumed tokens: 317030400.0 | grad norm avg: 1.29 | grad norm last: 1.27 | 
2025-12-29T17:03:31 | step: 38800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.5041837943717837e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.44 | consumed tokens: 317849600.0 | grad norm avg: 1.29 | grad norm last: 1.29 | 
2025-12-29T17:03:51 | step: 38900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4938730348367244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.48 | consumed tokens: 318668800.0 | grad norm avg: 1.3 | grad norm last: 1.25 | 
2025-12-29T17:04:12 | step: 39000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.4835626390995458e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.62 | consumed tokens: 319488000.0 | grad norm avg: 1.3 | grad norm last: 1.33 | 
2025-12-29T17:04:32 | step: 39100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.4732524252613075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.66 | consumed tokens: 320307200.0 | grad norm avg: 1.32 | grad norm last: 1.53 | 
2025-12-29T17:04:53 | step: 39200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.4629427571198903e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.33 | consumed tokens: 321126400.0 | grad norm avg: 1.32 | grad norm last: 1.38 | 
2025-12-29T17:05:13 | step: 39300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.4526338165742345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 3.03 | consumed tokens: 321945600.0 | grad norm avg: 1.3 | grad norm last: 1.29 | 
2025-12-29T17:05:34 | step: 39400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.44232560362434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.64 | consumed tokens: 322764800.0 | grad norm avg: 1.31 | grad norm last: 1.48 | 
2025-12-29T17:05:55 | step: 39500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.432018482068088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.8 | consumed tokens: 323584000.0 | grad norm avg: 1.33 | grad norm last: 1.36 | 
2025-12-29T17:06:15 | step: 39600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.421712633804418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.77 | consumed tokens: 324403200.0 | grad norm avg: 1.3 | grad norm last: 1.25 | 
2025-12-29T17:06:36 | step: 39700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.411408058833331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.72 | consumed tokens: 325222400.0 | grad norm avg: 1.3 | grad norm last: 1.33 | 
2025-12-29T17:06:56 | step: 39800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.4011051209527068e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.84 | consumed tokens: 326041600.0 | grad norm avg: 1.32 | grad norm last: 1.26 | 
2025-12-29T17:07:17 | step: 39900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.390803820162546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.06 | consumed tokens: 326860800.0 | grad norm avg: 1.31 | grad norm last: 1.37 | 
2025-12-29T17:07:37 | step: 40000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.380504520260729e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.41 | consumed tokens: 327680000.0 | grad norm avg: 1.3 | grad norm last: 1.33 | 
2025-12-29T17:07:59 | step: 40100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.3702072212472558e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.97 | consumed tokens: 328499200.0 | grad norm avg: 1.33 | grad norm last: 1.34 | 
2025-12-29T17:08:20 | step: 40200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3599122869200073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.03 | consumed tokens: 329318400.0 | grad norm avg: 1.33 | grad norm last: 1.32 | 
2025-12-29T17:08:40 | step: 40300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.3496197172789834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.42 | consumed tokens: 330137600.0 | grad norm avg: 1.33 | grad norm last: 1.28 | 
2025-12-29T17:09:01 | step: 40400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.3393296942231245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.8 | consumed tokens: 330956800.0 | grad norm avg: 1.35 | grad norm last: 1.35 | 
2025-12-29T17:09:21 | step: 40500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3290425815503113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.72 | consumed tokens: 331776000.0 | grad norm avg: 1.31 | grad norm last: 1.49 | 
2025-12-29T17:09:42 | step: 40600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.3187581973616034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.67 | consumed tokens: 332595200.0 | grad norm avg: 1.3 | grad norm last: 1.31 | 
2025-12-29T17:10:03 | step: 40700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.308477087353822e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.92 | consumed tokens: 333414400.0 | grad norm avg: 1.31 | grad norm last: 1.18 | 
2025-12-29T17:10:23 | step: 40800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.2981992515269667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.41 | consumed tokens: 334233600.0 | grad norm avg: 1.32 | grad norm last: 1.31 | 
2025-12-29T17:10:44 | step: 40900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.2879250536789186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.89 | consumed tokens: 335052800.0 | grad norm avg: 1.32 | grad norm last: 1.23 | 
2025-12-29T17:11:04 | step: 41000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.2776543119107373e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.7 | consumed tokens: 335872000.0 | grad norm avg: 1.32 | grad norm last: 1.28 | 
2025-12-29T17:11:25 | step: 41100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.2673875719192438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.83 | consumed tokens: 336691200.0 | grad norm avg: 1.31 | grad norm last: 1.27 | 
2025-12-29T17:11:45 | step: 41200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.2571246518054977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.38 | consumed tokens: 337510400.0 | grad norm avg: 1.34 | grad norm last: 1.48 | 
2025-12-29T17:12:06 | step: 41300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.24686609726632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.55 | consumed tokens: 338329600.0 | grad norm avg: 1.33 | grad norm last: 1.31 | 
2025-12-29T17:12:27 | step: 41400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2366117264027707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.98 | consumed tokens: 339148800.0 | grad norm avg: 1.31 | grad norm last: 1.23 | 
2025-12-29T17:12:47 | step: 41500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2263620849116705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.3 | consumed tokens: 339968000.0 | grad norm avg: 1.32 | grad norm last: 1.3 | 
2025-12-29T17:13:08 | step: 41600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.2161169908940792e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.06 | consumed tokens: 340787200.0 | grad norm avg: 1.32 | grad norm last: 1.29 | 
2025-12-29T17:13:28 | step: 41700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.2058768081478775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.94 | consumed tokens: 341606400.0 | grad norm avg: 1.32 | grad norm last: 1.26 | 
2025-12-29T17:13:49 | step: 41800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.1956417185720056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.09 | consumed tokens: 342425600.0 | grad norm avg: 1.34 | grad norm last: 1.38 | 
2025-12-29T17:14:09 | step: 41900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.1854117221664637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.25 | consumed tokens: 343244800.0 | grad norm avg: 1.32 | grad norm last: 1.32 | 
2025-12-29T17:14:30 | step: 42000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.1751873646280728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.45 | consumed tokens: 344064000.0 | grad norm avg: 1.33 | grad norm last: 1.29 | 
2025-12-29T17:14:50 | step: 42100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.1649684640578926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.61 | consumed tokens: 344883200.0 | grad norm avg: 1.33 | grad norm last: 1.47 | 
2025-12-29T17:15:11 | step: 42200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1547552023548633e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.05 | consumed tokens: 345702400.0 | grad norm avg: 1.32 | grad norm last: 1.37 | 
2025-12-29T17:15:32 | step: 42300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.144548125215806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.48 | consumed tokens: 346521600.0 | grad norm avg: 1.34 | grad norm last: 1.27 | 
2025-12-29T17:15:52 | step: 42400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.1343468688428402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.06 | consumed tokens: 347340800.0 | grad norm avg: 1.33 | grad norm last: 1.3 | 
2025-12-29T17:16:13 | step: 42500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.124152160831727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.58 | consumed tokens: 348160000.0 | grad norm avg: 1.34 | grad norm last: 1.32 | 
2025-12-29T17:16:33 | step: 42600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.113963637384586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.2 | consumed tokens: 348979200.0 | grad norm avg: 1.34 | grad norm last: 1.41 | 
2025-12-29T17:16:54 | step: 42700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.1037820260971785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.45 | consumed tokens: 349798400.0 | grad norm avg: 1.33 | grad norm last: 1.26 | 
2025-12-29T17:17:14 | step: 42800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.0936069631716236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.14 | consumed tokens: 350617600.0 | grad norm avg: 1.32 | grad norm last: 1.3 | 
2025-12-29T17:17:35 | step: 42900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0834389943047427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.62 | consumed tokens: 351436800.0 | grad norm avg: 1.34 | grad norm last: 1.33 | 
2025-12-29T17:17:55 | step: 43000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.0732781194965355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 3.05 | consumed tokens: 352256000.0 | grad norm avg: 1.33 | grad norm last: 1.28 | 
2025-12-29T17:18:16 | step: 43100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.0631245206459425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.66 | consumed tokens: 353075200.0 | grad norm avg: 1.33 | grad norm last: 1.37 | 
2025-12-29T17:18:36 | step: 43200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0529785615508445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.16 | consumed tokens: 353894400.0 | grad norm avg: 1.32 | grad norm last: 1.32 | 
2025-12-29T17:18:57 | step: 43300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.042840060312301e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 2.81 | consumed tokens: 354713600.0 | grad norm avg: 1.33 | grad norm last: 1.32 | 
2025-12-29T17:19:18 | step: 43400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.032709562627133e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.69 | consumed tokens: 355532800.0 | grad norm avg: 1.33 | grad norm last: 1.33 | 
2025-12-29T17:19:38 | step: 43500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0225868865964003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.7 | consumed tokens: 356352000.0 | grad norm avg: 1.33 | grad norm last: 1.24 | 
2025-12-29T17:19:59 | step: 43600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.012472577916924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.0 | consumed tokens: 357171200.0 | grad norm avg: 1.34 | grad norm last: 1.35 | 
2025-12-29T17:20:19 | step: 43700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.0023664546897635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.39 | consumed tokens: 357990400.0 | grad norm avg: 1.34 | grad norm last: 1.36 | 
2025-12-29T17:20:40 | step: 43800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.99226906261174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.8 | consumed tokens: 358809600.0 | grad norm avg: 1.33 | grad norm last: 1.28 | 
2025-12-29T17:21:01 | step: 43900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.9821802197839133e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.81 | consumed tokens: 359628800.0 | grad norm avg: 1.34 | grad norm last: 1.41 | 
2025-12-29T17:21:21 | step: 44000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.972100290004164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.8 | consumed tokens: 360448000.0 | grad norm avg: 1.33 | grad norm last: 1.33 | 
2025-12-29T17:21:42 | step: 44100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.9620294551714323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.72 | train loss last: 3.05 | consumed tokens: 361267200.0 | grad norm avg: 1.35 | grad norm last: 1.28 | 
2025-12-29T17:22:02 | step: 44200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.9519677152857184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.95 | consumed tokens: 362086400.0 | grad norm avg: 1.36 | grad norm last: 1.34 | 
2025-12-29T17:22:23 | step: 44300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.941915434144903e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.75 | consumed tokens: 362905600.0 | grad norm avg: 1.33 | grad norm last: 1.23 | 
2025-12-29T17:22:43 | step: 44400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.931872611748986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.2 | consumed tokens: 363724800.0 | grad norm avg: 1.34 | grad norm last: 1.44 | 
2025-12-29T17:23:04 | step: 44500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.921839611895848e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.55 | consumed tokens: 364544000.0 | grad norm avg: 1.35 | grad norm last: 1.49 | 
2025-12-29T17:23:25 | step: 44600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.9118164345854893e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.33 | consumed tokens: 365363200.0 | grad norm avg: 1.33 | grad norm last: 1.26 | 
2025-12-29T17:23:45 | step: 44700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.9018034436157905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.66 | consumed tokens: 366182400.0 | grad norm avg: 1.32 | grad norm last: 1.32 | 
2025-12-29T17:24:06 | step: 44800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.8918006389867514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.64 | consumed tokens: 367001600.0 | grad norm avg: 1.35 | grad norm last: 1.61 | 
2025-12-29T17:24:26 | step: 44900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.8818082025973126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.2 | consumed tokens: 367820800.0 | grad norm avg: 1.34 | grad norm last: 1.43 | 
2025-12-29T17:24:47 | step: 45000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.8718263163464144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.92 | consumed tokens: 368640000.0 | grad norm avg: 1.33 | grad norm last: 1.4 | 
2025-12-29T17:25:09 | step: 45100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.8618553440319374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.23 | consumed tokens: 369459200.0 | grad norm avg: 1.34 | grad norm last: 1.33 | 
2025-12-29T17:25:29 | step: 45200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.8518951037549414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.58 | consumed tokens: 370278400.0 | grad norm avg: 1.34 | grad norm last: 1.34 | 
2025-12-29T17:25:50 | step: 45300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.841945959313307e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.25 | consumed tokens: 371097600.0 | grad norm avg: 1.34 | grad norm last: 1.39 | 
2025-12-29T17:26:11 | step: 45400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.832008274504915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.55 | consumed tokens: 371916800.0 | grad norm avg: 1.33 | grad norm last: 1.34 | 
2025-12-29T17:26:31 | step: 45500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.8220818674308248e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.17 | consumed tokens: 372736000.0 | grad norm avg: 1.35 | grad norm last: 1.37 | 
2025-12-29T17:26:52 | step: 45600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.812166919989977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.05 | consumed tokens: 373555200.0 | grad norm avg: 1.35 | grad norm last: 1.32 | 
2025-12-29T17:27:12 | step: 45700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.8022639778791927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.31 | consumed tokens: 374374400.0 | grad norm avg: 1.35 | grad norm last: 1.18 | 
2025-12-29T17:27:33 | step: 45800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.7923728591995314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.64 | consumed tokens: 375193600.0 | grad norm avg: 1.35 | grad norm last: 1.47 | 
2025-12-29T17:27:54 | step: 45900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.7824939277488738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.73 | train loss last: 2.8 | consumed tokens: 376012800.0 | grad norm avg: 1.34 | grad norm last: 1.31 | 
2025-12-29T17:28:14 | step: 46000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.77262718352722e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.64 | consumed tokens: 376832000.0 | grad norm avg: 1.35 | grad norm last: 1.45 | 
2025-12-29T17:28:35 | step: 46100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.7627728084335104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.66 | consumed tokens: 377651200.0 | grad norm avg: 1.32 | grad norm last: 1.37 | 
2025-12-29T17:28:55 | step: 46200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7529311662656255e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.73 | consumed tokens: 378470400.0 | grad norm avg: 1.34 | grad norm last: 1.3 | 
2025-12-29T17:29:16 | step: 46300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.7431022570235655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.73 | consumed tokens: 379289600.0 | grad norm avg: 1.32 | grad norm last: 1.29 | 
2025-12-29T17:29:36 | step: 46400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.7332862626062706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.78 | consumed tokens: 380108800.0 | grad norm avg: 1.33 | grad norm last: 1.33 | 
2025-12-29T17:29:57 | step: 46500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.7234835468116216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.5 | consumed tokens: 380928000.0 | grad norm avg: 1.36 | grad norm last: 1.43 | 
2025-12-29T17:30:17 | step: 46600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.7136939277406782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.56 | consumed tokens: 381747200.0 | grad norm avg: 1.35 | grad norm last: 1.33 | 
2025-12-29T17:30:38 | step: 46700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.703917769191321e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.09 | consumed tokens: 382566400.0 | grad norm avg: 1.35 | grad norm last: 1.3 | 
2025-12-29T17:30:59 | step: 46800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.6941552530624904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.16 | consumed tokens: 383385600.0 | grad norm avg: 1.35 | grad norm last: 1.3 | 
2025-12-29T17:31:19 | step: 46900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.6844065612531267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.58 | consumed tokens: 384204800.0 | grad norm avg: 1.34 | grad norm last: 1.45 | 
2025-12-29T17:31:40 | step: 47000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.67467169376323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.95 | consumed tokens: 385024000.0 | grad norm avg: 1.35 | grad norm last: 1.32 | 
2025-12-29T17:32:00 | step: 47100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.664951014390681e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.83 | consumed tokens: 385843200.0 | grad norm avg: 1.35 | grad norm last: 1.48 | 
2025-12-29T17:32:21 | step: 47200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.65524470503442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.5 | consumed tokens: 386662400.0 | grad norm avg: 1.36 | grad norm last: 1.24 | 
2025-12-29T17:32:42 | step: 47300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.645552765694447e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.66 | consumed tokens: 387481600.0 | grad norm avg: 1.35 | grad norm last: 1.53 | 
2025-12-29T17:33:02 | step: 47400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.6358753782697022e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.17 | consumed tokens: 388300800.0 | grad norm avg: 1.34 | grad norm last: 1.29 | 
2025-12-29T17:33:23 | step: 47500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.6262127246591263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.64 | consumed tokens: 389120000.0 | grad norm avg: 1.36 | grad norm last: 1.27 | 
2025-12-29T17:33:43 | step: 47600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.6165649867616594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.08 | consumed tokens: 389939200.0 | grad norm avg: 1.36 | grad norm last: 1.49 | 
2025-12-29T17:34:04 | step: 47700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.6069325283751823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.72 | consumed tokens: 390758400.0 | grad norm avg: 1.35 | grad norm last: 1.2 | 
2025-12-29T17:34:24 | step: 47800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5973151676007546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.3 | consumed tokens: 391577600.0 | grad norm avg: 1.36 | grad norm last: 1.37 | 
2025-12-29T17:34:45 | step: 47900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.587713268236257e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.23 | consumed tokens: 392396800.0 | grad norm avg: 1.35 | grad norm last: 1.2 | 
2025-12-29T17:35:06 | step: 48000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.57812701218063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.67 | consumed tokens: 393216000.0 | grad norm avg: 1.37 | grad norm last: 1.28 | 
2025-12-29T17:35:26 | step: 48100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.568556581332814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.39 | consumed tokens: 394035200.0 | grad norm avg: 1.34 | grad norm last: 1.4 | 
2025-12-29T17:35:47 | step: 48200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5590019756928086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.39 | consumed tokens: 394854400.0 | grad norm avg: 1.37 | grad norm last: 1.5 | 
2025-12-29T17:36:07 | step: 48300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.5494633771595545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.48 | consumed tokens: 395673600.0 | grad norm avg: 1.36 | grad norm last: 1.29 | 
2025-12-29T17:36:28 | step: 48400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5399411495309323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.33 | consumed tokens: 396492800.0 | grad norm avg: 1.37 | grad norm last: 1.43 | 
2025-12-29T17:36:49 | step: 48500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.530435292806942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.0 | consumed tokens: 397312000.0 | grad norm avg: 1.37 | grad norm last: 1.32 | 
2025-12-29T17:37:09 | step: 48600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.5209458979370538e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.47 | consumed tokens: 398131200.0 | grad norm avg: 1.36 | grad norm last: 1.25 | 
2025-12-29T17:37:30 | step: 48700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5114733287191484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.94 | consumed tokens: 398950400.0 | grad norm avg: 1.36 | grad norm last: 1.53 | 
2025-12-29T17:37:50 | step: 48800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.502017676102696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.52 | consumed tokens: 399769600.0 | grad norm avg: 1.36 | grad norm last: 1.42 | 
2025-12-29T17:38:11 | step: 48900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.4925790310371667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.36 | consumed tokens: 400588800.0 | grad norm avg: 1.35 | grad norm last: 1.6 | 
2025-12-29T17:38:31 | step: 49000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.483157575421501e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.17 | consumed tokens: 401408000.0 | grad norm avg: 1.35 | grad norm last: 1.29 | 
2025-12-29T17:38:52 | step: 49100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4737535821041092e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.02 | consumed tokens: 402227200.0 | grad norm avg: 1.36 | grad norm last: 1.38 | 
2025-12-29T17:39:12 | step: 49200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4643670510849915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.59 | consumed tokens: 403046400.0 | grad norm avg: 1.36 | grad norm last: 1.29 | 
2025-12-29T17:39:33 | step: 49300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4549981642630883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.03 | consumed tokens: 403865600.0 | grad norm avg: 1.36 | grad norm last: 1.3 | 
2025-12-29T17:39:54 | step: 49400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.4456471944868099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.97 | consumed tokens: 404684800.0 | grad norm avg: 1.35 | grad norm last: 1.27 | 
2025-12-29T17:40:14 | step: 49500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.4363142327056266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.98 | consumed tokens: 405504000.0 | grad norm avg: 1.35 | grad norm last: 1.32 | 
2025-12-29T17:40:35 | step: 49600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4269994608184788e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.84 | consumed tokens: 406323200.0 | grad norm avg: 1.37 | grad norm last: 1.43 | 
2025-12-29T17:40:55 | step: 49700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4177030607243069e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.23 | consumed tokens: 407142400.0 | grad norm avg: 1.36 | grad norm last: 1.47 | 
2025-12-29T17:41:16 | step: 49800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4084250324231107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.17 | consumed tokens: 407961600.0 | grad norm avg: 1.35 | grad norm last: 1.31 | 
2025-12-29T17:41:37 | step: 49900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.399165739712771e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.7 | consumed tokens: 408780800.0 | grad norm avg: 1.37 | grad norm last: 1.46 | 
2025-12-29T17:41:57 | step: 50000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.3899251825932879e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.72 | consumed tokens: 409600000.0 | grad norm avg: 1.38 | grad norm last: 1.28 | 
2025-12-29T17:42:19 | step: 50100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.3807036339130718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.61 | consumed tokens: 410419200.0 | grad norm avg: 1.36 | grad norm last: 1.42 | 
2025-12-29T17:42:40 | step: 50200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3715012755710632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.91 | consumed tokens: 411238400.0 | grad norm avg: 1.37 | grad norm last: 1.41 | 
2025-12-29T17:43:00 | step: 50300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.3623181075672619e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.28 | consumed tokens: 412057600.0 | grad norm avg: 1.37 | grad norm last: 1.31 | 
2025-12-29T17:43:21 | step: 50400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3531543118006084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.56 | consumed tokens: 412876800.0 | grad norm avg: 1.38 | grad norm last: 1.31 | 
2025-12-29T17:43:42 | step: 50500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3440102520689834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.28 | consumed tokens: 413696000.0 | grad norm avg: 1.38 | grad norm last: 1.28 | 
2025-12-29T17:44:02 | step: 50600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3348858374229167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.72 | consumed tokens: 414515200.0 | grad norm avg: 1.36 | grad norm last: 1.28 | 
2025-12-29T17:44:23 | step: 50700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.3257813407108188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.61 | consumed tokens: 415334400.0 | grad norm avg: 1.37 | grad norm last: 1.34 | 
2025-12-29T17:44:43 | step: 50800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.3166968528821599e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.2 | consumed tokens: 416153600.0 | grad norm avg: 1.37 | grad norm last: 1.32 | 
2025-12-29T17:45:04 | step: 50900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3076326467853505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.38 | consumed tokens: 416972800.0 | grad norm avg: 1.38 | grad norm last: 1.35 | 
2025-12-29T17:45:24 | step: 51000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2985888133698609e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.72 | consumed tokens: 417792000.0 | grad norm avg: 1.37 | grad norm last: 1.24 | 
2025-12-29T17:45:45 | step: 51100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.2895654435851611e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.12 | consumed tokens: 418611200.0 | grad norm avg: 1.36 | grad norm last: 1.33 | 
2025-12-29T17:46:06 | step: 51200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2805628102796618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.56 | consumed tokens: 419430400.0 | grad norm avg: 1.37 | grad norm last: 1.33 | 
2025-12-29T17:46:26 | step: 51300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2715809134533629e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.75 | consumed tokens: 420249600.0 | grad norm avg: 1.37 | grad norm last: 1.43 | 
2025-12-29T17:46:47 | step: 51400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2626201169041451e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.22 | consumed tokens: 421068800.0 | grad norm avg: 1.38 | grad norm last: 1.38 | 
2025-12-29T17:47:07 | step: 51500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.2536803296825383e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.25 | consumed tokens: 421888000.0 | grad norm avg: 1.38 | grad norm last: 1.23 | 
2025-12-29T17:47:28 | step: 51600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.2447619155864231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.86 | consumed tokens: 422707200.0 | grad norm avg: 1.36 | grad norm last: 1.55 | 
2025-12-29T17:47:48 | step: 51700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2358648746157996e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.02 | consumed tokens: 423526400.0 | grad norm avg: 1.39 | grad norm last: 1.36 | 
2025-12-29T17:48:09 | step: 51800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2269894796190783e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.31 | consumed tokens: 424345600.0 | grad norm avg: 1.37 | grad norm last: 1.3 | 
2025-12-29T17:48:30 | step: 51900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2181358215457294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.73 | consumed tokens: 425164800.0 | grad norm avg: 1.37 | grad norm last: 1.43 | 
2025-12-29T17:48:50 | step: 52000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.209303991345223e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.91 | consumed tokens: 425984000.0 | grad norm avg: 1.37 | grad norm last: 1.31 | 
2025-12-29T17:49:11 | step: 52100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2004942618659697e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.11 | consumed tokens: 426803200.0 | grad norm avg: 1.37 | grad norm last: 1.34 | 
2025-12-29T17:49:31 | step: 52200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1917066331079695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.61 | consumed tokens: 427622400.0 | grad norm avg: 1.38 | grad norm last: 1.32 | 
2025-12-29T17:49:52 | step: 52300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1829414688691031e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.62 | consumed tokens: 428441600.0 | grad norm avg: 1.37 | grad norm last: 1.3 | 
2025-12-29T17:50:13 | step: 52400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.1741986781999003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.53 | consumed tokens: 429260800.0 | grad norm avg: 1.36 | grad norm last: 1.33 | 
2025-12-29T17:50:33 | step: 52500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.1654786248982418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 2.53 | consumed tokens: 430080000.0 | grad norm avg: 1.38 | grad norm last: 1.37 | 
2025-12-29T17:50:54 | step: 52600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1567813089641277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.59 | consumed tokens: 430899200.0 | grad norm avg: 1.35 | grad norm last: 1.41 | 
2025-12-29T17:51:14 | step: 52700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.1481069122964982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.52 | consumed tokens: 431718400.0 | grad norm avg: 1.37 | grad norm last: 1.42 | 
2025-12-29T17:51:35 | step: 52800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.1394556167942937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.98 | consumed tokens: 432537600.0 | grad norm avg: 1.38 | grad norm last: 1.25 | 
2025-12-29T17:51:55 | step: 52900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1308276043564547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.58 | consumed tokens: 433356800.0 | grad norm avg: 1.38 | grad norm last: 1.37 | 
2025-12-29T17:52:16 | step: 53000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.1222229659324512e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.84 | consumed tokens: 434176000.0 | grad norm avg: 1.38 | grad norm last: 1.41 | 
2025-12-29T17:52:37 | step: 53100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1136417924717534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.8 | consumed tokens: 434995200.0 | grad norm avg: 1.37 | grad norm last: 1.37 | 
2025-12-29T17:52:57 | step: 53200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.1050842658733018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.28 | consumed tokens: 435814400.0 | grad norm avg: 1.38 | grad norm last: 1.33 | 
2025-12-29T17:53:18 | step: 53300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.0965506589855067e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.47 | consumed tokens: 436633600.0 | grad norm avg: 1.38 | grad norm last: 1.3 | 
2025-12-29T17:53:38 | step: 53400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0880409718083683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.94 | consumed tokens: 437452800.0 | grad norm avg: 1.37 | grad norm last: 1.26 | 
2025-12-29T17:53:59 | step: 53500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0795552952913567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.59 | consumed tokens: 438272000.0 | grad norm avg: 1.37 | grad norm last: 1.36 | 
2025-12-29T17:54:19 | step: 53600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.0710939932323527e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.0 | consumed tokens: 439091200.0 | grad norm avg: 1.37 | grad norm last: 1.42 | 
2025-12-29T17:54:40 | step: 53700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.0626570656313561e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.73 | consumed tokens: 439910400.0 | grad norm avg: 1.38 | grad norm last: 1.21 | 
2025-12-29T17:55:01 | step: 53800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0542446034378372e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.97 | consumed tokens: 440729600.0 | grad norm avg: 1.38 | grad norm last: 1.27 | 
2025-12-29T17:55:21 | step: 53900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0458568795002066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.69 | consumed tokens: 441548800.0 | grad norm avg: 1.39 | grad norm last: 1.44 | 
2025-12-29T17:55:42 | step: 54000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0374939847679343e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 1.9 | consumed tokens: 442368000.0 | grad norm avg: 1.39 | grad norm last: 1.27 | 
2025-12-29T17:56:02 | step: 54100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.0291561011399608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.08 | consumed tokens: 443187200.0 | grad norm avg: 1.37 | grad norm last: 1.42 | 
2025-12-29T17:56:23 | step: 54200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0208433195657562e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 1.85 | consumed tokens: 444006400.0 | grad norm avg: 1.38 | grad norm last: 1.47 | 
2025-12-29T17:56:44 | step: 54300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0125557309947908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.62 | consumed tokens: 444825600.0 | grad norm avg: 1.4 | grad norm last: 1.46 | 
2025-12-29T17:57:04 | step: 54400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0042935173260048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.38 | consumed tokens: 445644800.0 | grad norm avg: 1.37 | grad norm last: 1.35 | 
2025-12-29T17:57:25 | step: 54500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 9.960569514078088e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.42 | consumed tokens: 446464000.0 | grad norm avg: 1.36 | grad norm last: 1.41 | 
2025-12-29T17:57:45 | step: 54600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.878459422907326e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.23 | consumed tokens: 447283200.0 | grad norm avg: 1.39 | grad norm last: 1.27 | 
2025-12-29T17:58:06 | step: 54700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.796607628231868e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.16 | consumed tokens: 448102400.0 | grad norm avg: 1.37 | grad norm last: 1.34 | 
2025-12-29T17:58:26 | step: 54800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.715015949041117e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.11 | consumed tokens: 448921600.0 | grad norm avg: 1.39 | grad norm last: 1.45 | 
2025-12-29T17:58:47 | step: 54900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 9.633685294829775e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.91 | consumed tokens: 449740800.0 | grad norm avg: 1.39 | grad norm last: 1.43 | 
2025-12-29T17:59:08 | step: 55000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 9.552616575092543e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.66 | consumed tokens: 450560000.0 | grad norm avg: 1.38 | grad norm last: 1.42 | 
2025-12-29T17:59:30 | step: 55100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 9.471811608818825e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.77 | consumed tokens: 451379200.0 | grad norm avg: 1.39 | grad norm last: 1.49 | 
2025-12-29T17:59:50 | step: 55200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 9.391271305503324e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.72 | consumed tokens: 452198400.0 | grad norm avg: 1.39 | grad norm last: 1.49 | 
2025-12-29T18:00:11 | step: 55300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.310998393630143e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.91 | consumed tokens: 453017600.0 | grad norm avg: 1.38 | grad norm last: 1.46 | 
2025-12-29T18:00:32 | step: 55400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.230992873199284e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.81 | consumed tokens: 453836800.0 | grad norm avg: 1.38 | grad norm last: 1.37 | 
2025-12-29T18:00:52 | step: 55500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.15125656320015e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.19 | consumed tokens: 454656000.0 | grad norm avg: 1.39 | grad norm last: 1.36 | 
2025-12-29T18:01:13 | step: 55600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.071790373127442e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.98 | consumed tokens: 455475200.0 | grad norm avg: 1.4 | grad norm last: 1.37 | 
2025-12-29T18:01:33 | step: 55700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.992596121970564e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.34 | consumed tokens: 456294400.0 | grad norm avg: 1.4 | grad norm last: 1.34 | 
2025-12-29T18:01:54 | step: 55800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.91367562871892e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 1.97 | consumed tokens: 457113600.0 | grad norm avg: 1.39 | grad norm last: 1.38 | 
2025-12-29T18:02:15 | step: 55900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.835029802867211e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.2 | consumed tokens: 457932800.0 | grad norm avg: 1.39 | grad norm last: 1.35 | 
2025-12-29T18:02:35 | step: 56000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.75665955391014e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.86 | consumed tokens: 458752000.0 | grad norm avg: 1.39 | grad norm last: 1.33 | 
2025-12-29T18:02:56 | step: 56100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.67856670083711e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.39 | consumed tokens: 459571200.0 | grad norm avg: 1.39 | grad norm last: 1.35 | 
2025-12-29T18:03:16 | step: 56200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 8.600752153142821e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 1.96 | consumed tokens: 460390400.0 | grad norm avg: 1.38 | grad norm last: 1.42 | 
2025-12-29T18:03:37 | step: 56300 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 8.523217729816679e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.39 | consumed tokens: 461209600.0 | grad norm avg: 1.4 | grad norm last: 1.4 | 
2025-12-29T18:03:58 | step: 56400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.445965249848086e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.55 | consumed tokens: 462028800.0 | grad norm avg: 1.39 | grad norm last: 1.45 | 
2025-12-29T18:04:18 | step: 56500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.368994713237043e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.75 | consumed tokens: 462848000.0 | grad norm avg: 1.39 | grad norm last: 1.38 | 
2025-12-29T18:04:39 | step: 56600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.292307938972954e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.05 | consumed tokens: 463667200.0 | grad norm avg: 1.39 | grad norm last: 1.39 | 
2025-12-29T18:04:59 | step: 56700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 8.215905836550519e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.31 | consumed tokens: 464486400.0 | grad norm avg: 1.39 | grad norm last: 1.45 | 
2025-12-29T18:05:20 | step: 56800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 8.139791134453844e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.56 | consumed tokens: 465305600.0 | grad norm avg: 1.41 | grad norm last: 1.54 | 
2025-12-29T18:05:41 | step: 56900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 8.06396383268293e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.64 | consumed tokens: 466124800.0 | grad norm avg: 1.39 | grad norm last: 1.42 | 
2025-12-29T18:06:01 | step: 57000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.98842575022718e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.53 | consumed tokens: 466944000.0 | grad norm avg: 1.38 | grad norm last: 1.35 | 
2025-12-29T18:06:22 | step: 57100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.913177796581294e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.67 | consumed tokens: 467763200.0 | grad norm avg: 1.41 | grad norm last: 1.44 | 
2025-12-29T18:06:42 | step: 57200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.838221790734679e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.69 | consumed tokens: 468582400.0 | grad norm avg: 1.41 | grad norm last: 1.45 | 
2025-12-29T18:07:03 | step: 57300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.763558642182034e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.22 | consumed tokens: 469401600.0 | grad norm avg: 1.4 | grad norm last: 1.34 | 
2025-12-29T18:07:24 | step: 57400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 7.689189260418061e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.7 | consumed tokens: 470220800.0 | grad norm avg: 1.4 | grad norm last: 1.32 | 
2025-12-29T18:07:44 | step: 57500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.615115919179516e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.67 | consumed tokens: 471040000.0 | grad norm avg: 1.41 | grad norm last: 1.47 | 
2025-12-29T18:08:05 | step: 57600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.541339073213749e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.84 | consumed tokens: 471859200.0 | grad norm avg: 1.39 | grad norm last: 1.39 | 
2025-12-29T18:08:25 | step: 57700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.467860541510163e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.03 | consumed tokens: 472678400.0 | grad norm avg: 1.42 | grad norm last: 1.38 | 
2025-12-29T18:08:46 | step: 57800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.3946807788161095e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.71 | train loss last: 2.83 | consumed tokens: 473497600.0 | grad norm avg: 1.42 | grad norm last: 1.46 | 
2025-12-29T18:09:07 | step: 57900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.321801604120992e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.42 | consumed tokens: 474316800.0 | grad norm avg: 1.41 | grad norm last: 1.4 | 
2025-12-29T18:09:27 | step: 58000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.249224381666863e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 3.28 | consumed tokens: 475136000.0 | grad norm avg: 1.42 | grad norm last: 1.38 | 
2025-12-29T18:09:48 | step: 58100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.176950020948425e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.86 | consumed tokens: 475955200.0 | grad norm avg: 1.41 | grad norm last: 1.39 | 
2025-12-29T18:10:08 | step: 58200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.104979431460379e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.58 | consumed tokens: 476774400.0 | grad norm avg: 1.43 | grad norm last: 1.3 | 
2025-12-29T18:10:29 | step: 58300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.033314886939479e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.38 | consumed tokens: 477593600.0 | grad norm avg: 1.42 | grad norm last: 1.3 | 
2025-12-29T18:10:49 | step: 58400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.961956387385726e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.33 | consumed tokens: 478412800.0 | grad norm avg: 1.42 | grad norm last: 1.56 | 
2025-12-29T18:11:10 | step: 58500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.890906206535874e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.17 | consumed tokens: 479232000.0 | grad norm avg: 1.41 | grad norm last: 1.31 | 
2025-12-29T18:11:31 | step: 58600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.820164799137274e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.61 | consumed tokens: 480051200.0 | grad norm avg: 1.39 | grad norm last: 1.4 | 
2025-12-29T18:11:51 | step: 58700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 6.749733984179329e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.47 | consumed tokens: 480870400.0 | grad norm avg: 1.4 | grad norm last: 1.58 | 
2025-12-29T18:12:12 | step: 58800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.679614671156742e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 2.66 | consumed tokens: 481689600.0 | grad norm avg: 1.41 | grad norm last: 1.45 | 
2025-12-29T18:12:33 | step: 58900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.609807769564213e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.25 | consumed tokens: 482508800.0 | grad norm avg: 1.42 | grad norm last: 1.35 | 
2025-12-29T18:12:53 | step: 59000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.5403146436437964e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.56 | consumed tokens: 483328000.0 | grad norm avg: 1.41 | grad norm last: 1.39 | 
2025-12-29T18:13:14 | step: 59100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.471136657637544e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.48 | consumed tokens: 484147200.0 | grad norm avg: 1.41 | grad norm last: 1.48 | 
2025-12-29T18:13:34 | step: 59200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.4022751757875085e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.66 | consumed tokens: 484966400.0 | grad norm avg: 1.44 | grad norm last: 1.31 | 
2025-12-29T18:13:55 | step: 59300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.333730652841041e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.73 | consumed tokens: 485785600.0 | grad norm avg: 1.42 | grad norm last: 1.42 | 
2025-12-29T18:14:15 | step: 59400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.265504907787545e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.69 | consumed tokens: 486604800.0 | grad norm avg: 1.42 | grad norm last: 1.26 | 
2025-12-29T18:14:36 | step: 59500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.197598850121722e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.52 | consumed tokens: 487424000.0 | grad norm avg: 1.41 | grad norm last: 1.36 | 
2025-12-29T18:14:57 | step: 59600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.1300138440856244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.59 | consumed tokens: 488243200.0 | grad norm avg: 1.41 | grad norm last: 1.3 | 
2025-12-29T18:15:17 | step: 59700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.062750799173955e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.69 | consumed tokens: 489062400.0 | grad norm avg: 1.42 | grad norm last: 1.3 | 
2025-12-29T18:15:38 | step: 59800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.995811079628766e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.84 | consumed tokens: 489881600.0 | grad norm avg: 1.42 | grad norm last: 1.45 | 
2025-12-29T18:15:58 | step: 59900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.9291951401974075e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.08 | consumed tokens: 490700800.0 | grad norm avg: 1.43 | grad norm last: 1.48 | 
2025-12-29T18:16:19 | step: 60000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.862905254616635e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.73 | consumed tokens: 491520000.0 | grad norm avg: 1.42 | grad norm last: 1.39 | 
2025-12-29T18:16:41 | step: 60100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.796941877633799e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.73 | consumed tokens: 492339200.0 | grad norm avg: 1.42 | grad norm last: 1.39 | 
2025-12-29T18:17:02 | step: 60200 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 5.731305918743601e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.72 | consumed tokens: 493158400.0 | grad norm avg: 1.44 | grad norm last: 1.4 | 
2025-12-29T18:17:23 | step: 60300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.665998742188094e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.39 | consumed tokens: 493977600.0 | grad norm avg: 1.42 | grad norm last: 1.42 | 
2025-12-29T18:17:43 | step: 60400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.601021712209331e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.8 | consumed tokens: 494796800.0 | grad norm avg: 1.43 | grad norm last: 1.55 | 
2025-12-29T18:18:04 | step: 60500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.536375738302013e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.56 | consumed tokens: 495616000.0 | grad norm avg: 1.42 | grad norm last: 1.36 | 
2025-12-29T18:18:24 | step: 60600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.472062184708193e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.86 | consumed tokens: 496435200.0 | grad norm avg: 1.42 | grad norm last: 1.5 | 
2025-12-29T18:18:45 | step: 60700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.408081506175222e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.91 | consumed tokens: 497254400.0 | grad norm avg: 1.43 | grad norm last: 1.42 | 
2025-12-29T18:19:05 | step: 60800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.344435066945152e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.7 | consumed tokens: 498073600.0 | grad norm avg: 1.43 | grad norm last: 1.23 | 
2025-12-29T18:19:26 | step: 60900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.281124231260037e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.48 | consumed tokens: 498892800.0 | grad norm avg: 1.41 | grad norm last: 1.38 | 
2025-12-29T18:19:46 | step: 61000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.218149908614578e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.05 | consumed tokens: 499712000.0 | grad norm avg: 1.43 | grad norm last: 1.43 | 
2025-12-29T18:20:07 | step: 61100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 5.155513463250827e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.62 | consumed tokens: 500531200.0 | grad norm avg: 1.44 | grad norm last: 1.48 | 
2025-12-29T18:20:28 | step: 61200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.093215349916136e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.47 | consumed tokens: 501350400.0 | grad norm avg: 1.45 | grad norm last: 1.44 | 
2025-12-29T18:20:48 | step: 61300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 5.031256932852557e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.75 | consumed tokens: 502169600.0 | grad norm avg: 1.42 | grad norm last: 1.43 | 
2025-12-29T18:21:09 | step: 61400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.969639576302143e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.98 | consumed tokens: 502988800.0 | grad norm avg: 1.41 | grad norm last: 1.38 | 
2025-12-29T18:21:30 | step: 61500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.9083637350122444e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.06 | consumed tokens: 503808000.0 | grad norm avg: 1.42 | grad norm last: 1.32 | 
2025-12-29T18:21:50 | step: 61600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.8474307732249144e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.5 | consumed tokens: 504627200.0 | grad norm avg: 1.42 | grad norm last: 1.44 | 
2025-12-29T18:22:11 | step: 61700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.786841600434855e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.58 | consumed tokens: 505446400.0 | grad norm avg: 1.42 | grad norm last: 1.59 | 
2025-12-29T18:22:31 | step: 61800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.726597580884118e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.25 | consumed tokens: 506265600.0 | grad norm avg: 1.41 | grad norm last: 1.47 | 
2025-12-29T18:22:52 | step: 61900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.666699624067405e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.69 | consumed tokens: 507084800.0 | grad norm avg: 1.41 | grad norm last: 1.61 | 
2025-12-29T18:23:12 | step: 62000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.607148184732068e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.31 | consumed tokens: 507904000.0 | grad norm avg: 1.42 | grad norm last: 1.41 | 
2025-12-29T18:23:33 | step: 62100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5479450818675105e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.48 | consumed tokens: 508723200.0 | grad norm avg: 1.42 | grad norm last: 1.35 | 
2025-12-29T18:23:53 | step: 62200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.4890907702210825e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.72 | consumed tokens: 509542400.0 | grad norm avg: 1.42 | grad norm last: 1.37 | 
2025-12-29T18:24:14 | step: 62300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.430586614034837e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.56 | consumed tokens: 510361600.0 | grad norm avg: 1.42 | grad norm last: 1.41 | 
2025-12-29T18:24:34 | step: 62400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.372433522803476e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.34 | consumed tokens: 511180800.0 | grad norm avg: 1.44 | grad norm last: 1.45 | 
2025-12-29T18:24:55 | step: 62500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.314632406021701e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.05 | consumed tokens: 512000000.0 | grad norm avg: 1.44 | grad norm last: 1.6 | 
2025-12-29T18:25:16 | step: 62600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.257184173184214e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.56 | consumed tokens: 512819200.0 | grad norm avg: 1.42 | grad norm last: 1.24 | 
2025-12-29T18:25:36 | step: 62700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.200090188533068e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.92 | consumed tokens: 513638400.0 | grad norm avg: 1.44 | grad norm last: 1.36 | 
2025-12-29T18:25:57 | step: 62800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.143350452068262e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.38 | consumed tokens: 514457600.0 | grad norm avg: 1.42 | grad norm last: 1.43 | 
2025-12-29T18:26:17 | step: 62900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.086967237526551e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.28 | consumed tokens: 515276800.0 | grad norm avg: 1.43 | grad norm last: 1.34 | 
2025-12-29T18:26:38 | step: 63000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.030940544907935e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.44 | consumed tokens: 516096000.0 | grad norm avg: 1.43 | grad norm last: 1.57 | 
2025-12-29T18:26:59 | step: 63100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.9752721932018176e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.67 | consumed tokens: 516915200.0 | grad norm avg: 1.43 | grad norm last: 1.37 | 
2025-12-29T18:27:19 | step: 63200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.919962182408199e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.88 | consumed tokens: 517734400.0 | grad norm avg: 1.43 | grad norm last: 1.45 | 
2025-12-29T18:27:40 | step: 63300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.865011876769131e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.73 | consumed tokens: 518553600.0 | grad norm avg: 1.43 | grad norm last: 1.51 | 
2025-12-29T18:28:00 | step: 63400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.8104221857793164e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.7 | consumed tokens: 519372800.0 | grad norm avg: 1.43 | grad norm last: 1.37 | 
2025-12-29T18:28:21 | step: 63500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.7561940189334564e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.55 | consumed tokens: 520192000.0 | grad norm avg: 1.44 | grad norm last: 1.39 | 
2025-12-29T18:28:41 | step: 63600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.7023285130999284e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.83 | consumed tokens: 521011200.0 | grad norm avg: 1.43 | grad norm last: 1.4 | 
2025-12-29T18:29:02 | step: 63700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.6488263503997587e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.64 | consumed tokens: 521830400.0 | grad norm avg: 1.42 | grad norm last: 1.35 | 
2025-12-29T18:29:23 | step: 63800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.5956886677013244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.53 | consumed tokens: 522649600.0 | grad norm avg: 1.42 | grad norm last: 1.42 | 
2025-12-29T18:29:43 | step: 63900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.5429159197519766e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.67 | consumed tokens: 523468800.0 | grad norm avg: 1.43 | grad norm last: 1.36 | 
2025-12-29T18:30:04 | step: 64000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.490509470793768e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.61 | consumed tokens: 524288000.0 | grad norm avg: 1.42 | grad norm last: 1.41 | 
2025-12-29T18:30:24 | step: 64100 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.4384700029477244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 2.75 | consumed tokens: 525107200.0 | grad norm avg: 1.42 | grad norm last: 1.4 | 
2025-12-29T18:30:45 | step: 64200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.3867986530822236e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.83 | consumed tokens: 525926400.0 | grad norm avg: 1.43 | grad norm last: 1.36 | 
2025-12-29T18:31:06 | step: 64300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.3354958759446163e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.38 | consumed tokens: 526745600.0 | grad norm avg: 1.43 | grad norm last: 1.55 | 
2025-12-29T18:31:26 | step: 64400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.2845628084032796e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.7 | consumed tokens: 527564800.0 | grad norm avg: 1.43 | grad norm last: 1.38 | 
2025-12-29T18:31:47 | step: 64500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.2340003599529155e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.75 | consumed tokens: 528384000.0 | grad norm avg: 1.42 | grad norm last: 1.5 | 
2025-12-29T18:32:07 | step: 64600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.18380921271455e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 1.8 | consumed tokens: 529203200.0 | grad norm avg: 1.41 | grad norm last: 1.39 | 
2025-12-29T18:32:28 | step: 64700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.1339902761828853e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.78 | consumed tokens: 530022400.0 | grad norm avg: 1.43 | grad norm last: 1.31 | 
2025-12-29T18:32:49 | step: 64800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.084544459852623e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 3.31 | consumed tokens: 530841600.0 | grad norm avg: 1.42 | grad norm last: 1.4 | 
2025-12-29T18:33:09 | step: 64900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.0354726732184645e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.97 | consumed tokens: 531660800.0 | grad norm avg: 1.43 | grad norm last: 1.42 | 
2025-12-29T18:33:30 | step: 65000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.9867755984014366e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.5 | consumed tokens: 532480000.0 | grad norm avg: 1.43 | grad norm last: 1.51 | 
2025-12-29T18:33:52 | step: 65100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.938454144896241e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.61 | consumed tokens: 533299200.0 | grad norm avg: 1.43 | grad norm last: 1.43 | 
2025-12-29T18:34:12 | step: 65200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.8905092221975792e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.62 | train loss last: 2.62 | consumed tokens: 534118400.0 | grad norm avg: 1.43 | grad norm last: 1.45 | 
2025-12-29T18:34:33 | step: 65300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.8429412850528024e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.34 | consumed tokens: 534937600.0 | grad norm avg: 1.45 | grad norm last: 1.48 | 
2025-12-29T18:34:54 | step: 65400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.795751697703963e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.64 | consumed tokens: 535756800.0 | grad norm avg: 1.45 | grad norm last: 1.42 | 
2025-12-29T18:35:14 | step: 65500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7489409148984123e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.72 | consumed tokens: 536576000.0 | grad norm avg: 1.46 | grad norm last: 1.43 | 
2025-12-29T18:35:35 | step: 65600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7025096187571762e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.39 | consumed tokens: 537395200.0 | grad norm avg: 1.45 | grad norm last: 1.36 | 
2025-12-29T18:35:55 | step: 65700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.656458946148632e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.48 | consumed tokens: 538214400.0 | grad norm avg: 1.42 | grad norm last: 1.37 | 
2025-12-29T18:36:16 | step: 65800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.610789351820131e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.81 | consumed tokens: 539033600.0 | grad norm avg: 1.43 | grad norm last: 1.38 | 
2025-12-29T18:36:37 | step: 65900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.5655019726400496e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.42 | consumed tokens: 539852800.0 | grad norm avg: 1.45 | grad norm last: 1.52 | 
2025-12-29T18:36:57 | step: 66000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.5205972633557394e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.03 | consumed tokens: 540672000.0 | grad norm avg: 1.44 | grad norm last: 1.41 | 
2025-12-29T18:37:18 | step: 66100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.476076133461902e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.5 | consumed tokens: 541491200.0 | grad norm avg: 1.43 | grad norm last: 1.4 | 
2025-12-29T18:37:38 | step: 66200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.431939492453239e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.86 | consumed tokens: 542310400.0 | grad norm avg: 1.43 | grad norm last: 1.46 | 
2025-12-29T18:37:59 | step: 66300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.388187567703426e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.62 | consumed tokens: 543129600.0 | grad norm avg: 1.43 | grad norm last: 1.53 | 
2025-12-29T18:38:20 | step: 66400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.3448217234545154e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.73 | consumed tokens: 543948800.0 | grad norm avg: 1.43 | grad norm last: 1.38 | 
2025-12-29T18:38:40 | step: 66500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.3018424144538585e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.34 | consumed tokens: 544768000.0 | grad norm avg: 1.43 | grad norm last: 1.37 | 
2025-12-29T18:39:01 | step: 66600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.2592503228224814e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.16 | consumed tokens: 545587200.0 | grad norm avg: 1.45 | grad norm last: 1.35 | 
2025-12-29T18:39:21 | step: 66700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.2170461306814104e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.83 | consumed tokens: 546406400.0 | grad norm avg: 1.43 | grad norm last: 1.32 | 
2025-12-29T18:39:42 | step: 66800 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 2.1752307475253474e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.66 | consumed tokens: 547225600.0 | grad norm avg: 1.44 | grad norm last: 1.32 | 
2025-12-29T18:40:03 | step: 66900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.1338048554753186e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.36 | consumed tokens: 548044800.0 | grad norm avg: 1.43 | grad norm last: 1.41 | 
2025-12-29T18:40:23 | step: 67000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0927691366523504e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.19 | consumed tokens: 548864000.0 | grad norm avg: 1.46 | grad norm last: 1.41 | 
2025-12-29T18:40:44 | step: 67100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.052124273177469e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.94 | consumed tokens: 549683200.0 | grad norm avg: 1.45 | grad norm last: 1.36 | 
2025-12-29T18:41:04 | step: 67200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.011870947171701e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.64 | consumed tokens: 550502400.0 | grad norm avg: 1.44 | grad norm last: 1.38 | 
2025-12-29T18:41:25 | step: 67300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.9720098407560727e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.59 | consumed tokens: 551321600.0 | grad norm avg: 1.44 | grad norm last: 1.47 | 
2025-12-29T18:41:46 | step: 67400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.9325414086779347e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.73 | consumed tokens: 552140800.0 | grad norm avg: 1.46 | grad norm last: 1.42 | 
2025-12-29T18:42:06 | step: 67500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.8934667878056644e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.58 | consumed tokens: 552960000.0 | grad norm avg: 1.45 | grad norm last: 1.53 | 
2025-12-29T18:42:27 | step: 67600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.8547864328866126e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.84 | consumed tokens: 553779200.0 | grad norm avg: 1.45 | grad norm last: 1.41 | 
2025-12-29T18:42:47 | step: 67700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.816500912354968e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.7 | consumed tokens: 554598400.0 | grad norm avg: 1.43 | grad norm last: 1.37 | 
2025-12-29T18:43:08 | step: 67800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.778610908331757e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.14 | consumed tokens: 555417600.0 | grad norm avg: 1.44 | grad norm last: 1.37 | 
2025-12-29T18:43:28 | step: 67900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7411171029380057e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 3.09 | consumed tokens: 556236800.0 | grad norm avg: 1.45 | grad norm last: 1.39 | 
2025-12-29T18:43:49 | step: 68000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.7040201782947406e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.67 | consumed tokens: 557056000.0 | grad norm avg: 1.44 | grad norm last: 1.47 | 
2025-12-29T18:44:10 | step: 68100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.6673207028361503e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.78 | consumed tokens: 557875200.0 | grad norm avg: 1.44 | grad norm last: 1.46 | 
2025-12-29T18:44:30 | step: 68200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.631019358683261e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.16 | consumed tokens: 558694400.0 | grad norm avg: 1.43 | grad norm last: 1.35 | 
2025-12-29T18:44:51 | step: 68300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5951167142702616e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.66 | consumed tokens: 559513600.0 | grad norm avg: 1.45 | grad norm last: 1.53 | 
2025-12-29T18:45:11 | step: 68400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.5596134517181781e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.44 | consumed tokens: 560332800.0 | grad norm avg: 1.46 | grad norm last: 1.59 | 
2025-12-29T18:45:32 | step: 68500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.5245100257743616e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.42 | consumed tokens: 561152000.0 | grad norm avg: 1.46 | grad norm last: 1.49 | 
2025-12-29T18:45:53 | step: 68600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.489807232246676e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.92 | consumed tokens: 561971200.0 | grad norm avg: 1.45 | grad norm last: 1.56 | 
2025-12-29T18:46:13 | step: 68700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4555055258824723e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.84 | consumed tokens: 562790400.0 | grad norm avg: 1.44 | grad norm last: 1.48 | 
2025-12-29T18:46:34 | step: 68800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.421605475115939e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.5 | consumed tokens: 563609600.0 | grad norm avg: 1.44 | grad norm last: 1.46 | 
2025-12-29T18:46:54 | step: 68900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.3881077620681026e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.3 | consumed tokens: 564428800.0 | grad norm avg: 1.46 | grad norm last: 1.34 | 
2025-12-29T18:47:15 | step: 69000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.3550128414863138e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.08 | consumed tokens: 565248000.0 | grad norm avg: 1.44 | grad norm last: 1.36 | 
2025-12-29T18:47:36 | step: 69100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.322321395491599e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.62 | consumed tokens: 566067200.0 | grad norm avg: 1.45 | grad norm last: 1.41 | 
2025-12-29T18:47:56 | step: 69200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.290033992518147e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.61 | consumed tokens: 566886400.0 | grad norm avg: 1.46 | grad norm last: 1.53 | 
2025-12-29T18:48:17 | step: 69300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2581509736264707e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.92 | consumed tokens: 567705600.0 | grad norm avg: 1.45 | grad norm last: 1.52 | 
2025-12-29T18:48:38 | step: 69400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.2266731346244342e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.42 | consumed tokens: 568524800.0 | grad norm avg: 1.44 | grad norm last: 1.34 | 
2025-12-29T18:48:58 | step: 69500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1956009302593884e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.3 | consumed tokens: 569344000.0 | grad norm avg: 1.46 | grad norm last: 1.34 | 
2025-12-29T18:49:19 | step: 69600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1649348152786843e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.47 | consumed tokens: 570163200.0 | grad norm avg: 1.45 | grad norm last: 1.45 | 
2025-12-29T18:49:39 | step: 69700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1346752444296726e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.05 | consumed tokens: 570982400.0 | grad norm avg: 1.44 | grad norm last: 1.45 | 
2025-12-29T18:50:00 | step: 69800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1048230135202175e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.12 | consumed tokens: 571801600.0 | grad norm avg: 1.44 | grad norm last: 1.41 | 
2025-12-29T18:50:21 | step: 69900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.0753783499239944e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.53 | consumed tokens: 572620800.0 | grad norm avg: 1.45 | grad norm last: 1.41 | 
2025-12-29T18:50:41 | step: 70000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0463419357620296e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 2.06 | consumed tokens: 573440000.0 | grad norm avg: 1.46 | grad norm last: 1.35 | 
2025-12-29T18:51:03 | step: 70100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.017714225781674e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.97 | consumed tokens: 574259200.0 | grad norm avg: 1.45 | grad norm last: 1.49 | 
2025-12-29T18:51:24 | step: 70200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 9.894956747302786e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.66 | consumed tokens: 575078400.0 | grad norm avg: 1.44 | grad norm last: 1.47 | 
2025-12-29T18:51:45 | step: 70300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.61686737355194e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.44 | consumed tokens: 575897600.0 | grad norm avg: 1.45 | grad norm last: 1.42 | 
2025-12-29T18:52:05 | step: 70400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 9.342879820906091e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.45 | consumed tokens: 576716800.0 | grad norm avg: 1.45 | grad norm last: 1.57 | 
2025-12-29T18:52:26 | step: 70500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.07299749997037e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.39 | consumed tokens: 577536000.0 | grad norm avg: 1.47 | grad norm last: 1.5 | 
2025-12-29T18:52:47 | step: 70600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 8.807226095086662e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.36 | consumed tokens: 578355200.0 | grad norm avg: 1.44 | grad norm last: 1.6 | 
2025-12-29T18:53:07 | step: 70700 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 8.545569585294288e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.44 | consumed tokens: 579174400.0 | grad norm avg: 1.46 | grad norm last: 1.49 | 
2025-12-29T18:53:28 | step: 70800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.288032518066757e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.61 | consumed tokens: 579993600.0 | grad norm avg: 1.47 | grad norm last: 1.52 | 
2025-12-29T18:53:49 | step: 70900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.034619440877577e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.72 | consumed tokens: 580812800.0 | grad norm avg: 1.44 | grad norm last: 1.54 | 
2025-12-29T18:54:09 | step: 71000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.785334901200258e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.38 | consumed tokens: 581632000.0 | grad norm avg: 1.46 | grad norm last: 1.6 | 
2025-12-29T18:54:30 | step: 71100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.540182309639931e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 1.76 | consumed tokens: 582451200.0 | grad norm avg: 1.45 | grad norm last: 1.37 | 
2025-12-29T18:54:50 | step: 71200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.299166782104294e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 2.38 | consumed tokens: 583270400.0 | grad norm avg: 1.46 | grad norm last: 1.39 | 
2025-12-29T18:55:11 | step: 71300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.062291729198478e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.95 | consumed tokens: 584089600.0 | grad norm avg: 1.43 | grad norm last: 1.41 | 
2025-12-29T18:55:31 | step: 71400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.829561129961803e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.55 | consumed tokens: 584908800.0 | grad norm avg: 1.46 | grad norm last: 1.45 | 
2025-12-29T18:55:52 | step: 71500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.600980100301967e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.08 | consumed tokens: 585728000.0 | grad norm avg: 1.44 | grad norm last: 1.54 | 
2025-12-29T18:56:13 | step: 71600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.376550913955725e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.14 | consumed tokens: 586547200.0 | grad norm avg: 1.45 | grad norm last: 1.65 | 
2025-12-29T18:56:33 | step: 71700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.156278686830774e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.62 | consumed tokens: 587366400.0 | grad norm avg: 1.46 | grad norm last: 1.63 | 
2025-12-29T18:56:54 | step: 71800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.940166261098057e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.3 | consumed tokens: 588185600.0 | grad norm avg: 1.45 | grad norm last: 1.41 | 
2025-12-29T18:57:15 | step: 71900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.728218184231082e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 3.39 | consumed tokens: 589004800.0 | grad norm avg: 1.44 | grad norm last: 1.63 | 
2025-12-29T18:57:35 | step: 72000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 5.520437298400793e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.64 | consumed tokens: 589824000.0 | grad norm avg: 1.44 | grad norm last: 1.51 | 
2025-12-29T18:57:56 | step: 72100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.316828151080699e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.56 | consumed tokens: 590643200.0 | grad norm avg: 1.46 | grad norm last: 1.57 | 
2025-12-29T18:58:17 | step: 72200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.117393016007554e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.39 | consumed tokens: 591462400.0 | grad norm avg: 1.46 | grad norm last: 1.37 | 
2025-12-29T18:58:37 | step: 72300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.922135872220679e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.2 | consumed tokens: 592281600.0 | grad norm avg: 1.46 | grad norm last: 1.46 | 
2025-12-29T18:58:58 | step: 72400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.7310595618910156e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.63 | train loss last: 2.31 | consumed tokens: 593100800.0 | grad norm avg: 1.47 | grad norm last: 1.38 | 
2025-12-29T18:59:18 | step: 72500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.5441683482749795e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 1.75 | consumed tokens: 593920000.0 | grad norm avg: 1.44 | grad norm last: 1.36 | 
2025-12-29T18:59:39 | step: 72600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.361464505109325e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.64 | consumed tokens: 594739200.0 | grad norm avg: 1.44 | grad norm last: 1.43 | 
2025-12-29T19:00:00 | step: 72700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.1829517272162775e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.59 | consumed tokens: 595558400.0 | grad norm avg: 1.45 | grad norm last: 1.32 | 
2025-12-29T19:00:20 | step: 72800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.0086325725496863e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 3.06 | consumed tokens: 596377600.0 | grad norm avg: 1.44 | grad norm last: 1.41 | 
2025-12-29T19:00:41 | step: 72900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.838510451714683e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.48 | consumed tokens: 597196800.0 | grad norm avg: 1.46 | grad norm last: 1.43 | 
2025-12-29T19:01:01 | step: 73000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.672587922665116e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.02 | consumed tokens: 598016000.0 | grad norm avg: 1.45 | grad norm last: 1.49 | 
2025-12-29T19:01:22 | step: 73100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.510868111789023e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.58 | consumed tokens: 598835200.0 | grad norm avg: 1.45 | grad norm last: 1.38 | 
2025-12-29T19:01:43 | step: 73200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.353353292823158e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.98 | consumed tokens: 599654400.0 | grad norm avg: 1.45 | grad norm last: 1.4 | 
2025-12-29T19:02:03 | step: 73300 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.200046592155559e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.53 | consumed tokens: 600473600.0 | grad norm avg: 1.46 | grad norm last: 1.56 | 
2025-12-29T19:02:24 | step: 73400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.050950851957168e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 3.08 | consumed tokens: 601292800.0 | grad norm avg: 1.46 | grad norm last: 1.5 | 
2025-12-29T19:02:45 | step: 73500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.906067777530552e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.41 | consumed tokens: 602112000.0 | grad norm avg: 1.45 | grad norm last: 1.53 | 
2025-12-29T19:03:05 | step: 73600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.765400495263748e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.92 | consumed tokens: 602931200.0 | grad norm avg: 1.45 | grad norm last: 1.4 | 
2025-12-29T19:03:26 | step: 73700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.62895127889351e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.55 | consumed tokens: 603750400.0 | grad norm avg: 1.46 | grad norm last: 1.58 | 
2025-12-29T19:03:46 | step: 73800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.4967224021565926e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.89 | consumed tokens: 604569600.0 | grad norm avg: 1.45 | grad norm last: 1.51 | 
2025-12-29T19:04:07 | step: 73900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.3687162808982976e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.55 | consumed tokens: 605388800.0 | grad norm avg: 1.45 | grad norm last: 1.51 | 
2025-12-29T19:04:28 | step: 74000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.2449347625297378e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.62 | consumed tokens: 606208000.0 | grad norm avg: 1.46 | grad norm last: 1.39 | 
2025-12-29T19:04:48 | step: 74100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.1253801207876677e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.8 | consumed tokens: 607027200.0 | grad norm avg: 1.47 | grad norm last: 1.49 | 
2025-12-29T19:05:09 | step: 74200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.0100543451917474e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.58 | consumed tokens: 607846400.0 | grad norm avg: 1.45 | grad norm last: 1.55 | 
2025-12-29T19:05:29 | step: 74300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.8989595673701842e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.55 | consumed tokens: 608665600.0 | grad norm avg: 1.45 | grad norm last: 1.72 | 
2025-12-29T19:05:50 | step: 74400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.792097634734091e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.23 | consumed tokens: 609484800.0 | grad norm avg: 1.45 | grad norm last: 1.61 | 
2025-12-29T19:06:10 | step: 74500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.6894701104774867e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.69 | consumed tokens: 610304000.0 | grad norm avg: 1.46 | grad norm last: 1.44 | 
2025-12-29T19:06:31 | step: 74600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.5910791262285784e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 3.38 | consumed tokens: 611123200.0 | grad norm avg: 1.43 | grad norm last: 1.38 | 
2025-12-29T19:06:52 | step: 74700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.4969261030728376e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.52 | consumed tokens: 611942400.0 | grad norm avg: 1.45 | grad norm last: 1.44 | 
2025-12-29T19:07:12 | step: 74800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.40701274631283e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.81 | consumed tokens: 612761600.0 | grad norm avg: 1.45 | grad norm last: 1.42 | 
2025-12-29T19:07:33 | step: 74900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.3213404770340276e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.7 | train loss last: 3.08 | consumed tokens: 613580800.0 | grad norm avg: 1.44 | grad norm last: 1.5 | 
2025-12-29T19:07:54 | step: 75000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2399108584304486e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.7 | consumed tokens: 614400000.0 | grad norm avg: 1.45 | grad norm last: 1.56 | 
2025-12-29T19:08:16 | step: 75100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1627253115875646e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 3.3 | consumed tokens: 615219200.0 | grad norm avg: 1.44 | grad norm last: 1.37 | 
2025-12-29T19:08:36 | step: 75200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0897850444280266e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.48 | consumed tokens: 616038400.0 | grad norm avg: 1.44 | grad norm last: 1.46 | 
2025-12-29T19:08:57 | step: 75300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.0210914069830324e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.8 | consumed tokens: 616857600.0 | grad norm avg: 1.45 | grad norm last: 1.44 | 
2025-12-29T19:09:18 | step: 75400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 9.566454650666856e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.27 | consumed tokens: 617676800.0 | grad norm avg: 1.46 | grad norm last: 1.39 | 
2025-12-29T19:09:38 | step: 75500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.96448426601637e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.45 | consumed tokens: 618496000.0 | grad norm avg: 1.43 | grad norm last: 1.42 | 
2025-12-29T19:09:59 | step: 75600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.405012152934432e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.62 | consumed tokens: 619315200.0 | grad norm avg: 1.44 | grad norm last: 1.39 | 
2025-12-29T19:10:19 | step: 75700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.888048259019342e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.68 | train loss last: 2.25 | consumed tokens: 620134400.0 | grad norm avg: 1.45 | grad norm last: 1.46 | 
2025-12-29T19:10:40 | step: 75800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.41360111078393e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 3.59 | consumed tokens: 620953600.0 | grad norm avg: 1.44 | grad norm last: 1.44 | 
2025-12-29T19:11:01 | step: 75900 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 6.981679945283759e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.81 | consumed tokens: 621772800.0 | grad norm avg: 1.45 | grad norm last: 1.34 | 
2025-12-29T19:11:22 | step: 76000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.592290446860716e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.66 | train loss last: 2.41 | consumed tokens: 622592000.0 | grad norm avg: 1.44 | grad norm last: 1.42 | 
2025-12-29T19:11:42 | step: 76100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.245440431484894e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.78 | consumed tokens: 623411200.0 | grad norm avg: 1.43 | grad norm last: 1.34 | 
2025-12-29T19:12:03 | step: 76200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.9411359387695484e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.67 | consumed tokens: 624230400.0 | grad norm avg: 1.45 | grad norm last: 1.42 | 
2025-12-29T19:12:23 | step: 76300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.6793812319710923e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.34 | consumed tokens: 625049600.0 | grad norm avg: 1.43 | grad norm last: 1.43 | 
2025-12-29T19:12:44 | step: 76400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 5.4601812848886766e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.65 | train loss last: 2.44 | consumed tokens: 625868800.0 | grad norm avg: 1.43 | grad norm last: 1.43 | 
2025-12-29T19:13:05 | step: 76500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.28353965023598e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.64 | train loss last: 2.83 | consumed tokens: 626688000.0 | grad norm avg: 1.44 | grad norm last: 1.44 | 
2025-12-29T19:13:25 | step: 76600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.149459525455313e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.41 | consumed tokens: 627507200.0 | grad norm avg: 1.44 | grad norm last: 1.4 | 
2025-12-29T19:13:46 | step: 76700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.057943042174884e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.67 | train loss last: 2.84 | consumed tokens: 628326400.0 | grad norm avg: 1.44 | grad norm last: 1.52 | 
2025-12-29T19:14:06 | step: 76800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.008991976751531e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 2.69 | train loss last: 2.77 | consumed tokens: 629145600.0 | grad norm avg: 1.43 | grad norm last: 1.36 | 
Training done at 2025-12-29 19:14:20.521129.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/s472389/modalities_test/wandb_storage/wandb/offline-run-20251229_145035-j6nc6tfr[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb_storage/wandb/offline-run-20251229_145035-j6nc6tfr/logs[0m
==========================================
Job finished at: Mon Dec 29 07:14:21 PM CET 2025
==========================================
