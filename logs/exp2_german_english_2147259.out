==========================================
Experiment 2: Fine-tuning GPT-2 on German + English
Job ID: 2147259
Node: jn003
Start time: Mon Dec 29 11:32:27 PM CET 2025
==========================================
Mon Dec 29 23:32:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40                     On  |   00000000:81:00.0 Off |                  Off |
| N/A   28C    P8             35W /  300W |       1MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2025-12-29__23-32-33_5734a86bbe85efff
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 124,439,808 parameters): weight_decay = 0.01
=> all (148 modules with 124,439,808 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2025-12-29 23:32:37.006884.



======================== Training Report ========================
Training target: 
	num_target_tokens: 2325897216
	num_target_steps: 283923 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 1
CUDA environment settings: 
	local_rank: 0
	world_size: 1
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (2325898240) does not match the number of target tokens (2325897216). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (283923) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (283923) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (283923) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2025-12-29 23:32:37.007249.
2025-12-29T23:32:56 | step: 100 | train samples/s: 89.2 | train mfu (16-bit): -1.0 | lr mean: 5.13769373355899e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.12 | train loss last: 4.66 | consumed tokens: 819200.0 | grad norm avg: 3.31 | grad norm last: 2.83 | 
2025-12-29T23:33:15 | step: 200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 5.549089109990746e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.04 | train loss last: 4.38 | consumed tokens: 1638400.0 | grad norm avg: 2.76 | grad norm last: 2.58 | 
2025-12-29T23:33:33 | step: 300 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 6.229151040315628e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.03 | train loss last: 4.16 | consumed tokens: 2457600.0 | grad norm avg: 2.64 | grad norm last: 2.58 | 
2025-12-29T23:33:51 | step: 400 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 7.169555829023011e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.0 | train loss last: 3.94 | consumed tokens: 3276800.0 | grad norm avg: 2.6 | grad norm last: 2.6 | 
2025-12-29T23:34:09 | step: 500 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 8.358793820661958e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.99 | train loss last: 4.25 | consumed tokens: 4096000.0 | grad norm avg: 2.51 | grad norm last: 2.75 | 
2025-12-29T23:34:27 | step: 600 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 9.782309462025296e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.95 | train loss last: 3.95 | consumed tokens: 4915200.0 | grad norm avg: 2.46 | grad norm last: 2.3 | 
2025-12-29T23:34:46 | step: 700 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 1.142267956311116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.95 | train loss last: 4.72 | consumed tokens: 5734400.0 | grad norm avg: 2.38 | grad norm last: 2.32 | 
2025-12-29T23:35:04 | step: 800 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 1.3259826118883211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.91 | train loss last: 4.03 | consumed tokens: 6553600.0 | grad norm avg: 2.31 | grad norm last: 2.22 | 
2025-12-29T23:35:22 | step: 900 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 1.527126551081892e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.95 | train loss last: 4.38 | consumed tokens: 7372800.0 | grad norm avg: 2.26 | grad norm last: 2.25 | 
2025-12-29T23:35:40 | step: 1000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 1.7432375898351893e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.91 | train loss last: 3.8 | consumed tokens: 8192000.0 | grad norm avg: 2.2 | grad norm last: 2.06 | 
2025-12-29T23:35:58 | step: 1100 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 1.971671008504927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.92 | train loss last: 3.83 | consumed tokens: 9011200.0 | grad norm avg: 2.13 | grad norm last: 2.06 | 
2025-12-29T23:36:16 | step: 1200 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 2.209630656579975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.88 | train loss last: 3.78 | consumed tokens: 9830400.0 | grad norm avg: 2.07 | grad norm last: 2.08 | 
2025-12-29T23:36:34 | step: 1300 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 2.4542039682273753e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.88 | train loss last: 3.7 | consumed tokens: 10649600.0 | grad norm avg: 2.0 | grad norm last: 2.0 | 
2025-12-29T23:36:52 | step: 1400 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 2.7023977963835932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.85 | train loss last: 3.95 | consumed tokens: 11468800.0 | grad norm avg: 1.92 | grad norm last: 1.9 | 
2025-12-29T23:37:10 | step: 1500 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 2.951174246845767e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.88 | train loss last: 4.06 | consumed tokens: 12288000.0 | grad norm avg: 1.88 | grad norm last: 1.89 | 
2025-12-29T23:37:29 | step: 1600 | train samples/s: 96.7 | train mfu (16-bit): -1.0 | lr mean: 3.1974883313523605e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.83 | train loss last: 3.78 | consumed tokens: 13107200.0 | grad norm avg: 1.79 | grad norm last: 1.73 | 
2025-12-29T23:37:47 | step: 1700 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 3.438325802562758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.84 | train loss last: 4.34 | consumed tokens: 13926400.0 | grad norm avg: 1.74 | grad norm last: 1.63 | 
2025-12-29T23:38:05 | step: 1800 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 3.6707380786538124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.85 | train loss last: 3.09 | consumed tokens: 14745600.0 | grad norm avg: 1.67 | grad norm last: 1.72 | 
2025-12-29T23:38:23 | step: 1900 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 3.891881351592019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.76 | train loss last: 3.39 | consumed tokens: 15564800.0 | grad norm avg: 1.59 | grad norm last: 1.53 | 
2025-12-29T23:38:41 | step: 2000 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.099048601347022e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.82 | train loss last: 3.72 | consumed tokens: 16384000.0 | grad norm avg: 1.57 | grad norm last: 1.46 | 
2025-12-29T23:38:59 | step: 2100 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.289704156690277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.82 | train loss last: 3.88 | consumed tokens: 17203200.0 | grad norm avg: 1.52 | grad norm last: 1.43 | 
2025-12-29T23:39:17 | step: 2200 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.461514618014917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.81 | train loss last: 3.83 | consumed tokens: 18022400.0 | grad norm avg: 1.45 | grad norm last: 1.45 | 
2025-12-29T23:39:35 | step: 2300 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.6123772335704416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.8 | train loss last: 4.16 | consumed tokens: 18841600.0 | grad norm avg: 1.41 | grad norm last: 1.39 | 
2025-12-29T23:39:53 | step: 2400 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.740445365314372e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 3.89 | consumed tokens: 19660800.0 | grad norm avg: 1.37 | grad norm last: 1.34 | 
2025-12-29T23:40:11 | step: 2500 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.844151408178732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.75 | train loss last: 3.84 | consumed tokens: 20480000.0 | grad norm avg: 1.33 | grad norm last: 1.41 | 
2025-12-29T23:40:29 | step: 2600 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.922226435155608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.79 | train loss last: 3.52 | consumed tokens: 21299200.0 | grad norm avg: 1.31 | grad norm last: 1.33 | 
2025-12-29T23:40:47 | step: 2700 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.973714749212377e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.76 | train loss last: 3.41 | consumed tokens: 22118400.0 | grad norm avg: 1.28 | grad norm last: 1.22 | 
2025-12-29T23:41:05 | step: 2800 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 4.99798588862177e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 4.22 | consumed tokens: 22937600.0 | grad norm avg: 1.24 | grad norm last: 1.35 | 
2025-12-29T23:41:23 | step: 2900 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.999999509891495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.75 | train loss last: 4.22 | consumed tokens: 23756800.0 | grad norm avg: 1.23 | grad norm last: 1.19 | 
2025-12-29T23:41:41 | step: 3000 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999995871912688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 4.12 | consumed tokens: 24576000.0 | grad norm avg: 1.21 | grad norm last: 1.25 | 
2025-12-29T23:41:59 | step: 3100 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.999989323550835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.75 | train loss last: 3.55 | consumed tokens: 25395200.0 | grad norm avg: 1.18 | grad norm last: 1.19 | 
2025-12-29T23:42:17 | step: 3200 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.999979501008056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.74 | train loss last: 3.48 | consumed tokens: 26214400.0 | grad norm avg: 1.17 | grad norm last: 1.15 | 
2025-12-29T23:42:35 | step: 3300 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999966768082231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.74 | train loss last: 3.69 | consumed tokens: 27033600.0 | grad norm avg: 1.14 | grad norm last: 1.22 | 
2025-12-29T23:42:54 | step: 3400 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.99995076097548e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.73 | train loss last: 3.62 | consumed tokens: 27852800.0 | grad norm avg: 1.13 | grad norm last: 1.05 | 
2025-12-29T23:43:12 | step: 3500 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999931843485683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 3.08 | consumed tokens: 28672000.0 | grad norm avg: 1.1 | grad norm last: 1.11 | 
2025-12-29T23:43:30 | step: 3600 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.99990965181496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.76 | train loss last: 3.77 | consumed tokens: 29491200.0 | grad norm avg: 1.1 | grad norm last: 1.1 | 
2025-12-29T23:43:48 | step: 3700 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.99988418596331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.72 | train loss last: 4.12 | consumed tokens: 30310400.0 | grad norm avg: 1.07 | grad norm last: 1.04 | 
2025-12-29T23:44:06 | step: 3800 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999855809728615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.69 | train loss last: 3.64 | consumed tokens: 31129600.0 | grad norm avg: 1.05 | grad norm last: 1.09 | 
2025-12-29T23:44:24 | step: 3900 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999824159312993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.69 | train loss last: 4.16 | consumed tokens: 31948800.0 | grad norm avg: 1.05 | grad norm last: 0.98 | 
2025-12-29T23:44:42 | step: 4000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.999789598514326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.71 | train loss last: 3.7 | consumed tokens: 32768000.0 | grad norm avg: 1.04 | grad norm last: 0.96 | 
2025-12-29T23:45:00 | step: 4100 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.999751763534732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.69 | train loss last: 2.78 | consumed tokens: 33587200.0 | grad norm avg: 1.01 | grad norm last: 1.01 | 
2025-12-29T23:45:18 | step: 4200 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.999710654374212e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 3.39 | consumed tokens: 34406400.0 | grad norm avg: 1.02 | grad norm last: 0.99 | 
2025-12-29T23:45:36 | step: 4300 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.999666634830646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.72 | train loss last: 3.31 | consumed tokens: 35225600.0 | grad norm avg: 1.01 | grad norm last: 0.98 | 
2025-12-29T23:45:54 | step: 4400 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999619704904035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 3.62 | consumed tokens: 36044800.0 | grad norm avg: 1.0 | grad norm last: 0.98 | 
2025-12-29T23:46:12 | step: 4500 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.999569136998616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.45 | consumed tokens: 36864000.0 | grad norm avg: 0.99 | grad norm last: 1.02 | 
2025-12-29T23:46:30 | step: 4600 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9995160225080326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.72 | train loss last: 3.64 | consumed tokens: 37683200.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-29T23:46:48 | step: 4700 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.999459270038642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 4.03 | consumed tokens: 38502400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-29T23:47:06 | step: 4800 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.999399607186206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.66 | train loss last: 3.69 | consumed tokens: 39321600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-29T23:47:25 | step: 4900 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 4.999337033950724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 3.86 | consumed tokens: 40140800.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-29T23:47:43 | step: 5000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9992711865343153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 3.48 | consumed tokens: 40960000.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-29T23:48:02 | step: 5100 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.9992020649369806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.77 | consumed tokens: 41779200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-29T23:48:20 | step: 5200 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.9991300329566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.81 | consumed tokens: 42598400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-29T23:48:38 | step: 5300 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.9990547267952934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.68 | train loss last: 4.25 | consumed tokens: 43417600.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-29T23:48:57 | step: 5400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.998976510250941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.62 | consumed tokens: 44236800.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-29T23:49:15 | step: 5500 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.998895019525662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.69 | train loss last: 3.02 | consumed tokens: 45056000.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-29T23:49:33 | step: 5600 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.998810254619457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.64 | consumed tokens: 45875200.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-29T23:49:51 | step: 5700 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.998722579330206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.88 | consumed tokens: 46694400.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-29T23:50:09 | step: 5800 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.9986316298600286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.68 | train loss last: 3.25 | consumed tokens: 47513600.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-29T23:50:27 | step: 5900 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.9985377700068057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.42 | consumed tokens: 48332800.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-29T23:50:45 | step: 6000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.998440635972656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.86 | consumed tokens: 49152000.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-29T23:51:03 | step: 6100 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.998340591555461e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.33 | consumed tokens: 49971200.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-29T23:51:21 | step: 6200 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.99823727295734e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.66 | train loss last: 3.53 | consumed tokens: 50790400.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-29T23:51:39 | step: 6300 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.998130680178292e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 2.75 | consumed tokens: 51609600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-29T23:51:57 | step: 6400 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9980211770161986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.42 | consumed tokens: 52428800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-29T23:52:15 | step: 6500 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.9979087634710595e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.66 | train loss last: 3.83 | consumed tokens: 53248000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-29T23:52:34 | step: 6600 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.997792711947113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.68 | train loss last: 4.09 | consumed tokens: 54067200.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-29T23:52:52 | step: 6700 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.997674113838002e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 4.44 | consumed tokens: 54886400.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-29T23:53:10 | step: 6800 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.997551877750084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 4.0 | consumed tokens: 55705600.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-29T23:53:28 | step: 6900 | train samples/s: 97.4 | train mfu (16-bit): -1.0 | lr mean: 4.99742673127912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 4.44 | consumed tokens: 56524800.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-29T23:53:46 | step: 7000 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.99729867442511e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.75 | consumed tokens: 57344000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-29T23:54:04 | step: 7100 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.997167343390174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 4.16 | consumed tokens: 58163200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-29T23:54:22 | step: 7200 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.997032738174312e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.77 | consumed tokens: 58982400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-29T23:54:40 | step: 7300 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.996895222575404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.28 | consumed tokens: 59801600.0 | grad norm avg: 0.88 | grad norm last: 0.8 | 
2025-12-29T23:54:58 | step: 7400 | train samples/s: 96.5 | train mfu (16-bit): -1.0 | lr mean: 4.996754432795569e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.62 | consumed tokens: 60620800.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2025-12-29T23:55:17 | step: 7500 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.996610732632689e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.47 | consumed tokens: 61440000.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-29T23:55:35 | step: 7600 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.996463758288883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.67 | consumed tokens: 62259200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-29T23:55:53 | step: 7700 | train samples/s: 96.7 | train mfu (16-bit): -1.0 | lr mean: 4.9963138735620305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.2 | consumed tokens: 63078400.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-29T23:56:11 | step: 7800 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.996160714654252e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.42 | consumed tokens: 63897600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-29T23:56:29 | step: 7900 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.996004281565547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.31 | consumed tokens: 64716800.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-29T23:56:47 | step: 8000 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.9958449380937964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 4.41 | consumed tokens: 65536000.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-29T23:57:06 | step: 8100 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9956823204411194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.48 | consumed tokens: 66355200.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-29T23:57:24 | step: 8200 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.995516792405397e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 2.94 | consumed tokens: 67174400.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-29T23:57:42 | step: 8300 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9953479901887476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.78 | consumed tokens: 67993600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-29T23:58:00 | step: 8400 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 4.995176277589053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.91 | consumed tokens: 68812800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-29T23:58:18 | step: 8500 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.995001290808432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.41 | consumed tokens: 69632000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-29T23:58:36 | step: 8600 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.994823029846884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.64 | consumed tokens: 70451200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-29T23:58:54 | step: 8700 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.994641858502291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.73 | consumed tokens: 71270400.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-29T23:59:12 | step: 8800 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.994457776774652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.8 | consumed tokens: 72089600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-29T23:59:30 | step: 8900 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.994270420866087e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.66 | train loss last: 3.91 | consumed tokens: 72908800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-29T23:59:48 | step: 9000 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9940797907765955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.56 | consumed tokens: 73728000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:00:07 | step: 9100 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9938858865061775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.11 | consumed tokens: 74547200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T00:00:25 | step: 9200 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.9936894356505945e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.05 | consumed tokens: 75366400.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T00:00:43 | step: 9300 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.9934893468162045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.66 | consumed tokens: 76185600.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T00:01:01 | step: 9400 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.993286347598769e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.19 | consumed tokens: 77004800.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T00:01:19 | step: 9500 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9930804379982874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.33 | consumed tokens: 77824000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T00:01:37 | step: 9600 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9928712542168796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 4.16 | consumed tokens: 78643200.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T00:01:55 | step: 9700 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9926587962545455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.25 | consumed tokens: 79462400.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T00:02:13 | step: 9800 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.9924434279091656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.08 | consumed tokens: 80281600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T00:02:31 | step: 9900 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.9922247853828594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.72 | consumed tokens: 81100800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T00:02:49 | step: 10000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9920032324735075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.91 | consumed tokens: 81920000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T00:03:09 | step: 10100 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.991778405383229e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.53 | consumed tokens: 82739200.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T00:03:27 | step: 10200 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 4.991550667909905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.73 | consumed tokens: 83558400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T00:03:45 | step: 10300 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.991319656255655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 2.94 | consumed tokens: 84377600.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T00:04:03 | step: 10400 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.991085734218359e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.11 | consumed tokens: 85196800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T00:04:21 | step: 10500 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.9908485380001366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.41 | consumed tokens: 86016000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T00:04:40 | step: 10600 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.990608067600988e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.81 | consumed tokens: 86835200.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T00:04:58 | step: 10700 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9903646868187934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.22 | consumed tokens: 87654400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T00:05:16 | step: 10800 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.9901180318556726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.45 | consumed tokens: 88473600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:05:34 | step: 10900 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.989868466509506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.84 | consumed tokens: 89292800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T00:05:52 | step: 11000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.989615990780294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.33 | consumed tokens: 90112000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:06:10 | step: 11100 | train samples/s: 96.7 | train mfu (16-bit): -1.0 | lr mean: 4.989359877072275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.81 | consumed tokens: 90931200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T00:06:28 | step: 11200 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9891012167790905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.25 | consumed tokens: 91750400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T00:06:46 | step: 11300 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.988838918507099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.52 | consumed tokens: 92569600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:07:04 | step: 11400 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.988573709852062e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.5 | consumed tokens: 93388800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:07:22 | step: 11500 | train samples/s: 97.3 | train mfu (16-bit): -1.0 | lr mean: 4.9883055908139795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.72 | consumed tokens: 94208000.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T00:07:40 | step: 11600 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9880341975949705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.78 | consumed tokens: 95027200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:07:59 | step: 11700 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.987759893992916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.31 | consumed tokens: 95846400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:08:17 | step: 11800 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9874823162099347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.27 | consumed tokens: 96665600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:08:35 | step: 11900 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.987201464246027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.48 | consumed tokens: 97484800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T00:08:53 | step: 12000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.986917701899074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.34 | consumed tokens: 98304000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:09:11 | step: 12100 | train samples/s: 96.5 | train mfu (16-bit): -1.0 | lr mean: 4.986631029169075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 4.06 | consumed tokens: 99123200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:09:30 | step: 12200 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 4.986340718460269e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.88 | consumed tokens: 99942400.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T00:09:48 | step: 12300 | train samples/s: 96.7 | train mfu (16-bit): -1.0 | lr mean: 4.9860478611662984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.52 | consumed tokens: 100761600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:10:06 | step: 12400 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 4.985751729691401e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 4.09 | consumed tokens: 101580800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:10:24 | step: 12500 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 4.9854523240355775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.61 | consumed tokens: 102400000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:10:42 | step: 12600 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 4.985150007996708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.98 | consumed tokens: 103219200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:11:00 | step: 12700 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 4.9848444177769125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.91 | consumed tokens: 104038400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T00:11:18 | step: 12800 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 4.984535917174071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.47 | consumed tokens: 104857600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T00:11:36 | step: 12900 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.984224142390303e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.22 | consumed tokens: 105676800.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T00:11:55 | step: 13000 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 4.98390945722349e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.22 | consumed tokens: 106496000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:12:13 | step: 13100 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.98359149787575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.66 | consumed tokens: 107315200.0 | grad norm avg: 0.82 | grad norm last: 0.92 | 
2025-12-30T00:12:31 | step: 13200 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.983270264347084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 4.16 | consumed tokens: 108134400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:12:49 | step: 13300 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.982946120435372e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.83 | consumed tokens: 108953600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T00:13:07 | step: 13400 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 4.9826190661406144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.7 | consumed tokens: 109772800.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T00:13:25 | step: 13500 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9822887376649305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 2.81 | consumed tokens: 110592000.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:13:43 | step: 13600 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.981955498806201e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.19 | consumed tokens: 111411200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:14:01 | step: 13700 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 4.981618985766545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.06 | consumed tokens: 112230400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:14:19 | step: 13800 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.9812791985459626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.73 | consumed tokens: 113049600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:14:38 | step: 13900 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.9809365009423345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.17 | consumed tokens: 113868800.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T00:14:56 | step: 14000 | train samples/s: 97.0 | train mfu (16-bit): -1.0 | lr mean: 4.980590892955661e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.33 | consumed tokens: 114688000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:15:14 | step: 14100 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.980242010788061e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.59 | consumed tokens: 115507200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:15:32 | step: 14200 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 4.979889854439534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.8 | consumed tokens: 116326400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:15:50 | step: 14300 | train samples/s: 97.2 | train mfu (16-bit): -1.0 | lr mean: 4.979534787707962e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.81 | consumed tokens: 117145600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:16:08 | step: 14400 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 4.979176810593344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.38 | consumed tokens: 117964800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T00:16:26 | step: 14500 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.9788155592978e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.58 | consumed tokens: 118784000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:16:44 | step: 14600 | train samples/s: 96.7 | train mfu (16-bit): -1.0 | lr mean: 4.9784510338213295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 4.06 | consumed tokens: 119603200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:17:03 | step: 14700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.978083597961813e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.77 | consumed tokens: 120422400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:17:21 | step: 14800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.977713251719251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.47 | consumed tokens: 121241600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:17:40 | step: 14900 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 4.977339631295763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.62 | consumed tokens: 122060800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T00:17:58 | step: 15000 | train samples/s: 96.6 | train mfu (16-bit): -1.0 | lr mean: 4.976962736691348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.45 | consumed tokens: 122880000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:18:17 | step: 15100 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.976582931703888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.48 | consumed tokens: 123699200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:18:35 | step: 15200 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.976200216333382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.66 | consumed tokens: 124518400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:18:54 | step: 15300 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.9758142267819494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.66 | consumed tokens: 125337600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:19:12 | step: 15400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.9754249630495906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.55 | consumed tokens: 126156800.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T00:19:31 | step: 15500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.975032788934186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 4.16 | consumed tokens: 126976000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:19:49 | step: 15600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.974637704435736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.36 | consumed tokens: 127795200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T00:20:08 | step: 15700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.9742393457563594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.34 | consumed tokens: 128614400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:20:26 | step: 15800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.973838076693937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.77 | consumed tokens: 129433600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:20:44 | step: 15900 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.9734335334505886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.61 | consumed tokens: 130252800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:21:02 | step: 16000 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.9730257160263136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.0 | consumed tokens: 131072000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:21:21 | step: 16100 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.972614988218993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.52 | consumed tokens: 131891200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:21:39 | step: 16200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.9722013500286266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.69 | consumed tokens: 132710400.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:21:58 | step: 16300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.971784437657334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.23 | consumed tokens: 133529600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:22:16 | step: 16400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.9713646149029955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 4.62 | consumed tokens: 134348800.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:22:35 | step: 16500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.970941517967731e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.06 | consumed tokens: 135168000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:22:53 | step: 16600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.97051551064942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.88 | consumed tokens: 135987200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:23:11 | step: 16700 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.9700862291501835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.75 | consumed tokens: 136806400.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T00:23:30 | step: 16800 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 4.969654037267901e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.2 | consumed tokens: 137625600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:23:48 | step: 16900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.969218571204692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.72 | consumed tokens: 138444800.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T00:24:07 | step: 17000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.9687801947584376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 2.97 | consumed tokens: 139264000.0 | grad norm avg: 0.81 | grad norm last: 0.92 | 
2025-12-30T00:24:25 | step: 17100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.9683385441312566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.61 | consumed tokens: 140083200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:24:44 | step: 17200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.96789398312103e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.73 | consumed tokens: 140902400.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T00:25:02 | step: 17300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.967446147929877e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 4.22 | consumed tokens: 141721600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T00:25:20 | step: 17400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.9669954023556784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.69 | consumed tokens: 142540800.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T00:25:39 | step: 17500 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.966541746398434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 2.88 | consumed tokens: 143360000.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:25:57 | step: 17600 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.966084816260263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.44 | consumed tokens: 144179200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:26:15 | step: 17700 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.965624611941166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.45 | consumed tokens: 144998400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T00:26:34 | step: 17800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.9651614972390234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.92 | consumed tokens: 145817600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:26:52 | step: 17900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.964695472153835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.03 | consumed tokens: 146636800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:27:11 | step: 18000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.96422617288772e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.98 | consumed tokens: 147456000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:27:29 | step: 18100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.96375396323856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.27 | consumed tokens: 148275200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:27:47 | step: 18200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.963278479408473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.45 | consumed tokens: 149094400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T00:28:06 | step: 18300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.96280008519534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.75 | consumed tokens: 149913600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:28:24 | step: 18400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.962318780599162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.48 | consumed tokens: 150732800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:28:43 | step: 18500 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.9618342018220574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.28 | consumed tokens: 151552000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:29:01 | step: 18600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.9613463488640264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.59 | consumed tokens: 152371200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:29:19 | step: 18700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.96085558552295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.55 | consumed tokens: 153190400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T00:29:38 | step: 18800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.9603619117988274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 2.81 | consumed tokens: 154009600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T00:29:56 | step: 18900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.9598649638937786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.5 | consumed tokens: 154828800.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T00:30:15 | step: 19000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.959365105605684e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.66 | consumed tokens: 155648000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:30:33 | step: 19100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.9588619731366634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.98 | consumed tokens: 156467200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:30:52 | step: 19200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.958355930284597e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.34 | consumed tokens: 157286400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:31:10 | step: 19300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.957846613251604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 4.09 | consumed tokens: 158105600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:31:29 | step: 19400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.9573343858355656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 4.09 | consumed tokens: 158924800.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:31:47 | step: 19500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.9568192480364814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.39 | consumed tokens: 159744000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:32:06 | step: 19600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.956300836056471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 2.97 | consumed tokens: 160563200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:32:24 | step: 19700 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.9557795136934146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.47 | consumed tokens: 161382400.0 | grad norm avg: 0.81 | grad norm last: 0.93 | 
2025-12-30T00:32:43 | step: 19800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.955254917149432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.64 | consumed tokens: 162201600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:33:01 | step: 19900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.954727410222404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.27 | consumed tokens: 163020800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:33:20 | step: 20000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.954196629114449e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.59 | consumed tokens: 163840000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:33:40 | step: 20100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.953662937623449e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 4.12 | consumed tokens: 164659200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:33:58 | step: 20200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.9531263357494026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.2 | consumed tokens: 165478400.0 | grad norm avg: 0.8 | grad norm last: 0.88 | 
2025-12-30T00:34:17 | step: 20300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.95258645969443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.52 | consumed tokens: 166297600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:34:35 | step: 20400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.952043673256412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.48 | consumed tokens: 167116800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:34:54 | step: 20500 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.9514979764353484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.31 | consumed tokens: 167936000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:35:12 | step: 20600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.9509486416354775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.5 | consumed tokens: 168755200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:35:31 | step: 20700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.950396760250442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.84 | consumed tokens: 169574400.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T00:35:49 | step: 20800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.9498416046844795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.92 | consumed tokens: 170393600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:36:08 | step: 20900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.949283538735472e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.83 | consumed tokens: 171212800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T00:36:26 | step: 21000 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.9487221986055374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.53 | consumed tokens: 172032000.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T00:36:45 | step: 21100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.9481579480925575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.06 | consumed tokens: 172851200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T00:37:03 | step: 21200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.947590787196532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.33 | consumed tokens: 173670400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T00:37:22 | step: 21300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.94702035211958e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.56 | consumed tokens: 174489600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:37:40 | step: 21400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.946447006659582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.78 | consumed tokens: 175308800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:37:59 | step: 21500 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.945870387018658e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.02 | consumed tokens: 176128000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:38:17 | step: 21600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.9452908569946885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 4.03 | consumed tokens: 176947200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:38:35 | step: 21700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.944708416587673e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.89 | consumed tokens: 177766400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:38:54 | step: 21800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.9441227019997314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 2.81 | consumed tokens: 178585600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:39:12 | step: 21900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.943534077028744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.95 | consumed tokens: 179404800.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T00:39:31 | step: 22000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.94294217787683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.55 | consumed tokens: 180224000.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:39:49 | step: 22100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.9423473683418706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.7 | consumed tokens: 181043200.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2025-12-30T00:40:07 | step: 22200 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.9417496484238654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.83 | consumed tokens: 181862400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T00:40:26 | step: 22300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.941148654324934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 4.59 | consumed tokens: 182681600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:40:44 | step: 22400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.940544749842957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.66 | consumed tokens: 183500800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:41:03 | step: 22500 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.939937934977934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.62 | consumed tokens: 184320000.0 | grad norm avg: 0.8 | grad norm last: 0.72 | 
2025-12-30T00:41:21 | step: 22600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.9393278459319845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.52 | consumed tokens: 185139200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T00:41:39 | step: 22700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.9387148465029895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.47 | consumed tokens: 185958400.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T00:41:58 | step: 22800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.938098572893068e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.31 | consumed tokens: 186777600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:42:17 | step: 22900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.937479752697982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.88 | consumed tokens: 187596800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:42:35 | step: 23000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.9368572945240885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.78 | consumed tokens: 188416000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T00:42:54 | step: 23100 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.93623228976503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.88 | consumed tokens: 189235200.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T00:43:12 | step: 23200 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.9356040108250454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.89 | consumed tokens: 190054400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:43:31 | step: 23300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.934972457704134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.67 | consumed tokens: 190873600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:43:49 | step: 23400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.934338357998058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.14 | consumed tokens: 191692800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:44:08 | step: 23500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.933700984111056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.83 | consumed tokens: 192512000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:44:26 | step: 23600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.933060336043127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.5 | consumed tokens: 193331200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:44:45 | step: 23700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.932417141390033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.88 | consumed tokens: 194150400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:45:03 | step: 23800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.931770672556013e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.81 | consumed tokens: 194969600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:45:22 | step: 23900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.931120929541066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.66 | consumed tokens: 195788800.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T00:45:41 | step: 24000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.930468276143074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.28 | consumed tokens: 196608000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:45:59 | step: 24100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.929812712362036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 2.95 | consumed tokens: 197427200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:46:18 | step: 24200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.9291542381979525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.17 | consumed tokens: 198246400.0 | grad norm avg: 0.8 | grad norm last: 0.9 | 
2025-12-30T00:46:36 | step: 24300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.9284924898529425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.8 | consumed tokens: 199065600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:46:54 | step: 24400 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.927827831124887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 4.03 | consumed tokens: 199884800.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:47:13 | step: 24500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.9271602620137855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.56 | consumed tokens: 200704000.0 | grad norm avg: 0.81 | grad norm last: 0.89 | 
2025-12-30T00:47:31 | step: 24600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.926489418721758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.69 | consumed tokens: 201523200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:47:50 | step: 24700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.9258156650466844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 2.97 | consumed tokens: 202342400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:48:09 | step: 24800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.925138637190685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.52 | consumed tokens: 203161600.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:48:27 | step: 24900 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.92445906274952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 4.12 | consumed tokens: 203980800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:48:46 | step: 25000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.923776214127429e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.48 | consumed tokens: 204800000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:49:05 | step: 25100 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.923090091324411e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.56 | consumed tokens: 205619200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:49:24 | step: 25200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.922401421936229e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.25 | consumed tokens: 206438400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T00:49:42 | step: 25300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.92170947836712e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.36 | consumed tokens: 207257600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:50:00 | step: 25400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.9210146244149655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 2.98 | consumed tokens: 208076800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T00:50:19 | step: 25500 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.9203164962818846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.27 | consumed tokens: 208896000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:50:38 | step: 25600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.919615457765758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 2.91 | consumed tokens: 209715200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:50:56 | step: 25700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.918911508866586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.47 | consumed tokens: 210534400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:51:15 | step: 25800 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.918204649584368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 4.38 | consumed tokens: 211353600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:51:34 | step: 25900 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.9174945161212236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.67 | consumed tokens: 212172800.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:51:53 | step: 26000 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.9167814722750336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.08 | consumed tokens: 212992000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:52:11 | step: 26100 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.916065154247917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.16 | consumed tokens: 213811200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:52:30 | step: 26200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.915346289635636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.97 | consumed tokens: 214630400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:52:49 | step: 26300 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.914624150842428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.38 | consumed tokens: 215449600.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:53:07 | step: 26400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.913899101666175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.05 | consumed tokens: 216268800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T00:53:26 | step: 26500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.913170778308995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.55 | consumed tokens: 217088000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:53:44 | step: 26600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.9124395445687696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.09 | consumed tokens: 217907200.0 | grad norm avg: 0.81 | grad norm last: 0.91 | 
2025-12-30T00:54:03 | step: 26700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.9117054004454985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.55 | consumed tokens: 218726400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:54:22 | step: 26800 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.910968345939182e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.89 | consumed tokens: 219545600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:54:40 | step: 26900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.910228381049819e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 4.31 | consumed tokens: 220364800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T00:54:58 | step: 27000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.9094851419795305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.56 | consumed tokens: 221184000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:55:17 | step: 27100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.908738992526196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.5 | consumed tokens: 222003200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T00:55:36 | step: 27200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.907989568891935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.42 | consumed tokens: 222822400.0 | grad norm avg: 0.81 | grad norm last: 0.96 | 
2025-12-30T00:55:54 | step: 27300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.907237598672509e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.88 | consumed tokens: 223641600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:56:13 | step: 27400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.906482354272157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.77 | consumed tokens: 224460800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:56:31 | step: 27500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.905724199488759e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 2.88 | consumed tokens: 225280000.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T00:56:50 | step: 27600 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.904962770524435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.64 | consumed tokens: 226099200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:57:08 | step: 27700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.9041987949749455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.09 | consumed tokens: 226918400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:57:27 | step: 27800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.90343154524453e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.89 | consumed tokens: 227737600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:57:46 | step: 27900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.9026613851310685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.81 | consumed tokens: 228556800.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T00:58:04 | step: 28000 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.901887950836681e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.19 | consumed tokens: 229376000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:58:23 | step: 28100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.901111969957128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.47 | consumed tokens: 230195200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T00:58:41 | step: 28200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.900332714896649e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.66 | consumed tokens: 231014400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:59:00 | step: 28300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.8995505494531244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.41 | consumed tokens: 231833600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T00:59:19 | step: 28400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.898765109828673e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 4.19 | consumed tokens: 232652800.0 | grad norm avg: 0.8 | grad norm last: 0.73 | 
2025-12-30T00:59:37 | step: 28500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.897977123619057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.45 | consumed tokens: 233472000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T00:59:55 | step: 28600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.897185863228515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.7 | consumed tokens: 234291200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:00:14 | step: 28700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.896391692454927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.7 | consumed tokens: 235110400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:00:32 | step: 28800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.895594611298293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.91 | consumed tokens: 235929600.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:00:51 | step: 28900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.8947946197586134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.0 | consumed tokens: 236748800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:01:09 | step: 29000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.8939913540380076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.67 | consumed tokens: 237568000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:01:28 | step: 29100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.893185177934356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.19 | consumed tokens: 238387200.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:01:46 | step: 29200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.892376091447659e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.77 | consumed tokens: 239206400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:02:05 | step: 29300 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.891564094577916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.7 | consumed tokens: 240025600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:02:23 | step: 29400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.890748823527247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.19 | consumed tokens: 240844800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:02:42 | step: 29500 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.8899310058914125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.91 | consumed tokens: 241664000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:03:00 | step: 29600 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.889109914074652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.06 | consumed tokens: 242483200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:03:19 | step: 29700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.8882859118748456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.2 | consumed tokens: 243302400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:03:37 | step: 29800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.887458635494113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.22 | consumed tokens: 244121600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:03:56 | step: 29900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.8866288125282153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.02 | consumed tokens: 244940800.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:04:15 | step: 30000 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.8857957153813913e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.0 | consumed tokens: 245760000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:04:34 | step: 30100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.8849600716494024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.69 | consumed tokens: 246579200.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T01:04:53 | step: 30200 | train samples/s: 92.2 | train mfu (16-bit): -1.0 | lr mean: 4.884121153736487e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.72 | consumed tokens: 247398400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:05:12 | step: 30300 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.883278961642645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.31 | consumed tokens: 248217600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:05:31 | step: 30400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.8824342229636386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.91 | consumed tokens: 249036800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:05:49 | step: 30500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.881586573901586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.34 | consumed tokens: 249856000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:06:08 | step: 30600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.8807356506586075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.95 | consumed tokens: 250675200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:06:27 | step: 30700 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.879881817032583e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 2.88 | consumed tokens: 251494400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:06:45 | step: 30800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.879025073023513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.31 | consumed tokens: 252313600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:07:04 | step: 30900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.878165418631397e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.12 | consumed tokens: 253132800.0 | grad norm avg: 0.8 | grad norm last: 0.73 | 
2025-12-30T01:07:22 | step: 31000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.877302490058355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.09 | consumed tokens: 253952000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:07:41 | step: 31100 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.876437014900148e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.88 | consumed tokens: 254771200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:07:59 | step: 31200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.8755682655610144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.58 | consumed tokens: 255590400.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T01:08:18 | step: 31300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.874696969636716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.42 | consumed tokens: 256409600.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T01:08:37 | step: 31400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.873822399531491e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.14 | consumed tokens: 257228800.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T01:08:56 | step: 31500 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 4.8729449190432206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.05 | consumed tokens: 258048000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:09:14 | step: 31600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.872064164374024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.59 | consumed tokens: 258867200.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:09:33 | step: 31700 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.871180863119662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.56 | consumed tokens: 259686400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:09:51 | step: 31800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.870294651482254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.0 | consumed tokens: 260505600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:10:10 | step: 31900 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.8694051656639203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 2.97 | consumed tokens: 261324800.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:10:29 | step: 32000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.868512769462541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.45 | consumed tokens: 262144000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:10:47 | step: 32100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.8676174628781155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 4.62 | consumed tokens: 262963200.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T01:11:06 | step: 32200 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.8667192459106445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.14 | consumed tokens: 263782400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:11:25 | step: 32300 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.865818118560128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.48 | consumed tokens: 264601600.0 | grad norm avg: 0.8 | grad norm last: 0.71 | 
2025-12-30T01:11:43 | step: 32400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.8649140808265656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.5 | consumed tokens: 265420800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:12:01 | step: 32500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.8640071327099577e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 2.97 | consumed tokens: 266240000.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:12:20 | step: 32600 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.863096910412423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 4.44 | consumed tokens: 267059200.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:12:38 | step: 32700 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.862184141529724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.64 | consumed tokens: 267878400.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T01:12:57 | step: 32800 | train samples/s: 92.8 | train mfu (16-bit): -1.0 | lr mean: 4.861268098466098e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.59 | consumed tokens: 268697600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:13:15 | step: 32900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.860349145019427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.67 | consumed tokens: 269516800.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T01:13:34 | step: 33000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.85942728118971e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.66 | consumed tokens: 270336000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:13:52 | step: 33100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.858502506976947e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.05 | consumed tokens: 271155200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:14:10 | step: 33200 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.857574822381139e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.12 | consumed tokens: 271974400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:14:29 | step: 33300 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.856644227402285e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.56 | consumed tokens: 272793600.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:14:47 | step: 33400 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.855710722040385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.19 | consumed tokens: 273612800.0 | grad norm avg: 0.81 | grad norm last: 0.89 | 
2025-12-30T01:15:05 | step: 33500 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.8547743062954396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 4.19 | consumed tokens: 274432000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:15:24 | step: 33600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.853834616369568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.11 | consumed tokens: 275251200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:15:42 | step: 33700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.852892379858531e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.38 | consumed tokens: 276070400.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:16:00 | step: 33800 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.8519472329644486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.73 | consumed tokens: 276889600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:16:19 | step: 33900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.85099881188944e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.0 | consumed tokens: 277708800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:16:37 | step: 34000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.850047480431385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 4.47 | consumed tokens: 278528000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:16:56 | step: 34100 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.849093602388166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.75 | consumed tokens: 279347200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:17:15 | step: 34200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.84813645016402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.41 | consumed tokens: 280166400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:17:33 | step: 34300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.8471763875568286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.41 | consumed tokens: 280985600.0 | grad norm avg: 0.8 | grad norm last: 0.9 | 
2025-12-30T01:17:52 | step: 34400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.8462134145665914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.12 | consumed tokens: 281804800.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:18:10 | step: 34500 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.8452475311933085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.5 | consumed tokens: 282624000.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T01:18:28 | step: 34600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.84427873743698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.98 | consumed tokens: 283443200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:18:46 | step: 34700 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.843307033297606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.48 | consumed tokens: 284262400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:19:05 | step: 34800 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.842332418775186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.67 | consumed tokens: 285081600.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T01:19:23 | step: 34900 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.8413548938697204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.94 | consumed tokens: 285900800.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T01:19:41 | step: 35000 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.840374458581209e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 4.34 | consumed tokens: 286720000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:20:01 | step: 35100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.839391112909652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 2.84 | consumed tokens: 287539200.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T01:20:19 | step: 35200 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 4.83840485685505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 4.0 | consumed tokens: 288358400.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T01:20:37 | step: 35300 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.8374156904174015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.25 | consumed tokens: 289177600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:20:56 | step: 35400 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.8364236135967076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 4.06 | consumed tokens: 289996800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:21:14 | step: 35500 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.835428262595087e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.7 | consumed tokens: 290816000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:21:32 | step: 35600 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.834430365008302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.36 | consumed tokens: 291635200.0 | grad norm avg: 0.81 | grad norm last: 0.91 | 
2025-12-30T01:21:51 | step: 35700 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.833429557038471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.53 | consumed tokens: 292454400.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T01:22:09 | step: 35800 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.8324258386855945e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.77 | consumed tokens: 293273600.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:22:28 | step: 35900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.831419209949672e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.33 | consumed tokens: 294092800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:22:46 | step: 36000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.8304093070328236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.7 | consumed tokens: 294912000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T01:23:04 | step: 36100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.82939685753081e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.89 | consumed tokens: 295731200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:23:23 | step: 36200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.8283814976457506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.67 | consumed tokens: 296550400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T01:23:41 | step: 36300 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.827363227377646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.52 | consumed tokens: 297369600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:23:59 | step: 36400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.826342046726495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.69 | consumed tokens: 298188800.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:24:18 | step: 36500 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.825317591894418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 4.06 | consumed tokens: 299008000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T01:24:36 | step: 36600 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.824290590477176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 4.06 | consumed tokens: 299827200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:24:54 | step: 36700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.8232606786768883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.92 | consumed tokens: 300646400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:25:13 | step: 36800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.822227856493555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.69 | consumed tokens: 301465600.0 | grad norm avg: 0.82 | grad norm last: 0.74 | 
2025-12-30T01:25:31 | step: 36900 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 4.821192123927176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.05 | consumed tokens: 302284800.0 | grad norm avg: 0.81 | grad norm last: 0.89 | 
2025-12-30T01:25:49 | step: 37000 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.820153480977751e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.09 | consumed tokens: 303104000.0 | grad norm avg: 0.81 | grad norm last: 0.89 | 
2025-12-30T01:26:07 | step: 37100 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.819111927645281e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.47 | consumed tokens: 303923200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:26:26 | step: 37200 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.818067463929765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 2.95 | consumed tokens: 304742400.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T01:26:44 | step: 37300 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.817020089831203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.22 | consumed tokens: 305561600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:27:02 | step: 37400 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.8159701691474766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 2.64 | consumed tokens: 306380800.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:27:20 | step: 37500 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 4.8149169742828235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.25 | consumed tokens: 307200000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:27:39 | step: 37600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.813860869035125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 2.89 | consumed tokens: 308019200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:27:57 | step: 37700 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 4.812802217202261e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.47 | consumed tokens: 308838400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:28:15 | step: 37800 | train samples/s: 96.5 | train mfu (16-bit): -1.0 | lr mean: 4.811740291188471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.61 | consumed tokens: 309657600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T01:28:33 | step: 37900 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.810675818589516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.86 | consumed tokens: 310476800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:28:52 | step: 38000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.8096080718096346e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 4.72 | consumed tokens: 311296000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:29:10 | step: 38100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.808537778444588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.16 | consumed tokens: 312115200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:29:29 | step: 38200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.807464574696496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.53 | consumed tokens: 312934400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:29:47 | step: 38300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.806388096767478e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.8 | consumed tokens: 313753600.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T01:30:06 | step: 38400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.805309072253294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.14 | consumed tokens: 314572800.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T01:30:24 | step: 38500 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.804227137356065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 2.98 | consumed tokens: 315392000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:30:42 | step: 38600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.8031422920757905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.28 | consumed tokens: 316211200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:31:01 | step: 38700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.802054900210351e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 4.12 | consumed tokens: 317030400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:31:19 | step: 38800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.8009642341639847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.22 | consumed tokens: 317849600.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:31:38 | step: 38900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.799870657734573e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.83 | consumed tokens: 318668800.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T01:31:56 | step: 39000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.798774534719996e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 2.86 | consumed tokens: 319488000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:32:14 | step: 39100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.797675501322374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.34 | consumed tokens: 320307200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:32:33 | step: 39200 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.796573193743825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.22 | consumed tokens: 321126400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:32:51 | step: 39300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.795468339580111e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.98 | consumed tokens: 321945600.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T01:33:10 | step: 39400 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.794360575033352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.5 | consumed tokens: 322764800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:33:28 | step: 39500 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.793249900103547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.5 | consumed tokens: 323584000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T01:33:46 | step: 39600 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.7921366785885766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.17 | consumed tokens: 324403200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T01:34:04 | step: 39700 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.79102018289268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.81 | consumed tokens: 325222400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:34:23 | step: 39800 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.789901140611619e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.38 | consumed tokens: 326041600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:34:41 | step: 39900 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.788778824149631e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.8 | consumed tokens: 326860800.0 | grad norm avg: 0.82 | grad norm last: 0.93 | 
2025-12-30T01:34:59 | step: 40000 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.787653961102478e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.66 | consumed tokens: 327680000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T01:35:19 | step: 40100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.78652618767228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.02 | consumed tokens: 328499200.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:35:37 | step: 40200 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.785395503859036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.95 | consumed tokens: 329318400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T01:35:55 | step: 40300 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.7842622734606266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.06 | consumed tokens: 330137600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:36:14 | step: 40400 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.783125768881291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.22 | consumed tokens: 330956800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T01:36:32 | step: 40500 | train samples/s: 96.5 | train mfu (16-bit): -1.0 | lr mean: 4.781986717716791e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.64 | consumed tokens: 331776000.0 | grad norm avg: 0.82 | grad norm last: 0.75 | 
2025-12-30T01:36:50 | step: 40600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.7808447561692446e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.19 | consumed tokens: 332595200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:37:08 | step: 40700 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.779699884238653e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.59 | consumed tokens: 333414400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:37:27 | step: 40800 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.7785521019250154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.92 | consumed tokens: 334233600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:37:45 | step: 40900 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.7774014092283323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.28 | consumed tokens: 335052800.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T01:38:03 | step: 41000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.776248169946484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.39 | consumed tokens: 335872000.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T01:38:22 | step: 41100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.7750920202815905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 4.28 | consumed tokens: 336691200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:38:41 | step: 41200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.773932960233651e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.33 | consumed tokens: 337510400.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T01:38:59 | step: 41300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.772770989802666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.23 | consumed tokens: 338329600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T01:39:18 | step: 41400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.771606108988635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.17 | consumed tokens: 339148800.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T01:39:36 | step: 41500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.7704386815894395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.72 | consumed tokens: 339968000.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T01:39:55 | step: 41600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.7692679800093174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 2.98 | consumed tokens: 340787200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:40:13 | step: 41700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.76809473184403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.03 | consumed tokens: 341606400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:40:31 | step: 41800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.766918937093578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.89 | consumed tokens: 342425600.0 | grad norm avg: 0.81 | grad norm last: 0.73 | 
2025-12-30T01:40:50 | step: 41900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.7657398681622e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.62 | consumed tokens: 343244800.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T01:41:08 | step: 42000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.7645582526456565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 2.94 | consumed tokens: 344064000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:41:27 | step: 42100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.7633737267460674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.34 | consumed tokens: 344883200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:41:45 | step: 42200 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.762186290463433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.66 | consumed tokens: 345702400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:42:03 | step: 42300 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.760995943797752e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.31 | consumed tokens: 346521600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T01:42:21 | step: 42400 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.759803050546907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.67 | consumed tokens: 347340800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T01:42:40 | step: 42500 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.758607246913016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.08 | consumed tokens: 348160000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:42:58 | step: 42600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.757408532896079e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.75 | consumed tokens: 348979200.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T01:43:16 | step: 42700 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.756206908496097e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 2.89 | consumed tokens: 349798400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T01:43:34 | step: 42800 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.7550027375109494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.16 | consumed tokens: 350617600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:43:53 | step: 42900 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 4.7537956561427563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 2.52 | consumed tokens: 351436800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:44:11 | step: 43000 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.7525856643915176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.78 | consumed tokens: 352256000.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:44:29 | step: 43100 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.751372762257233e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.42 | consumed tokens: 353075200.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T01:44:47 | step: 43200 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 4.750157313537784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.92 | consumed tokens: 353894400.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T01:45:06 | step: 43300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.748938954435289e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.31 | consumed tokens: 354713600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:45:25 | step: 43400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.747717684949748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.75 | consumed tokens: 355532800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:45:43 | step: 43500 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.7464938688790426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.38 | consumed tokens: 356352000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:46:02 | step: 43600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.745267142425291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.95 | consumed tokens: 357171200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T01:46:20 | step: 43700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.744037505588494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.06 | consumed tokens: 357990400.0 | grad norm avg: 0.82 | grad norm last: 0.94 | 
2025-12-30T01:46:39 | step: 43800 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.742805322166532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.45 | consumed tokens: 358809600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T01:46:58 | step: 43900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.741569864563644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.84 | consumed tokens: 359628800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:47:16 | step: 44000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.7403318603755906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.36 | consumed tokens: 360448000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T01:47:35 | step: 44100 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.7390913096023723e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.09 | consumed tokens: 361267200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T01:47:53 | step: 44200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.7378478484461084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.28 | consumed tokens: 362086400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T01:48:12 | step: 44300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.736601476906799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.41 | consumed tokens: 362905600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T01:48:31 | step: 44400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.7353521949844435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.22 | consumed tokens: 363724800.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T01:48:49 | step: 44500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.734100366476923e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.27 | consumed tokens: 364544000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T01:49:08 | step: 44600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.732845627586357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.23 | consumed tokens: 365363200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:49:26 | step: 44700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.731587978312746e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.02 | consumed tokens: 366182400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T01:49:45 | step: 44800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.730327782453969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.34 | consumed tokens: 367001600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T01:50:04 | step: 44900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.729064676212147e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.41 | consumed tokens: 367820800.0 | grad norm avg: 0.82 | grad norm last: 0.91 | 
2025-12-30T01:50:22 | step: 45000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.72779902338516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.56 | consumed tokens: 368640000.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:50:42 | step: 45100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.726530096377246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.8 | consumed tokens: 369459200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:51:01 | step: 45200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.725258986582048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.88 | consumed tokens: 370278400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:51:19 | step: 45300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.723984602605924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.12 | consumed tokens: 371097600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T01:51:38 | step: 45400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.722707672044635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.7 | consumed tokens: 371916800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:51:57 | step: 45500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.7214278311003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.44 | consumed tokens: 372736000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T01:52:15 | step: 45600 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.7201454435708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.25 | consumed tokens: 373555200.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T01:52:34 | step: 45700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.7188601456582546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.27 | consumed tokens: 374374400.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:52:52 | step: 45800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.717572301160544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.89 | consumed tokens: 375193600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:53:11 | step: 45900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.716281182481907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.75 | consumed tokens: 376012800.0 | grad norm avg: 0.82 | grad norm last: 0.75 | 
2025-12-30T01:53:30 | step: 46000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.714987881015986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.83 | consumed tokens: 376832000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T01:53:48 | step: 46100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.713691305369139e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 4.0 | consumed tokens: 377651200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:54:07 | step: 46200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.712392183137126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.97 | consumed tokens: 378470400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T01:54:25 | step: 46300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.711090514319949e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.17 | consumed tokens: 379289600.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T01:54:44 | step: 46400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.709785935119726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.16 | consumed tokens: 380108800.0 | grad norm avg: 0.81 | grad norm last: 0.73 | 
2025-12-30T01:55:02 | step: 46500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.708478445536457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.45 | consumed tokens: 380928000.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T01:55:21 | step: 46600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.707168409368023e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.83 | consumed tokens: 381747200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:55:39 | step: 46700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.705855462816544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.09 | consumed tokens: 382566400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:55:58 | step: 46800 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.704539605882019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.06 | consumed tokens: 383385600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:56:17 | step: 46900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.7032215661602095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.97 | consumed tokens: 384204800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T01:56:35 | step: 47000 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.701900252257474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.42 | consumed tokens: 385024000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T01:56:54 | step: 47100 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.700576391769573e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.44 | consumed tokens: 385843200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T01:57:13 | step: 47200 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.699249620898627e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.33 | consumed tokens: 386662400.0 | grad norm avg: 0.82 | grad norm last: 0.75 | 
2025-12-30T01:57:31 | step: 47300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.6979203034425154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.16 | consumed tokens: 387481600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:57:50 | step: 47400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.6965880756033584e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.95 | consumed tokens: 388300800.0 | grad norm avg: 0.81 | grad norm last: 0.71 | 
2025-12-30T01:58:08 | step: 47500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.6952533011790365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.33 | consumed tokens: 389120000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:58:27 | step: 47600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.693915616371669e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.84 | consumed tokens: 389939200.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T01:58:46 | step: 47700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.692575384979136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.39 | consumed tokens: 390758400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T01:59:04 | step: 47800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.691232243203558e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.36 | consumed tokens: 391577600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T01:59:23 | step: 47900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.689886554842815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.5 | consumed tokens: 392396800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T01:59:41 | step: 48000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.688537956099026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.72 | consumed tokens: 393216000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T02:00:00 | step: 48100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.687186810770072e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.23 | consumed tokens: 394035200.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T02:00:18 | step: 48200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.6858327550580725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 2.58 | consumed tokens: 394854400.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T02:00:37 | step: 48300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.684476152760908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.44 | consumed tokens: 395673600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:00:56 | step: 48400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.683116640080698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.23 | consumed tokens: 396492800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:01:15 | step: 48500 | train samples/s: 92.8 | train mfu (16-bit): -1.0 | lr mean: 4.681754217017442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.09 | consumed tokens: 397312000.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T02:01:33 | step: 48600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.680389611166902e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.48 | consumed tokens: 398131200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:01:51 | step: 48700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.6790217311354354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.08 | consumed tokens: 398950400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:02:10 | step: 48800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.677651668316685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.09 | consumed tokens: 399769600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:02:29 | step: 48900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.6762783313170075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.5 | consumed tokens: 400588800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:02:47 | step: 49000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.674902811530046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.45 | consumed tokens: 401408000.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T02:03:05 | step: 49100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.6735240175621584e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.45 | consumed tokens: 402227200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:03:24 | step: 49200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.6721430408069864e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.66 | consumed tokens: 403046400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T02:03:42 | step: 49300 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.670759153668769e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.02 | consumed tokens: 403865600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T02:04:01 | step: 49400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.6693723561475053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.8 | consumed tokens: 404684800.0 | grad norm avg: 0.83 | grad norm last: 0.95 | 
2025-12-30T02:04:19 | step: 49500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.667983012041077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.62 | consumed tokens: 405504000.0 | grad norm avg: 0.81 | grad norm last: 0.94 | 
2025-12-30T02:04:38 | step: 49600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.666591121349484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.19 | consumed tokens: 406323200.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:04:56 | step: 49700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.665196320274845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.14 | consumed tokens: 407142400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:05:15 | step: 49800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.663798972615041e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.8 | consumed tokens: 407961600.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T02:05:33 | step: 49900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.662398714572191e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.89 | consumed tokens: 408780800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T02:05:51 | step: 50000 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.660995909944177e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.97 | consumed tokens: 409600000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T02:06:12 | step: 50100 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.659590558730997e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.78 | consumed tokens: 410419200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:06:30 | step: 50200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.658182297134772e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.44 | consumed tokens: 411238400.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T02:06:49 | step: 50300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.656771488953382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.52 | consumed tokens: 412057600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:07:07 | step: 50400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.655357770388946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.88 | consumed tokens: 412876800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T02:07:25 | step: 50500 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.653941505239345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.77 | consumed tokens: 413696000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:07:44 | step: 50600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.6525223297066987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.3 | consumed tokens: 414515200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T02:08:03 | step: 50700 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.651100607588887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.42 | consumed tokens: 415334400.0 | grad norm avg: 0.81 | grad norm last: 0.73 | 
2025-12-30T02:08:21 | step: 50800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.649676338885911e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.58 | consumed tokens: 416153600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:08:40 | step: 50900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.6482495235977694e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.45 | consumed tokens: 416972800.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T02:08:58 | step: 51000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.6468197979265824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.94 | consumed tokens: 417792000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:09:17 | step: 51100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.64538716187235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.39 | consumed tokens: 418611200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:09:35 | step: 51200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.643952343030833e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 4.19 | consumed tokens: 419430400.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T02:09:54 | step: 51300 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.64251461380627e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.22 | consumed tokens: 420249600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:10:12 | step: 51400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.641073974198662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.59 | consumed tokens: 421068800.0 | grad norm avg: 0.82 | grad norm last: 0.73 | 
2025-12-30T02:10:31 | step: 51500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.6396307880058885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.47 | consumed tokens: 421888000.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T02:10:50 | step: 51600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.63818505522795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.91 | consumed tokens: 422707200.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T02:11:08 | step: 51700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.636736775864847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.28 | consumed tokens: 423526400.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T02:11:27 | step: 51800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.635285586118698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.59 | consumed tokens: 424345600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T02:11:45 | step: 51900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.633831849787384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.59 | consumed tokens: 425164800.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T02:12:04 | step: 52000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.6323755668709055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.48 | consumed tokens: 425984000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:12:22 | step: 52100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.630916373571381e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.03 | consumed tokens: 426803200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:12:41 | step: 52200 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.6294546336866915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.81 | consumed tokens: 427622400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:12:59 | step: 52300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.627990347216837e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.58 | consumed tokens: 428441600.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:13:18 | step: 52400 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 4.626523150363937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.59 | consumed tokens: 429260800.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T02:13:37 | step: 52500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.625053406925872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.52 | consumed tokens: 430080000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:13:55 | step: 52600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.623581116902642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.27 | consumed tokens: 430899200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T02:14:14 | step: 52700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.622106280294247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.39 | consumed tokens: 431718400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:14:33 | step: 52800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.620628533302806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.53 | consumed tokens: 432537600.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T02:14:51 | step: 52900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.619148239726201e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.69 | consumed tokens: 433356800.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T02:15:10 | step: 53000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.61766539956443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.88 | consumed tokens: 434176000.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T02:15:28 | step: 53100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.616179649019614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.03 | consumed tokens: 434995200.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T02:15:47 | step: 53200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.6146913518896326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.16 | consumed tokens: 435814400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:16:05 | step: 53300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.6132005081744865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.52 | consumed tokens: 436633600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:16:24 | step: 53400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.611707117874175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.33 | consumed tokens: 437452800.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T02:16:42 | step: 53500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.6102108171908185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.55 | consumed tokens: 438272000.0 | grad norm avg: 0.82 | grad norm last: 0.91 | 
2025-12-30T02:17:01 | step: 53600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.608711969922297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.7 | consumed tokens: 439091200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:17:19 | step: 53700 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.60721057606861e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.56 | consumed tokens: 439910400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:17:38 | step: 53800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.605706635629758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 2.81 | consumed tokens: 440729600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:17:56 | step: 53900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.6042001486057416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.12 | consumed tokens: 441548800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:18:15 | step: 54000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.602690751198679e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.94 | consumed tokens: 442368000.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T02:18:33 | step: 54100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.601178807206452e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.23 | consumed tokens: 443187200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T02:18:52 | step: 54200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.5996643166290596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.53 | consumed tokens: 444006400.0 | grad norm avg: 0.8 | grad norm last: 0.93 | 
2025-12-30T02:19:10 | step: 54300 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.5981472794665024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.7 | consumed tokens: 444825600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:19:29 | step: 54400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.5966273319208995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.02 | consumed tokens: 445644800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T02:19:47 | step: 54500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.595105201588012e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.66 | consumed tokens: 446464000.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T02:20:06 | step: 54600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.5935801608720794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.41 | consumed tokens: 447283200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:20:24 | step: 54700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.5920525735709816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.97 | consumed tokens: 448102400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T02:20:43 | step: 54800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.590522075886838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.75 | consumed tokens: 448921600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:21:02 | step: 54900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.5889893954154104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.22 | consumed tokens: 449740800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T02:21:21 | step: 55000 | train samples/s: 92.3 | train mfu (16-bit): -1.0 | lr mean: 4.587453804560937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.81 | consumed tokens: 450560000.0 | grad norm avg: 0.83 | grad norm last: 0.72 | 
2025-12-30T02:21:41 | step: 55100 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.585916030919179e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.5 | consumed tokens: 451379200.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:21:59 | step: 55200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.584375346894376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.12 | consumed tokens: 452198400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T02:22:18 | step: 55300 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.582832116284408e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.66 | consumed tokens: 453017600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:22:36 | step: 55400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.5812863390892744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.64 | consumed tokens: 453836800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T02:22:55 | step: 55500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.5797376515110955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.89 | consumed tokens: 454656000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:23:13 | step: 55600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.578186781145632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.86 | consumed tokens: 455475200.0 | grad norm avg: 0.83 | grad norm last: 0.76 | 
2025-12-30T02:23:32 | step: 55700 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.5766330003971234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 4.16 | consumed tokens: 456294400.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T02:23:51 | step: 55800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.57507703686133e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.61 | consumed tokens: 457113600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:24:09 | step: 55900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.5735181629424915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.88 | consumed tokens: 457932800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:24:28 | step: 56000 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.571956742438488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.33 | consumed tokens: 458752000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:24:47 | step: 56100 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 4.570392775349319e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.64 | consumed tokens: 459571200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T02:25:05 | step: 56200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.568826261674985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.67 | consumed tokens: 460390400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:25:24 | step: 56300 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.567256837617606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.48 | consumed tokens: 461209600.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T02:25:43 | step: 56400 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.565685230772942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.48 | consumed tokens: 462028800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T02:26:02 | step: 56500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.564111077343114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.52 | consumed tokens: 462848000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:26:20 | step: 56600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.5625340135302395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.55 | consumed tokens: 463667200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T02:26:39 | step: 56700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.560954766930081e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.06 | consumed tokens: 464486400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:26:57 | step: 56800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.559372609946877e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.09 | consumed tokens: 465305600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T02:27:16 | step: 56900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.5577879063785076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.77 | consumed tokens: 466124800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T02:27:34 | step: 57000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.556201020022854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.27 | consumed tokens: 466944000.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T02:27:53 | step: 57100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.554611223284155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.81 | consumed tokens: 467763200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T02:28:11 | step: 57200 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.553018879960291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.11 | consumed tokens: 468582400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:28:29 | step: 57300 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.551423990051262e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.14 | consumed tokens: 469401600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:28:48 | step: 57400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.549826553557068e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.17 | consumed tokens: 470220800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:29:07 | step: 57500 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.548226570477709e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.61 | consumed tokens: 471040000.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:29:25 | step: 57600 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.546624040813185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.97 | consumed tokens: 471859200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:29:44 | step: 57700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.5450189645634964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.3 | consumed tokens: 472678400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:30:02 | step: 57800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.5434113417286426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 4.19 | consumed tokens: 473497600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:30:20 | step: 57900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.541801172308624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.16 | consumed tokens: 474316800.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T02:30:39 | step: 58000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.54018845630344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.83 | consumed tokens: 475136000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T02:30:58 | step: 58100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.538573193713091e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.0 | consumed tokens: 475955200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:31:16 | step: 58200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.5369553845375776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.31 | consumed tokens: 476774400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T02:31:35 | step: 58300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.535335028776899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.17 | consumed tokens: 477593600.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T02:31:53 | step: 58400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.5337121264310554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.5 | consumed tokens: 478412800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T02:32:12 | step: 58500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.532086677500047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.08 | consumed tokens: 479232000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:32:30 | step: 58600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.530459045781754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.56 | consumed tokens: 480051200.0 | grad norm avg: 0.82 | grad norm last: 0.75 | 
2025-12-30T02:32:49 | step: 58700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.5288285036804155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.52 | consumed tokens: 480870400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:33:07 | step: 58800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.527195414993912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.19 | consumed tokens: 481689600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:33:26 | step: 58900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.5255597797222435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.36 | consumed tokens: 482508800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T02:33:44 | step: 59000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.52392159786541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 483328000.0 | grad norm avg: 0.82 | grad norm last: 1.0 | 
2025-12-30T02:34:03 | step: 59100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.5222812332212925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.97 | consumed tokens: 484147200.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T02:34:21 | step: 59200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.520637958194129e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 2.92 | consumed tokens: 484966400.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:34:39 | step: 59300 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.518992136581801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.78 | consumed tokens: 485785600.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:34:58 | step: 59400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.517344132182188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.66 | consumed tokens: 486604800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T02:35:16 | step: 59500 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.515693581197411e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.59 | consumed tokens: 487424000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:35:34 | step: 59600 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.5140401198295876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 4.41 | consumed tokens: 488243200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:35:53 | step: 59700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.51238447567448e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.22 | consumed tokens: 489062400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T02:36:11 | step: 59800 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.510726284934208e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.66 | consumed tokens: 489881600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T02:36:29 | step: 59900 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.5090655476087704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.19 | consumed tokens: 490700800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T02:36:48 | step: 60000 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.507402263698168e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.28 | consumed tokens: 491520000.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T02:37:08 | step: 60100 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.505736433202401e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.27 | consumed tokens: 492339200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:37:26 | step: 60200 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 4.504068419919349e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.8 | consumed tokens: 493158400.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T02:37:45 | step: 60300 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.502397496253252e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.03 | consumed tokens: 493977600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:38:04 | step: 60400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.5007243897998706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.16 | consumed tokens: 494796800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T02:38:22 | step: 60500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.499048736761324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.67 | consumed tokens: 495616000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:38:41 | step: 60600 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.497370173339732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.34 | consumed tokens: 496435200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:38:59 | step: 60700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.495689790928736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.12 | consumed tokens: 497254400.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T02:39:18 | step: 60800 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 4.494006498134695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.58 | consumed tokens: 498073600.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T02:39:37 | step: 60900 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.492320658755489e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 4.81 | consumed tokens: 498892800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:39:55 | step: 61000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.490632636588998e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.25 | consumed tokens: 499712000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:40:14 | step: 61100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.488941704039462e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.56 | consumed tokens: 500531200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:40:33 | step: 61200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.4872485887026414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.89 | consumed tokens: 501350400.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:40:51 | step: 61300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.485552926780656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.55 | consumed tokens: 502169600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T02:41:10 | step: 61400 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.483855082071386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.03 | consumed tokens: 502988800.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:41:28 | step: 61500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.482154326979071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.78 | consumed tokens: 503808000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:41:47 | step: 61600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.480451389099471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.36 | consumed tokens: 504627200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T02:42:06 | step: 61700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.478745904634707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.06 | consumed tokens: 505446400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T02:42:24 | step: 61800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.477037873584777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.42 | consumed tokens: 506265600.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T02:42:43 | step: 61900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.4753272959496826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.36 | consumed tokens: 507084800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:43:01 | step: 62000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.473614171729423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.06 | consumed tokens: 507904000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:43:20 | step: 62100 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.4718988647218794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.42 | consumed tokens: 508723200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:43:38 | step: 62200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.4701810111291707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.23 | consumed tokens: 509542400.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T02:43:56 | step: 62300 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.468460610951297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.34 | consumed tokens: 510361600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T02:44:15 | step: 62400 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.466738027986139e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.06 | consumed tokens: 511180800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:44:33 | step: 62500 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.465012898435816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.95 | consumed tokens: 512000000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T02:44:51 | step: 62600 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.463285222300328e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.59 | consumed tokens: 512819200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:45:10 | step: 62700 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 4.4615549995796755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.59 | consumed tokens: 513638400.0 | grad norm avg: 0.82 | grad norm last: 0.94 | 
2025-12-30T02:45:28 | step: 62800 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.459822230273858e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.0 | consumed tokens: 514457600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:45:46 | step: 62900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.458087278180756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.44 | consumed tokens: 515276800.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T02:46:05 | step: 63000 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.456349779502489e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.98 | consumed tokens: 516096000.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T02:46:23 | step: 63100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.4546100980369374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.88 | consumed tokens: 516915200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:46:42 | step: 63200 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.4528675061883405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.88 | consumed tokens: 517734400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:47:00 | step: 63300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.451122731552459e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.06 | consumed tokens: 518553600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T02:47:19 | step: 63400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.449375774129294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 4.22 | consumed tokens: 519372800.0 | grad norm avg: 0.82 | grad norm last: 0.91 | 
2025-12-30T02:47:37 | step: 63500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.447625906323083e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.53 | consumed tokens: 520192000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T02:47:56 | step: 63600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.4458738557295874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.12 | consumed tokens: 521011200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:48:14 | step: 63700 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.444119622348808e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.22 | consumed tokens: 521830400.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T02:48:33 | step: 63800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.4423624785849825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.98 | consumed tokens: 522649600.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T02:48:52 | step: 63900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.440603152033873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.25 | consumed tokens: 523468800.0 | grad norm avg: 0.82 | grad norm last: 0.72 | 
2025-12-30T02:49:10 | step: 64000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.4388412788975984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 4.16 | consumed tokens: 524288000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:49:29 | step: 64100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.4370772229740396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.27 | consumed tokens: 525107200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T02:49:47 | step: 64200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.435310620465316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.48 | consumed tokens: 525926400.0 | grad norm avg: 0.82 | grad norm last: 0.75 | 
2025-12-30T02:50:06 | step: 64300 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.433541471371427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.16 | consumed tokens: 526745600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:50:24 | step: 64400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.431770139490254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.0 | consumed tokens: 527564800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:50:43 | step: 64500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.429996261023916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.41 | consumed tokens: 528384000.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T02:51:02 | step: 64600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.428220199770294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.95 | consumed tokens: 529203200.0 | grad norm avg: 0.83 | grad norm last: 0.76 | 
2025-12-30T02:51:20 | step: 64700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.426441591931507e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.8 | consumed tokens: 530022400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T02:51:39 | step: 64800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.424660437507555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.08 | consumed tokens: 530841600.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T02:51:57 | step: 64900 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.4228771002963185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.72 | consumed tokens: 531660800.0 | grad norm avg: 0.83 | grad norm last: 0.91 | 
2025-12-30T02:52:16 | step: 65000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.421091216499917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.28 | consumed tokens: 532480000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T02:52:36 | step: 65100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.419302786118351e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.09 | consumed tokens: 533299200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T02:52:54 | step: 65200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.4175121729495004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.22 | consumed tokens: 534118400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T02:53:13 | step: 65300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.415719013195485e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.56 | consumed tokens: 534937600.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T02:53:31 | step: 65400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.413923670654185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.61 | consumed tokens: 535756800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T02:53:50 | step: 65500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.4121257815277204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.41 | consumed tokens: 536576000.0 | grad norm avg: 0.82 | grad norm last: 1.0 | 
2025-12-30T02:54:08 | step: 65600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.4103257096139714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.67 | consumed tokens: 537395200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T02:54:26 | step: 65700 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.4085230911150575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.02 | consumed tokens: 538214400.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T02:54:45 | step: 65800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.406718289828859e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.3 | consumed tokens: 539033600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:55:04 | step: 65900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.404910941957496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.25 | consumed tokens: 539852800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T02:55:22 | step: 66000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.403101047500968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.16 | consumed tokens: 540672000.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T02:55:41 | step: 66100 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.4012889702571556e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.91 | consumed tokens: 541491200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T02:56:00 | step: 66200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.399474346428178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.41 | consumed tokens: 542310400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T02:56:18 | step: 66300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.3976575398119166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.86 | consumed tokens: 543129600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T02:56:37 | step: 66400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.395838550408371e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.42 | consumed tokens: 543948800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T02:56:55 | step: 66500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.39401701441966e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.94 | consumed tokens: 544768000.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T02:57:14 | step: 66600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.392192931845784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.89 | consumed tokens: 545587200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T02:57:32 | step: 66700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.390366666484624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.41 | consumed tokens: 546406400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T02:57:51 | step: 66800 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 4.388537854538299e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.81 | consumed tokens: 547225600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T02:58:10 | step: 66900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.38670685980469e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.95 | consumed tokens: 548044800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T02:58:28 | step: 67000 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.3848736822837964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.77 | consumed tokens: 548864000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T02:58:47 | step: 67100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.383037958177738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.45 | consumed tokens: 549683200.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T02:59:06 | step: 67200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.3811996874865144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.53 | consumed tokens: 550502400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T02:59:24 | step: 67300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.3793595978058875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.3 | consumed tokens: 551321600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T02:59:43 | step: 67400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.377516597742215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.08 | consumed tokens: 552140800.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:00:01 | step: 67500 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 4.375671414891258e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.16 | consumed tokens: 552960000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:00:20 | step: 67600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.373824049253017e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 2.58 | consumed tokens: 553779200.0 | grad norm avg: 0.82 | grad norm last: 0.7 | 
2025-12-30T03:00:39 | step: 67700 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.371974500827491e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.52 | consumed tokens: 554598400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:00:57 | step: 67800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.370122405816801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.34 | consumed tokens: 555417600.0 | grad norm avg: 0.83 | grad norm last: 0.93 | 
2025-12-30T03:01:16 | step: 67900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.3682677642209455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.44 | consumed tokens: 556236800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:01:35 | step: 68000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.366410939837806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.7 | consumed tokens: 557056000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:01:53 | step: 68100 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.364551932667382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.55 | consumed tokens: 557875200.0 | grad norm avg: 0.83 | grad norm last: 0.74 | 
2025-12-30T03:02:12 | step: 68200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.362690378911793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.64 | consumed tokens: 558694400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:02:30 | step: 68300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.36082664236892e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.75 | consumed tokens: 559513600.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T03:02:48 | step: 68400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.358960723038763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.09 | consumed tokens: 560332800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:03:07 | step: 68500 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.3570922571234405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.78 | consumed tokens: 561152000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:03:26 | step: 68600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.355221608420834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.56 | consumed tokens: 561971200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T03:03:44 | step: 68700 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.3533484131330624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.36 | consumed tokens: 562790400.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T03:04:03 | step: 68800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.3514730350580066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.47 | consumed tokens: 563609600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:04:22 | step: 68900 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.3495954741956666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.36 | consumed tokens: 564428800.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:04:40 | step: 69000 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.347715730546042e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.64 | consumed tokens: 565248000.0 | grad norm avg: 0.83 | grad norm last: 0.76 | 
2025-12-30T03:04:59 | step: 69100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.345833440311253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.22 | consumed tokens: 566067200.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:05:17 | step: 69200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.3439489672891796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.3 | consumed tokens: 566886400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T03:05:36 | step: 69300 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.342061947681941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.78 | consumed tokens: 567705600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:05:55 | step: 69400 | train samples/s: 92.3 | train mfu (16-bit): -1.0 | lr mean: 4.3401727452874184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.22 | consumed tokens: 568524800.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T03:06:14 | step: 69500 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.3382813601056114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.2 | consumed tokens: 569344000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T03:06:32 | step: 69600 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.3363874283386394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 570163200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:06:51 | step: 69700 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.334491677582264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.06 | consumed tokens: 570982400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:07:10 | step: 69800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.3325933802407235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.83 | consumed tokens: 571801600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:07:28 | step: 69900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.330692536314018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.05 | consumed tokens: 572620800.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T03:07:47 | step: 70000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.328789873397909e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.56 | consumed tokens: 573440000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:08:07 | step: 70100 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.326884663896635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.95 | consumed tokens: 574259200.0 | grad norm avg: 0.83 | grad norm last: 0.91 | 
2025-12-30T03:08:26 | step: 70200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.324976907810196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.34 | consumed tokens: 575078400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:08:44 | step: 70300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.323067332734354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.5 | consumed tokens: 575897600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:09:03 | step: 70400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.3211552110733464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.22 | consumed tokens: 576716800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:09:21 | step: 70500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.319240906625055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.67 | consumed tokens: 577536000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:09:40 | step: 70600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.317324419389479e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.11 | consumed tokens: 578355200.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T03:09:59 | step: 70700 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 4.315405749366619e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.92 | consumed tokens: 579174400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:10:17 | step: 70800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.3134845327585936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.28 | consumed tokens: 579993600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:10:36 | step: 70900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.311561133363284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.06 | consumed tokens: 580812800.0 | grad norm avg: 0.83 | grad norm last: 0.93 | 
2025-12-30T03:10:54 | step: 71000 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.3096355511806905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.56 | consumed tokens: 581632000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T03:11:13 | step: 71100 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.3077077862108126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.48 | consumed tokens: 582451200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:11:32 | step: 71200 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.3057778384536505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.08 | consumed tokens: 583270400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:11:50 | step: 71300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.3038453441113234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.11 | consumed tokens: 584089600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T03:12:09 | step: 71400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.301910666981712e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.25 | consumed tokens: 584908800.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T03:12:28 | step: 71500 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.2999738070648164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.27 | consumed tokens: 585728000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T03:12:46 | step: 71600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.2980347643606365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.28 | consumed tokens: 586547200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:13:05 | step: 71700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.2960931750712916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.77 | consumed tokens: 587366400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:13:23 | step: 71800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.294149766792543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.91 | consumed tokens: 588185600.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T03:13:42 | step: 71900 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.29220381192863e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.72 | consumed tokens: 589004800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:14:01 | step: 72000 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 4.290255674277432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.91 | consumed tokens: 589824000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:14:20 | step: 72100 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.288305717636831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.31 | consumed tokens: 590643200.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:14:38 | step: 72200 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.286352850613184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.12 | consumed tokens: 591462400.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T03:14:57 | step: 72300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.284398164600134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.03 | consumed tokens: 592281600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:15:15 | step: 72400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.282441295799799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.3 | consumed tokens: 593100800.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T03:15:34 | step: 72500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.2804818804143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.34 | consumed tokens: 593920000.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:15:52 | step: 72600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.2785206460393965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.33 | consumed tokens: 594739200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:16:11 | step: 72700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.2765568650793284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 595558400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:16:30 | step: 72800 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.274591265129857e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.16 | consumed tokens: 596377600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:16:48 | step: 72900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.27262311859522e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.38 | consumed tokens: 597196800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:17:06 | step: 73000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.270652789273299e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.53 | consumed tokens: 598016000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T03:17:25 | step: 73100 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.268680277164094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.59 | consumed tokens: 598835200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:17:44 | step: 73200 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.266705582267605e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.09 | consumed tokens: 599654400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:18:03 | step: 73300 | train samples/s: 92.8 | train mfu (16-bit): -1.0 | lr mean: 4.264728704583831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.38 | consumed tokens: 600473600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:18:21 | step: 73400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.262749644112773e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.22 | consumed tokens: 601292800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:18:40 | step: 73500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.2607680370565504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.84 | consumed tokens: 602112000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:18:59 | step: 73600 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.258784611010924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.33 | consumed tokens: 602931200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:19:17 | step: 73700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.256799002178013e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.5 | consumed tokens: 603750400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:19:36 | step: 73800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.2548112105578184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.78 | consumed tokens: 604569600.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:19:54 | step: 73900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.2528208723524585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.5 | consumed tokens: 605388800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:20:13 | step: 74000 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.250828715157695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.94 | consumed tokens: 606208000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:20:32 | step: 74100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.2488343751756474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.98 | consumed tokens: 607027200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:20:50 | step: 74200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.246837488608435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.62 | consumed tokens: 607846400.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T03:21:09 | step: 74300 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.2448387830518186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.47 | consumed tokens: 608665600.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:21:27 | step: 74400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.242837894707918e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.3 | consumed tokens: 609484800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T03:21:46 | step: 74500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.2408348235767335e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 610304000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:22:05 | step: 74600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.238829205860384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.47 | consumed tokens: 611123200.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T03:22:23 | step: 74700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.2368217691546306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.94 | consumed tokens: 611942400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:22:42 | step: 74800 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.234812149661593e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.41 | consumed tokens: 612761600.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T03:23:00 | step: 74900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.2328003473812714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 613580800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:23:19 | step: 75000 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.2307863623136654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.75 | consumed tokens: 614400000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:23:39 | step: 75100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.228770194458775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 4.44 | consumed tokens: 615219200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:23:57 | step: 75200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.226751843816601e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.06 | consumed tokens: 616038400.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:24:16 | step: 75300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.224731310387142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.5 | consumed tokens: 616857600.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:24:34 | step: 75400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.22270895796828e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.72 | consumed tokens: 617676800.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T03:24:53 | step: 75500 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.2206840589642525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.55 | consumed tokens: 618496000.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T03:25:12 | step: 75600 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.218656977172941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.77 | consumed tokens: 619315200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:25:31 | step: 75700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.216628076392226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.7 | consumed tokens: 620134400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:25:49 | step: 75800 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.2145969928242266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.08 | consumed tokens: 620953600.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T03:26:08 | step: 75900 | train samples/s: 92.5 | train mfu (16-bit): -1.0 | lr mean: 4.2125633626710624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.66 | consumed tokens: 621772800.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T03:26:27 | step: 76000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.2105279135284945e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.41 | consumed tokens: 622592000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:26:45 | step: 76100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.2084902815986425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.62 | consumed tokens: 623411200.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T03:27:04 | step: 76200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.206450830679387e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.81 | consumed tokens: 624230400.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:27:22 | step: 76300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.204408833174966e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.83 | consumed tokens: 625049600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:27:41 | step: 76400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.202365016681142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.53 | consumed tokens: 625868800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:27:59 | step: 76500 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.200318653602153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.89 | consumed tokens: 626688000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T03:28:18 | step: 76600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.1982704715337604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.81 | consumed tokens: 627507200.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T03:28:37 | step: 76700 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.1962201066780835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.41 | consumed tokens: 628326400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T03:28:55 | step: 76800 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.1941675590351224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.64 | consumed tokens: 629145600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:29:14 | step: 76900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.192113192402758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.17 | consumed tokens: 629964800.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:29:33 | step: 77000 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.190056279185228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.25 | consumed tokens: 630784000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:29:51 | step: 77100 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.187997546978295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.0 | consumed tokens: 631603200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:30:10 | step: 77200 | train samples/s: 92.5 | train mfu (16-bit): -1.0 | lr mean: 4.1859366319840774e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.27 | consumed tokens: 632422400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:30:29 | step: 77300 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 4.183873534202576e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.97 | consumed tokens: 633241600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:30:48 | step: 77400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.1818086174316704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.36 | consumed tokens: 634060800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:31:06 | step: 77500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.179741517873481e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.38 | consumed tokens: 634880000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:31:25 | step: 77600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.177672235528007e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 4.0 | consumed tokens: 635699200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:31:43 | step: 77700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.175600770395249e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.64 | consumed tokens: 636518400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:32:02 | step: 77800 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 4.173527122475207e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.19 | consumed tokens: 637337600.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T03:32:21 | step: 77900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.171451655565761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.28 | consumed tokens: 638156800.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:32:39 | step: 78000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.169374005869031e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 638976000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:32:58 | step: 78100 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 4.1672941733850166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.89 | consumed tokens: 639795200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:33:17 | step: 78200 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 4.165212521911599e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.91 | consumed tokens: 640614400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:33:36 | step: 78300 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 4.163128323853016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.89 | consumed tokens: 641433600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:33:54 | step: 78400 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.16104267060291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.17 | consumed tokens: 642252800.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2025-12-30T03:34:13 | step: 78500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.1589544707676396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.08 | consumed tokens: 643072000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T03:34:32 | step: 78600 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.1568644519429654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.81 | consumed tokens: 643891200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T03:34:51 | step: 78700 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.154772250331007e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.25 | consumed tokens: 644710400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:35:09 | step: 78800 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.152677865931764e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.05 | consumed tokens: 645529600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:35:28 | step: 78900 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.150581662543118e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.0 | consumed tokens: 646348800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:35:47 | step: 79000 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.1484832763671875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 647168000.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T03:36:05 | step: 79100 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.146382707403973e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.7 | consumed tokens: 647987200.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:36:24 | step: 79200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.1442803194513544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.7 | consumed tokens: 648806400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:36:43 | step: 79300 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.142175748711452e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.42 | consumed tokens: 649625600.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T03:37:01 | step: 79400 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 4.140069358982146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.59 | consumed tokens: 650444800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:37:20 | step: 79500 | train samples/s: 92.6 | train mfu (16-bit): -1.0 | lr mean: 4.1379607864655554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.64 | consumed tokens: 651264000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:37:39 | step: 79600 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.135850031161681e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.38 | consumed tokens: 652083200.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T03:37:58 | step: 79700 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.1337374568684027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.88 | consumed tokens: 652902400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T03:38:16 | step: 79800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.13162269978784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 4.06 | consumed tokens: 653721600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:38:35 | step: 79900 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 4.1295057599199936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.23 | consumed tokens: 654540800.0 | grad norm avg: 0.83 | grad norm last: 0.73 | 
2025-12-30T03:38:53 | step: 80000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.1273870010627434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.83 | consumed tokens: 655360000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:39:14 | step: 80100 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.125266059418209e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.58 | consumed tokens: 656179200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:39:32 | step: 80200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.123143298784271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.61 | consumed tokens: 656998400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:39:51 | step: 80300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.1210183553630486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.3 | consumed tokens: 657817600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:40:09 | step: 80400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.118891592952423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.83 | consumed tokens: 658636800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T03:40:28 | step: 80500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.116762647754513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 4.0 | consumed tokens: 659456000.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T03:40:46 | step: 80600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.1146315197693184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.75 | consumed tokens: 660275200.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T03:41:04 | step: 80700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.1124985727947205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.88 | consumed tokens: 661094400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:41:23 | step: 80800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.110363806830719e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.39 | consumed tokens: 661913600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T03:41:41 | step: 80900 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.1082268580794334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.5 | consumed tokens: 662732800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:42:00 | step: 81000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.1060877265408635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 663552000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:42:18 | step: 81100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 4.10394677601289e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.66 | consumed tokens: 664371200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T03:42:37 | step: 81200 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 4.101803642697632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 665190400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:42:55 | step: 81300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.099658690392971e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.66 | consumed tokens: 666009600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:43:14 | step: 81400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.097511919098906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.56 | consumed tokens: 666828800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:43:33 | step: 81500 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.095362965017557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.91 | consumed tokens: 667648000.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T03:43:51 | step: 81600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.093211828148924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 668467200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:44:10 | step: 81700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.091058872290887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.69 | consumed tokens: 669286400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T03:44:28 | step: 81800 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 4.0889040974434465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.45 | consumed tokens: 670105600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:44:47 | step: 81900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.086747139808722e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 670924800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:45:06 | step: 82000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.084587999386713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.59 | consumed tokens: 671744000.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:45:24 | step: 82100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.082427403773181e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.59 | consumed tokens: 672563200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:45:42 | step: 82200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.0802642615744844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.86 | consumed tokens: 673382400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:46:01 | step: 82300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.078099664184265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.0 | consumed tokens: 674201600.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T03:46:19 | step: 82400 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.075932884006761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.42 | consumed tokens: 675020800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T03:46:38 | step: 82500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.073763921041973e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.7 | consumed tokens: 675840000.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T03:46:56 | step: 82600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.071593502885662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 4.09 | consumed tokens: 676659200.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:47:15 | step: 82700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.069420538144186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.36 | consumed tokens: 677478400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T03:47:33 | step: 82800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.0672461182111874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 678297600.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T03:47:52 | step: 82900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.0650695154909045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.53 | consumed tokens: 679116800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:48:10 | step: 83000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.062891093781218e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.17 | consumed tokens: 679936000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T03:48:29 | step: 83100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.060710489284247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.69 | consumed tokens: 680755200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:48:47 | step: 83200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.058528065797873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 681574400.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T03:49:06 | step: 83300 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.056343823322095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.2 | consumed tokens: 682393600.0 | grad norm avg: 0.84 | grad norm last: 0.94 | 
2025-12-30T03:49:25 | step: 83400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.054157398059033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.53 | consumed tokens: 683212800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T03:49:43 | step: 83500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.051969153806567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.3 | consumed tokens: 684032000.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T03:50:02 | step: 83600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.049778726766817e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.91 | consumed tokens: 684851200.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T03:50:20 | step: 83700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.0475868445355445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.0 | consumed tokens: 685670400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:50:39 | step: 83800 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.0453927795169875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.59 | consumed tokens: 686489600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T03:50:57 | step: 83900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.043196895509027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.95 | consumed tokens: 687308800.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T03:51:16 | step: 84000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.040998828713782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.14 | consumed tokens: 688128000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T03:51:34 | step: 84100 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 4.038798942929134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.31 | consumed tokens: 688947200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T03:51:53 | step: 84200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 4.036597238155082e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.59 | consumed tokens: 689766400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:52:12 | step: 84300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.0343937143916264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.41 | consumed tokens: 690585600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T03:52:30 | step: 84400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.032188007840887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.77 | consumed tokens: 691404800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:52:48 | step: 84500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.0299804823007435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.98 | consumed tokens: 692224000.0 | grad norm avg: 0.84 | grad norm last: 0.72 | 
2025-12-30T03:53:07 | step: 84600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.027771137771197e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.52 | consumed tokens: 693043200.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T03:53:25 | step: 84700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.025559974252246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.09 | consumed tokens: 693862400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:53:44 | step: 84800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.023346627946012e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.52 | consumed tokens: 694681600.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T03:54:03 | step: 84900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.0211314626503736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.77 | consumed tokens: 695500800.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T03:54:21 | step: 85000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.018914478365332e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.38 | consumed tokens: 696320000.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T03:54:41 | step: 85100 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.0166956750908867e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 697139200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T03:55:00 | step: 85200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.014474689029157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.77 | consumed tokens: 697958400.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T03:55:18 | step: 85300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.012252247775905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.77 | consumed tokens: 698777600.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T03:55:37 | step: 85400 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.010027623735368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.72 | consumed tokens: 699596800.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T03:55:55 | step: 85500 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.007801180705428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 2.19 | consumed tokens: 700416000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:56:14 | step: 85600 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 4.0055729186860844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.22 | consumed tokens: 701235200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:56:33 | step: 85700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.0033424738794565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.17 | consumed tokens: 702054400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T03:56:51 | step: 85800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.001110573881306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.23 | consumed tokens: 702873600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T03:57:10 | step: 85900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.998876491095871e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.31 | consumed tokens: 703692800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:57:28 | step: 86000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.996640589321032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.5 | consumed tokens: 704512000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:57:46 | step: 86100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 3.994403232354671e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.28 | consumed tokens: 705331200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T03:58:05 | step: 86200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.992163692601025e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.61 | consumed tokens: 706150400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T03:58:24 | step: 86300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.989921970060095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.69 | consumed tokens: 706969600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T03:58:43 | step: 86400 | train samples/s: 92.4 | train mfu (16-bit): -1.0 | lr mean: 3.9876787923276424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.53 | consumed tokens: 707788800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T03:59:01 | step: 86500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.985433795605786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.5 | consumed tokens: 708608000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T03:59:20 | step: 86600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.9831866160966456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.19 | consumed tokens: 709427200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T03:59:38 | step: 86700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.980937981395982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.17 | consumed tokens: 710246400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T03:59:57 | step: 86800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.9786871639080346e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.19 | consumed tokens: 711065600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:00:15 | step: 86900 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.976434891228564e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.52 | consumed tokens: 711884800.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T04:00:34 | step: 87000 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.9741804357618093e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.98 | consumed tokens: 712704000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T04:00:53 | step: 87100 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.971924161305651e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.05 | consumed tokens: 713523200.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T04:01:12 | step: 87200 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.969666067860089e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.91 | consumed tokens: 714342400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:01:30 | step: 87300 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.967406155425124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.88 | consumed tokens: 715161600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T04:01:49 | step: 87400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.965144424000755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.55 | consumed tokens: 715980800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T04:02:08 | step: 87500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.9628808735869825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.38 | consumed tokens: 716800000.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T04:02:26 | step: 87600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.960615867981687e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.11 | consumed tokens: 717619200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:02:45 | step: 87700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.9583486795891076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.27 | consumed tokens: 718438400.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T04:03:03 | step: 87800 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.9560796722071245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.42 | consumed tokens: 719257600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:03:22 | step: 87900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.953808845835738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.95 | consumed tokens: 720076800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T04:03:40 | step: 88000 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.951536200474948e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.42 | consumed tokens: 720896000.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T04:03:58 | step: 88100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.949261736124754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.41 | consumed tokens: 721715200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:04:17 | step: 88200 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.946985452785157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.38 | consumed tokens: 722534400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:04:36 | step: 88300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.944707350456156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.38 | consumed tokens: 723353600.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:04:55 | step: 88400 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.9424274291377515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.33 | consumed tokens: 724172800.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T04:05:13 | step: 88500 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.940146052627824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.14 | consumed tokens: 724992000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:05:32 | step: 88600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.937862493330613e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.95 | consumed tokens: 725811200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T04:05:50 | step: 88700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.9355774788418785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.69 | consumed tokens: 726630400.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:06:09 | step: 88800 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.93329028156586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.41 | consumed tokens: 727449600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:06:27 | step: 88900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.9310016290983185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.56 | consumed tokens: 728268800.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T04:06:46 | step: 89000 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.928710793843493e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.33 | consumed tokens: 729088000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T04:07:05 | step: 89100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.9264185033971444e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.34 | consumed tokens: 729907200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:07:23 | step: 89200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.9241243939613923e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.91 | consumed tokens: 730726400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:07:42 | step: 89300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.921828465536237e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.17 | consumed tokens: 731545600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:08:00 | step: 89400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.9195307181216776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 732364800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T04:08:18 | step: 89500 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 3.917231151717715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.89 | consumed tokens: 733184000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:08:37 | step: 89600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.9149301301222295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.09 | consumed tokens: 734003200.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:08:55 | step: 89700 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.91262692573946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 734822400.0 | grad norm avg: 0.85 | grad norm last: 1.04 | 
2025-12-30T04:09:14 | step: 89800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.910322266165167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.33 | consumed tokens: 735641600.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T04:09:32 | step: 89900 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 3.908015787601471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.38 | consumed tokens: 736460800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T04:09:50 | step: 90000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.905707490048371e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.55 | consumed tokens: 737280000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T04:10:10 | step: 90100 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 3.903397373505868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.06 | consumed tokens: 738099200.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T04:10:28 | step: 90200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.901085801771842e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.84 | consumed tokens: 738918400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T04:10:47 | step: 90300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.8987720472505316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.61 | consumed tokens: 739737600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:11:06 | step: 90400 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.8964568375376984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 740556800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:11:24 | step: 90500 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.894139808835462e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.89 | consumed tokens: 741376000.0 | grad norm avg: 0.83 | grad norm last: 0.74 | 
2025-12-30T04:11:43 | step: 90600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.891821324941702e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.92 | consumed tokens: 742195200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:12:02 | step: 90700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.8895006582606584e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.17 | consumed tokens: 743014400.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T04:12:20 | step: 90800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.887178536388092e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.75 | consumed tokens: 743833600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T04:12:39 | step: 90900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.8848545955261216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.55 | consumed tokens: 744652800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T04:12:57 | step: 91000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.882528835674748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.44 | consumed tokens: 745472000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T04:13:16 | step: 91100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.8802012568339705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.64 | consumed tokens: 746291200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:13:34 | step: 91200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.8778722228016704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.77 | consumed tokens: 747110400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T04:13:53 | step: 91300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.875541369779967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.03 | consumed tokens: 747929600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T04:14:11 | step: 91400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.87320906156674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.12 | consumed tokens: 748748800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:14:30 | step: 91500 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.8708745705662295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.92 | consumed tokens: 749568000.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T04:14:49 | step: 91600 | train samples/s: 92.6 | train mfu (16-bit): -1.0 | lr mean: 3.868538624374196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.22 | consumed tokens: 750387200.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T04:15:08 | step: 91700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.866200859192759e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.38 | consumed tokens: 751206400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T04:15:26 | step: 91800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.863861638819799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.92 | consumed tokens: 752025600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:15:44 | step: 91900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.861520599457435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.25 | consumed tokens: 752844800.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T04:16:03 | step: 92000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.859177741105668e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.36 | consumed tokens: 753664000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T04:16:21 | step: 92100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.8568334275623783e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.42 | consumed tokens: 754483200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:16:40 | step: 92200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.854486931231804e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.92 | consumed tokens: 755302400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:16:59 | step: 92300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.852139343507588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.11 | consumed tokens: 756121600.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T04:17:17 | step: 92400 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.8497895729960874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.75 | consumed tokens: 756940800.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:17:36 | step: 92500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.847438347293064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.55 | consumed tokens: 757760000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T04:17:54 | step: 92600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.845085666398518e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.58 | consumed tokens: 758579200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:18:13 | step: 92700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.842731166514568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.33 | consumed tokens: 759398400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:18:31 | step: 92800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.840374847641215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.53 | consumed tokens: 760217600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T04:18:50 | step: 92900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.838016709778458e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.8 | consumed tokens: 761036800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:19:08 | step: 93000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.835657116724178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 761856000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:19:27 | step: 93100 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 3.833296068478376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.81 | consumed tokens: 762675200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:19:45 | step: 93200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.8309332012431696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.25 | consumed tokens: 763494400.0 | grad norm avg: 0.84 | grad norm last: 0.74 | 
2025-12-30T04:20:04 | step: 93300 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.82856851501856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.62 | consumed tokens: 764313600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T04:20:22 | step: 93400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.8262023736024275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.81 | consumed tokens: 765132800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T04:20:41 | step: 93500 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.8238344131968915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.08 | consumed tokens: 765952000.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T04:20:59 | step: 93600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.821464633801952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.88 | consumed tokens: 766771200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:21:18 | step: 93700 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.8190933992154896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.88 | consumed tokens: 767590400.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T04:21:37 | step: 93800 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.8167207094375044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.03 | consumed tokens: 768409600.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:21:55 | step: 93900 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.8143462006701156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.03 | consumed tokens: 769228800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T04:22:14 | step: 94000 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.811970236711204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.69 | consumed tokens: 770048000.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:22:33 | step: 94100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.809592453762889e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.97 | consumed tokens: 770867200.0 | grad norm avg: 0.84 | grad norm last: 0.97 | 
2025-12-30T04:22:52 | step: 94200 | train samples/s: 92.3 | train mfu (16-bit): -1.0 | lr mean: 3.80721285182517e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.81 | consumed tokens: 771686400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:23:11 | step: 94300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.804831794695929e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.66 | consumed tokens: 772505600.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:23:29 | step: 94400 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.802449282375164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.09 | consumed tokens: 773324800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T04:23:48 | step: 94500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.8000649510649964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.11 | consumed tokens: 774144000.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T04:24:07 | step: 94600 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.797679164563306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.69 | consumed tokens: 774963200.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T04:24:25 | step: 94700 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.7952915590722114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 775782400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T04:24:44 | step: 94800 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.792902498389594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.16 | consumed tokens: 776601600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:25:03 | step: 94900 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.790511982515454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.09 | consumed tokens: 777420800.0 | grad norm avg: 0.84 | grad norm last: 0.96 | 
2025-12-30T04:25:22 | step: 95000 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.788119647651911e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.42 | consumed tokens: 778240000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:25:42 | step: 95100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.785725493798964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.45 | consumed tokens: 779059200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:26:00 | step: 95200 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.783329884754494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.7 | consumed tokens: 779878400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T04:26:19 | step: 95300 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.780932820518501e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.86 | consumed tokens: 780697600.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T04:26:38 | step: 95400 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.778533937293105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.02 | consumed tokens: 781516800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:26:57 | step: 95500 | train samples/s: 92.5 | train mfu (16-bit): -1.0 | lr mean: 3.776133598876186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.39 | consumed tokens: 782336000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:27:15 | step: 95600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.773731805267744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.97 | consumed tokens: 783155200.0 | grad norm avg: 0.84 | grad norm last: 0.97 | 
2025-12-30T04:27:34 | step: 95700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.771328192669898e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.45 | consumed tokens: 783974400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:27:52 | step: 95800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.76892312488053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.47 | consumed tokens: 784793600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T04:28:11 | step: 95900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.766516601899639e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.28 | consumed tokens: 785612800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:28:29 | step: 96000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.764108259929344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.77 | consumed tokens: 786432000.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T04:28:48 | step: 96100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.7616984627675265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.06 | consumed tokens: 787251200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:29:06 | step: 96200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.7592868466163054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.03 | consumed tokens: 788070400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T04:29:25 | step: 96300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.756874139071442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.31 | consumed tokens: 788889600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:29:44 | step: 96400 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.754459248739295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.59 | consumed tokens: 789708800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:30:02 | step: 96500 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.752043267013505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.94 | consumed tokens: 790528000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T04:30:21 | step: 96600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.749625466298312e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.44 | consumed tokens: 791347200.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T04:30:40 | step: 96700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.747206574189477e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.56 | consumed tokens: 792166400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:30:58 | step: 96800 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.744785499293357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.94 | consumed tokens: 792985600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:31:17 | step: 96900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.7423633330035955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.42 | consumed tokens: 793804800.0 | grad norm avg: 0.83 | grad norm last: 0.72 | 
2025-12-30T04:31:35 | step: 97000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.73993934772443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.72 | consumed tokens: 794624000.0 | grad norm avg: 0.84 | grad norm last: 0.74 | 
2025-12-30T04:31:54 | step: 97100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.737513907253742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.89 | consumed tokens: 795443200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:32:12 | step: 97200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.735087011591531e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.16 | consumed tokens: 796262400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T04:32:31 | step: 97300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.732658296939917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.28 | consumed tokens: 797081600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T04:32:49 | step: 97400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.7302284908946604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.38 | consumed tokens: 797900800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T04:33:08 | step: 97500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.72779686586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.27 | consumed tokens: 798720000.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T04:33:27 | step: 97600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.725363785633817e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.38 | consumed tokens: 799539200.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T04:33:45 | step: 97700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.722928886418231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.08 | consumed tokens: 800358400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:34:04 | step: 97800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.720492895809002e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.22 | consumed tokens: 801177600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T04:34:22 | step: 97900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.71805508621037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.44 | consumed tokens: 801996800.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T04:34:41 | step: 98000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.715615821420215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.83 | consumed tokens: 802816000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:35:00 | step: 98100 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.713175101438537e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.47 | consumed tokens: 803635200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T04:35:18 | step: 98200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.7107329262653366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.52 | consumed tokens: 804454400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T04:35:37 | step: 98300 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.708289295900613e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.97 | consumed tokens: 805273600.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:35:55 | step: 98400 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.705843846546486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.89 | consumed tokens: 806092800.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2025-12-30T04:36:13 | step: 98500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.703397305798717e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.0 | consumed tokens: 806912000.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T04:36:32 | step: 98600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.700948946061544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.95 | consumed tokens: 807731200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T04:36:50 | step: 98700 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.6984991311328486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.66 | consumed tokens: 808550400.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T04:37:09 | step: 98800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.69604786101263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.56 | consumed tokens: 809369600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T04:37:27 | step: 98900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.693595135700889e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.88 | consumed tokens: 810188800.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:37:46 | step: 99000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.691140955197625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.86 | consumed tokens: 811008000.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T04:38:04 | step: 99100 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 3.688685319502838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.25 | consumed tokens: 811827200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:38:22 | step: 99200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.686228228616528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.61 | consumed tokens: 812646400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:38:41 | step: 99300 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 3.683769318740815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.2 | consumed tokens: 813465600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:39:00 | step: 99400 | train samples/s: 92.8 | train mfu (16-bit): -1.0 | lr mean: 3.6813093174714595e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.5 | consumed tokens: 814284800.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:39:18 | step: 99500 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.6788474972127005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.92 | consumed tokens: 815104000.0 | grad norm avg: 0.84 | grad norm last: 0.75 | 
2025-12-30T04:39:37 | step: 99600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.6763845855602995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.28 | consumed tokens: 815923200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T04:39:56 | step: 99700 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.673919854918495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.48 | consumed tokens: 816742400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:40:14 | step: 99800 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.671454032883048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.41 | consumed tokens: 817561600.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-30T04:40:33 | step: 99900 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.668986391858198e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.92 | consumed tokens: 818380800.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:40:52 | step: 100000 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.666517659439705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.02 | consumed tokens: 819200000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T04:41:12 | step: 100100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.664047108031809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.22 | consumed tokens: 820019200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T04:41:30 | step: 100200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.661575465230271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.53 | consumed tokens: 820838400.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T04:41:49 | step: 100300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.6591020034393296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 821657600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:42:07 | step: 100400 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.656627086456865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.84 | consumed tokens: 822476800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T04:42:26 | step: 100500 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.6541510780807585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.86 | consumed tokens: 823296000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T04:42:45 | step: 100600 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.651673614513129e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.28 | consumed tokens: 824115200.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:43:04 | step: 100700 | train samples/s: 91.3 | train mfu (16-bit): -1.0 | lr mean: 3.649194331956096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.75 | consumed tokens: 824934400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T04:43:23 | step: 100800 | train samples/s: 91.5 | train mfu (16-bit): -1.0 | lr mean: 3.646713958005421e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.88 | consumed tokens: 825753600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T04:43:42 | step: 100900 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.644232128863223e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.92 | consumed tokens: 826572800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T04:44:00 | step: 101000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.641748844529502e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.89 | consumed tokens: 827392000.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T04:44:19 | step: 101100 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.639263741206378e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.36 | consumed tokens: 828211200.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T04:44:38 | step: 101200 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.636777910287492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.83 | consumed tokens: 829030400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T04:44:57 | step: 101300 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.634290260379203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.31 | consumed tokens: 829849600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T04:45:15 | step: 101400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.6318011552793905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 830668800.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T04:45:34 | step: 101500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.6293105949880555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.34 | consumed tokens: 831488000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:45:53 | step: 101600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.6268189433030784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.34 | consumed tokens: 832307200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:46:11 | step: 101700 | train samples/s: 92.6 | train mfu (16-bit): -1.0 | lr mean: 3.624325472628698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.98 | consumed tokens: 833126400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T04:46:30 | step: 101800 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.621830910560675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.38 | consumed tokens: 833945600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:46:49 | step: 101900 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.6193348933011293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.67 | consumed tokens: 834764800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:47:08 | step: 102000 | train samples/s: 92.0 | train mfu (16-bit): -1.0 | lr mean: 3.616837420850061e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.44 | consumed tokens: 835584000.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T04:47:27 | step: 102100 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 3.61433885700535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.67 | consumed tokens: 836403200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:47:45 | step: 102200 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.611838474171236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.97 | consumed tokens: 837222400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T04:48:04 | step: 102300 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.60933699994348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.36 | consumed tokens: 838041600.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T04:48:23 | step: 102400 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.606834070524201e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.84 | consumed tokens: 838860800.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T04:48:41 | step: 102500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.604329685913399e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.42 | consumed tokens: 839680000.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:49:00 | step: 102600 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.601823846111074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.55 | consumed tokens: 840499200.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T04:49:19 | step: 102700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.5993165511172265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.84 | consumed tokens: 841318400.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:49:37 | step: 102800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.596808164729737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.17 | consumed tokens: 842137600.0 | grad norm avg: 0.83 | grad norm last: 0.93 | 
2025-12-30T04:49:56 | step: 102900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.594298323150724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.72 | consumed tokens: 842956800.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:50:14 | step: 103000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.591787026380189e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.39 | consumed tokens: 843776000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T04:50:33 | step: 103100 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.589274638216011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.39 | consumed tokens: 844595200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T04:50:51 | step: 103200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.586760794860311e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.33 | consumed tokens: 845414400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:51:10 | step: 103300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.5842454963130876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.69 | consumed tokens: 846233600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T04:51:28 | step: 103400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.5817287425743416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.06 | consumed tokens: 847052800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:51:47 | step: 103500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.5792108974419534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.27 | consumed tokens: 847872000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T04:52:06 | step: 103600 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 3.576691233320162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.3 | consumed tokens: 848691200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T04:52:24 | step: 103700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.5741708416026086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.39 | consumed tokens: 849510400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:52:43 | step: 103800 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.571648630895652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.55 | consumed tokens: 850329600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:53:02 | step: 103900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.569125328795053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.64 | consumed tokens: 851148800.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T04:53:20 | step: 104000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.5666005715029314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.78 | consumed tokens: 851968000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T04:53:39 | step: 104100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.5640747228171676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.19 | consumed tokens: 852787200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:53:57 | step: 104200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.561547418939881e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.62 | consumed tokens: 853606400.0 | grad norm avg: 0.84 | grad norm last: 0.94 | 
2025-12-30T04:54:16 | step: 104300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.5590186598710716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.23 | consumed tokens: 854425600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:54:34 | step: 104400 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.55648880940862e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.17 | consumed tokens: 855244800.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T04:54:53 | step: 104500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.5539575037546456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.33 | consumed tokens: 856064000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T04:55:12 | step: 104600 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.551424742909148e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.55 | consumed tokens: 856883200.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T04:55:30 | step: 104700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.548890890670009e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.66 | consumed tokens: 857702400.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T04:55:49 | step: 104800 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.546355583239347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.52 | consumed tokens: 858521600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:56:07 | step: 104900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.5438191844150424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.66 | consumed tokens: 859340800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:56:25 | step: 105000 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.541281330399215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.66 | consumed tokens: 860160000.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T04:56:45 | step: 105100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.538742384989746e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.72 | consumed tokens: 860979200.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T04:57:04 | step: 105200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.536201620590873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 861798400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T04:57:22 | step: 105300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.533660128596239e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.61 | consumed tokens: 862617600.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T04:57:41 | step: 105400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.531117181410082e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.72 | consumed tokens: 863436800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:57:59 | step: 105500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.528572779032402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.25 | consumed tokens: 864256000.0 | grad norm avg: 0.85 | grad norm last: 0.93 | 
2025-12-30T04:58:18 | step: 105600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.52602728526108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.28 | consumed tokens: 865075200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T04:58:37 | step: 105700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.523480336298235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 865894400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T04:58:55 | step: 105800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.520932295941748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.61 | consumed tokens: 866713600.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T04:59:14 | step: 105900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.518382800393738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 867532800.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T04:59:32 | step: 106000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.515832213452086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.02 | consumed tokens: 868352000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T04:59:51 | step: 106100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.513280171318911e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 869171200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:00:10 | step: 106200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.510727037792094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.86 | consumed tokens: 869990400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T05:00:28 | step: 106300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.508172812871635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.39 | consumed tokens: 870809600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T05:00:47 | step: 106400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.505616768961772e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.81 | consumed tokens: 871628800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T05:01:05 | step: 106500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.503059997456148e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.39 | consumed tokens: 872448000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:01:24 | step: 106600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.5005017707590014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.62 | consumed tokens: 873267200.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:01:43 | step: 106700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.4979424526682124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.53 | consumed tokens: 874086400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:02:01 | step: 106800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.4953816793859005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.03 | consumed tokens: 874905600.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T05:02:20 | step: 106900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.4928198147099465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.22 | consumed tokens: 875724800.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:02:38 | step: 107000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.49025649484247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.36 | consumed tokens: 876544000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:02:57 | step: 107100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.487692083581351e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.72 | consumed tokens: 877363200.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T05:03:15 | step: 107200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.48512658092659e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.83 | consumed tokens: 878182400.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T05:03:34 | step: 107300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.482559623080306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.61 | consumed tokens: 879001600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T05:03:52 | step: 107400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.47999157384038e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.22 | consumed tokens: 879820800.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T05:04:11 | step: 107500 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 3.477422069408931e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.22 | consumed tokens: 880640000.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T05:04:29 | step: 107600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.4748518373817205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.0 | consumed tokens: 881459200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T05:04:48 | step: 107700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.472279786365107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.86 | consumed tokens: 882278400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T05:05:06 | step: 107800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.4697070077527314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.89 | consumed tokens: 883097600.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T05:05:25 | step: 107900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.4671327739488333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.59 | consumed tokens: 883916800.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:05:43 | step: 108000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.464557448751293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.97 | consumed tokens: 884736000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T05:06:02 | step: 108100 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.46198066836223e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.11 | consumed tokens: 885555200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:06:21 | step: 108200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.4594031603774056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.55 | consumed tokens: 886374400.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:06:39 | step: 108300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.456824197201058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.22 | consumed tokens: 887193600.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T05:06:58 | step: 108400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.454243778833188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.97 | consumed tokens: 888012800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:07:16 | step: 108500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.4516626328695565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.28 | consumed tokens: 888832000.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T05:07:35 | step: 108600 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 3.449080031714402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.7 | consumed tokens: 889651200.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:07:54 | step: 108700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.4464963391656056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.86 | consumed tokens: 890470400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T05:08:12 | step: 108800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.443911555223167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.42 | consumed tokens: 891289600.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T05:08:31 | step: 108900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.4413253160892054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.66 | consumed tokens: 892108800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:08:49 | step: 109000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.438737985561602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.75 | consumed tokens: 892928000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:09:08 | step: 109100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.436149563640356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.61 | consumed tokens: 893747200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T05:09:27 | step: 109200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.433560050325468e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.22 | consumed tokens: 894566400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T05:09:45 | step: 109300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.430969445616938e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.62 | consumed tokens: 895385600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:10:04 | step: 109400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.428377385716885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.17 | consumed tokens: 896204800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:10:22 | step: 109500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.425784598221071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.72 | consumed tokens: 897024000.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T05:10:41 | step: 109600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.423190355533734e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.86 | consumed tokens: 897843200.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:10:59 | step: 109700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.420595021452755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 898662400.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:11:18 | step: 109800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.417998232180253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.28 | consumed tokens: 899481600.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T05:11:37 | step: 109900 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 3.415400715311989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 900300800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T05:11:55 | step: 110000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.412801743252203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.03 | consumed tokens: 901120000.0 | grad norm avg: 0.85 | grad norm last: 0.74 | 
2025-12-30T05:12:15 | step: 110100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.410202043596655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 901939200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:12:34 | step: 110200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.4076008887495846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.41 | consumed tokens: 902758400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:12:52 | step: 110300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.404998642508872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 903577600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:13:11 | step: 110400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.402395304874517e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 904396800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:13:29 | step: 110500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.39979087584652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.73 | consumed tokens: 905216000.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:13:48 | step: 110600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.397185355424881e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.03 | consumed tokens: 906035200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:14:06 | step: 110700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.3945787436096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.67 | consumed tokens: 906854400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T05:14:25 | step: 110800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.3919710404006764e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.62 | consumed tokens: 907673600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:14:43 | step: 110900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.38936188200023e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.97 | consumed tokens: 908492800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:15:02 | step: 111000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.3867519960040227e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 909312000.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T05:15:20 | step: 111100 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.384141018614173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.61 | consumed tokens: 910131200.0 | grad norm avg: 0.85 | grad norm last: 0.76 | 
2025-12-30T05:15:39 | step: 111200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 3.3815285860328004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.47 | consumed tokens: 910950400.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:15:57 | step: 111300 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.3789154258556664e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.64 | consumed tokens: 911769600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T05:16:16 | step: 111400 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.3763008104870096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.52 | consumed tokens: 912588800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:16:34 | step: 111500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.3736854675225914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.25 | consumed tokens: 913408000.0 | grad norm avg: 0.85 | grad norm last: 0.99 | 
2025-12-30T05:16:52 | step: 111600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.37106866936665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.06 | consumed tokens: 914227200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T05:17:11 | step: 111700 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.368451143614948e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.39 | consumed tokens: 915046400.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T05:17:29 | step: 111800 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.3658321626717225e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 915865600.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T05:17:48 | step: 111900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.363212454132736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.38 | consumed tokens: 916684800.0 | grad norm avg: 0.84 | grad norm last: 0.93 | 
2025-12-30T05:18:06 | step: 112000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.360591654200107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.62 | consumed tokens: 917504000.0 | grad norm avg: 0.84 | grad norm last: 0.94 | 
2025-12-30T05:18:25 | step: 112100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.357969762873836e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.95 | consumed tokens: 918323200.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-30T05:18:43 | step: 112200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.355346416356042e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.56 | consumed tokens: 919142400.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T05:19:02 | step: 112300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.352722342242487e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.98 | consumed tokens: 919961600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T05:19:20 | step: 112400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.3500971767352894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.78 | consumed tokens: 920780800.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T05:19:39 | step: 112500 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.34747091983445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.2 | consumed tokens: 921600000.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T05:19:58 | step: 112600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.344843571539968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.73 | consumed tokens: 922419200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:20:16 | step: 112700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.342215495649725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.55 | consumed tokens: 923238400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T05:20:35 | step: 112800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.339585964567959e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.7 | consumed tokens: 924057600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:20:53 | step: 112900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.336955342092551e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.7 | consumed tokens: 924876800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:21:12 | step: 113000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.334323992021382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.75 | consumed tokens: 925696000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:21:30 | step: 113100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.33169155055657e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.86 | consumed tokens: 926515200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:21:48 | step: 113200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.3290580176981166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.62 | consumed tokens: 927334400.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T05:22:07 | step: 113300 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.326423393446021e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.48 | consumed tokens: 928153600.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T05:22:25 | step: 113400 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.323787677800283e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.38 | consumed tokens: 928972800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:22:43 | step: 113500 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.321150870760903e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 929792000.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T05:23:02 | step: 113600 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.318513336125761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.23 | consumed tokens: 930611200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:23:20 | step: 113700 | train samples/s: 96.6 | train mfu (16-bit): -1.0 | lr mean: 3.3158747100969777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.11 | consumed tokens: 931430400.0 | grad norm avg: 0.84 | grad norm last: 0.75 | 
2025-12-30T05:23:38 | step: 113800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.313234992674552e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.36 | consumed tokens: 932249600.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T05:23:56 | step: 113900 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 3.310594183858484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.11 | consumed tokens: 933068800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:24:15 | step: 114000 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 3.307952647446655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.0 | consumed tokens: 933888000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T05:24:33 | step: 114100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.3053096558433026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.94 | consumed tokens: 934707200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T05:24:51 | step: 114200 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.302665936644189e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.67 | consumed tokens: 935526400.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T05:25:09 | step: 114300 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.3000211260514334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.98 | consumed tokens: 936345600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:25:28 | step: 114400 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.297375587862916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.62 | consumed tokens: 937164800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T05:25:46 | step: 114500 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 3.294728958280757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.95 | consumed tokens: 937984000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:26:04 | step: 114600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 3.292081237304956e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.75 | consumed tokens: 938803200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:26:22 | step: 114700 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 3.289432424935512e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.06 | consumed tokens: 939622400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:26:40 | step: 114800 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.2867825211724266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.88 | consumed tokens: 940441600.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:26:59 | step: 114900 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 3.2841318898135796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.78 | consumed tokens: 941260800.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T05:27:17 | step: 115000 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 3.281480530858971e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.44 | consumed tokens: 942080000.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T05:27:37 | step: 115100 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.27882771671284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.7 | consumed tokens: 942899200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:27:55 | step: 115200 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.276174174970947e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.92 | consumed tokens: 943718400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:28:13 | step: 115300 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 3.2735195418354124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.0 | consumed tokens: 944537600.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T05:28:31 | step: 115400 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 3.270864181104116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.52 | consumed tokens: 945356800.0 | grad norm avg: 0.84 | grad norm last: 0.74 | 
2025-12-30T05:28:49 | step: 115500 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 3.268207728979178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.73 | consumed tokens: 946176000.0 | grad norm avg: 0.87 | grad norm last: 0.78 | 
2025-12-30T05:29:08 | step: 115600 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 3.265550185460597e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 946995200.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:29:26 | step: 115700 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 3.2628919143462554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.22 | consumed tokens: 947814400.0 | grad norm avg: 0.84 | grad norm last: 0.96 | 
2025-12-30T05:29:44 | step: 115800 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.260232551838271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.5 | consumed tokens: 948633600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:30:02 | step: 115900 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 3.257572097936645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.48 | consumed tokens: 949452800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T05:30:20 | step: 116000 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.2549109164392576e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.56 | consumed tokens: 950272000.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T05:30:38 | step: 116100 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.252248643548228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.14 | consumed tokens: 951091200.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T05:30:57 | step: 116200 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.249585643061437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.16 | consumed tokens: 951910400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T05:31:15 | step: 116300 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.2469215511810035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.11 | consumed tokens: 952729600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:31:33 | step: 116400 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 3.244256731704809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.88 | consumed tokens: 953548800.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:31:51 | step: 116500 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.241590820834972e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.12 | consumed tokens: 954368000.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T05:32:10 | step: 116600 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.238923818571493e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.67 | consumed tokens: 955187200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T05:32:28 | step: 116700 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.236256088712253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.89 | consumed tokens: 956006400.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:32:46 | step: 116800 | train samples/s: 96.6 | train mfu (16-bit): -1.0 | lr mean: 3.233587631257251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.19 | consumed tokens: 956825600.0 | grad norm avg: 0.86 | grad norm last: 0.77 | 
2025-12-30T05:33:04 | step: 116900 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 3.230918082408607e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.81 | consumed tokens: 957644800.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T05:33:23 | step: 117000 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 3.228247442166321e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.53 | consumed tokens: 958464000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:33:41 | step: 117100 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 3.2255760743282735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.69 | consumed tokens: 959283200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T05:33:59 | step: 117200 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 3.222903978894465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.44 | consumed tokens: 960102400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:34:18 | step: 117300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.220230792067014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.53 | consumed tokens: 960921600.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T05:34:36 | step: 117400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.2175565138459206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.23 | consumed tokens: 961740800.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:34:55 | step: 117500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.214881508029066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.64 | consumed tokens: 962560000.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:35:13 | step: 117600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.21220577461645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.27 | consumed tokens: 963379200.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T05:35:32 | step: 117700 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 3.209528949810192e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.09 | consumed tokens: 964198400.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T05:35:50 | step: 117800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.2068513974081725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.12 | consumed tokens: 965017600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:36:09 | step: 117900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.204172753612511e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.95 | consumed tokens: 965836800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T05:36:27 | step: 118000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.201493382221088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.45 | consumed tokens: 966656000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T05:36:46 | step: 118100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.1988132832339033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.41 | consumed tokens: 967475200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:37:05 | step: 118200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.196132092853077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.34 | consumed tokens: 968294400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T05:37:23 | step: 118300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.193450174876489e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.36 | consumed tokens: 969113600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T05:37:42 | step: 118400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.1907671655062586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.19 | consumed tokens: 969932800.0 | grad norm avg: 0.84 | grad norm last: 0.96 | 
2025-12-30T05:38:00 | step: 118500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.188083428540267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.55 | consumed tokens: 970752000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T05:38:19 | step: 118600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.185398963978514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.11 | consumed tokens: 971571200.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T05:38:37 | step: 118700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.182713408023119e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 972390400.0 | grad norm avg: 0.85 | grad norm last: 0.96 | 
2025-12-30T05:38:56 | step: 118800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.1800271244719625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 973209600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T05:39:14 | step: 118900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.1773401133250445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.89 | consumed tokens: 974028800.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2025-12-30T05:39:33 | step: 119000 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 3.1746520107844844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.03 | consumed tokens: 974848000.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:39:52 | step: 119100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.171963180648163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.22 | consumed tokens: 975667200.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:40:10 | step: 119200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.16927362291608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.39 | consumed tokens: 976486400.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T05:40:29 | step: 119300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.166582973790355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.7 | consumed tokens: 977305600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:40:47 | step: 119400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.163891960866749e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.59 | consumed tokens: 978124800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:41:05 | step: 119500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.1611998565495014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.89 | consumed tokens: 978944000.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:41:24 | step: 119600 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.1585066608386114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 4.16 | consumed tokens: 979763200.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T05:41:42 | step: 119700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.155813101329841e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.98 | consumed tokens: 980582400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T05:42:01 | step: 119800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.153118450427428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.77 | consumed tokens: 981401600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:42:19 | step: 119900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.1504230719292536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.86 | consumed tokens: 982220800.0 | grad norm avg: 0.84 | grad norm last: 0.75 | 
2025-12-30T05:42:38 | step: 120000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.147726602037437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.0 | consumed tokens: 983040000.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:42:58 | step: 120100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.14502976834774e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.42 | consumed tokens: 983859200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:43:16 | step: 120200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.142331843264401e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.0 | consumed tokens: 984678400.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T05:43:35 | step: 120300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.1396331905853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.27 | consumed tokens: 985497600.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T05:43:53 | step: 120400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.136933810310438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.67 | consumed tokens: 986316800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T05:44:12 | step: 120500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.134233702439815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.48 | consumed tokens: 987136000.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:44:30 | step: 120600 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.131532503175549e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.16 | consumed tokens: 987955200.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:44:48 | step: 120700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.128830940113403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.44 | consumed tokens: 988774400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:45:07 | step: 120800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.1261282856576145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.5 | consumed tokens: 989593600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:45:26 | step: 120900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.1234249036060646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.27 | consumed tokens: 990412800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:45:44 | step: 121000 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.1207207939587533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.39 | consumed tokens: 991232000.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:46:03 | step: 121100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.1180159567156807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.64 | consumed tokens: 992051200.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:46:21 | step: 121200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.1153103918768466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.28 | consumed tokens: 992870400.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T05:46:40 | step: 121300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.11260373564437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.59 | consumed tokens: 993689600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T05:46:58 | step: 121400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.1098967156140134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.34 | consumed tokens: 994508800.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:47:17 | step: 121500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.107188604190014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 995328000.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T05:47:35 | step: 121600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.104479765170254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.89 | consumed tokens: 996147200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:47:54 | step: 121700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.1017705623526126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.45 | consumed tokens: 996966400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:48:12 | step: 121800 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 3.099060268141329e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.12 | consumed tokens: 997785600.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:48:30 | step: 121900 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.0963492463342845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.38 | consumed tokens: 998604800.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:48:49 | step: 122000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.0936374969314784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.41 | consumed tokens: 999424000.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T05:49:07 | step: 122100 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 3.090925019932911e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.44 | consumed tokens: 1000243200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T05:49:25 | step: 122200 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 3.0882121791364625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 1001062400.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T05:49:44 | step: 122300 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 3.085498246946372e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.31 | consumed tokens: 1001881600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:50:02 | step: 122400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 3.08278358716052e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.55 | consumed tokens: 1002700800.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T05:50:20 | step: 122500 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 3.080068199778907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 1003520000.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:50:39 | step: 122600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.077352084801532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.41 | consumed tokens: 1004339200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:50:57 | step: 122700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.074635242228396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.19 | consumed tokens: 1005158400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:51:16 | step: 122800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.071917672059499e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.27 | consumed tokens: 1005977600.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:51:35 | step: 122900 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.06919937429484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.59 | consumed tokens: 1006796800.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T05:51:53 | step: 123000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.0664807127323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.42 | consumed tokens: 1007616000.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T05:52:12 | step: 123100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.0637609597761184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.47 | consumed tokens: 1008435200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:52:30 | step: 123200 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.061040479224175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.72 | consumed tokens: 1009254400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:52:49 | step: 123300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.058319634874351e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 1010073600.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T05:53:07 | step: 123400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.055597699130885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 1010892800.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:53:26 | step: 123500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.0528753995895386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.62 | consumed tokens: 1011712000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T05:53:44 | step: 123600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.0501523724524304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 1012531200.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T05:54:03 | step: 123700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.047428617719561e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.56 | consumed tokens: 1013350400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:54:22 | step: 123800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.04470413539093e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.0 | consumed tokens: 1014169600.0 | grad norm avg: 0.84 | grad norm last: 0.94 | 
2025-12-30T05:54:40 | step: 123900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.0419789254665375e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.62 | consumed tokens: 1014988800.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T05:54:59 | step: 124000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.039253169845324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.16 | consumed tokens: 1015808000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T05:55:18 | step: 124100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.036526686628349e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.7 | consumed tokens: 1016627200.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T05:55:36 | step: 124200 | train samples/s: 92.5 | train mfu (16-bit): -1.0 | lr mean: 3.0337994758156128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.33 | consumed tokens: 1017446400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:55:55 | step: 124300 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.0310717193060555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.16 | consumed tokens: 1018265600.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T05:56:14 | step: 124400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.0283432352007367e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.17 | consumed tokens: 1019084800.0 | grad norm avg: 0.85 | grad norm last: 0.97 | 
2025-12-30T05:56:32 | step: 124500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.0256140234996565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.94 | consumed tokens: 1019904000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T05:56:51 | step: 124600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.0228842661017552e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.08 | consumed tokens: 1020723200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T05:57:10 | step: 124700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.0201537811080925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.22 | consumed tokens: 1021542400.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T05:57:28 | step: 124800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.0174227504176088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1022361600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T05:57:46 | step: 124900 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.0146909921313636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.78 | consumed tokens: 1023180800.0 | grad norm avg: 0.85 | grad norm last: 0.75 | 
2025-12-30T05:58:05 | step: 125000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.011958506249357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.56 | consumed tokens: 1024000000.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:58:25 | step: 125100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 3.0092256565694697e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.53 | consumed tokens: 1024819200.0 | grad norm avg: 0.85 | grad norm last: 0.93 | 
2025-12-30T05:58:43 | step: 125200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.0064918973948807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.67 | consumed tokens: 1025638400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T05:59:02 | step: 125300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.0037575925234705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.67 | consumed tokens: 1026457600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:59:20 | step: 125400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.0010227419552393e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.97 | consumed tokens: 1027276800.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T05:59:39 | step: 125500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.9982871637912467e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.2 | consumed tokens: 1028096000.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T05:59:57 | step: 125600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.995551039930433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.64 | consumed tokens: 1028915200.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T06:00:16 | step: 125700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.9928143703727983e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.58 | consumed tokens: 1029734400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T06:00:34 | step: 125800 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.990076973219402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.03 | consumed tokens: 1030553600.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T06:00:53 | step: 125900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.987339030369185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.86 | consumed tokens: 1031372800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T06:01:11 | step: 126000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.9846003599232063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.19 | consumed tokens: 1032192000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T06:01:29 | step: 126100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.981861325679347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.94 | consumed tokens: 1033011200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:01:48 | step: 126200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.9791213819407858e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.23 | consumed tokens: 1033830400.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T06:02:07 | step: 126300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.976381074404344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.91 | consumed tokens: 1034649600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T06:02:25 | step: 126400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.9736400392721407e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.56 | consumed tokens: 1035468800.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T06:02:44 | step: 126500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.9708986403420568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.53 | consumed tokens: 1036288000.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T06:03:02 | step: 126600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.968156331917271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.88 | consumed tokens: 1037107200.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2025-12-30T06:03:21 | step: 126700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.9654136596946046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.06 | consumed tokens: 1037926400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T06:03:39 | step: 126800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.962670441775117e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.22 | consumed tokens: 1038745600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T06:03:58 | step: 126900 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 2.959926496259868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.52 | consumed tokens: 1039564800.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:04:17 | step: 127000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.957182005047798e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.67 | consumed tokens: 1040384000.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:04:35 | step: 127100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.954436968138907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.08 | consumed tokens: 1041203200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:04:54 | step: 127200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.951691385533195e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.22 | consumed tokens: 1042022400.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T06:05:12 | step: 127300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.9489452572306618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.77 | consumed tokens: 1042841600.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T06:05:31 | step: 127400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.9461985832313076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1043660800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T06:05:49 | step: 127500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.9434513635351323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.44 | consumed tokens: 1044480000.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T06:06:07 | step: 127600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.940703598142136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 1045299200.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T06:06:26 | step: 127700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.937955105153378e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.8 | consumed tokens: 1046118400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T06:06:45 | step: 127800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.9352062483667396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.81 | consumed tokens: 1046937600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:07:03 | step: 127900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.93245684588328e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.05 | consumed tokens: 1047756800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T06:07:22 | step: 128000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.9297068977029994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.52 | consumed tokens: 1048576000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:07:40 | step: 128100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.9269564038258977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.91 | consumed tokens: 1049395200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:07:59 | step: 128200 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 2.924205364251975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.44 | consumed tokens: 1050214400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T06:08:18 | step: 128300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.921453778981231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.73 | consumed tokens: 1051033600.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2025-12-30T06:08:36 | step: 128400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.9187016480136663e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.58 | consumed tokens: 1051852800.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T06:08:54 | step: 128500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.9159489713492803e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1052672000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T06:09:13 | step: 128600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.9131959308870137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.02 | consumed tokens: 1053491200.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T06:09:31 | step: 128700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.910442344727926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.81 | consumed tokens: 1054310400.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T06:09:50 | step: 128800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.9076882128720172e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.95 | consumed tokens: 1055129600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:10:08 | step: 128900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.9049335353192873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.5 | consumed tokens: 1055948800.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T06:10:27 | step: 129000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.9021783120697364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.8 | consumed tokens: 1056768000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:10:45 | step: 129100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.8994227250223048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.86 | consumed tokens: 1057587200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:11:04 | step: 129200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.896666592278052e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.03 | consumed tokens: 1058406400.0 | grad norm avg: 0.85 | grad norm last: 0.97 | 
2025-12-30T06:11:22 | step: 129300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.8939099138369784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.03 | consumed tokens: 1059225600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T06:11:41 | step: 129400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.8911526896990836e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.06 | consumed tokens: 1060044800.0 | grad norm avg: 0.87 | grad norm last: 0.78 | 
2025-12-30T06:12:00 | step: 129500 | train samples/s: 92.2 | train mfu (16-bit): -1.0 | lr mean: 2.888395101763308e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.94 | consumed tokens: 1060864000.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T06:12:18 | step: 129600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.8856369681307115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1061683200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T06:12:37 | step: 129700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.8828784707002342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.17 | consumed tokens: 1062502400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:12:56 | step: 129800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.8801194275729358e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.42 | consumed tokens: 1063321600.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T06:13:14 | step: 129900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.8773598387488164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.72 | consumed tokens: 1064140800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T06:13:33 | step: 130000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.8745998861268163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.69 | consumed tokens: 1064960000.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T06:13:53 | step: 130100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.871839387807995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.16 | consumed tokens: 1065779200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T06:14:11 | step: 130200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.8690785256912932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.73 | consumed tokens: 1066598400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T06:14:30 | step: 130300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.8663171178777702e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.8 | consumed tokens: 1067417600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:14:49 | step: 130400 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.8635551643674262e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.41 | consumed tokens: 1068236800.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T06:15:07 | step: 130500 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.8607930289581418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.19 | consumed tokens: 1069056000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T06:15:26 | step: 130600 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.858030165953096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.55 | consumed tokens: 1069875200.0 | grad norm avg: 0.86 | grad norm last: 0.95 | 
2025-12-30T06:15:45 | step: 130700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.85526712104911e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.48 | consumed tokens: 1070694400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:16:03 | step: 130800 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 2.8525033485493623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 1071513600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:16:22 | step: 130900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.8497393941506743e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.83 | consumed tokens: 1072332800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T06:16:40 | step: 131000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.8469748940551654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.97 | consumed tokens: 1073152000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T06:16:59 | step: 131100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.8442100301617756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.92 | consumed tokens: 1073971200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:17:17 | step: 131200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.841444620571565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.75 | consumed tokens: 1074790400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T06:17:36 | step: 131300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.8386788471834734e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.83 | consumed tokens: 1075609600.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T06:17:54 | step: 131400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.8359127099975012e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.58 | consumed tokens: 1076428800.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T06:18:13 | step: 131500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.833146027114708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.67 | consumed tokens: 1077248000.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T06:18:31 | step: 131600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.830378980434034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.97 | consumed tokens: 1078067200.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T06:18:50 | step: 131700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.8276115699554794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.84 | consumed tokens: 1078886400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:19:08 | step: 131800 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.824843795679044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.17 | consumed tokens: 1079705600.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T06:19:27 | step: 131900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.8220754757057875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.52 | consumed tokens: 1080524800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T06:19:45 | step: 132000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.8193069738335907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.48 | consumed tokens: 1081344000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:20:04 | step: 132100 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 2.816537926264573e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.16 | consumed tokens: 1082163200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T06:20:23 | step: 132200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.8137685148976743e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.52 | consumed tokens: 1082982400.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T06:20:41 | step: 132300 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.8109985578339547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.95 | consumed tokens: 1083801600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:21:00 | step: 132400 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.8082284188712947e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.02 | consumed tokens: 1084620800.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T06:21:18 | step: 132500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.805457916110754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.97 | consumed tokens: 1085440000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:21:37 | step: 132600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.8026868676533923e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.88 | consumed tokens: 1086259200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:21:55 | step: 132700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.79991563729709e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 1087078400.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:22:14 | step: 132800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.797143861243967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 4.09 | consumed tokens: 1087897600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T06:22:33 | step: 132900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.794371721392963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.81 | consumed tokens: 1088716800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T06:22:51 | step: 133000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.791599399643019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.39 | consumed tokens: 1089536000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T06:23:10 | step: 133100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.7888265321962535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.38 | consumed tokens: 1090355200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:23:28 | step: 133200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.7860533009516075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.66 | consumed tokens: 1091174400.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T06:23:47 | step: 133300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.783279887808021e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.41 | consumed tokens: 1091993600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:24:06 | step: 133400 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 2.7805059289676137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.47 | consumed tokens: 1092812800.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T06:24:24 | step: 133500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.777731788228266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.23 | consumed tokens: 1093632000.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T06:24:43 | step: 133600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.7749572836910374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.53 | consumed tokens: 1094451200.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T06:25:02 | step: 133700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.7721824153559282e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 1095270400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T06:25:20 | step: 133800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.7694071832229383e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.14 | consumed tokens: 1096089600.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:25:39 | step: 133900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.7666315872920677e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.83 | consumed tokens: 1096908800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:25:57 | step: 134000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.7638556275633164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.47 | consumed tokens: 1097728000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:26:16 | step: 134100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.7610794859356247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.97 | consumed tokens: 1098547200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T06:26:34 | step: 134200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.7583029805100523e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.84 | consumed tokens: 1099366400.0 | grad norm avg: 0.86 | grad norm last: 0.94 | 
2025-12-30T06:26:53 | step: 134300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.7555261112865992e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.94 | consumed tokens: 1100185600.0 | grad norm avg: 0.86 | grad norm last: 0.98 | 
2025-12-30T06:27:12 | step: 134400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.7527488782652654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.61 | consumed tokens: 1101004800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T06:27:30 | step: 134500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.749971281446051e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 1101824000.0 | grad norm avg: 0.87 | grad norm last: 0.79 | 
2025-12-30T06:27:49 | step: 134600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.747193502727896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.0 | consumed tokens: 1102643200.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T06:28:07 | step: 134700 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 2.7444153602118604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.53 | consumed tokens: 1103462400.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T06:28:26 | step: 134800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.7416370357968844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.09 | consumed tokens: 1104281600.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:28:44 | step: 134900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.7388583475840278e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.44 | consumed tokens: 1105100800.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:29:03 | step: 135000 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.7360792955732904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.41 | consumed tokens: 1105920000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:29:23 | step: 135100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.7333000616636127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.41 | consumed tokens: 1106739200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T06:29:41 | step: 135200 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.7305204639560543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.59 | consumed tokens: 1107558400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:30:00 | step: 135300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.727740502450615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 1108377600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T06:30:19 | step: 135400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.7249603590462357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.36 | consumed tokens: 1109196800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:30:37 | step: 135500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.722180033742916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 1110016000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T06:30:56 | step: 135600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.7193993446417153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.05 | consumed tokens: 1110835200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:31:14 | step: 135700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.716618291742634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.09 | consumed tokens: 1111654400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:31:33 | step: 135800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.7138370569446124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.47 | consumed tokens: 1112473600.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:31:51 | step: 135900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.7110556402476504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1113292800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T06:32:10 | step: 136000 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 2.7082738597528078e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.5 | consumed tokens: 1114112000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T06:32:29 | step: 136100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.7054918973590247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.58 | consumed tokens: 1114931200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:32:47 | step: 136200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.702709571167361e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.28 | consumed tokens: 1115750400.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T06:33:06 | step: 136300 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.699927063076757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.2 | consumed tokens: 1116569600.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:33:24 | step: 136400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.697144191188272e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.45 | consumed tokens: 1117388800.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T06:33:42 | step: 136500 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.694361137400847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.31 | consumed tokens: 1118208000.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T06:34:01 | step: 136600 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.6915779017144814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.52 | consumed tokens: 1119027200.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T06:34:19 | step: 136700 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.6887944841291755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.98 | consumed tokens: 1119846400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:34:37 | step: 136800 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.686010702745989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.39 | consumed tokens: 1120665600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T06:34:56 | step: 136900 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.683226739463862e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.28 | consumed tokens: 1121484800.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T06:35:14 | step: 137000 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 2.6804425942827947e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 1122304000.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T06:35:32 | step: 137100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 2.677658267202787e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.98 | consumed tokens: 1123123200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:35:51 | step: 137200 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 2.6748735763248987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 1123942400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T06:36:09 | step: 137300 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 2.67208870354807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.16 | consumed tokens: 1124761600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:36:28 | step: 137400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.669303648872301e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.89 | consumed tokens: 1125580800.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:36:46 | step: 137500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.6665184122975916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.17 | consumed tokens: 1126400000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:37:05 | step: 137600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.6637329938239418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.61 | consumed tokens: 1127219200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T06:37:24 | step: 137700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.6609473934513517e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.92 | consumed tokens: 1128038400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:37:42 | step: 137800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.658161429280881e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.53 | consumed tokens: 1128857600.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T06:38:01 | step: 137900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.65537546511041e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.98 | consumed tokens: 1129676800.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:38:19 | step: 138000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.6525891371420585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 1130496000.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T06:38:38 | step: 138100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.649802809173707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.17 | consumed tokens: 1131315200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:38:56 | step: 138200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.6470161174074747e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 1132134400.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:39:15 | step: 138300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.6442294256412424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1132953600.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T06:39:33 | step: 138400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.6414423700771295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.22 | consumed tokens: 1133772800.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T06:39:52 | step: 138500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.638655132614076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.33 | consumed tokens: 1134592000.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T06:40:11 | step: 138600 | train samples/s: 92.8 | train mfu (16-bit): -1.0 | lr mean: 2.6358678951510228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.61 | consumed tokens: 1135411200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:40:29 | step: 138700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.633080475789029e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.92 | consumed tokens: 1136230400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:40:48 | step: 138800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.6302926926291548e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.95 | consumed tokens: 1137049600.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T06:41:07 | step: 138900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.6275049094692804e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.33 | consumed tokens: 1137868800.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T06:41:25 | step: 139000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.6247169444104657e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.61 | consumed tokens: 1138688000.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T06:41:44 | step: 139100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.6219287974527106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.41 | consumed tokens: 1139507200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:42:02 | step: 139200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.6191406504949555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.44 | consumed tokens: 1140326400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:42:21 | step: 139300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.6163521397393197e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.41 | consumed tokens: 1141145600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T06:42:39 | step: 139400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.613563628983684e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 1141964800.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:42:58 | step: 139500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.6107749363291077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.06 | consumed tokens: 1142784000.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T06:43:16 | step: 139600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.6079860617755912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.03 | consumed tokens: 1143603200.0 | grad norm avg: 0.86 | grad norm last: 0.98 | 
2025-12-30T06:43:35 | step: 139700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.6051971872220747e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 1144422400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:43:53 | step: 139800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.602408130769618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.17 | consumed tokens: 1145241600.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T06:44:12 | step: 139900 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 2.5996188924182206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.94 | consumed tokens: 1146060800.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T06:44:30 | step: 140000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.596829472167883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.2 | consumed tokens: 1146880000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:44:50 | step: 140100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.5940400519175455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.88 | consumed tokens: 1147699200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T06:45:09 | step: 140200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.5912504497682676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.92 | consumed tokens: 1148518400.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T06:45:28 | step: 140300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.5884608476189896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.83 | consumed tokens: 1149337600.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T06:45:46 | step: 140400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.5856710635707714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.03 | consumed tokens: 1150156800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T06:46:05 | step: 140500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.5828810976236127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.09 | consumed tokens: 1150976000.0 | grad norm avg: 0.87 | grad norm last: 0.94 | 
2025-12-30T06:46:23 | step: 140600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.580091131676454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.94 | consumed tokens: 1151795200.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:46:41 | step: 140700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.5773011657292955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.73 | consumed tokens: 1152614400.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T06:47:00 | step: 140800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.5745110178831965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.06 | consumed tokens: 1153433600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:47:18 | step: 140900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.571720688138157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.33 | consumed tokens: 1154252800.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T06:47:36 | step: 141000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.5689303583931178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.44 | consumed tokens: 1155072000.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T06:47:55 | step: 141100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.566139846749138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.38 | consumed tokens: 1155891200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:48:14 | step: 141200 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 2.5633493351051584e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.38 | consumed tokens: 1156710400.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T06:48:33 | step: 141300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.5605588234611787e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.23 | consumed tokens: 1157529600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:48:51 | step: 141400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.5577681299182586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 2.73 | consumed tokens: 1158348800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:49:10 | step: 141500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.5549774363753386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 1159168000.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T06:49:28 | step: 141600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.5521865609334782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.84 | consumed tokens: 1159987200.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T06:49:47 | step: 141700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.5493956854916178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 1160806400.0 | grad norm avg: 0.87 | grad norm last: 0.95 | 
2025-12-30T06:50:05 | step: 141800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.5466048100497574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.56 | consumed tokens: 1161625600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T06:50:24 | step: 141900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.5438137527089566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.25 | consumed tokens: 1162444800.0 | grad norm avg: 0.88 | grad norm last: 0.79 | 
2025-12-30T06:50:43 | step: 142000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.541022695368156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 1163264000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:51:01 | step: 142100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.538231638027355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.69 | consumed tokens: 1164083200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T06:51:20 | step: 142200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.5354405806865543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 1164902400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T06:51:38 | step: 142300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.5326493414468132e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.89 | consumed tokens: 1165721600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T06:51:57 | step: 142400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.5298582841060124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.61 | consumed tokens: 1166540800.0 | grad norm avg: 0.86 | grad norm last: 0.99 | 
2025-12-30T06:52:16 | step: 142500 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 2.5270670448662713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 1167360000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:52:34 | step: 142600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.5242758056265302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.08 | consumed tokens: 1168179200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T06:52:53 | step: 142700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.5214843844878487e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.23 | consumed tokens: 1168998400.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T06:53:11 | step: 142800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.5186931452481076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.41 | consumed tokens: 1169817600.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T06:53:30 | step: 142900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.515901724109426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 1170636800.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T06:53:48 | step: 143000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.513110484869685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.25 | consumed tokens: 1171456000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T06:54:07 | step: 143100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.5103190637310036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 1172275200.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T06:54:25 | step: 143200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.507527642592322e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.62 | consumed tokens: 1173094400.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:54:43 | step: 143300 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.504736403352581e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.0 | consumed tokens: 1173913600.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T06:55:02 | step: 143400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.5019449822138995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.36 | consumed tokens: 1174732800.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T06:55:20 | step: 143500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.499153561075218e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 1175552000.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T06:55:39 | step: 143600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.4963621399365366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 1176371200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:55:57 | step: 143700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.493570718797855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.19 | consumed tokens: 1177190400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T06:56:16 | step: 143800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.490779479558114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.69 | consumed tokens: 1178009600.0 | grad norm avg: 0.87 | grad norm last: 0.79 | 
2025-12-30T06:56:34 | step: 143900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.4879880584194325e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.69 | consumed tokens: 1178828800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:56:53 | step: 144000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.4851968191796914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 1179648000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T06:57:11 | step: 144100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.48240539804101e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 1180467200.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T06:57:30 | step: 144200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.4796141588012688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.89 | consumed tokens: 1181286400.0 | grad norm avg: 0.87 | grad norm last: 0.94 | 
2025-12-30T06:57:48 | step: 144300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.4768229195615277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.12 | consumed tokens: 1182105600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:58:07 | step: 144400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.4740316803217866e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 1182924800.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:58:25 | step: 144500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.4712404410820454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.05 | consumed tokens: 1183744000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T06:58:43 | step: 144600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.4684493837412447e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 1184563200.0 | grad norm avg: 0.86 | grad norm last: 0.76 | 
2025-12-30T06:59:02 | step: 144700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.465658326400444e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.23 | consumed tokens: 1185382400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T06:59:20 | step: 144800 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 2.462867269059643e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.03 | consumed tokens: 1186201600.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T06:59:39 | step: 144900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.4600762117188424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.61 | consumed tokens: 1187020800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T06:59:57 | step: 145000 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.457285336276982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.42 | consumed tokens: 1187840000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T07:00:17 | step: 145100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.4544942789361812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.31 | consumed tokens: 1188659200.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T07:00:35 | step: 145200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.451703585393261e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.77 | consumed tokens: 1189478400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:00:53 | step: 145300 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 2.4489127099514008e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1190297600.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:01:12 | step: 145400 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 2.4461220164084807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1191116800.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:01:30 | step: 145500 | train samples/s: 96.8 | train mfu (16-bit): -1.0 | lr mean: 2.4433313228655607e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.08 | consumed tokens: 1191936000.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T07:01:48 | step: 145600 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 2.440540811221581e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.41 | consumed tokens: 1192755200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:02:06 | step: 145700 | train samples/s: 96.5 | train mfu (16-bit): -1.0 | lr mean: 2.4377502995776013e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.44 | consumed tokens: 1193574400.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:02:25 | step: 145800 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 2.434959969832562e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.19 | consumed tokens: 1194393600.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:02:43 | step: 145900 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.4321696400875226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.22 | consumed tokens: 1195212800.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T07:03:01 | step: 146000 | train samples/s: 96.6 | train mfu (16-bit): -1.0 | lr mean: 2.4293793103424832e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.89 | consumed tokens: 1196032000.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T07:03:19 | step: 146100 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 2.4265893443953246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.22 | consumed tokens: 1196851200.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T07:03:38 | step: 146200 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 2.4237991965492256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.39 | consumed tokens: 1197670400.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T07:03:56 | step: 146300 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 2.421009230602067e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.92 | consumed tokens: 1198489600.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T07:04:14 | step: 146400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.4182194465538487e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 1199308800.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:04:33 | step: 146500 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 2.4154296625056304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.06 | consumed tokens: 1200128000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T07:04:51 | step: 146600 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 2.4126400603563525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.14 | consumed tokens: 1200947200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:05:09 | step: 146700 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 2.4098504582070746e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 1201766400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:05:27 | step: 146800 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 2.407061037956737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 1202585600.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T07:05:45 | step: 146900 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 2.4042717996053398e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.72 | consumed tokens: 1203404800.0 | grad norm avg: 0.87 | grad norm last: 0.96 | 
2025-12-30T07:06:04 | step: 147000 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 2.401482743152883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.31 | consumed tokens: 1204224000.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T07:06:22 | step: 147100 | train samples/s: 96.9 | train mfu (16-bit): -1.0 | lr mean: 2.398693686700426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 1205043200.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2025-12-30T07:06:40 | step: 147200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.3959048121469095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.52 | consumed tokens: 1205862400.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:06:59 | step: 147300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.393115937593393e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.58 | consumed tokens: 1206681600.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T07:07:17 | step: 147400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.390327244938817e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.78 | consumed tokens: 1207500800.0 | grad norm avg: 0.88 | grad norm last: 0.96 | 
2025-12-30T07:07:36 | step: 147500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.387538734183181e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 1208320000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:07:54 | step: 147600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.3847504053264856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.73 | consumed tokens: 1209139200.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T07:08:13 | step: 147700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.3819622583687305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.64 | consumed tokens: 1209958400.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T07:08:31 | step: 147800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.3791742933099158e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.44 | consumed tokens: 1210777600.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T07:08:50 | step: 147900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.376386328251101e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.91 | consumed tokens: 1211596800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:09:08 | step: 148000 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 2.3735985450912267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.94 | consumed tokens: 1212416000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:09:26 | step: 148100 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 2.3708109438302927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.3 | consumed tokens: 1213235200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T07:09:45 | step: 148200 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.368023524468299e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.0 | consumed tokens: 1214054400.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:10:03 | step: 148300 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.3652362870052457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.25 | consumed tokens: 1214873600.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T07:10:21 | step: 148400 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 2.3624492314411327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.08 | consumed tokens: 1215692800.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T07:10:39 | step: 148500 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 2.35966235777596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.59 | consumed tokens: 1216512000.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T07:10:58 | step: 148600 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 2.356875666009728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.95 | consumed tokens: 1217331200.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T07:11:16 | step: 148700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.3540889742434956e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.95 | consumed tokens: 1218150400.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T07:11:35 | step: 148800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.351302646275144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.42 | consumed tokens: 1218969600.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T07:11:54 | step: 148900 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.348516500205733e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.91 | consumed tokens: 1219788800.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:12:12 | step: 149000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.345730536035262e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 1220608000.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2025-12-30T07:12:31 | step: 149100 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 2.3429447537637316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.17 | consumed tokens: 1221427200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:12:49 | step: 149200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.3401591533911414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.97 | consumed tokens: 1222246400.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T07:13:08 | step: 149300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.3373737349174917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.94 | consumed tokens: 1223065600.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T07:13:26 | step: 149400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.3345886802417226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.69 | consumed tokens: 1223884800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T07:13:45 | step: 149500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.3318036255659536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.52 | consumed tokens: 1224704000.0 | grad norm avg: 0.88 | grad norm last: 0.81 | 
2025-12-30T07:14:03 | step: 149600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.3290189346880652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.06 | consumed tokens: 1225523200.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T07:14:22 | step: 149700 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.3262344257091172e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.05 | consumed tokens: 1226342400.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:14:40 | step: 149800 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.3234500986291096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.94 | consumed tokens: 1227161600.0 | grad norm avg: 0.87 | grad norm last: 1.01 | 
2025-12-30T07:14:58 | step: 149900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.3206659534480423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.28 | consumed tokens: 1227980800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T07:15:17 | step: 150000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.3178821720648557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.92 | consumed tokens: 1228800000.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:15:37 | step: 150100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.3150985725806095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.91 | consumed tokens: 1229619200.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T07:15:55 | step: 150200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.3123151549953036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.06 | consumed tokens: 1230438400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:16:14 | step: 150300 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.309531919308938e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.08 | consumed tokens: 1231257600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:16:33 | step: 150400 | train samples/s: 92.8 | train mfu (16-bit): -1.0 | lr mean: 2.3067490474204533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.8 | consumed tokens: 1232076800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:16:52 | step: 150500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.3039663574309088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.28 | consumed tokens: 1232896000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:17:10 | step: 150600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.3011838493403047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.98 | consumed tokens: 1233715200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T07:17:29 | step: 150700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.2984017050475813e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.94 | consumed tokens: 1234534400.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T07:17:47 | step: 150800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2956197426537983e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.94 | consumed tokens: 1235353600.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:18:06 | step: 150900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.292838144057896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.58 | consumed tokens: 1236172800.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T07:18:24 | step: 151000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.290056727360934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.78 | consumed tokens: 1236992000.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:18:43 | step: 151100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2872756744618528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.69 | consumed tokens: 1237811200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:19:02 | step: 151200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.284494803461712e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.03 | consumed tokens: 1238630400.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2025-12-30T07:19:20 | step: 151300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.2817141143605113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.5 | consumed tokens: 1239449600.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:19:39 | step: 151400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2789337890571915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.16 | consumed tokens: 1240268800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T07:19:57 | step: 151500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.2761538275517523e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.73 | consumed tokens: 1241088000.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T07:20:16 | step: 151600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.2733740479452536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.66 | consumed tokens: 1241907200.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T07:20:34 | step: 151700 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.2705946321366355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.59 | consumed tokens: 1242726400.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:20:53 | step: 151800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.2678153982269578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.17 | consumed tokens: 1243545600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T07:21:11 | step: 151900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.2650365281151608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.19 | consumed tokens: 1244364800.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:21:29 | step: 152000 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 2.2622580218012445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.8 | consumed tokens: 1245184000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T07:21:48 | step: 152100 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.2594796973862685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.03 | consumed tokens: 1246003200.0 | grad norm avg: 0.87 | grad norm last: 0.79 | 
2025-12-30T07:22:06 | step: 152200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.2567017367691733e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.66 | consumed tokens: 1246822400.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:22:25 | step: 152300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.2539239580510184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1247641600.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:22:44 | step: 152400 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.2511465431307442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.89 | consumed tokens: 1248460800.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:23:02 | step: 152500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.2483694920083508e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.02 | consumed tokens: 1249280000.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T07:23:21 | step: 152600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.245592804683838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.38 | consumed tokens: 1250099200.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:23:39 | step: 152700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.242816481157206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.41 | consumed tokens: 1250918400.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T07:23:58 | step: 152800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2400403395295143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.31 | consumed tokens: 1251737600.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T07:24:17 | step: 152900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2372645616997033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.03 | consumed tokens: 1252556800.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T07:24:35 | step: 153000 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 2.234489147667773e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.11 | consumed tokens: 1253376000.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:24:54 | step: 153100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.2317140974337235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 1254195200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:25:12 | step: 153200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.2289392290986143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.7 | consumed tokens: 1255014400.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:25:31 | step: 153300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.226164906460326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 1255833600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T07:25:49 | step: 153400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.2233907657209784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.34 | consumed tokens: 1256652800.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T07:26:08 | step: 153500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.2206169887795113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.66 | consumed tokens: 1257472000.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:26:26 | step: 153600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2178437575348653e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.0 | consumed tokens: 1258291200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:26:45 | step: 153700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.2150707081891596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.64 | consumed tokens: 1259110400.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:27:03 | step: 153800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.2122980226413347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.19 | consumed tokens: 1259929600.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:27:22 | step: 153900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.2095257008913904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 1260748800.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:27:40 | step: 154000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.206753742939327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.23 | consumed tokens: 1261568000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:27:59 | step: 154100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.2039823306840844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.09 | consumed tokens: 1262387200.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T07:28:17 | step: 154200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.2012111003277823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.98 | consumed tokens: 1263206400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:28:36 | step: 154300 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 2.198440233769361e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.3 | consumed tokens: 1264025600.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:28:55 | step: 154400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.1956697310088202e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 1264844800.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T07:29:13 | step: 154500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.1928997739451006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.75 | consumed tokens: 1265664000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T07:29:32 | step: 154600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.1901301806792617e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.58 | consumed tokens: 1266483200.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:29:50 | step: 154700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.187360769312363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.06 | consumed tokens: 1267302400.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:30:09 | step: 154800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.1845919036422856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.45 | consumed tokens: 1268121600.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T07:30:27 | step: 154900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.1818234017700888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.41 | consumed tokens: 1268940800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:30:46 | step: 155000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.179055445594713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 1269760000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T07:31:06 | step: 155100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.1762876713182777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.86 | consumed tokens: 1270579200.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T07:31:24 | step: 155200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.1735204427386634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.83 | consumed tokens: 1271398400.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:31:43 | step: 155300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.1707535779569298e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.53 | consumed tokens: 1272217600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T07:32:01 | step: 155400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.1679872588720173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.14 | consumed tokens: 1273036800.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:32:20 | step: 155500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.165221121686045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 1273856000.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T07:32:39 | step: 155600 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 2.162455530196894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.39 | consumed tokens: 1274675200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:32:57 | step: 155700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.1596903025056235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.53 | consumed tokens: 1275494400.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:33:16 | step: 155800 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.1569256205111742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.12 | consumed tokens: 1276313600.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:33:35 | step: 155900 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.1541613023146056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.22 | consumed tokens: 1277132800.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:33:53 | step: 156000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.1513973479159176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.97 | consumed tokens: 1277952000.0 | grad norm avg: 0.88 | grad norm last: 0.97 | 
2025-12-30T07:34:12 | step: 156100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.1486339392140508e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.92 | consumed tokens: 1278771200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:34:30 | step: 156200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.1458708943100646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.42 | consumed tokens: 1279590400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:34:49 | step: 156300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.1431083951028995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.39 | consumed tokens: 1280409600.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T07:35:07 | step: 156400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.140346259693615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.97 | consumed tokens: 1281228800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:35:26 | step: 156500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.1375846699811518e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.06 | consumed tokens: 1282048000.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:35:44 | step: 156600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.1348234440665692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.59 | consumed tokens: 1282867200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:36:03 | step: 156700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.1320627638488077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.05 | consumed tokens: 1283686400.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:36:22 | step: 156800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.1293024474289268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.72 | consumed tokens: 1284505600.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T07:36:40 | step: 156900 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 2.126542676705867e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.56 | consumed tokens: 1285324800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T07:36:59 | step: 157000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.123783269780688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.69 | consumed tokens: 1286144000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T07:37:18 | step: 157100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.12102440855233e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.31 | consumed tokens: 1286963200.0 | grad norm avg: 0.89 | grad norm last: 0.95 | 
2025-12-30T07:37:36 | step: 157200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.118266093020793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 1287782400.0 | grad norm avg: 0.87 | grad norm last: 0.79 | 
2025-12-30T07:37:55 | step: 157300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.1155081412871368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 1288601600.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:38:13 | step: 157400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.1127507352503017e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 1289420800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:38:32 | step: 157500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.1099936930113472e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.55 | consumed tokens: 1290240000.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:38:50 | step: 157600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.107237378368154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.48 | consumed tokens: 1291059200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T07:39:09 | step: 157700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.104481427522842e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 1291878400.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T07:39:27 | step: 157800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.1017258404754102e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.45 | consumed tokens: 1292697600.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2025-12-30T07:39:46 | step: 157900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 2.09897098102374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 1293516800.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T07:40:04 | step: 158000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.0962164853699505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 1294336000.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:40:22 | step: 158100 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.093462535412982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.05 | consumed tokens: 1295155200.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:40:41 | step: 158200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.0907091311528347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.62 | consumed tokens: 1295974400.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:40:59 | step: 158300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.087956090690568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.56 | consumed tokens: 1296793600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T07:41:18 | step: 158400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.085203777824063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.86 | consumed tokens: 1297612800.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T07:41:37 | step: 158500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.0824518287554383e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.73 | consumed tokens: 1298432000.0 | grad norm avg: 0.88 | grad norm last: 1.01 | 
2025-12-30T07:41:55 | step: 158600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.0797006072825752e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.39 | consumed tokens: 1299251200.0 | grad norm avg: 0.88 | grad norm last: 1.06 | 
2025-12-30T07:42:14 | step: 158700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.0769497496075928e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.38 | consumed tokens: 1300070400.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:42:32 | step: 158800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.0741994376294315e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.56 | consumed tokens: 1300889600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T07:42:51 | step: 158900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.0714496713480912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.33 | consumed tokens: 1301708800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T07:43:09 | step: 159000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.068700450763572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.81 | consumed tokens: 1302528000.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:43:28 | step: 159100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.065951775875874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.66 | consumed tokens: 1303347200.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T07:43:46 | step: 159200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.0632036466849968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.8 | consumed tokens: 1304166400.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T07:44:05 | step: 159300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.0604560631909408e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.23 | consumed tokens: 1304985600.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T07:44:23 | step: 159400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.0577090253937058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.28 | consumed tokens: 1305804800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:44:42 | step: 159500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 2.054962533293292e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 1306624000.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T07:45:00 | step: 159600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.052216586889699e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.5 | consumed tokens: 1307443200.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T07:45:19 | step: 159700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.0494711861829273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.67 | consumed tokens: 1308262400.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:45:38 | step: 159800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.0467263311729766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.31 | consumed tokens: 1309081600.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:45:56 | step: 159900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.0439822037587874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.88 | consumed tokens: 1309900800.0 | grad norm avg: 0.88 | grad norm last: 0.81 | 
2025-12-30T07:46:15 | step: 160000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.041238622041419e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.8 | consumed tokens: 1310720000.0 | grad norm avg: 0.89 | grad norm last: 0.82 | 
2025-12-30T07:46:35 | step: 160100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.0384954041219316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.33 | consumed tokens: 1311539200.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:46:53 | step: 160200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 2.0357529137982056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.58 | consumed tokens: 1312358400.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:47:12 | step: 160300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.0330109691713005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.19 | consumed tokens: 1313177600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T07:47:31 | step: 160400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.030269752140157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.48 | consumed tokens: 1313996800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:47:49 | step: 160500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.0275290808058344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.12 | consumed tokens: 1314816000.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:48:08 | step: 160600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.0247887732693925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.94 | consumed tokens: 1315635200.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-30T07:48:27 | step: 160700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.0220493752276525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.78 | consumed tokens: 1316454400.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T07:48:45 | step: 160800 | train samples/s: 92.6 | train mfu (16-bit): -1.0 | lr mean: 2.019310340983793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.22 | consumed tokens: 1317273600.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T07:49:04 | step: 160900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.0165720343356952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.69 | consumed tokens: 1318092800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T07:49:23 | step: 161000 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.0138342733844183e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.02 | consumed tokens: 1318912000.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2025-12-30T07:49:41 | step: 161100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.011097240028903e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.03 | consumed tokens: 1319731200.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T07:50:00 | step: 161200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.008360570471268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.91 | consumed tokens: 1320550400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:50:19 | step: 161300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.0056248104083352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.0 | consumed tokens: 1321369600.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T07:50:37 | step: 161400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.002889414143283e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.47 | consumed tokens: 1322188800.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:50:56 | step: 161500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.000154745473992e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.0 | consumed tokens: 1323008000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:51:14 | step: 161600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.9974208044004627e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.3 | consumed tokens: 1323827200.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:51:33 | step: 161700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.9946874090237543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.97 | consumed tokens: 1324646400.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:51:51 | step: 161800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.9919547412428074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.23 | consumed tokens: 1325465600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T07:52:10 | step: 161900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.9892226191586815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.95 | consumed tokens: 1326284800.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T07:52:29 | step: 162000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.9864910427713767e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 1327104000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:52:48 | step: 162100 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 1.9837601939798333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.95 | consumed tokens: 1327923200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:53:06 | step: 162200 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.9810300727840513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.11 | consumed tokens: 1328742400.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T07:53:25 | step: 162300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.9783004972850904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.7 | consumed tokens: 1329561600.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T07:53:43 | step: 162400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.975571649381891e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.92 | consumed tokens: 1330380800.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T07:54:02 | step: 162500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.9728433471755125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.86 | consumed tokens: 1331200000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T07:54:21 | step: 162600 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.9701159544638358e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.14 | consumed tokens: 1332019200.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T07:54:39 | step: 162700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.96738892555004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.44 | consumed tokens: 1332838400.0 | grad norm avg: 0.88 | grad norm last: 0.8 | 
2025-12-30T07:54:58 | step: 162800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.9646628061309457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.36 | consumed tokens: 1333657600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T07:55:17 | step: 162900 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.9619372324086726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 1334476800.0 | grad norm avg: 0.89 | grad norm last: 1.0 | 
2025-12-30T07:55:35 | step: 163000 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 1.959212386282161e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.86 | consumed tokens: 1335296000.0 | grad norm avg: 0.89 | grad norm last: 0.78 | 
2025-12-30T07:55:54 | step: 163100 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.9564880858524702e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 1336115200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:56:13 | step: 163200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.953764513018541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.91 | consumed tokens: 1336934400.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T07:56:31 | step: 163300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.9510418496793136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.61 | consumed tokens: 1337753600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T07:56:50 | step: 163400 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 1.948319550137967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 1338572800.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T07:57:08 | step: 163500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.945598160091322e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.38 | consumed tokens: 1339392000.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T07:57:27 | step: 163600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.9428774976404384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.73 | consumed tokens: 1340211200.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T07:57:45 | step: 163700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.940157380886376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.97 | consumed tokens: 1341030400.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T07:58:04 | step: 163800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.937437991728075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.12 | consumed tokens: 1341849600.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:58:22 | step: 163900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.9347193301655352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.23 | consumed tokens: 1342668800.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:58:41 | step: 164000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.932001396198757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 1343488000.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T07:58:59 | step: 164100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.9292841898277402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.16 | consumed tokens: 1344307200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T07:59:18 | step: 164200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.9265677110524848e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.06 | consumed tokens: 1345126400.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T07:59:36 | step: 164300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.923851959872991e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.39 | consumed tokens: 1345945600.0 | grad norm avg: 0.88 | grad norm last: 0.96 | 
2025-12-30T07:59:55 | step: 164400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.921136754390318e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.92 | consumed tokens: 1346764800.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:00:13 | step: 164500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.9184224584023468e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.22 | consumed tokens: 1347584000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:00:32 | step: 164600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.915708890010137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.34 | consumed tokens: 1348403200.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T08:00:51 | step: 164700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.9129960492136888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.69 | consumed tokens: 1349222400.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T08:01:09 | step: 164800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.910283936013002e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.56 | consumed tokens: 1350041600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T08:01:28 | step: 164900 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 1.9075725504080765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.33 | consumed tokens: 1350860800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T08:01:46 | step: 165000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.9048618923989125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.86 | consumed tokens: 1351680000.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:02:06 | step: 165100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.90215196198551e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.3 | consumed tokens: 1352499200.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T08:02:25 | step: 165200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.8994427591678686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.67 | consumed tokens: 1353318400.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T08:02:43 | step: 165300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.896734283945989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.92 | consumed tokens: 1354137600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:03:02 | step: 165400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.894026718218811e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1354956800.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T08:03:21 | step: 165500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.891319698188454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.23 | consumed tokens: 1355776000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T08:03:39 | step: 165600 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 1.8886135876527987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 1356595200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T08:03:58 | step: 165700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.885908204712905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.02 | consumed tokens: 1357414400.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T08:04:17 | step: 165800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.883203731267713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.22 | consumed tokens: 1358233600.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T08:04:35 | step: 165900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.880499803519342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 1359052800.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T08:04:54 | step: 166000 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.877796785265673e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.25 | consumed tokens: 1359872000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T08:05:12 | step: 166100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.8750944946077652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.84 | consumed tokens: 1360691200.0 | grad norm avg: 0.9 | grad norm last: 0.99 | 
2025-12-30T08:05:31 | step: 166200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.8723931134445593e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.17 | consumed tokens: 1361510400.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T08:05:50 | step: 166300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.8696924598771147e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.19 | consumed tokens: 1362329600.0 | grad norm avg: 0.9 | grad norm last: 0.81 | 
2025-12-30T08:06:08 | step: 166400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.8669925339054316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 1363148800.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T08:06:26 | step: 166500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.86429333552951e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.38 | consumed tokens: 1363968000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:06:45 | step: 166600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.86159504664829e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 1364787200.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T08:07:03 | step: 166700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.858897667261772e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.28 | consumed tokens: 1365606400.0 | grad norm avg: 0.89 | grad norm last: 0.8 | 
2025-12-30T08:07:22 | step: 166800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.8562008335720748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.8 | consumed tokens: 1366425600.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:07:40 | step: 166900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.8535049093770795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.92 | consumed tokens: 1367244800.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:07:59 | step: 167000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.850809894676786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.14 | consumed tokens: 1368064000.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T08:08:17 | step: 167100 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 1.8481156075722538e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.97 | consumed tokens: 1368883200.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:08:35 | step: 167200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.8454222299624234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.78 | consumed tokens: 1369702400.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T08:08:54 | step: 167300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.8427295799483545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.55 | consumed tokens: 1370521600.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:09:12 | step: 167400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.840037657530047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.16 | consumed tokens: 1371340800.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T08:09:30 | step: 167500 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 1.8373468265053816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.33 | consumed tokens: 1372160000.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T08:09:49 | step: 167600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.8346565411775373e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.11 | consumed tokens: 1372979200.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T08:10:07 | step: 167700 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.831967347243335e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.2 | consumed tokens: 1373798400.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T08:10:25 | step: 167800 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.8292788809048943e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.0 | consumed tokens: 1374617600.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T08:10:44 | step: 167900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.826591142162215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.23 | consumed tokens: 1375436800.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:11:02 | step: 168000 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 1.8239043129142374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 1376256000.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T08:11:21 | step: 168100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.8212183931609616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.97 | consumed tokens: 1377075200.0 | grad norm avg: 0.89 | grad norm last: 0.94 | 
2025-12-30T08:11:39 | step: 168200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.8185333829023875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.95 | consumed tokens: 1377894400.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T08:11:58 | step: 168300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.815849100239575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.02 | consumed tokens: 1378713600.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:12:16 | step: 168400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.813165727071464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.11 | consumed tokens: 1379532800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:12:35 | step: 168500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.810483263398055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.19 | consumed tokens: 1380352000.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T08:12:53 | step: 168600 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 1.8078015273204073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.03 | consumed tokens: 1381171200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:13:12 | step: 168700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.8051207007374614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 1381990400.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:13:31 | step: 168800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.8024407836492173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 1382809600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T08:13:49 | step: 168900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.799761776055675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.73 | consumed tokens: 1383628800.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:14:08 | step: 169000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.7970836779568344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.23 | consumed tokens: 1384448000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T08:14:26 | step: 169100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.7944063074537553e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.11 | consumed tokens: 1385267200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T08:14:44 | step: 169200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.791729846445378e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.3 | consumed tokens: 1386086400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:15:03 | step: 169300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.7890544768306427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.0 | consumed tokens: 1386905600.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T08:15:21 | step: 169400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.786379834811669e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.3 | consumed tokens: 1387724800.0 | grad norm avg: 0.89 | grad norm last: 0.94 | 
2025-12-30T08:15:40 | step: 169500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.7837061022873968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.86 | consumed tokens: 1388544000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T08:15:58 | step: 169600 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 1.7810332792578265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.78 | consumed tokens: 1389363200.0 | grad norm avg: 0.89 | grad norm last: 0.96 | 
2025-12-30T08:16:16 | step: 169700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.778361365722958e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.2 | consumed tokens: 1390182400.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:16:35 | step: 169800 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 1.7756903616827913e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.94 | consumed tokens: 1391001600.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T08:16:53 | step: 169900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.7730202671373263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.73 | consumed tokens: 1391820800.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T08:17:11 | step: 170000 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 1.7703509001876228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1392640000.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T08:17:31 | step: 170100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.7676826246315613e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.45 | consumed tokens: 1393459200.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:17:50 | step: 170200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7650152585702017e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.14 | consumed tokens: 1394278400.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T08:18:09 | step: 170300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.7623488020035438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 1395097600.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T08:18:27 | step: 170400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.759683436830528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.77 | consumed tokens: 1395916800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T08:18:46 | step: 170500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.7570187992532738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.61 | consumed tokens: 1396736000.0 | grad norm avg: 0.89 | grad norm last: 0.8 | 
2025-12-30T08:19:04 | step: 170600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.7543550711707212e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.95 | consumed tokens: 1397555200.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:19:23 | step: 170700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.7516924344818108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.44 | consumed tokens: 1398374400.0 | grad norm avg: 0.89 | grad norm last: 0.95 | 
2025-12-30T08:19:41 | step: 170800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.7490305253886618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.44 | consumed tokens: 1399193600.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T08:20:00 | step: 170900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.746369707689155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.69 | consumed tokens: 1400012800.0 | grad norm avg: 0.89 | grad norm last: 0.95 | 
2025-12-30T08:20:18 | step: 171000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.7437097994843498e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 1400832000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T08:20:37 | step: 171100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.7410508007742465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.19 | consumed tokens: 1401651200.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:20:55 | step: 171200 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 1.7383928934577852e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.38 | consumed tokens: 1402470400.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:21:14 | step: 171300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.7357357137370855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.19 | consumed tokens: 1403289600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:21:33 | step: 171400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.7330796254100278e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.84 | consumed tokens: 1404108800.0 | grad norm avg: 0.89 | grad norm last: 0.81 | 
2025-12-30T08:21:51 | step: 171500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.7304246284766123e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.34 | consumed tokens: 1404928000.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:22:10 | step: 171600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.727770359138958e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 1405747200.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T08:22:28 | step: 171700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.7251171811949462e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.67 | consumed tokens: 1406566400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T08:22:47 | step: 171800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.722464912745636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.47 | consumed tokens: 1407385600.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T08:23:05 | step: 171900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.719813735689968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.2 | consumed tokens: 1408204800.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T08:23:24 | step: 172000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.7171634681290016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 1409024000.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:23:42 | step: 172100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.714514110062737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.77 | consumed tokens: 1409843200.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:24:01 | step: 172200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7118658433901146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 1410662400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:24:20 | step: 172300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.709218486212194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.31 | consumed tokens: 1411481600.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:24:38 | step: 172400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.7065722204279155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.44 | consumed tokens: 1412300800.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-30T08:24:57 | step: 172500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.7039268641383387e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.22 | consumed tokens: 1413120000.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:25:15 | step: 172600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.7012824173434637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.31 | consumed tokens: 1413939200.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T08:25:34 | step: 172700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.698639061942231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1414758400.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T08:25:52 | step: 172800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.6959967979346402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.64 | consumed tokens: 1415577600.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:26:11 | step: 172900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.6933554434217513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.84 | consumed tokens: 1416396800.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:26:29 | step: 173000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.6907151803025045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.38 | consumed tokens: 1417216000.0 | grad norm avg: 0.9 | grad norm last: 0.99 | 
2025-12-30T08:26:48 | step: 173100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.6880758266779594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.75 | consumed tokens: 1418035200.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T08:27:06 | step: 173200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.6854375644470565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.89 | consumed tokens: 1418854400.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:27:25 | step: 173300 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.6828002117108554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.42 | consumed tokens: 1419673600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:27:43 | step: 173400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.6801639503682964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.25 | consumed tokens: 1420492800.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T08:28:02 | step: 173500 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.6775287804193795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.7 | consumed tokens: 1421312000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:28:20 | step: 173600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.6748947018641047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.72 | consumed tokens: 1422131200.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T08:28:39 | step: 173700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.6722615328035317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.64 | consumed tokens: 1422950400.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:28:58 | step: 173800 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 1.6696292732376605e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.34 | consumed tokens: 1423769600.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T08:29:16 | step: 173900 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 1.6669982869643718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.03 | consumed tokens: 1424588800.0 | grad norm avg: 0.9 | grad norm last: 0.79 | 
2025-12-30T08:29:35 | step: 174000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.664368210185785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.17 | consumed tokens: 1425408000.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T08:29:54 | step: 174100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.66173922480084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.08 | consumed tokens: 1426227200.0 | grad norm avg: 0.9 | grad norm last: 0.83 | 
2025-12-30T08:30:12 | step: 174200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.6591113308095373e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.7 | consumed tokens: 1427046400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:30:31 | step: 174300 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.6564845282118767e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.09 | consumed tokens: 1427865600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:30:50 | step: 174400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.653858635108918e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.03 | consumed tokens: 1428684800.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T08:31:08 | step: 174500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.6512338333996013e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.19 | consumed tokens: 1429504000.0 | grad norm avg: 0.9 | grad norm last: 0.99 | 
2025-12-30T08:31:27 | step: 174600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.6486101230839267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.94 | consumed tokens: 1430323200.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T08:31:45 | step: 174700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.6459875041618943e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.66 | consumed tokens: 1431142400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T08:32:04 | step: 174800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.643365976633504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.52 | consumed tokens: 1431961600.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:32:23 | step: 174900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.640745540498756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 1432780800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T08:32:41 | step: 175000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.6381260138587095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.59 | consumed tokens: 1433600000.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T08:33:01 | step: 175100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.6355077605112456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.14 | consumed tokens: 1434419200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T08:33:20 | step: 175200 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 1.6328904166584834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 1435238400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T08:33:38 | step: 175300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.6302743460983038e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.81 | consumed tokens: 1436057600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T08:33:57 | step: 175400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.627659185032826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.05 | consumed tokens: 1436876800.0 | grad norm avg: 0.89 | grad norm last: 0.78 | 
2025-12-30T08:34:15 | step: 175500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.6250452972599305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.64 | consumed tokens: 1437696000.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T08:34:34 | step: 175600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.622432318981737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.69 | consumed tokens: 1438515200.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T08:34:53 | step: 175700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.6198206139961258e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.78 | consumed tokens: 1439334400.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T08:35:11 | step: 175800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.6172098185052164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 1440153600.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T08:35:30 | step: 175900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.6146002963068895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.22 | consumed tokens: 1440972800.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:35:48 | step: 176000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.6119918655022047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.84 | consumed tokens: 1441792000.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T08:36:07 | step: 176100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.609384526091162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.64 | consumed tokens: 1442611200.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:36:25 | step: 176200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.6067782780737616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.3 | consumed tokens: 1443430400.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T08:36:43 | step: 176300 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.6041731214500032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1444249600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:37:02 | step: 176400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.601569056219887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.64 | consumed tokens: 1445068800.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T08:37:21 | step: 176500 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 1.5989662642823532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 1445888000.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T08:37:39 | step: 176600 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.5963645637384616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.73 | consumed tokens: 1446707200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:37:58 | step: 176700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.593763954588212e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 1447526400.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:38:16 | step: 176800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.5911644368316047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.0 | consumed tokens: 1448345600.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T08:38:35 | step: 176900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.5885660104686394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.41 | consumed tokens: 1449164800.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:38:54 | step: 177000 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.5859688573982567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.0 | consumed tokens: 1449984000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T08:39:12 | step: 177100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.583372795721516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.28 | consumed tokens: 1450803200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T08:39:31 | step: 177200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.5807778254384175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.17 | consumed tokens: 1451622400.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:39:49 | step: 177300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.5781841284479015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.02 | consumed tokens: 1452441600.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T08:40:08 | step: 177400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.5755915228510275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.91 | consumed tokens: 1453260800.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T08:40:27 | step: 177500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.573000190546736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.72 | consumed tokens: 1454080000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T08:40:45 | step: 177600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.5704097677371465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.8 | consumed tokens: 1454899200.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:41:04 | step: 177700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.5678208001190796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 1455718400.0 | grad norm avg: 0.9 | grad norm last: 0.8 | 
2025-12-30T08:41:23 | step: 177800 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 1.5652327419957146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 1456537600.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T08:41:41 | step: 177900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.562645957164932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.94 | consumed tokens: 1457356800.0 | grad norm avg: 0.89 | grad norm last: 0.94 | 
2025-12-30T08:42:00 | step: 178000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.560060445626732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.39 | consumed tokens: 1458176000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T08:42:19 | step: 178100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.557476025482174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.05 | consumed tokens: 1458995200.0 | grad norm avg: 0.89 | grad norm last: 0.96 | 
2025-12-30T08:42:37 | step: 178200 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.5548926967312582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.52 | consumed tokens: 1459814400.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T08:42:56 | step: 178300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.552310641272925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.05 | consumed tokens: 1460633600.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T08:43:14 | step: 178400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.549729859107174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.75 | consumed tokens: 1461452800.0 | grad norm avg: 0.89 | grad norm last: 1.04 | 
2025-12-30T08:43:33 | step: 178500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5471501683350652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 1462272000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:43:51 | step: 178600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5445715689565986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.52 | consumed tokens: 1463091200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T08:44:10 | step: 178700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5419942428707145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 1463910400.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T08:44:28 | step: 178800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.539418190077413e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.03 | consumed tokens: 1464729600.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T08:44:47 | step: 178900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.5368434105766937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.95 | consumed tokens: 1465548800.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T08:45:05 | step: 179000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.5342697224696167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.67 | consumed tokens: 1466368000.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T08:45:24 | step: 179100 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.5316971257561818e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.41 | consumed tokens: 1467187200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T08:45:42 | step: 179200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.5291259842342697e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.5 | consumed tokens: 1468006400.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T08:46:01 | step: 179300 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 1.5265559341059998e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.41 | consumed tokens: 1468825600.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T08:46:19 | step: 179400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.5239870663208421e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.2 | consumed tokens: 1469644800.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T08:46:37 | step: 179500 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 1.5214193808787968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.02 | consumed tokens: 1470464000.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T08:46:56 | step: 179600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.5188530596788041e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 1471283200.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T08:47:14 | step: 179700 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 1.5162879208219238e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 1472102400.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T08:47:32 | step: 179800 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 1.5137239643081557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.58 | consumed tokens: 1472921600.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T08:47:51 | step: 179900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.5111612810869701e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.92 | consumed tokens: 1473740800.0 | grad norm avg: 0.9 | grad norm last: 1.03 | 
2025-12-30T08:48:09 | step: 180000 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 1.508599871158367e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.78 | consumed tokens: 1474560000.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T08:48:29 | step: 180100 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.5060396435728762e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.81 | consumed tokens: 1475379200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T08:48:47 | step: 180200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.503480689279968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 1476198400.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T08:49:05 | step: 180300 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 1.5009229173301719e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.38 | consumed tokens: 1477017600.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T08:49:24 | step: 180400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.4983665096224286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.89 | consumed tokens: 1477836800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T08:49:42 | step: 180500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.4958112842577975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.89 | consumed tokens: 1478656000.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T08:50:00 | step: 180600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 1.493257332185749e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.72 | consumed tokens: 1479475200.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T08:50:19 | step: 180700 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 1.4907046534062829e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.83 | consumed tokens: 1480294400.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T08:50:37 | step: 180800 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 1.4881532479193993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 4.06 | consumed tokens: 1481113600.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T08:50:55 | step: 180900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.4856031157250982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.02 | consumed tokens: 1481932800.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T08:51:13 | step: 181000 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 1.4830541658739094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.95 | consumed tokens: 1482752000.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T08:51:32 | step: 181100 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 1.4805065802647732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.02 | consumed tokens: 1483571200.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T08:51:50 | step: 181200 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.4779602679482196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.47 | consumed tokens: 1484390400.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:52:08 | step: 181300 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.4754151379747782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1485209600.0 | grad norm avg: 0.89 | grad norm last: 0.94 | 
2025-12-30T08:52:26 | step: 181400 | train samples/s: 96.4 | train mfu (16-bit): -1.0 | lr mean: 1.4728713722433895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.0 | consumed tokens: 1486028800.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T08:52:45 | step: 181500 | train samples/s: 96.5 | train mfu (16-bit): -1.0 | lr mean: 1.4703288798045833e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.7 | consumed tokens: 1486848000.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T08:53:03 | step: 181600 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 1.4677877516078297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 1487667200.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T08:53:22 | step: 181700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.4652478057541884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.17 | consumed tokens: 1488486400.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T08:53:40 | step: 181800 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 1.4627092241425999e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1489305600.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T08:53:58 | step: 181900 | train samples/s: 96.2 | train mfu (16-bit): -1.0 | lr mean: 1.4601719158235937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.59 | consumed tokens: 1490124800.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T08:54:17 | step: 182000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.4576358807971701e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.17 | consumed tokens: 1490944000.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T08:54:35 | step: 182100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.4551012100127991e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.83 | consumed tokens: 1491763200.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:54:54 | step: 182200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.4525678125210106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 1492582400.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T08:55:12 | step: 182300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.4500356883218046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.84 | consumed tokens: 1493401600.0 | grad norm avg: 0.9 | grad norm last: 0.81 | 
2025-12-30T08:55:31 | step: 182400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.4475049283646513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.2 | consumed tokens: 1494220800.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T08:55:49 | step: 182500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4449754417000804e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 1495040000.0 | grad norm avg: 0.9 | grad norm last: 1.01 | 
2025-12-30T08:56:08 | step: 182600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.4424473192775622e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.73 | consumed tokens: 1495859200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T08:56:26 | step: 182700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4399205610970967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.84 | consumed tokens: 1496678400.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:56:45 | step: 182800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.4373950762092136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 1497497600.0 | grad norm avg: 0.89 | grad norm last: 0.96 | 
2025-12-30T08:57:03 | step: 182900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.4348709555633832e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.39 | consumed tokens: 1498316800.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T08:57:22 | step: 183000 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 1.4323481082101353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 1499136000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T08:57:41 | step: 183100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.42982662509894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.55 | consumed tokens: 1499955200.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:57:59 | step: 183200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.4273065062297974e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.09 | consumed tokens: 1500774400.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T08:58:17 | step: 183300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4247876606532373e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 1501593600.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T08:58:36 | step: 183400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4222702702682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.97 | consumed tokens: 1502412800.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T08:58:54 | step: 183500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.4197541531757452e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.52 | consumed tokens: 1503232000.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T08:59:13 | step: 183600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.417239400325343e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.88 | consumed tokens: 1504051200.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T08:59:31 | step: 183700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.4147260117169935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.08 | consumed tokens: 1504870400.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T08:59:50 | step: 183800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.4122139873506967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.56 | consumed tokens: 1505689600.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T09:00:08 | step: 183900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.4097033272264525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.41 | consumed tokens: 1506508800.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T09:00:27 | step: 184000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.407194031344261e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.73 | consumed tokens: 1507328000.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T09:00:45 | step: 184100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.4046860997041222e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.42 | consumed tokens: 1508147200.0 | grad norm avg: 0.89 | grad norm last: 0.81 | 
2025-12-30T09:01:04 | step: 184200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.402179532306036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.94 | consumed tokens: 1508966400.0 | grad norm avg: 0.9 | grad norm last: 0.99 | 
2025-12-30T09:01:23 | step: 184300 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.3996743291500024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.84 | consumed tokens: 1509785600.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T09:01:41 | step: 184400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.3971704902360216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.25 | consumed tokens: 1510604800.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T09:02:00 | step: 184500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.3946681065135635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.45 | consumed tokens: 1511424000.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T09:02:18 | step: 184600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.392166996083688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 1512243200.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:02:37 | step: 184700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.3896673408453353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.77 | consumed tokens: 1513062400.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:02:56 | step: 184800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.3871690498490352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.25 | consumed tokens: 1513881600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T09:03:14 | step: 184900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.384672214044258e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.75 | consumed tokens: 1514700800.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:03:33 | step: 185000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.3821767424815334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.75 | consumed tokens: 1515520000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T09:03:53 | step: 185100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.3796826351608615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.11 | consumed tokens: 1516339200.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T09:04:11 | step: 185200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.3771899830317125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.25 | consumed tokens: 1517158400.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:04:30 | step: 185300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.374698695144616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 1517977600.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T09:04:48 | step: 185400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.3722088624490425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.62 | consumed tokens: 1518796800.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T09:05:07 | step: 185500 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.3697203939955216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.22 | consumed tokens: 1519616000.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T09:05:26 | step: 185600 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 1.3672333807335235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.42 | consumed tokens: 1520435200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T09:05:45 | step: 185700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.3647477317135781e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.73 | consumed tokens: 1521254400.0 | grad norm avg: 0.91 | grad norm last: 0.81 | 
2025-12-30T09:06:03 | step: 185800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.3622635378851555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.81 | consumed tokens: 1522073600.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:06:22 | step: 185900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.3597807082987856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 1522892800.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T09:06:40 | step: 186000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.3572994248534087e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 1523712000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:06:58 | step: 186100 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.3548195056500845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 1524531200.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T09:07:17 | step: 186200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.352341041638283e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.98 | consumed tokens: 1525350400.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T09:07:35 | step: 186300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.3498639418685343e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.25 | consumed tokens: 1526169600.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T09:07:54 | step: 186400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.3473883882397786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.97 | consumed tokens: 1526988800.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T09:08:12 | step: 186500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.3449141988530755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.22 | consumed tokens: 1527808000.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T09:08:31 | step: 186600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.3424414646578953e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.53 | consumed tokens: 1528627200.0 | grad norm avg: 0.9 | grad norm last: 0.82 | 
2025-12-30T09:08:49 | step: 186700 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.3399701856542379e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.81 | consumed tokens: 1529446400.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T09:09:07 | step: 186800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.3375003618421033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.53 | consumed tokens: 1530265600.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T09:09:26 | step: 186900 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 1.3350319932214916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.94 | consumed tokens: 1531084800.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T09:09:45 | step: 187000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.3325651707418729e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 1531904000.0 | grad norm avg: 0.9 | grad norm last: 0.82 | 
2025-12-30T09:10:03 | step: 187100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.3300997125043068e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.22 | consumed tokens: 1532723200.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T09:10:22 | step: 187200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.3276357094582636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.88 | consumed tokens: 1533542400.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:10:40 | step: 187300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.3251732525532134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 1534361600.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T09:10:59 | step: 187400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.3227121598902158e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.56 | consumed tokens: 1535180800.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T09:11:17 | step: 187500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.3202526133682113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.56 | consumed tokens: 1536000000.0 | grad norm avg: 0.9 | grad norm last: 1.05 | 
2025-12-30T09:11:36 | step: 187600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.3177945220377296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.03 | consumed tokens: 1536819200.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:11:55 | step: 187700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.3153378858987708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.03 | consumed tokens: 1537638400.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T09:12:13 | step: 187800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.312882795900805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.31 | consumed tokens: 1538457600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T09:12:32 | step: 187900 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.310429161094362e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.3 | consumed tokens: 1539276800.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:12:50 | step: 188000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.3079769814794417e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.64 | consumed tokens: 1540096000.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T09:13:09 | step: 188100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.3055263480055146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.42 | consumed tokens: 1540915200.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T09:13:28 | step: 188200 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.3030771697231103e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.28 | consumed tokens: 1541734400.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:13:46 | step: 188300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.300629537581699e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.88 | consumed tokens: 1542553600.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T09:14:05 | step: 188400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2981833606318105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.73 | consumed tokens: 1543372800.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T09:14:24 | step: 188500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.295738729822915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 1544192000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:14:42 | step: 188600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.2932955542055424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 1545011200.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T09:15:01 | step: 188700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.2908539247291628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.73 | consumed tokens: 1545830400.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:15:19 | step: 188800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.288413750444306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.05 | consumed tokens: 1546649600.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2025-12-30T09:15:38 | step: 188900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.2859752132499125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.89 | consumed tokens: 1547468800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T09:15:56 | step: 189000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.2835381312470417e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.34 | consumed tokens: 1548288000.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T09:16:15 | step: 189100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.2811025044356938e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.48 | consumed tokens: 1549107200.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T09:16:34 | step: 189200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.2786685147148091e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 1549926400.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T09:16:52 | step: 189300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.2762359801854473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.97 | consumed tokens: 1550745600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T09:17:11 | step: 189400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.2738049917970784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.12 | consumed tokens: 1551564800.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T09:17:29 | step: 189500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.2713755495497026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1552384000.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T09:17:48 | step: 189600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.2689476534433197e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.91 | consumed tokens: 1553203200.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T09:18:07 | step: 189700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.2665213034779299e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.97 | consumed tokens: 1554022400.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T09:18:25 | step: 189800 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.2640964996535331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.7 | consumed tokens: 1554841600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T09:18:44 | step: 189900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.2616731510206591e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1555660800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:19:02 | step: 190000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.2592514394782484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.97 | consumed tokens: 1556480000.0 | grad norm avg: 0.91 | grad norm last: 1.01 | 
2025-12-30T09:19:22 | step: 190100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2568312740768306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.19 | consumed tokens: 1557299200.0 | grad norm avg: 0.91 | grad norm last: 0.99 | 
2025-12-30T09:19:41 | step: 190200 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.2544126548164058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.72 | consumed tokens: 1558118400.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T09:20:00 | step: 190300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2519955816969741e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.95 | consumed tokens: 1558937600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T09:20:18 | step: 190400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2495801456680056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.62 | consumed tokens: 1559756800.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:20:37 | step: 190500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.2471661648305599e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 1560576000.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T09:20:55 | step: 190600 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 1.2447538210835773e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.16 | consumed tokens: 1561395200.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T09:21:14 | step: 190700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.2423430234775878e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.34 | consumed tokens: 1562214400.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T09:21:33 | step: 190800 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 1.2399338629620615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.92 | consumed tokens: 1563033600.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T09:21:51 | step: 190900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.237526157638058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.62 | consumed tokens: 1563852800.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:22:10 | step: 191000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.2351200894045178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.72 | consumed tokens: 1564672000.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:22:28 | step: 191100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.2327156582614407e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 1565491200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T09:22:47 | step: 191200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.2303127732593566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.06 | consumed tokens: 1566310400.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T09:23:05 | step: 191300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.2279114343982656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.03 | consumed tokens: 1567129600.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T09:23:24 | step: 191400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.2255117326276377e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.55 | consumed tokens: 1567948800.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T09:23:42 | step: 191500 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.223113667947473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.45 | consumed tokens: 1568768000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T09:24:01 | step: 191600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.2207171494083013e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.7 | consumed tokens: 1569587200.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:24:19 | step: 191700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.2183221770101227e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.84 | consumed tokens: 1570406400.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T09:24:37 | step: 191800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.2159289326518774e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.48 | consumed tokens: 1571225600.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:24:56 | step: 191900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.2135372344346251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.14 | consumed tokens: 1572044800.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T09:25:14 | step: 192000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.2111470823583659e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.69 | consumed tokens: 1572864000.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T09:25:33 | step: 192100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.20875865832204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.44 | consumed tokens: 1573683200.0 | grad norm avg: 0.91 | grad norm last: 0.99 | 
2025-12-30T09:25:51 | step: 192200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2063717804267071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.52 | consumed tokens: 1574502400.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:26:10 | step: 192300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2039865396218374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.25 | consumed tokens: 1575321600.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:26:29 | step: 192400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.201602935907431e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.64 | consumed tokens: 1576140800.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T09:26:47 | step: 192500 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.1992208783340175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.56 | consumed tokens: 1576960000.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T09:27:06 | step: 192600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.1968405488005374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1577779200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T09:27:24 | step: 192700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1944617654080503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.05 | consumed tokens: 1578598400.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T09:27:43 | step: 192800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.1920847100554965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.67 | consumed tokens: 1579417600.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:28:01 | step: 192900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.1897092008439358e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.31 | consumed tokens: 1580236800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:28:20 | step: 193000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.1873354196723085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.84 | consumed tokens: 1581056000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:28:39 | step: 193100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.1849631846416742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.52 | consumed tokens: 1581875200.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T09:28:57 | step: 193200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.1825926776509732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 1582694400.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T09:29:16 | step: 193300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1802238077507354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 1583513600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T09:29:34 | step: 193400 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 1.1778565749409609e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.94 | consumed tokens: 1584332800.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T09:29:53 | step: 193500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.1754909792216495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 4.12 | consumed tokens: 1585152000.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T09:30:12 | step: 193600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1731270205928013e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.02 | consumed tokens: 1585971200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:30:30 | step: 193700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.1707647900038864e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 1586790400.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:30:49 | step: 193800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.1684041965054348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.95 | consumed tokens: 1587609600.0 | grad norm avg: 0.92 | grad norm last: 1.0 | 
2025-12-30T09:31:07 | step: 193900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.1660452400974464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.58 | consumed tokens: 1588428800.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T09:31:26 | step: 194000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.1636880117293913e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.14 | consumed tokens: 1589248000.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T09:31:44 | step: 194100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.1613324204517994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.78 | consumed tokens: 1590067200.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T09:32:03 | step: 194200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.1589784662646707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 1590886400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:32:21 | step: 194300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.1566262401174754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.64 | consumed tokens: 1591705600.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T09:32:39 | step: 194400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.1542756510607433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.69 | consumed tokens: 1592524800.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:32:58 | step: 194500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.1519267900439445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.8 | consumed tokens: 1593344000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:33:16 | step: 194600 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.1495796570670791e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.52 | consumed tokens: 1594163200.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T09:33:35 | step: 194700 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 1.1472341611806769e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 1594982400.0 | grad norm avg: 0.91 | grad norm last: 1.03 | 
2025-12-30T09:33:54 | step: 194800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.1448903023847379e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.5 | consumed tokens: 1595801600.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:34:12 | step: 194900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1425481716287322e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.94 | consumed tokens: 1596620800.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:34:31 | step: 195000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.14020776891266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.81 | consumed tokens: 1597440000.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T09:34:51 | step: 195100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.137869094236521e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.53 | consumed tokens: 1598259200.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T09:35:09 | step: 195200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1355320566508453e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 1599078400.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T09:35:28 | step: 195300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.1331968380545732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.83 | consumed tokens: 1599897600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T09:35:46 | step: 195400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.1308632565487642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.97 | consumed tokens: 1600716800.0 | grad norm avg: 0.92 | grad norm last: 1.03 | 
2025-12-30T09:36:05 | step: 195500 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.1285313121334184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.31 | consumed tokens: 1601536000.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:36:24 | step: 195600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.1262011867074762e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.41 | consumed tokens: 1602355200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:36:42 | step: 195700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.1238727893214673e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.36 | consumed tokens: 1603174400.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T09:37:01 | step: 195800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.1215460290259216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.62 | consumed tokens: 1603993600.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T09:37:20 | step: 195900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.1192210877197795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.67 | consumed tokens: 1604812800.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T09:37:38 | step: 196000 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.1168977835041005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.3 | consumed tokens: 1605632000.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T09:37:57 | step: 196100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.1145762982778251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.92 | consumed tokens: 1606451200.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:38:15 | step: 196200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.1122564501420129e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.92 | consumed tokens: 1607270400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T09:38:34 | step: 196300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.1099384209956042e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 1608089600.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:38:52 | step: 196400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.107622119889129e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.27 | consumed tokens: 1608908800.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:39:11 | step: 196500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.105307546822587e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.7 | consumed tokens: 1609728000.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-30T09:39:29 | step: 196600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.1029947017959785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1610547200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T09:39:48 | step: 196700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.1006835848093033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 1611366400.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T09:40:06 | step: 196800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.0983742868120316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.3 | consumed tokens: 1612185600.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:40:25 | step: 196900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.0960667168546934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.45 | consumed tokens: 1613004800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T09:40:43 | step: 197000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.0937608749372885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1613824000.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-30T09:41:02 | step: 197100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0914568520092871e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 1614643200.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T09:41:20 | step: 197200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0891545571212191e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1615462400.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T09:41:39 | step: 197300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0868539902730845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.48 | consumed tokens: 1616281600.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:41:57 | step: 197400 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.0845552424143534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.31 | consumed tokens: 1617100800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T09:42:16 | step: 197500 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.0822582225955557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.64 | consumed tokens: 1617920000.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:42:34 | step: 197600 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.0799630217661615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.88 | consumed tokens: 1618739200.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T09:42:52 | step: 197700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.0776695489767008e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.55 | consumed tokens: 1619558400.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T09:43:11 | step: 197800 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.0753778951766435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.86 | consumed tokens: 1620377600.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T09:43:29 | step: 197900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.0730880603659898e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 1621196800.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T09:43:48 | step: 198000 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 1.0707999535952695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.78 | consumed tokens: 1622016000.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T09:44:06 | step: 198100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.0685136658139527e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 1622835200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T09:44:25 | step: 198200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.0662291060725693e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.3 | consumed tokens: 1623654400.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T09:44:43 | step: 198300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.0639464562700596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 1624473600.0 | grad norm avg: 0.91 | grad norm last: 0.99 | 
2025-12-30T09:45:02 | step: 198400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.0616655345074832e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.23 | consumed tokens: 1625292800.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:45:20 | step: 198500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.0593863407848403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.59 | consumed tokens: 1626112000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:45:39 | step: 198600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.057109057001071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.2 | consumed tokens: 1626931200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T09:45:57 | step: 198700 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.0548335012572352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.44 | consumed tokens: 1627750400.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:46:16 | step: 198800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.052559855452273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.67 | consumed tokens: 1628569600.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T09:46:35 | step: 198900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.0502879376872443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.78 | consumed tokens: 1629388800.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T09:46:53 | step: 199000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.048017838911619e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.19 | consumed tokens: 1630208000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:47:12 | step: 199100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.0457495591253974e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 1631027200.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T09:47:30 | step: 199200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.0434831892780494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.47 | consumed tokens: 1631846400.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T09:47:49 | step: 199300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.0412185474706348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.5 | consumed tokens: 1632665600.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T09:48:07 | step: 199400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0389557246526238e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.66 | consumed tokens: 1633484800.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T09:48:26 | step: 199500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0366947208240163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.86 | consumed tokens: 1634304000.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:48:44 | step: 199600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0344356269342825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.22 | consumed tokens: 1635123200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T09:49:02 | step: 199700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.0321783520339523e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.08 | consumed tokens: 1635942400.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T09:49:21 | step: 199800 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 1.0299228961230256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.55 | consumed tokens: 1636761600.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:49:39 | step: 199900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.0276692592015024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.44 | consumed tokens: 1637580800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T09:49:58 | step: 200000 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.0254174412693828e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 1638400000.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T09:50:17 | step: 200100 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 1.023167533276137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.61 | consumed tokens: 1639219200.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T09:50:36 | step: 200200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.0209194442722946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.3 | consumed tokens: 1640038400.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:50:54 | step: 200300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.0186731742578559e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.83 | consumed tokens: 1640857600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T09:51:13 | step: 200400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.0164288141822908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 1641676800.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T09:51:32 | step: 200500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.0141862730961293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.12 | consumed tokens: 1642496000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:51:50 | step: 200600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.0119455509993713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 1643315200.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T09:52:08 | step: 200700 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0097067388414871e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.8 | consumed tokens: 1644134400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:52:27 | step: 200800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0074698366224766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.16 | consumed tokens: 1644953600.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T09:52:45 | step: 200900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.0052347533928696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.45 | consumed tokens: 1645772800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T09:53:04 | step: 201000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.0030015801021364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.56 | consumed tokens: 1646592000.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T09:53:23 | step: 201100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.0007702258008067e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 1647411200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T09:53:41 | step: 201200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 9.985407814383507e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 1648230400.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T09:54:00 | step: 201300 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 9.963131560652982e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.77 | consumed tokens: 1649049600.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T09:54:18 | step: 201400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 9.940875315805897e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.8 | consumed tokens: 1649868800.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T09:54:37 | step: 201500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 9.918637260852847e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 1650688000.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T09:54:55 | step: 201600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 9.896417395793833e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.05 | consumed tokens: 1651507200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T09:55:14 | step: 201700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 9.874217539618257e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 1652326400.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T09:55:32 | step: 201800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 9.852035873336717e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 1653145600.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T09:55:51 | step: 201900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 9.829873306443915e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.84 | consumed tokens: 1653964800.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T09:56:09 | step: 202000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.80772983893985e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.06 | consumed tokens: 1654784000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T09:56:28 | step: 202100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 9.785605470824521e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 1655603200.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T09:56:46 | step: 202200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 9.76350020209793e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.8 | consumed tokens: 1656422400.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:57:05 | step: 202300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 9.741413123265374e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.48 | consumed tokens: 1657241600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T09:57:23 | step: 202400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 9.719346053316258e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.33 | consumed tokens: 1658060800.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T09:57:42 | step: 202500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 9.697298082755879e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 1658880000.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:58:00 | step: 202600 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 9.675269211584236e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.92 | consumed tokens: 1659699200.0 | grad norm avg: 0.92 | grad norm last: 1.0 | 
2025-12-30T09:58:19 | step: 202700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 9.653259439801332e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.12 | consumed tokens: 1660518400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T09:58:37 | step: 202800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 9.631268767407164e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.72 | consumed tokens: 1661337600.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T09:58:56 | step: 202900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 9.609297194401734e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.06 | consumed tokens: 1662156800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T09:59:14 | step: 203000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 9.58734472078504e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.92 | consumed tokens: 1662976000.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T09:59:33 | step: 203100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 9.565412256051786e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.2 | consumed tokens: 1663795200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T09:59:51 | step: 203200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 9.54349889070727e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 1664614400.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T10:00:10 | step: 203300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 9.52160462475149e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.61 | consumed tokens: 1665433600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T10:00:29 | step: 203400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 9.499729458184447e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.98 | consumed tokens: 1666252800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T10:00:47 | step: 203500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 9.477874300500844e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 1667072000.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:01:06 | step: 203600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.456038242205977e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.31 | consumed tokens: 1667891200.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T10:01:24 | step: 203700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 9.434221283299848e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1668710400.0 | grad norm avg: 0.93 | grad norm last: 1.0 | 
2025-12-30T10:01:43 | step: 203800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 9.412424333277158e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 1669529600.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T10:02:01 | step: 203900 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 9.390646482643206e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.72 | consumed tokens: 1670348800.0 | grad norm avg: 0.93 | grad norm last: 0.86 | 
2025-12-30T10:02:20 | step: 204000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 9.368888640892692e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.66 | consumed tokens: 1671168000.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T10:02:38 | step: 204100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 9.347149898530915e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.72 | consumed tokens: 1671987200.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T10:02:57 | step: 204200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 9.325431165052578e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.58 | consumed tokens: 1672806400.0 | grad norm avg: 0.93 | grad norm last: 1.01 | 
2025-12-30T10:03:15 | step: 204300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 9.303731530962978e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.02 | consumed tokens: 1673625600.0 | grad norm avg: 0.92 | grad norm last: 1.03 | 
2025-12-30T10:03:33 | step: 204400 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 9.282051905756816e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.88 | consumed tokens: 1674444800.0 | grad norm avg: 0.92 | grad norm last: 0.83 | 
2025-12-30T10:03:52 | step: 204500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 9.260392289434094e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.94 | consumed tokens: 1675264000.0 | grad norm avg: 0.92 | grad norm last: 0.82 | 
2025-12-30T10:04:10 | step: 204600 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 9.238751772500109e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.38 | consumed tokens: 1676083200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T10:04:29 | step: 204700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 9.217131264449563e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.89 | consumed tokens: 1676902400.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T10:04:47 | step: 204800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 9.195529855787754e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.59 | consumed tokens: 1677721600.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T10:05:06 | step: 204900 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 9.173949365504086e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 1678540800.0 | grad norm avg: 0.93 | grad norm last: 0.83 | 
2025-12-30T10:05:24 | step: 205000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 9.152387974609155e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.14 | consumed tokens: 1679360000.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T10:05:44 | step: 205100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.130846592597663e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.56 | consumed tokens: 1680179200.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T10:06:03 | step: 205200 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 9.109324309974909e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.3 | consumed tokens: 1680998400.0 | grad norm avg: 0.93 | grad norm last: 0.85 | 
2025-12-30T10:06:22 | step: 205300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 9.087822945730295e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 1681817600.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T10:06:40 | step: 205400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 9.06634159036912e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.86 | consumed tokens: 1682636800.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:06:58 | step: 205500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 9.044879334396683e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.73 | consumed tokens: 1683456000.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:07:17 | step: 205600 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 9.023437087307684e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.69 | consumed tokens: 1684275200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T10:07:36 | step: 205700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 9.002015758596826e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.83 | consumed tokens: 1685094400.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T10:07:54 | step: 205800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.980613529274706e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 1685913600.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T10:08:13 | step: 205900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.959231308836024e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.44 | consumed tokens: 1686732800.0 | grad norm avg: 0.93 | grad norm last: 0.8 | 
2025-12-30T10:08:31 | step: 206000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 8.937870006775483e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 1687552000.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T10:08:50 | step: 206100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 8.91652780410368e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.45 | consumed tokens: 1688371200.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-30T10:09:08 | step: 206200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 8.895206519810017e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.42 | consumed tokens: 1689190400.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T10:09:27 | step: 206300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.873905244399793e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.19 | consumed tokens: 1690009600.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T10:09:45 | step: 206400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.852623977873009e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.91 | consumed tokens: 1690828800.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T10:10:04 | step: 206500 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 8.831362720229663e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.14 | consumed tokens: 1691648000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T10:10:23 | step: 206600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 8.810122380964458e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.41 | consumed tokens: 1692467200.0 | grad norm avg: 0.93 | grad norm last: 1.0 | 
2025-12-30T10:10:41 | step: 206700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 8.78890114108799e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.2 | consumed tokens: 1693286400.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T10:11:00 | step: 206800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 8.767700819589663e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.97 | consumed tokens: 1694105600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:11:18 | step: 206900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 8.746521416469477e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 1694924800.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T10:11:37 | step: 207000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.725361112738028e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 1695744000.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T10:11:55 | step: 207100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.704222636879422e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.09 | consumed tokens: 1696563200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T10:12:14 | step: 207200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.683103260409553e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.02 | consumed tokens: 1697382400.0 | grad norm avg: 0.93 | grad norm last: 1.0 | 
2025-12-30T10:12:32 | step: 207300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 8.662004802317824e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.92 | consumed tokens: 1698201600.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T10:12:51 | step: 207400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 8.640926353109535e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.42 | consumed tokens: 1699020800.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T10:13:09 | step: 207500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 8.619868822279386e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.62 | consumed tokens: 1699840000.0 | grad norm avg: 0.93 | grad norm last: 1.01 | 
2025-12-30T10:13:28 | step: 207600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 8.598832209827378e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 1700659200.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T10:13:46 | step: 207700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 8.57781560625881e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.23 | consumed tokens: 1701478400.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:14:05 | step: 207800 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 8.556819921068382e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.73 | consumed tokens: 1702297600.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T10:14:24 | step: 207900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 8.535844244761392e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.67 | consumed tokens: 1703116800.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:14:42 | step: 208000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.514889486832544e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1703936000.0 | grad norm avg: 0.93 | grad norm last: 1.02 | 
2025-12-30T10:15:01 | step: 208100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 8.493954737787135e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.2 | consumed tokens: 1704755200.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T10:15:19 | step: 208200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.473041816614568e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.0 | consumed tokens: 1705574400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T10:15:38 | step: 208300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.45214890432544e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 1706393600.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T10:15:56 | step: 208400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 8.431276000919752e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 1707212800.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T10:16:15 | step: 208500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.410424925386906e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 1708032000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:16:34 | step: 208600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.389593858737499e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.61 | consumed tokens: 1708851200.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T10:16:52 | step: 208700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.368784619960934e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.84 | consumed tokens: 1709670400.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T10:17:11 | step: 208800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.347995390067808e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.17 | consumed tokens: 1710489600.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T10:17:29 | step: 208900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.327227078552824e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 1711308800.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:17:48 | step: 209000 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 8.30647968541598e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.41 | consumed tokens: 1712128000.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T10:18:07 | step: 209100 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 8.285753210657276e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.58 | consumed tokens: 1712947200.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T10:18:25 | step: 209200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.265046744782012e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.22 | consumed tokens: 1713766400.0 | grad norm avg: 0.93 | grad norm last: 1.0 | 
2025-12-30T10:18:44 | step: 209300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 8.24436210677959e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.88 | consumed tokens: 1714585600.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T10:19:03 | step: 209400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 8.22369838715531e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 1715404800.0 | grad norm avg: 0.94 | grad norm last: 1.03 | 
2025-12-30T10:19:21 | step: 209500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.20305558590917e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.58 | consumed tokens: 1716224000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:19:40 | step: 209600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.182434612535872e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.7 | consumed tokens: 1717043200.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:19:58 | step: 209700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 8.161833648046013e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.8 | consumed tokens: 1717862400.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:20:17 | step: 209800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 8.141253601934295e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.06 | consumed tokens: 1718681600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:20:35 | step: 209900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.12069538369542e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.53 | consumed tokens: 1719500800.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T10:20:54 | step: 210000 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 8.100158083834685e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.16 | consumed tokens: 1720320000.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T10:21:14 | step: 210100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 8.079641702352092e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.09 | consumed tokens: 1721139200.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T10:21:33 | step: 210200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 8.059146239247639e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.53 | consumed tokens: 1721958400.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:21:51 | step: 210300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.038672604016028e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.36 | consumed tokens: 1722777600.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-30T10:22:10 | step: 210400 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 8.018219887162559e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.67 | consumed tokens: 1723596800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T10:22:29 | step: 210500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 7.99778808868723e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.5 | consumed tokens: 1724416000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T10:22:47 | step: 210600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 7.977378118084744e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.08 | consumed tokens: 1725235200.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T10:23:06 | step: 210700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.956989065860398e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.86 | consumed tokens: 1726054400.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T10:23:24 | step: 210800 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 7.936621841508895e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.34 | consumed tokens: 1726873600.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T10:23:43 | step: 210900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 7.916275535535533e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.94 | consumed tokens: 1727692800.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:24:02 | step: 211000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.895951057435013e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 1728512000.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T10:24:20 | step: 211100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.875647497712635e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.28 | consumed tokens: 1729331200.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:24:39 | step: 211200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 7.855364856368396e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.61 | consumed tokens: 1730150400.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T10:24:57 | step: 211300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 7.835104952391703e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.36 | consumed tokens: 1730969600.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T10:25:16 | step: 211400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 7.81486596679315e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.11 | consumed tokens: 1731788800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:25:34 | step: 211500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.794647899572738e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.23 | consumed tokens: 1732608000.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T10:25:53 | step: 211600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 7.774451660225168e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 1733427200.0 | grad norm avg: 0.94 | grad norm last: 0.86 | 
2025-12-30T10:26:11 | step: 211700 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 7.75427724875044e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 1734246400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T10:26:30 | step: 211800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 7.734124665148556e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.97 | consumed tokens: 1735065600.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:26:48 | step: 211900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 7.713992999924812e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.81 | consumed tokens: 1735884800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:27:07 | step: 212000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.693883162573911e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.73 | consumed tokens: 1736704000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:27:25 | step: 212100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 7.673795153095853e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.23 | consumed tokens: 1737523200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:27:44 | step: 212200 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 7.653728971490636e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.42 | consumed tokens: 1738342400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T10:28:02 | step: 212300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 7.633683708263561e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.5 | consumed tokens: 1739161600.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T10:28:21 | step: 212400 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 7.61366118240403e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.98 | consumed tokens: 1739980800.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:28:39 | step: 212500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 7.59365957492264e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.91 | consumed tokens: 1740800000.0 | grad norm avg: 0.93 | grad norm last: 1.08 | 
2025-12-30T10:28:58 | step: 212600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.573680250061443e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.08 | consumed tokens: 1741619200.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T10:29:16 | step: 212700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 7.553722753073089e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 1742438400.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T10:29:35 | step: 212800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 7.533786629210226e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.48 | consumed tokens: 1743257600.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T10:29:53 | step: 212900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.513872787967557e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.47 | consumed tokens: 1744076800.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T10:30:12 | step: 213000 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 7.4939807745977305e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 1744896000.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T10:30:30 | step: 213100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.4741105891007464e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.92 | consumed tokens: 1745715200.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:30:49 | step: 213200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 7.454262231476605e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.94 | consumed tokens: 1746534400.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T10:31:07 | step: 213300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 7.434435701725306e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 1747353600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:31:26 | step: 213400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 7.4146314545942005e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.94 | consumed tokens: 1748172800.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:31:44 | step: 213500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 7.3948490353359375e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.5 | consumed tokens: 1748992000.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:32:02 | step: 213600 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 7.375088898697868e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.5 | consumed tokens: 1749811200.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T10:32:21 | step: 213700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 7.355350589932641e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.95 | consumed tokens: 1750630400.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T10:32:39 | step: 213800 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 7.335634109040257e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.41 | consumed tokens: 1751449600.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T10:32:58 | step: 213900 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 7.315939910768066e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 1752268800.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T10:33:16 | step: 214000 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 7.296267995116068e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.45 | consumed tokens: 1753088000.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:33:34 | step: 214100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 7.276618362084264e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.03 | consumed tokens: 1753907200.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T10:33:53 | step: 214200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 7.256990556925302e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.75 | consumed tokens: 1754726400.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:34:11 | step: 214300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 7.237385034386534e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 1755545600.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T10:34:29 | step: 214400 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 7.2178017944679596e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.86 | consumed tokens: 1756364800.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T10:34:48 | step: 214500 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 7.198240837169578e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.97 | consumed tokens: 1757184000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T10:35:06 | step: 214600 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 7.1787021624913905e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.88 | consumed tokens: 1758003200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:35:24 | step: 214700 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 7.159185770433396e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.5 | consumed tokens: 1758822400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:35:43 | step: 214800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 7.139691206248244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.44 | consumed tokens: 1759641600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:36:01 | step: 214900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.120219379430637e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.06 | consumed tokens: 1760460800.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T10:36:20 | step: 215000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 7.100770289980574e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.8 | consumed tokens: 1761280000.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T10:36:40 | step: 215100 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.081343028403353e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.62 | consumed tokens: 1762099200.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T10:36:58 | step: 215200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 7.061938504193677e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.77 | consumed tokens: 1762918400.0 | grad norm avg: 0.94 | grad norm last: 0.85 | 
2025-12-30T10:37:17 | step: 215300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 7.042556262604194e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.48 | consumed tokens: 1763737600.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T10:37:36 | step: 215400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.023196758382255e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1764556800.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:37:54 | step: 215500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.003859082033159e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.12 | consumed tokens: 1765376000.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T10:38:13 | step: 215600 | train samples/s: 92.7 | train mfu (16-bit): -1.0 | lr mean: 6.984544597798958e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.05 | consumed tokens: 1766195200.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T10:38:32 | step: 215700 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.965252396184951e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.88 | consumed tokens: 1767014400.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T10:38:50 | step: 215800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 6.945982931938488e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.75 | consumed tokens: 1767833600.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T10:39:09 | step: 215900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 6.9267357503122184e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.44 | consumed tokens: 1768652800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T10:39:27 | step: 216000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 6.907511306053493e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.17 | consumed tokens: 1769472000.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T10:39:46 | step: 216100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 6.888309599162312e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.45 | consumed tokens: 1770291200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:40:04 | step: 216200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 6.869130174891325e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.58 | consumed tokens: 1771110400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:40:23 | step: 216300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.849973942735232e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 1771929600.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T10:40:41 | step: 216400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 6.8308399931993335e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1772748800.0 | grad norm avg: 0.93 | grad norm last: 0.85 | 
2025-12-30T10:41:00 | step: 216500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 6.811728781030979e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.53 | consumed tokens: 1773568000.0 | grad norm avg: 0.94 | grad norm last: 0.82 | 
2025-12-30T10:41:18 | step: 216600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 6.79264076097752e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 1774387200.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:41:37 | step: 216700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.773575023544254e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.5 | consumed tokens: 1775206400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:41:55 | step: 216800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.754532478225883e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.91 | consumed tokens: 1776025600.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T10:42:14 | step: 216900 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 6.735512670275057e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.84 | consumed tokens: 1776844800.0 | grad norm avg: 0.94 | grad norm last: 0.85 | 
2025-12-30T10:42:32 | step: 217000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.716515599691775e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1777664000.0 | grad norm avg: 0.95 | grad norm last: 0.88 | 
2025-12-30T10:42:51 | step: 217100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 6.697541266476037e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1778483200.0 | grad norm avg: 0.94 | grad norm last: 1.0 | 
2025-12-30T10:43:09 | step: 217200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 6.6785901253751945e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 1779302400.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:43:28 | step: 217300 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 6.659661721641896e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.16 | consumed tokens: 1780121600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:43:46 | step: 217400 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 6.640756510023493e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.33 | consumed tokens: 1780940800.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T10:44:04 | step: 217500 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 6.621874035772635e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.92 | consumed tokens: 1781760000.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T10:44:23 | step: 217600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 6.60301429888932e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.94 | consumed tokens: 1782579200.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:44:41 | step: 217700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.584177754120901e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1783398400.0 | grad norm avg: 0.94 | grad norm last: 0.84 | 
2025-12-30T10:45:00 | step: 217800 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 6.565364401467377e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.09 | consumed tokens: 1784217600.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T10:45:18 | step: 217900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.546574240928749e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 1785036800.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T10:45:37 | step: 218000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.527806817757664e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.62 | consumed tokens: 1785856000.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T10:45:56 | step: 218100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.509062586701475e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.28 | consumed tokens: 1786675200.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:46:14 | step: 218200 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 6.490341547760181e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 1787494400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:46:33 | step: 218300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 6.471643700933782e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 1788313600.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:46:51 | step: 218400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 6.452969046222279e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 1789132800.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T10:47:10 | step: 218500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 6.4343175836256705e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.31 | consumed tokens: 1789952000.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:47:28 | step: 218600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 6.415689313143957e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.08 | consumed tokens: 1790771200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:47:47 | step: 218700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.3970842347771395e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.92 | consumed tokens: 1791590400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T10:48:05 | step: 218800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.378502348525217e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.44 | consumed tokens: 1792409600.0 | grad norm avg: 0.95 | grad norm last: 1.04 | 
2025-12-30T10:48:24 | step: 218900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 6.35994410913554e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.19 | consumed tokens: 1793228800.0 | grad norm avg: 0.93 | grad norm last: 1.04 | 
2025-12-30T10:48:42 | step: 219000 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 6.341408607113408e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.89 | consumed tokens: 1794048000.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T10:49:01 | step: 219100 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 6.322897206700873e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.36 | consumed tokens: 1794867200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T10:49:19 | step: 219200 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 6.304408543655882e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 1795686400.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T10:49:37 | step: 219300 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 6.285943527473137e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.36 | consumed tokens: 1796505600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:49:56 | step: 219400 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 6.267501703405287e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.62 | consumed tokens: 1797324800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T10:50:14 | step: 219500 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.2490835261996835e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 1798144000.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T10:50:32 | step: 219600 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 6.230688995856326e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 1798963200.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T10:50:51 | step: 219700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 6.212317657627864e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.42 | consumed tokens: 1799782400.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T10:51:09 | step: 219800 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 6.193969511514297e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1800601600.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T10:51:27 | step: 219900 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 6.175645467010327e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.64 | consumed tokens: 1801420800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T10:51:46 | step: 220000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 6.157344614621252e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.22 | consumed tokens: 1802240000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T10:52:05 | step: 220100 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 6.139067409094423e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.22 | consumed tokens: 1803059200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T10:52:24 | step: 220200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 6.12081385042984e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.88 | consumed tokens: 1803878400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T10:52:42 | step: 220300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.102583938627504e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.0 | consumed tokens: 1804697600.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:53:01 | step: 220400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 6.084377673687413e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.47 | consumed tokens: 1805516800.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T10:53:20 | step: 220500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 6.066195055609569e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.98 | consumed tokens: 1806336000.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T10:53:38 | step: 220600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 6.048036084393971e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.28 | consumed tokens: 1807155200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T10:53:57 | step: 220700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 6.0299007600406185e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.88 | consumed tokens: 1807974400.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T10:54:16 | step: 220800 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 6.011789082549512e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.36 | consumed tokens: 1808793600.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:54:34 | step: 220900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.993701506668003e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.52 | consumed tokens: 1809612800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T10:54:53 | step: 221000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 5.97563757764874e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.77 | consumed tokens: 1810432000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:55:11 | step: 221100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.957597295491723e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.61 | consumed tokens: 1811251200.0 | grad norm avg: 0.95 | grad norm last: 1.03 | 
2025-12-30T10:55:30 | step: 221200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.9395806601969525e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.83 | consumed tokens: 1812070400.0 | grad norm avg: 0.94 | grad norm last: 0.86 | 
2025-12-30T10:55:48 | step: 221300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.921588126511779e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.5 | consumed tokens: 1812889600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T10:56:07 | step: 221400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.903619694436202e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.91 | consumed tokens: 1813708800.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T10:56:25 | step: 221500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.885674909222871e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 1814528000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T10:56:44 | step: 221600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.867753770871786e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.33 | consumed tokens: 1815347200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T10:57:02 | step: 221700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.849856734130299e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.08 | consumed tokens: 1816166400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T10:57:21 | step: 221800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.831983798998408e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.58 | consumed tokens: 1816985600.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T10:57:39 | step: 221900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.814134965476114e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 1817804800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T10:57:58 | step: 222000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.796309778816067e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.31 | consumed tokens: 1818624000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T10:58:17 | step: 222100 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 5.778508693765616e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.61 | consumed tokens: 1819443200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T10:58:35 | step: 222200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 5.760731710324762e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.7 | consumed tokens: 1820262400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T10:58:54 | step: 222300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.742978828493506e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.92 | consumed tokens: 1821081600.0 | grad norm avg: 0.94 | grad norm last: 0.86 | 
2025-12-30T10:59:12 | step: 222400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.725250048271846e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 1821900800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T10:59:31 | step: 222500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.707545369659783e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.88 | consumed tokens: 1822720000.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T10:59:49 | step: 222600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.689864792657318e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.38 | consumed tokens: 1823539200.0 | grad norm avg: 0.95 | grad norm last: 1.03 | 
2025-12-30T11:00:08 | step: 222700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.672208317264449e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.56 | consumed tokens: 1824358400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T11:00:26 | step: 222800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.654575943481177e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.2 | consumed tokens: 1825177600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T11:00:45 | step: 222900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.636968126054853e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 1825996800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:01:03 | step: 223000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.619384410238126e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 1826816000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T11:01:22 | step: 223100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.601824796030996e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.5 | consumed tokens: 1827635200.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T11:01:40 | step: 223200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 5.5842892834334634e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.55 | consumed tokens: 1828454400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T11:01:59 | step: 223300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.566778327192878e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.44 | consumed tokens: 1829273600.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T11:02:18 | step: 223400 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 5.54929147256189e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 1830092800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:02:36 | step: 223500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.53182917428785e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.0 | consumed tokens: 1830912000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:02:55 | step: 223600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.514390977623407e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.2 | consumed tokens: 1831731200.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2025-12-30T11:03:13 | step: 223700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.4969773373159114e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.12 | consumed tokens: 1832550400.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T11:03:32 | step: 223800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.479587798618013e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 1833369600.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T11:03:50 | step: 223900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.462223271024413e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.75 | consumed tokens: 1834188800.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T11:04:09 | step: 224000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 5.444882845040411e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 1835008000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T11:04:27 | step: 224100 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 5.427566520666005e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.81 | consumed tokens: 1835827200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T11:04:46 | step: 224200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.410275207395898e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.44 | consumed tokens: 1836646400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T11:05:04 | step: 224300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.393007995735388e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.84 | consumed tokens: 1837465600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T11:05:23 | step: 224400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.375765795179177e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.11 | consumed tokens: 1838284800.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T11:05:41 | step: 224500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.358547696232563e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 1839104000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T11:06:00 | step: 224600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.3413546083902474e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.91 | consumed tokens: 1839923200.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T11:06:18 | step: 224700 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 5.324185622157529e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.06 | consumed tokens: 1840742400.0 | grad norm avg: 0.94 | grad norm last: 1.01 | 
2025-12-30T11:06:37 | step: 224800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.307041647029109e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 1841561600.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T11:06:55 | step: 224900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.289921773510287e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.41 | consumed tokens: 1842380800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T11:07:14 | step: 225000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.272826911095763e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.86 | consumed tokens: 1843200000.0 | grad norm avg: 0.96 | grad norm last: 0.83 | 
2025-12-30T11:07:34 | step: 225100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.255757059785537e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.91 | consumed tokens: 1844019200.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:07:52 | step: 225200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 5.238711310084909e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.48 | consumed tokens: 1844838400.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:08:10 | step: 225300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.22169057148858e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.83 | consumed tokens: 1845657600.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T11:08:29 | step: 225400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 5.204694389249198e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.55 | consumed tokens: 1846476800.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T11:08:47 | step: 225500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 5.187723218114115e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.7 | consumed tokens: 1847296000.0 | grad norm avg: 0.95 | grad norm last: 0.87 | 
2025-12-30T11:09:06 | step: 225600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 5.17077660333598e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.55 | consumed tokens: 1848115200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:09:24 | step: 225700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 5.153854999662144e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.77 | consumed tokens: 1848934400.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T11:09:42 | step: 225800 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 5.136957952345256e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.25 | consumed tokens: 1849753600.0 | grad norm avg: 0.95 | grad norm last: 0.89 | 
2025-12-30T11:10:01 | step: 225900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 5.120085916132666e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.91 | consumed tokens: 1850572800.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T11:10:19 | step: 226000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.103238891024375e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.67 | consumed tokens: 1851392000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:10:38 | step: 226100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 5.086416422273032e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.83 | consumed tokens: 1852211200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T11:10:56 | step: 226200 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 5.069618964625988e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.06 | consumed tokens: 1853030400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T11:11:14 | step: 226300 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 5.052846518083243e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.61 | consumed tokens: 1853849600.0 | grad norm avg: 0.95 | grad norm last: 0.87 | 
2025-12-30T11:11:33 | step: 226400 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 5.036099082644796e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.19 | consumed tokens: 1854668800.0 | grad norm avg: 0.95 | grad norm last: 0.87 | 
2025-12-30T11:11:51 | step: 226500 | train samples/s: 96.3 | train mfu (16-bit): -1.0 | lr mean: 5.019376658310648e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.19 | consumed tokens: 1855488000.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T11:12:09 | step: 226600 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 5.002679245080799e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 1856307200.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T11:12:27 | step: 226700 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 4.9860063882078975e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.89 | consumed tokens: 1857126400.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T11:12:46 | step: 226800 | train samples/s: 96.1 | train mfu (16-bit): -1.0 | lr mean: 4.969358997186646e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.86 | consumed tokens: 1857945600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T11:13:04 | step: 226900 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.952736617269693e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 1858764800.0 | grad norm avg: 0.95 | grad norm last: 0.86 | 
2025-12-30T11:13:22 | step: 227000 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.936139248457039e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.58 | consumed tokens: 1859584000.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T11:13:41 | step: 227100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.919566890748683e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1860403200.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T11:13:59 | step: 227200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.903019544144627e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.69 | consumed tokens: 1861222400.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T11:14:18 | step: 227300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.88649766339222e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 1862041600.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:14:37 | step: 227400 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 4.8700007937441114e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.22 | consumed tokens: 1862860800.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T11:14:55 | step: 227500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.853528935200302e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.22 | consumed tokens: 1863680000.0 | grad norm avg: 0.95 | grad norm last: 0.88 | 
2025-12-30T11:15:13 | step: 227600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.837082542508142e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.27 | consumed tokens: 1864499200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:15:32 | step: 227700 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.820661160920281e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.03 | consumed tokens: 1865318400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T11:15:50 | step: 227800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.8042652451840695e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.42 | consumed tokens: 1866137600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T11:16:09 | step: 227900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.787894340552157e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.45 | consumed tokens: 1866956800.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T11:16:27 | step: 228000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.771548901771894e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.2 | consumed tokens: 1867776000.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T11:16:46 | step: 228100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.75522892884328e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.19 | consumed tokens: 1868595200.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T11:17:04 | step: 228200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.738933967018966e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.58 | consumed tokens: 1869414400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T11:17:23 | step: 228300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.722664471046301e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.78 | consumed tokens: 1870233600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T11:17:41 | step: 228400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.706419986177934e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.81 | consumed tokens: 1871052800.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T11:18:00 | step: 228500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.690201421908569e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.84 | consumed tokens: 1871872000.0 | grad norm avg: 0.95 | grad norm last: 0.87 | 
2025-12-30T11:18:18 | step: 228600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.674007868743502e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.61 | consumed tokens: 1872691200.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T11:18:37 | step: 228700 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 4.657839781430084e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.81 | consumed tokens: 1873510400.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T11:18:55 | step: 228800 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.6416976147156674e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.2 | consumed tokens: 1874329600.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T11:19:14 | step: 228900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.625580459105549e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1875148800.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T11:19:32 | step: 229000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.609488769347081e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.83 | consumed tokens: 1875968000.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T11:19:50 | step: 229100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.593422545440262e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 1876787200.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:20:09 | step: 229200 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.577382242132444e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.34 | consumed tokens: 1877606400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T11:20:27 | step: 229300 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.5613669499289244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 1878425600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T11:20:45 | step: 229400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 4.5453775783244055e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.33 | consumed tokens: 1879244800.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T11:21:04 | step: 229500 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.529413672571536e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.11 | consumed tokens: 1880064000.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T11:21:22 | step: 229600 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.513475687417667e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.12 | consumed tokens: 1880883200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:21:40 | step: 229700 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.497562713368097e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.59 | consumed tokens: 1881702400.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T11:21:59 | step: 229800 | train samples/s: 96.0 | train mfu (16-bit): -1.0 | lr mean: 4.481676114664879e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 1882521600.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:22:17 | step: 229900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.465814527065959e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 1883340800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:22:36 | step: 230000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.4499788600660395e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.2 | consumed tokens: 1884160000.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T11:22:55 | step: 230100 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.434169113665121e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.97 | consumed tokens: 1884979200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T11:23:14 | step: 230200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.4183848331158515e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.03 | consumed tokens: 1885798400.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:23:32 | step: 230300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.402626018418232e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.62 | consumed tokens: 1886617600.0 | grad norm avg: 0.95 | grad norm last: 0.89 | 
2025-12-30T11:23:51 | step: 230400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.386893579066964e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.69 | consumed tokens: 1887436800.0 | grad norm avg: 0.95 | grad norm last: 1.04 | 
2025-12-30T11:24:09 | step: 230500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.371186605567345e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.17 | consumed tokens: 1888256000.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T11:24:28 | step: 230600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.3555055526667275e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.59 | consumed tokens: 1889075200.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T11:24:46 | step: 230700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.339849965617759e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.08 | consumed tokens: 1889894400.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T11:25:05 | step: 230800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.324220753915142e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.55 | consumed tokens: 1890713600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:25:23 | step: 230900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.308617008064175e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.06 | consumed tokens: 1891532800.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T11:25:42 | step: 231000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 4.293039182812208e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 1892352000.0 | grad norm avg: 0.95 | grad norm last: 0.89 | 
2025-12-30T11:26:00 | step: 231100 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 4.277487278159242e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 1893171200.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T11:26:19 | step: 231200 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.2619612941052765e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.52 | consumed tokens: 1893990400.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T11:26:37 | step: 231300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 4.246461230650311e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.81 | consumed tokens: 1894809600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T11:26:55 | step: 231400 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 4.230987087794347e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.41 | consumed tokens: 1895628800.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T11:27:14 | step: 231500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.2155393202847335e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 1896448000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:27:33 | step: 231600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.20011701862677e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.67 | consumed tokens: 1897267200.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T11:27:51 | step: 231700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.184721092315158e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 1898086400.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T11:28:10 | step: 231800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.169351086602546e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.47 | consumed tokens: 1898905600.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T11:28:28 | step: 231900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.154007001488935e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.14 | consumed tokens: 1899724800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:28:46 | step: 232000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.1386892917216755e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 1900544000.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T11:29:05 | step: 232100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 4.123397502553416e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.88 | consumed tokens: 1901363200.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T11:29:23 | step: 232200 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 4.108131633984158e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.03 | consumed tokens: 1902182400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T11:29:42 | step: 232300 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.092892140761251e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.2 | consumed tokens: 1903001600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T11:30:00 | step: 232400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 4.077678568137344e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.52 | consumed tokens: 1903820800.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T11:30:19 | step: 232500 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 4.062491370859789e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.16 | consumed tokens: 1904640000.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:30:37 | step: 232600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 4.047330548928585e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 1905459200.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T11:30:55 | step: 232700 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.032195647596382e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 1906278400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T11:31:14 | step: 232800 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 4.017086666863179e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.09 | consumed tokens: 1907097600.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T11:31:32 | step: 232900 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 4.002004516223678e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.23 | consumed tokens: 1907916800.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T11:31:51 | step: 233000 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.9869482861831784e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.31 | consumed tokens: 1908736000.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T11:32:09 | step: 233100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.97191843148903e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.12 | consumed tokens: 1909555200.0 | grad norm avg: 0.94 | grad norm last: 0.83 | 
2025-12-30T11:32:27 | step: 233200 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.956914952141233e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 1910374400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T11:32:46 | step: 233300 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.941937848139787e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.3 | consumed tokens: 1911193600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T11:33:04 | step: 233400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.926986664737342e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.19 | consumed tokens: 1912012800.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T11:33:23 | step: 233500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.912062311428599e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.42 | consumed tokens: 1912832000.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T11:33:41 | step: 233600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.897163878718857e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.38 | consumed tokens: 1913651200.0 | grad norm avg: 0.95 | grad norm last: 1.02 | 
2025-12-30T11:34:00 | step: 233700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.882292276102817e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.28 | consumed tokens: 1914470400.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T11:34:18 | step: 233800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.867447048833128e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.11 | consumed tokens: 1915289600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T11:34:37 | step: 233900 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 3.85262774216244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.33 | consumed tokens: 1916108800.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T11:34:56 | step: 234000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.837835265585454e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 1916928000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T11:35:14 | step: 234100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.823069619102171e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.83 | consumed tokens: 1917747200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T11:35:33 | step: 234200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.8083298932178877e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.78 | consumed tokens: 1918566400.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T11:35:51 | step: 234300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.7936167700536316e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.72 | consumed tokens: 1919385600.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T11:36:10 | step: 234400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.7789302496094024e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.56 | consumed tokens: 1920204800.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T11:36:28 | step: 234500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.7642703318852e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.75 | consumed tokens: 1921024000.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T11:36:47 | step: 234600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.7496370168810245e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.11 | consumed tokens: 1921843200.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:37:05 | step: 234700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.7350300772232004e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.81 | consumed tokens: 1922662400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T11:37:24 | step: 234800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.720449740285403e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.09 | consumed tokens: 1923481600.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T11:37:42 | step: 234900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.705896233441308e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.86 | consumed tokens: 1924300800.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T11:38:01 | step: 235000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.69136932931724e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 1925120000.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T11:38:21 | step: 235100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.6768688005395234e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 1925939200.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T11:38:39 | step: 235200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 3.662395101855509e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.56 | consumed tokens: 1926758400.0 | grad norm avg: 0.95 | grad norm last: 0.86 | 
2025-12-30T11:38:58 | step: 235300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.647948233265197e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.34 | consumed tokens: 1927577600.0 | grad norm avg: 0.96 | grad norm last: 0.88 | 
2025-12-30T11:39:16 | step: 235400 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.6335279673949117e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.12 | consumed tokens: 1928396800.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T11:39:35 | step: 235500 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.6191343042446533e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 1929216000.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:39:53 | step: 235600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.6047674711880973e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.11 | consumed tokens: 1930035200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T11:40:11 | step: 235700 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 3.5904274682252435e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.09 | consumed tokens: 1930854400.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T11:40:30 | step: 235800 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.5761140679824166e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.22 | consumed tokens: 1931673600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T11:40:48 | step: 235900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 3.5618277252069674e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.14 | consumed tokens: 1932492800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T11:41:07 | step: 236000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.547567985151545e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.39 | consumed tokens: 1933312000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T11:41:25 | step: 236100 | train samples/s: 95.9 | train mfu (16-bit): -1.0 | lr mean: 3.533335075189825e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.97 | consumed tokens: 1934131200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T11:41:43 | step: 236200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 3.5191289953218075e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 1934950400.0 | grad norm avg: 0.96 | grad norm last: 0.88 | 
2025-12-30T11:42:02 | step: 236300 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 3.5049499729211675e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.83 | consumed tokens: 1935769600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T11:42:20 | step: 236400 | train samples/s: 95.5 | train mfu (16-bit): -1.0 | lr mean: 3.49079778061423e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.39 | consumed tokens: 1936588800.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T11:42:39 | step: 236500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 3.476672191027319e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.92 | consumed tokens: 1937408000.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:42:57 | step: 236600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.4625738862814615e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.67 | consumed tokens: 1938227200.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T11:43:16 | step: 236700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.448502411629306e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 1939046400.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T11:43:34 | step: 236800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.434457767070853e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.95 | consumed tokens: 1939865600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T11:43:53 | step: 236900 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.420440179979778e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.59 | consumed tokens: 1940684800.0 | grad norm avg: 0.96 | grad norm last: 1.08 | 
2025-12-30T11:44:11 | step: 237000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 3.4064496503560804e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.48 | consumed tokens: 1941504000.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T11:44:30 | step: 237100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.392485950826085e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.36 | consumed tokens: 1942323200.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T11:44:48 | step: 237200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.378549536137143e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.75 | consumed tokens: 1943142400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T11:45:07 | step: 237300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.3646399515419034e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.61 | consumed tokens: 1943961600.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T11:45:26 | step: 237400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.3507574244140415e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.81 | consumed tokens: 1944780800.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T11:45:44 | step: 237500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.3369019547535572e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.8 | consumed tokens: 1945600000.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T11:46:03 | step: 237600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.323073769934126e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.64 | consumed tokens: 1946419200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T11:46:21 | step: 237700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.309272642582073e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.08 | consumed tokens: 1947238400.0 | grad norm avg: 0.96 | grad norm last: 1.05 | 
2025-12-30T11:46:40 | step: 237800 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 3.2954985726973973e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.73 | consumed tokens: 1948057600.0 | grad norm avg: 0.96 | grad norm last: 0.87 | 
2025-12-30T11:46:58 | step: 237900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.2817515602800995e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.28 | consumed tokens: 1948876800.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T11:47:17 | step: 238000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.268031832703855e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.69 | consumed tokens: 1949696000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T11:47:35 | step: 238100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.254339162594988e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 1950515200.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T11:47:54 | step: 238200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.2406737773271743e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1951334400.0 | grad norm avg: 0.95 | grad norm last: 1.06 | 
2025-12-30T11:48:12 | step: 238300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.227035676900414e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.81 | consumed tokens: 1952153600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:48:31 | step: 238400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.213424633941031e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.41 | consumed tokens: 1952972800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:48:49 | step: 238500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.1998408758227015e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.38 | consumed tokens: 1953792000.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T11:49:08 | step: 238600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.1862846299191006e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 1954611200.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T11:49:26 | step: 238700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.1727554414828774e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.41 | consumed tokens: 1955430400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T11:49:45 | step: 238800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.1592535378877074e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.23 | consumed tokens: 1956249600.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T11:50:03 | step: 238900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.1457789191335905e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.38 | consumed tokens: 1957068800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T11:50:22 | step: 239000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.1323318125942023e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.33 | consumed tokens: 1957888000.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T11:50:41 | step: 239100 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 3.1189119908958673e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.91 | consumed tokens: 1958707200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T11:50:59 | step: 239200 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 3.1055194540385855e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.19 | consumed tokens: 1959526400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T11:51:18 | step: 239300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.092154202022357e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 1960345600.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T11:51:36 | step: 239400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.0788166895945324e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 1961164800.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T11:51:55 | step: 239500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.0655062346340856e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.09 | consumed tokens: 1961984000.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T11:52:13 | step: 239600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.052223519262043e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.8 | consumed tokens: 1962803200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T11:52:32 | step: 239700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.0389680887310533e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.08 | consumed tokens: 1963622400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T11:52:50 | step: 239800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 3.025739943041117e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.83 | consumed tokens: 1964441600.0 | grad norm avg: 0.96 | grad norm last: 1.05 | 
2025-12-30T11:53:09 | step: 239900 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 3.0125395369395847e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.27 | consumed tokens: 1965260800.0 | grad norm avg: 0.97 | grad norm last: 1.1 | 
2025-12-30T11:53:27 | step: 240000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.999366643052781e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.45 | consumed tokens: 1966080000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T11:53:47 | step: 240100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.9862210340070305e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.52 | consumed tokens: 1966899200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T11:54:05 | step: 240200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.973103164549684e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.78 | consumed tokens: 1967718400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T11:54:24 | step: 240300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.9600128073070664e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 1968537600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T11:54:43 | step: 240400 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 2.9469499622791773e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.23 | consumed tokens: 1969356800.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T11:55:01 | step: 240500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.9339146294660168e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.55 | consumed tokens: 1970176000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T11:55:20 | step: 240600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 2.9209070362412604e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.64 | consumed tokens: 1970995200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T11:55:38 | step: 240700 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.9079269552312326e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.73 | consumed tokens: 1971814400.0 | grad norm avg: 0.96 | grad norm last: 0.87 | 
2025-12-30T11:55:57 | step: 240800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.894974613809609e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.44 | consumed tokens: 1972633600.0 | grad norm avg: 0.96 | grad norm last: 0.87 | 
2025-12-30T11:56:15 | step: 240900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.8820497846027138e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.84 | consumed tokens: 1973452800.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:56:34 | step: 241000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.8691526949842228e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.88 | consumed tokens: 1974272000.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T11:56:52 | step: 241100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.856283344954136e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.95 | consumed tokens: 1975091200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T11:57:11 | step: 241200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.8434415071387775e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.45 | consumed tokens: 1975910400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T11:57:29 | step: 241300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.8306276362854987e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.11 | consumed tokens: 1976729600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T11:57:48 | step: 241400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.8178412776469486e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.2 | consumed tokens: 1977548800.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T11:58:06 | step: 241500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.8050826585968025e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.27 | consumed tokens: 1978368000.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T11:58:25 | step: 241600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.792352006508736e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.92 | consumed tokens: 1979187200.0 | grad norm avg: 0.95 | grad norm last: 1.05 | 
2025-12-30T11:58:43 | step: 241700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 2.779648866635398e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.3 | consumed tokens: 1980006400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T11:59:02 | step: 241800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.7669736937241396e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.08 | consumed tokens: 1980825600.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T11:59:20 | step: 241900 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.7543262604012853e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.7 | consumed tokens: 1981644800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T11:59:39 | step: 242000 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.741706566666835e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.72 | consumed tokens: 1982464000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T11:59:57 | step: 242100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.7291148398944642e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.58 | consumed tokens: 1983283200.0 | grad norm avg: 0.95 | grad norm last: 1.07 | 
2025-12-30T12:00:16 | step: 242200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.7165508527104976e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.48 | consumed tokens: 1984102400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T12:00:34 | step: 242300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.7040148324886104e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 1984921600.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:00:53 | step: 242400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.6915065518551273e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.31 | consumed tokens: 1985740800.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T12:01:11 | step: 242500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.679026465557399e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 4.12 | consumed tokens: 1986560000.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T12:01:30 | step: 242600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.6665738914743997e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.73 | consumed tokens: 1987379200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T12:01:48 | step: 242700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.654149511727155e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.31 | consumed tokens: 1988198400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T12:02:07 | step: 242800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.64175309894199e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.88 | consumed tokens: 1989017600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:02:25 | step: 242900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.6293846531189047e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.5 | consumed tokens: 1989836800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:02:44 | step: 243000 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.6170439468842233e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.67 | consumed tokens: 1990656000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:03:02 | step: 243100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.604731434985297e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.2 | consumed tokens: 1991475200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:03:21 | step: 243200 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.59244689004845e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.34 | consumed tokens: 1992294400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:03:39 | step: 243300 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.5801903120736824e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.97 | consumed tokens: 1993113600.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T12:03:57 | step: 243400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.56796192843467e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.84 | consumed tokens: 1993932800.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T12:04:16 | step: 243500 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 2.555761511757737e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.73 | consumed tokens: 1994752000.0 | grad norm avg: 0.97 | grad norm last: 1.07 | 
2025-12-30T12:04:34 | step: 243600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.5435890620428836e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.78 | consumed tokens: 1995571200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:04:53 | step: 243700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.531444806663785e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.48 | consumed tokens: 1996390400.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:05:11 | step: 243800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.519328518246766e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.22 | consumed tokens: 1997209600.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T12:05:30 | step: 243900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.507240424165502e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.12 | consumed tokens: 1998028800.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T12:05:48 | step: 244000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.495180524419993e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.16 | consumed tokens: 1998848000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T12:06:07 | step: 244100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.483148819010239e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.05 | consumed tokens: 1999667200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T12:06:25 | step: 244200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.4711450805625645e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 2000486400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T12:06:44 | step: 244300 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.459169536450645e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.72 | consumed tokens: 2001305600.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T12:07:02 | step: 244400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.4472224140481558e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.22 | consumed tokens: 2002124800.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T12:07:21 | step: 244500 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.435303258607746e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.89 | consumed tokens: 2002944000.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T12:07:39 | step: 244600 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.423412524876767e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 2003763200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:07:58 | step: 244700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.411549758107867e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.39 | consumed tokens: 2004582400.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T12:08:16 | step: 244800 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.3997154130483977e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.38 | consumed tokens: 2005401600.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T12:08:35 | step: 244900 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.3879092623246834e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.75 | consumed tokens: 2006220800.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T12:08:53 | step: 245000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 2.3761315333103994e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.84 | consumed tokens: 2007040000.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T12:09:13 | step: 245100 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.3643819986318704e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 2007859200.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T12:09:31 | step: 245200 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 2.3526606582890963e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.59 | consumed tokens: 2008678400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T12:09:49 | step: 245300 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 2.3409677396557527e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.97 | consumed tokens: 2009497600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:10:08 | step: 245400 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 2.3293032427318394e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.06 | consumed tokens: 2010316800.0 | grad norm avg: 0.96 | grad norm last: 1.03 | 
2025-12-30T12:10:26 | step: 245500 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 2.317666940143681e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.88 | consumed tokens: 2011136000.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T12:10:45 | step: 245600 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 2.3060590592649532e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.72 | consumed tokens: 2011955200.0 | grad norm avg: 0.96 | grad norm last: 1.08 | 
2025-12-30T12:11:03 | step: 245700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2944796000956558e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.94 | consumed tokens: 2012774400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:11:22 | step: 245800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.2829285626357887e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.42 | consumed tokens: 2013593600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T12:11:40 | step: 245900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.271405946885352e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.62 | consumed tokens: 2014412800.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:11:59 | step: 246000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.2599117528443458e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.98 | consumed tokens: 2015232000.0 | grad norm avg: 0.98 | grad norm last: 0.88 | 
2025-12-30T12:12:18 | step: 246100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.2484457531390944e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.06 | consumed tokens: 2016051200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:12:36 | step: 246200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.237008402516949e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.19 | consumed tokens: 2016870400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:12:55 | step: 246300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.225599473604234e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.28 | consumed tokens: 2017689600.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:13:13 | step: 246400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.2142191937746247e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.02 | consumed tokens: 2018508800.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T12:13:32 | step: 246500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.202867335654446e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.83 | consumed tokens: 2019328000.0 | grad norm avg: 0.96 | grad norm last: 0.87 | 
2025-12-30T12:13:50 | step: 246600 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.1915438992436975e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.33 | consumed tokens: 2020147200.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T12:14:09 | step: 246700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.1802488845423795e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 2020966400.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T12:14:27 | step: 246800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.1689825189241674e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.75 | consumed tokens: 2021785600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:14:46 | step: 246900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.157744802389061e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.7 | consumed tokens: 2022604800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:15:05 | step: 247000 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 2.146535507563385e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.7 | consumed tokens: 2023424000.0 | grad norm avg: 0.97 | grad norm last: 0.88 | 
2025-12-30T12:15:23 | step: 247100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.135354861820815e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.34 | consumed tokens: 2024243200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:15:42 | step: 247200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 2.124202865161351e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.83 | consumed tokens: 2025062400.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T12:16:00 | step: 247300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.1130795175849926e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.95 | consumed tokens: 2025881600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:16:19 | step: 247400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.1019845917180646e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.08 | consumed tokens: 2026700800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T12:16:37 | step: 247500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.0909183149342425e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.69 | consumed tokens: 2027520000.0 | grad norm avg: 0.98 | grad norm last: 1.05 | 
2025-12-30T12:16:56 | step: 247600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.0798809146072017e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.14 | consumed tokens: 2028339200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:17:14 | step: 247700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.0688719359895913e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 2029158400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:17:33 | step: 247800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.057891833828762e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.22 | consumed tokens: 2029977600.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T12:17:51 | step: 247900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.0469401533773635e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.83 | consumed tokens: 2030796800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:18:10 | step: 248000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.036017349382746e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.02 | consumed tokens: 2031616000.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T12:18:28 | step: 248100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.02512342184491e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.05 | consumed tokens: 2032435200.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T12:18:47 | step: 248200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.014257916016504e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.69 | consumed tokens: 2033254400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T12:19:06 | step: 248300 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 2.0034212866448797e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.36 | consumed tokens: 2034073600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T12:19:24 | step: 248400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.9926135337300366e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.5 | consumed tokens: 2034892800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:19:43 | step: 248500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.9818344298982993e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 2035712000.0 | grad norm avg: 0.96 | grad norm last: 1.05 | 
2025-12-30T12:20:01 | step: 248600 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.9710842025233433e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.72 | consumed tokens: 2036531200.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T12:20:20 | step: 248700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.960362624231493e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 4.56 | consumed tokens: 2037350400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:20:38 | step: 248800 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 1.9496699223964242e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.62 | consumed tokens: 2038169600.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:20:56 | step: 248900 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.9390060970181366e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.61 | consumed tokens: 2038988800.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T12:21:15 | step: 249000 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.928370920722955e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.39 | consumed tokens: 2039808000.0 | grad norm avg: 0.96 | grad norm last: 1.03 | 
2025-12-30T12:21:33 | step: 249100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.91776484825823e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.86 | consumed tokens: 2040627200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T12:21:52 | step: 249200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.9071875385634485e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.5 | consumed tokens: 2041446400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:22:10 | step: 249300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.8966391053254483e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.17 | consumed tokens: 2042265600.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T12:22:29 | step: 249400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.8861195485442295e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.44 | consumed tokens: 2043084800.0 | grad norm avg: 0.99 | grad norm last: 0.9 | 
2025-12-30T12:22:47 | step: 249500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.8756289819066296e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.19 | consumed tokens: 2043904000.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:23:06 | step: 249600 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.8651671780389734e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.98 | consumed tokens: 2044723200.0 | grad norm avg: 0.96 | grad norm last: 1.03 | 
2025-12-30T12:23:24 | step: 249700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.8547344780017738e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.75 | consumed tokens: 2045542400.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T12:23:43 | step: 249800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.8443305407345179e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 2046361600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T12:24:01 | step: 249900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.8339557072977186e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.03 | consumed tokens: 2047180800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T12:24:20 | step: 250000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.8236097503177007e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.58 | consumed tokens: 2048000000.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T12:24:40 | step: 250100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.8132928971681395e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.77 | consumed tokens: 2048819200.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:24:58 | step: 250200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.8030049204753595e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 2049638400.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:25:17 | step: 250300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.7927459339261986e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 2050457600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:25:36 | step: 250400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.7825160512074945e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.86 | consumed tokens: 2051276800.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T12:25:54 | step: 250500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7723151586324093e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.7 | consumed tokens: 2052096000.0 | grad norm avg: 0.96 | grad norm last: 1.03 | 
2025-12-30T12:26:13 | step: 250600 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7621433698877809e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.92 | consumed tokens: 2052915200.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T12:26:31 | step: 250700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7520005712867714e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.56 | consumed tokens: 2053734400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:26:50 | step: 250800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.7418868765162188e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.08 | consumed tokens: 2054553600.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T12:27:09 | step: 250900 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.7318022855761228e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.0 | consumed tokens: 2055372800.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:27:27 | step: 251000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.7217467984664836e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.84 | consumed tokens: 2056192000.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:27:46 | step: 251100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.7117204151873011e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.31 | consumed tokens: 2057011200.0 | grad norm avg: 0.96 | grad norm last: 0.87 | 
2025-12-30T12:28:04 | step: 251200 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.7017231357385754e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.02 | consumed tokens: 2057830400.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T12:28:23 | step: 251300 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.6917549601203064e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.06 | consumed tokens: 2058649600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T12:28:41 | step: 251400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.6818160020193318e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.95 | consumed tokens: 2059468800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:29:00 | step: 251500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.671906147748814e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.72 | consumed tokens: 2060288000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:29:18 | step: 251600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.6620255109955906e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 2061107200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T12:29:37 | step: 251700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.652173978072824e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.41 | consumed tokens: 2061926400.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T12:29:55 | step: 251800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.6423517763541895e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.27 | consumed tokens: 2062745600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:30:14 | step: 251900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.6325586784660118e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 2063564800.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T12:30:32 | step: 252000 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.6227947980951285e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 2064384000.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T12:30:51 | step: 252100 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.6130602489283774e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.5 | consumed tokens: 2065203200.0 | grad norm avg: 0.96 | grad norm last: 1.05 | 
2025-12-30T12:31:09 | step: 252200 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 1.603354803592083e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 2066022400.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T12:31:28 | step: 252300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5936786894599209e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.67 | consumed tokens: 2066841600.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T12:31:46 | step: 252400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5840319065318909e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.16 | consumed tokens: 2067660800.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T12:32:05 | step: 252500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.5744143411211553e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.78 | consumed tokens: 2068480000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:32:24 | step: 252600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5648259932277142e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 2069299200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:32:42 | step: 252700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.555267090225243e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.55 | consumed tokens: 2070118400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T12:33:01 | step: 252800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.5457374047400663e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.81 | consumed tokens: 2070937600.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T12:33:19 | step: 252900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.5362371641458594e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.97 | consumed tokens: 2071756800.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T12:33:38 | step: 253000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.526766141068947e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.98 | consumed tokens: 2072576000.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:33:56 | step: 253100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.5173245628830045e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.47 | consumed tokens: 2073395200.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T12:34:14 | step: 253200 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.5079123159011942e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.64 | consumed tokens: 2074214400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:34:33 | step: 253300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.498529400123516e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.28 | consumed tokens: 2075033600.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:34:51 | step: 253400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4891759292368079e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.45 | consumed tokens: 2075852800.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T12:35:10 | step: 253500 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 1.4798519032410695e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.2 | consumed tokens: 2076672000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:35:28 | step: 253600 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.4705572084494634e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.81 | consumed tokens: 2077491200.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:35:47 | step: 253700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.461291958548827e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 2078310400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:36:05 | step: 253800 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 1.4520561535391607e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.58 | consumed tokens: 2079129600.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T12:36:24 | step: 253900 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 1.4428497934204643e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.95 | consumed tokens: 2079948800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:36:42 | step: 254000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.4336728781927377e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.48 | consumed tokens: 2080768000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:37:01 | step: 254100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.424525407855981e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.14 | consumed tokens: 2081587200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:37:19 | step: 254200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.415407496097032e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.03 | consumed tokens: 2082406400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T12:37:38 | step: 254300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.4063190292290528e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.62 | consumed tokens: 2083225600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:37:56 | step: 254400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.3972600072520436e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.83 | consumed tokens: 2084044800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T12:38:15 | step: 254500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.3882306575396797e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.11 | consumed tokens: 2084864000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:38:33 | step: 254600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.3792307527182857e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.77 | consumed tokens: 2085683200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:38:52 | step: 254700 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.3702602927878615e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.86 | consumed tokens: 2086502400.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T12:39:10 | step: 254800 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.3613195051220828e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.59 | consumed tokens: 2087321600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T12:39:29 | step: 254900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.3524082760341116e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.78 | consumed tokens: 2088140800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:39:47 | step: 255000 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 1.3435264918371104e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.48 | consumed tokens: 2088960000.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T12:40:07 | step: 255100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.3346743799047545e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.7 | consumed tokens: 2089779200.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T12:40:26 | step: 255200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.3258519402370439e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.97 | consumed tokens: 2090598400.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T12:40:44 | step: 255300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.3170589454603032e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 4.22 | consumed tokens: 2091417600.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T12:41:03 | step: 255400 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.3082957366350456e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 4.19 | consumed tokens: 2092236800.0 | grad norm avg: 0.97 | grad norm last: 0.88 | 
2025-12-30T12:41:21 | step: 255500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.299561972700758e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.17 | consumed tokens: 2093056000.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T12:41:40 | step: 255600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.2908579947179533e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 2093875200.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T12:41:58 | step: 255700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.2821835753129562e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.66 | consumed tokens: 2094694400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:42:17 | step: 255800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.2735388281726046e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.89 | consumed tokens: 2095513600.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T12:42:35 | step: 255900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.264923866983736e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 2096332800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:42:54 | step: 256000 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 1.256338464372675e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 2097152000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:43:13 | step: 256100 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.2477827340262593e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 2097971200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T12:43:31 | step: 256200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.2392567896313267e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.91 | consumed tokens: 2098790400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:43:50 | step: 256300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.2307605175010394e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.58 | consumed tokens: 2099609600.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T12:44:08 | step: 256400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.2222939176353975e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.69 | consumed tokens: 2100428800.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:44:27 | step: 256500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.2138571037212387e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.41 | consumed tokens: 2101248000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:44:45 | step: 256600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.2054500757585629e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.86 | consumed tokens: 2102067200.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T12:45:04 | step: 256700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1970728337473702e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.67 | consumed tokens: 2102886400.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T12:45:22 | step: 256800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.1887252640008228e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 2103705600.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T12:45:41 | step: 256900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.1804074802057585e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.02 | consumed tokens: 2104524800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T12:45:59 | step: 257000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.1721194823621772e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.67 | consumed tokens: 2105344000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:46:18 | step: 257100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.163861270470079e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.81 | consumed tokens: 2106163200.0 | grad norm avg: 0.96 | grad norm last: 0.87 | 
2025-12-30T12:46:36 | step: 257200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.1556329582163016e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 2106982400.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T12:46:55 | step: 257300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.1474343182271696e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.41 | consumed tokens: 2107801600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:47:14 | step: 257400 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.1392655778763583e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.5 | consumed tokens: 2108620800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:47:32 | step: 257500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.1311267371638678e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.89 | consumed tokens: 2109440000.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:47:51 | step: 257600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.1230176824028604e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.75 | consumed tokens: 2110259200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T12:48:09 | step: 257700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.1149385272801737e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.67 | consumed tokens: 2111078400.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T12:48:28 | step: 257800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.1068891581089702e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 2111897600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:48:46 | step: 257900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.0988696885760874e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.84 | consumed tokens: 2112716800.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T12:49:05 | step: 258000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.0908801186815253e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.03 | consumed tokens: 2113536000.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T12:49:23 | step: 258100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.0829204484252841e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.66 | consumed tokens: 2114355200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:49:42 | step: 258200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.0749907914942014e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 2115174400.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T12:50:00 | step: 258300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.0670909205146017e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.39 | consumed tokens: 2115993600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:50:19 | step: 258400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0592210628601606e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.25 | consumed tokens: 2116812800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:50:37 | step: 258500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0513809911572025e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 2117632000.0 | grad norm avg: 0.98 | grad norm last: 0.9 | 
2025-12-30T12:50:56 | step: 258600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0435710464662407e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.89 | consumed tokens: 2118451200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:51:14 | step: 258700 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 1.0357910014135996e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.33 | consumed tokens: 2119270400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:51:33 | step: 258800 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.0280408559992793e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.42 | consumed tokens: 2120089600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T12:51:51 | step: 258900 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.0203207239101175e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.88 | consumed tokens: 2120908800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T12:52:10 | step: 259000 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.0126306051461142e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.5 | consumed tokens: 2121728000.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T12:52:28 | step: 259100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.0049704997072695e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 2122547200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:52:47 | step: 259200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.973404075935832e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.33 | consumed tokens: 2123366400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T12:53:05 | step: 259300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.897402151182177e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.27 | consumed tokens: 2124185600.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T12:53:24 | step: 259400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.821701496548485e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 2125004800.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:53:42 | step: 259500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.746300975166378e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.34 | consumed tokens: 2125824000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:54:01 | step: 259600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 9.671200587035855e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.09 | consumed tokens: 2126643200.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T12:54:19 | step: 259700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 9.596401469025295e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.12 | consumed tokens: 2127462400.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T12:54:38 | step: 259800 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 9.52190248426632e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.12 | consumed tokens: 2128281600.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T12:54:56 | step: 259900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 9.447704201193119e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.02 | consumed tokens: 2129100800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T12:55:15 | step: 260000 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 9.373806619805691e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 2129920000.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T12:55:35 | step: 260100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 9.300210308538226e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 2130739200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T12:55:54 | step: 260200 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 9.226914698956534e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.38 | consumed tokens: 2131558400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T12:56:12 | step: 260300 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 9.153919791060616e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.97 | consumed tokens: 2132377600.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T12:56:31 | step: 260400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 9.081226721718849e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.41 | consumed tokens: 2133196800.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T12:56:49 | step: 260500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 9.008834922497044e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.7 | consumed tokens: 2134016000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T12:57:08 | step: 260600 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.936743824961013e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.75 | consumed tokens: 2134835200.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T12:57:27 | step: 260700 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 8.864954565979133e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.95 | consumed tokens: 2135654400.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T12:57:45 | step: 260800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.793467145551404e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.0 | consumed tokens: 2136473600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:58:04 | step: 260900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 8.722280995243636e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 2137292800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T12:58:22 | step: 261000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 8.65139668349002e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.58 | consumed tokens: 2138112000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T12:58:41 | step: 261100 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.580814210290555e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.39 | consumed tokens: 2138931200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T12:58:59 | step: 261200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.51053357564524e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.59 | consumed tokens: 2139750400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T12:59:18 | step: 261300 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 8.440554779554077e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.42 | consumed tokens: 2140569600.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T12:59:37 | step: 261400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.370877822017064e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.42 | consumed tokens: 2141388800.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T12:59:55 | step: 261500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.30150383990258e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.33 | consumed tokens: 2142208000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T13:00:14 | step: 261600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 8.232431127908058e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.2 | consumed tokens: 2143027200.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T13:00:32 | step: 261700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 8.163661391336063e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.61 | consumed tokens: 2143846400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:00:51 | step: 261800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.095194061752409e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.36 | consumed tokens: 2144665600.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:01:09 | step: 261900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 8.027029139157094e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.45 | consumed tokens: 2145484800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:01:28 | step: 262000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 7.959166623550118e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.3 | consumed tokens: 2146304000.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T13:01:46 | step: 262100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 7.89160708336567e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.34 | consumed tokens: 2147123200.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T13:02:05 | step: 262200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.824349950169562e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.41 | consumed tokens: 2147942400.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:02:23 | step: 262300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 7.757395792395982e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.5 | consumed tokens: 2148761600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:02:42 | step: 262400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.690744610044931e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.84 | consumed tokens: 2149580800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:03:00 | step: 262500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.624396403116407e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.02 | consumed tokens: 2150400000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:03:19 | step: 262600 | train samples/s: 93.0 | train mfu (16-bit): -1.0 | lr mean: 7.558351171610411e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 2151219200.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T13:03:38 | step: 262700 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 7.492608915526944e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.09 | consumed tokens: 2152038400.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T13:03:56 | step: 262800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.427170203300193e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.06 | consumed tokens: 2152857600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:04:15 | step: 262900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 7.362035034930159e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.22 | consumed tokens: 2153676800.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T13:04:33 | step: 263000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.297202841982653e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.84 | consumed tokens: 2154496000.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T13:04:52 | step: 263100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.232674192891864e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.39 | consumed tokens: 2155315200.0 | grad norm avg: 0.98 | grad norm last: 1.09 | 
2025-12-30T13:05:11 | step: 263200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 7.16844965609198e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.62 | consumed tokens: 2156134400.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:05:29 | step: 263300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 7.104528094714624e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.89 | consumed tokens: 2156953600.0 | grad norm avg: 0.98 | grad norm last: 0.88 | 
2025-12-30T13:05:48 | step: 263400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 7.040910645628173e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.19 | consumed tokens: 2157772800.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:06:06 | step: 263500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 6.97759674039844e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.45 | consumed tokens: 2158592000.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T13:06:25 | step: 263600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 6.914586947459611e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.5 | consumed tokens: 2159411200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:06:43 | step: 263700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 6.851881266811688e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.09 | consumed tokens: 2160230400.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T13:07:02 | step: 263800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 6.78947969845467e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.12 | consumed tokens: 2161049600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:07:21 | step: 263900 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 6.727381673954369e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 2161868800.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T13:07:39 | step: 264000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 6.665588330179162e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.28 | consumed tokens: 2162688000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:07:58 | step: 264100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.60409909869486e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.44 | consumed tokens: 2163507200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:08:16 | step: 264200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 6.542914547935652e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.27 | consumed tokens: 2164326400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T13:08:34 | step: 264300 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 6.48203410946735e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.22 | consumed tokens: 2165145600.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T13:08:53 | step: 264400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 6.421458351724141e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.48 | consumed tokens: 2165964800.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T13:09:11 | step: 264500 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 6.361186706271837e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.39 | consumed tokens: 2166784000.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T13:09:30 | step: 264600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 6.301220309978817e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.84 | consumed tokens: 2167603200.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T13:09:48 | step: 264700 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 6.241558025976701e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.94 | consumed tokens: 2168422400.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T13:10:07 | step: 264800 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 6.182200991133868e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.0 | consumed tokens: 2169241600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T13:10:25 | step: 264900 | train samples/s: 95.3 | train mfu (16-bit): -1.0 | lr mean: 6.123149205450318e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.22 | consumed tokens: 2170060800.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:10:43 | step: 265000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 6.064401532057673e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.44 | consumed tokens: 2170880000.0 | grad norm avg: 0.97 | grad norm last: 0.86 | 
2025-12-30T13:11:03 | step: 265100 | train samples/s: 95.4 | train mfu (16-bit): -1.0 | lr mean: 6.005959676258499e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.31 | consumed tokens: 2171699200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:11:22 | step: 265200 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 5.947822501184419e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.8 | consumed tokens: 2172518400.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:11:40 | step: 265300 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 5.889990575269621e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.06 | consumed tokens: 2173337600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:11:58 | step: 265400 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 5.832463898514106e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.31 | consumed tokens: 2174156800.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T13:12:17 | step: 265500 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 5.775242470917874e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 2174976000.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T13:12:35 | step: 265600 | train samples/s: 95.6 | train mfu (16-bit): -1.0 | lr mean: 5.718326860915113e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.86 | consumed tokens: 2175795200.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T13:12:53 | step: 265700 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 5.661716500071634e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 2176614400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T13:13:12 | step: 265800 | train samples/s: 95.2 | train mfu (16-bit): -1.0 | lr mean: 5.605411388387438e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.25 | consumed tokens: 2177433600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T13:13:30 | step: 265900 | train samples/s: 95.7 | train mfu (16-bit): -1.0 | lr mean: 5.549412094296713e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.78 | consumed tokens: 2178252800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:13:49 | step: 266000 | train samples/s: 95.8 | train mfu (16-bit): -1.0 | lr mean: 5.49371804936527e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.06 | consumed tokens: 2179072000.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:14:07 | step: 266100 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 5.438330390461488e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.3 | consumed tokens: 2179891200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T13:14:26 | step: 266200 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 5.383248549151176e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.05 | consumed tokens: 2180710400.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T13:14:44 | step: 266300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.328471957000147e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.97 | consumed tokens: 2181529600.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T13:15:03 | step: 266400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.274001750876778e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 2182348800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:15:22 | step: 266500 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 5.219837930781068e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.23 | consumed tokens: 2183168000.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T13:15:40 | step: 266600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.165979359844641e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.27 | consumed tokens: 2183987200.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T13:15:59 | step: 266700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.112427743370063e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 2184806400.0 | grad norm avg: 0.98 | grad norm last: 1.05 | 
2025-12-30T13:16:17 | step: 266800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.059181944488955e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.05 | consumed tokens: 2185625600.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T13:16:36 | step: 266900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.006242531635507e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.31 | consumed tokens: 2186444800.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T13:16:54 | step: 267000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.953608936375531e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 2187264000.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T13:17:13 | step: 267100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.901282295577403e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.36 | consumed tokens: 2188083200.0 | grad norm avg: 0.96 | grad norm last: 1.03 | 
2025-12-30T13:17:31 | step: 267200 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.849262040806934e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.11 | consumed tokens: 2188902400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T13:17:50 | step: 267300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.797548740498314e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.89 | consumed tokens: 2189721600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:18:09 | step: 267400 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.7461415420002595e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.25 | consumed tokens: 2190540800.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T13:18:27 | step: 267500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.695041297964053e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.95 | consumed tokens: 2191360000.0 | grad norm avg: 0.98 | grad norm last: 1.14 | 
2025-12-30T13:18:46 | step: 267600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.644247724172601e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.98 | consumed tokens: 2192179200.0 | grad norm avg: 0.98 | grad norm last: 1.06 | 
2025-12-30T13:19:04 | step: 267700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.593760820625903e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 2192998400.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T13:19:23 | step: 267800 | train samples/s: 93.3 | train mfu (16-bit): -1.0 | lr mean: 4.543580871541053e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.42 | consumed tokens: 2193817600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T13:19:41 | step: 267900 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.4937078769180516e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.2 | consumed tokens: 2194636800.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T13:20:00 | step: 268000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.4441418367568986e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.06 | consumed tokens: 2195456000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T13:20:18 | step: 268100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 4.394883035274688e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.53 | consumed tokens: 2196275200.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T13:20:37 | step: 268200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.345931188254326e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.34 | consumed tokens: 2197094400.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:20:55 | step: 268300 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.297286579912907e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.66 | consumed tokens: 2197913600.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:21:14 | step: 268400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 4.24894921025043e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.41 | consumed tokens: 2198732800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T13:21:32 | step: 268500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.200919079266896e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 2199552000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:21:51 | step: 268600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 4.1531961869623046e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.52 | consumed tokens: 2200371200.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T13:22:10 | step: 268700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 4.10578081755375e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.83 | consumed tokens: 2201190400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:22:28 | step: 268800 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.058672971041233e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 2202009600.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T13:22:47 | step: 268900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 4.0118729316418467e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.91 | consumed tokens: 2202828800.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:23:05 | step: 269000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.965380130921403e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.41 | consumed tokens: 2203648000.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T13:23:24 | step: 269100 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 3.919195137314091e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 2204467200.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T13:23:42 | step: 269200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.8733176666028157e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.31 | consumed tokens: 2205286400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:24:01 | step: 269300 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 3.827748287221766e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.86 | consumed tokens: 2206105600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:24:19 | step: 269400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.7824864307367534e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.39 | consumed tokens: 2206924800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T13:24:38 | step: 269500 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.7375326655819663e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.92 | consumed tokens: 2207744000.0 | grad norm avg: 0.98 | grad norm last: 1.06 | 
2025-12-30T13:24:56 | step: 269600 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.6928867075403105e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 2208563200.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T13:25:15 | step: 269700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.64854884082888e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.16 | consumed tokens: 2209382400.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:25:33 | step: 269800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.6045190654476755e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.73 | consumed tokens: 2210201600.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T13:25:52 | step: 269900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.560797097179602e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.11 | consumed tokens: 2211020800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:26:10 | step: 270000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.5173835044588486e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 2211840000.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:26:30 | step: 270100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.474278287285415e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.33 | consumed tokens: 2212659200.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:26:49 | step: 270200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.431481161442207e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.09 | consumed tokens: 2213478400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T13:27:07 | step: 270300 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.388992411146319e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.83 | consumed tokens: 2214297600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:27:26 | step: 270400 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 3.3468120363977505e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.25 | consumed tokens: 2215116800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T13:27:44 | step: 270500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 3.304940037196502e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.0 | consumed tokens: 2215936000.0 | grad norm avg: 0.98 | grad norm last: 1.05 | 
2025-12-30T13:28:03 | step: 270600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.2633764135425736e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.56 | consumed tokens: 2216755200.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:28:21 | step: 270700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 3.222121449653059e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 2217574400.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:28:40 | step: 270800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.181175145527959e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.91 | consumed tokens: 2218393600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:28:59 | step: 270900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 3.140537501167273e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 2219212800.0 | grad norm avg: 0.99 | grad norm last: 0.99 | 
2025-12-30T13:29:17 | step: 271000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 3.100208232353907e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.38 | consumed tokens: 2220032000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:29:36 | step: 271100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 3.0601879075220495e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 2220851200.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T13:29:54 | step: 271200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 3.0204765266717004e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.09 | consumed tokens: 2221670400.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T13:30:13 | step: 271300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.9810738055857655e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.56 | consumed tokens: 2222489600.0 | grad norm avg: 0.97 | grad norm last: 1.11 | 
2025-12-30T13:30:31 | step: 271400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.941980028481339e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.42 | consumed tokens: 2223308800.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T13:30:49 | step: 271500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.903195195358421e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.11 | consumed tokens: 2224128000.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T13:31:08 | step: 271600 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.864719306217012e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.48 | consumed tokens: 2224947200.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:31:27 | step: 271700 | train samples/s: 93.8 | train mfu (16-bit): -1.0 | lr mean: 2.826552361057111e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.59 | consumed tokens: 2225766400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T13:31:45 | step: 271800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.7886946440958127e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.12 | consumed tokens: 2226585600.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T13:32:04 | step: 271900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.751145871116023e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.06 | consumed tokens: 2227404800.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T13:32:22 | step: 272000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.713906326334836e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.09 | consumed tokens: 2228224000.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T13:32:41 | step: 272100 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.6769762939693464e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.86 | consumed tokens: 2229043200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T13:32:59 | step: 272200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.640355205585365e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.36 | consumed tokens: 2229862400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T13:33:18 | step: 272300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.604043629617081e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.86 | consumed tokens: 2230681600.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T13:33:36 | step: 272400 | train samples/s: 93.9 | train mfu (16-bit): -1.0 | lr mean: 2.5680412818473997e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.3 | consumed tokens: 2231500800.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T13:33:55 | step: 272500 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.532348162276321e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.28 | consumed tokens: 2232320000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:34:13 | step: 272600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.496964839338034e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 4.28 | consumed tokens: 2233139200.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T13:34:32 | step: 272700 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.4618907445983496e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.17 | consumed tokens: 2233958400.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T13:34:50 | step: 272800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 2.4271261622743623e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 2234777600.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:35:09 | step: 272900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.3926713765831664e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.81 | consumed tokens: 2235596800.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:35:28 | step: 273000 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 2.3585261033076677e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.7 | consumed tokens: 2236416000.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T13:35:46 | step: 273100 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 2.3246906266649603e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.61 | consumed tokens: 2237235200.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T13:36:05 | step: 273200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.29116466243795e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.98 | consumed tokens: 2238054400.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:36:23 | step: 273300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.2579484948437312e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.67 | consumed tokens: 2238873600.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T13:36:42 | step: 273400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 2.2250421238823037e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 2239692800.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T13:37:00 | step: 273500 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.1924456916622148e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.33 | consumed tokens: 2240512000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T13:37:19 | step: 273600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 2.1601590560749173e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 2241331200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:37:37 | step: 273700 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.1281822171204112e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 4.47 | consumed tokens: 2242150400.0 | grad norm avg: 0.98 | grad norm last: 0.9 | 
2025-12-30T13:37:56 | step: 273800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 2.0965154590157908e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.62 | consumed tokens: 2242969600.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T13:38:14 | step: 273900 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 2.065158639652509e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.84 | consumed tokens: 2243788800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:38:33 | step: 274000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 2.034111901139113e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 2244608000.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T13:38:51 | step: 274100 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 2.0033752434756025e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.98 | consumed tokens: 2245427200.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:39:10 | step: 274200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.9729485245534306e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.69 | consumed tokens: 2246246400.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T13:39:28 | step: 274300 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.9428321706982388e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.47 | consumed tokens: 2247065600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:39:47 | step: 274400 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.9130258976929326e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.28 | consumed tokens: 2247884800.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T13:40:05 | step: 274500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.8835298476460594e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.22 | consumed tokens: 2248704000.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T13:40:24 | step: 274600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.854344020557619e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.75 | consumed tokens: 2249523200.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T13:40:42 | step: 274700 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.8254685585361585e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 2250342400.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T13:41:00 | step: 274800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.796903319473131e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.7 | consumed tokens: 2251161600.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T13:41:19 | step: 274900 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7686485875856306e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 2251980800.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:41:38 | step: 275000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.7407042207651102e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 2252800000.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T13:41:58 | step: 275100 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 1.7130702190115699e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.11 | consumed tokens: 2253619200.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:42:16 | step: 275200 | train samples/s: 93.6 | train mfu (16-bit): -1.0 | lr mean: 1.6857467244335567e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.2 | consumed tokens: 2254438400.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T13:42:35 | step: 275300 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.6587337370310706e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.19 | consumed tokens: 2255257600.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T13:42:53 | step: 275400 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.6320312568041118e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.19 | consumed tokens: 2256076800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:43:12 | step: 275500 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.60563928375268e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.28 | consumed tokens: 2256896000.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T13:43:31 | step: 275600 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.5795581020938698e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.11 | consumed tokens: 2257715200.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T13:43:49 | step: 275700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 1.5537874276105867e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.38 | consumed tokens: 2258534400.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:44:08 | step: 275800 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.528327402411378e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.55 | consumed tokens: 2259353600.0 | grad norm avg: 0.97 | grad norm last: 0.85 | 
2025-12-30T13:44:26 | step: 275900 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.5031781686047907e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.41 | consumed tokens: 2260172800.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T13:44:45 | step: 276000 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.4783395840822777e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.8 | consumed tokens: 2260992000.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:45:03 | step: 276100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.4538117909523862e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.69 | consumed tokens: 2261811200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:45:22 | step: 276200 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4295947892151162e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.45 | consumed tokens: 2262630400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T13:45:40 | step: 276300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.4056887209790148e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.41 | consumed tokens: 2263449600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:45:59 | step: 276400 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.3820933020269877e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.28 | consumed tokens: 2264268800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T13:46:17 | step: 276500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.3588089586846763e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.73 | consumed tokens: 2265088000.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:46:36 | step: 276600 | train samples/s: 93.7 | train mfu (16-bit): -1.0 | lr mean: 1.3358354067349865e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.25 | consumed tokens: 2265907200.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T13:46:55 | step: 276700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 1.3131727882864652e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.48 | consumed tokens: 2266726400.0 | grad norm avg: 0.97 | grad norm last: 1.09 | 
2025-12-30T13:47:13 | step: 276800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 1.2908212454476597e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.66 | consumed tokens: 2267545600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:47:32 | step: 276900 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 1.268780636110023e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 2268364800.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:47:50 | step: 277000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 1.2470509602735547e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.41 | consumed tokens: 2269184000.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T13:48:09 | step: 277100 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.2256325021553494e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.73 | consumed tokens: 2270003200.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T13:48:27 | step: 277200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 1.2045249775383127e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.78 | consumed tokens: 2270822400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:48:46 | step: 277300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 1.1837286706395389e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.23 | consumed tokens: 2271641600.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T13:49:04 | step: 277400 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.1632434393504809e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 2272460800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T13:49:23 | step: 277500 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 1.1430694257796858e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.77 | consumed tokens: 2273280000.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T13:49:41 | step: 277600 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.1232064878186065e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 1.96 | consumed tokens: 2274099200.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T13:50:00 | step: 277700 | train samples/s: 95.1 | train mfu (16-bit): -1.0 | lr mean: 1.1036548386300638e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.58 | consumed tokens: 2274918400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T13:50:18 | step: 277800 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0844144071597839e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.47 | consumed tokens: 2275737600.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T13:50:36 | step: 277900 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0654851934077669e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.22 | consumed tokens: 2276556800.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:50:55 | step: 278000 | train samples/s: 95.0 | train mfu (16-bit): -1.0 | lr mean: 1.0468672684282865e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.48 | consumed tokens: 2277376000.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T13:51:13 | step: 278100 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 1.0285607032756161e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.5 | consumed tokens: 2278195200.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:51:32 | step: 278200 | train samples/s: 93.2 | train mfu (16-bit): -1.0 | lr mean: 1.0105654268954822e-07 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.36 | consumed tokens: 2279014400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:51:51 | step: 278300 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.928814392878849e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.31 | consumed tokens: 2279833600.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:52:09 | step: 278400 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 9.755088825613711e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.81 | consumed tokens: 2280652800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T13:52:28 | step: 278500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 9.584477567159411e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.22 | consumed tokens: 2281472000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:52:46 | step: 278600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 9.416979906973211e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.27 | consumed tokens: 2282291200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:53:05 | step: 278700 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 9.252596555597847e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.12 | consumed tokens: 2283110400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T13:53:24 | step: 278800 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 9.09132751303332e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.73 | consumed tokens: 2283929600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T13:53:42 | step: 278900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 8.933173489822366e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.92 | consumed tokens: 2284748800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T13:54:01 | step: 279000 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.778134485964983e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.8 | consumed tokens: 2285568000.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:54:19 | step: 279100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.626210501461173e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.91 | consumed tokens: 2286387200.0 | grad norm avg: 0.96 | grad norm last: 1.1 | 
2025-12-30T13:54:38 | step: 279200 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 8.477401536310936e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.16 | consumed tokens: 2287206400.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T13:54:56 | step: 279300 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 8.331708301057006e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.27 | consumed tokens: 2288025600.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T13:55:15 | step: 279400 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 8.189130795699384e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.56 | consumed tokens: 2288844800.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T13:55:34 | step: 279500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 8.049669730780806e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.45 | consumed tokens: 2289664000.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T13:55:52 | step: 279600 | train samples/s: 93.1 | train mfu (16-bit): -1.0 | lr mean: 7.9133236852158e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.62 | consumed tokens: 2290483200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T13:56:11 | step: 279700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 7.780094790632575e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.94 | consumed tokens: 2291302400.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T13:56:29 | step: 279800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 7.649981625945657e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 2292121600.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T13:56:48 | step: 279900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 7.522984901697782e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.36 | consumed tokens: 2292940800.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:57:06 | step: 280000 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 7.399105328431688e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.66 | consumed tokens: 2293760000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T13:57:26 | step: 280100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.278342906147373e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.02 | consumed tokens: 2294579200.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T13:57:45 | step: 280200 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.160696924302101e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.8 | consumed tokens: 2295398400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:58:03 | step: 280300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 7.04616809343861e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.25 | consumed tokens: 2296217600.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:58:22 | step: 280400 | train samples/s: 94.0 | train mfu (16-bit): -1.0 | lr mean: 6.934756413556897e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 2297036800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T13:58:40 | step: 280500 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 6.8264625951997e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.88 | consumed tokens: 2297856000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T13:58:59 | step: 280600 | train samples/s: 94.1 | train mfu (16-bit): -1.0 | lr mean: 6.721285927824283e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.5 | consumed tokens: 2298675200.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T13:59:17 | step: 280700 | train samples/s: 94.2 | train mfu (16-bit): -1.0 | lr mean: 6.619227121973381e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.81 | consumed tokens: 2299494400.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T13:59:36 | step: 280800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 6.520286177646994e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 2300313600.0 | grad norm avg: 0.98 | grad norm last: 1.05 | 
2025-12-30T13:59:55 | step: 280900 | train samples/s: 92.9 | train mfu (16-bit): -1.0 | lr mean: 6.424463805387859e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.25 | consumed tokens: 2301132800.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T14:00:13 | step: 281000 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 6.331758584110503e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 2301952000.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T14:00:32 | step: 281100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 6.242171934900398e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.77 | consumed tokens: 2302771200.0 | grad norm avg: 0.98 | grad norm last: 1.08 | 
2025-12-30T14:00:51 | step: 281200 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 6.155703857757544e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 2303590400.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T14:01:09 | step: 281300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 6.072353642139205e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.44 | consumed tokens: 2304409600.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T14:01:27 | step: 281400 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.992121998588118e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.7 | consumed tokens: 2305228800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T14:01:46 | step: 281500 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.9150092823756495e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.06 | consumed tokens: 2306048000.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T14:02:04 | step: 281600 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.841015138230432e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.62 | consumed tokens: 2306867200.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T14:02:23 | step: 281700 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.770139566152466e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.7 | consumed tokens: 2307686400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T14:02:42 | step: 281800 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.702383276684486e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.38 | consumed tokens: 2308505600.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T14:03:00 | step: 281900 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.637745559283758e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 2309324800.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T14:03:19 | step: 282000 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.576227124493016e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.31 | consumed tokens: 2310144000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T14:03:37 | step: 282100 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.5178279723122614e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.38 | consumed tokens: 2310963200.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T14:03:56 | step: 282200 | train samples/s: 93.5 | train mfu (16-bit): -1.0 | lr mean: 5.4625477474701256e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.98 | consumed tokens: 2311782400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T14:04:14 | step: 282300 | train samples/s: 94.9 | train mfu (16-bit): -1.0 | lr mean: 5.4103868052379767e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.16 | consumed tokens: 2312601600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T14:04:33 | step: 282400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.3613455008871824e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.73 | consumed tokens: 2313420800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T14:04:51 | step: 282500 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 5.315423123875007e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 2314240000.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T14:05:10 | step: 282600 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.2726207400155545e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.52 | consumed tokens: 2315059200.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T14:05:28 | step: 282700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.2329376387660886e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.14 | consumed tokens: 2315878400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T14:05:47 | step: 282800 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.1963738201266096e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.08 | consumed tokens: 2316697600.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T14:06:05 | step: 282900 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.162929994639853e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 4.09 | consumed tokens: 2317516800.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T14:06:24 | step: 283000 | train samples/s: 94.5 | train mfu (16-bit): -1.0 | lr mean: 5.132605451763084e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 2318336000.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T14:06:42 | step: 283100 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.105400902039037e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.53 | consumed tokens: 2319155200.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T14:07:01 | step: 283200 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.0813159901963445e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 2319974400.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T14:07:19 | step: 283300 | train samples/s: 94.8 | train mfu (16-bit): -1.0 | lr mean: 5.060350716235007e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.11 | consumed tokens: 2320793600.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T14:07:38 | step: 283400 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.042505435426392e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.25 | consumed tokens: 2321612800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T14:07:57 | step: 283500 | train samples/s: 93.4 | train mfu (16-bit): -1.0 | lr mean: 5.027779792499132e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 2322432000.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T14:08:15 | step: 283600 | train samples/s: 94.4 | train mfu (16-bit): -1.0 | lr mean: 5.0161737874532264e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.95 | consumed tokens: 2323251200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T14:08:34 | step: 283700 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.0076877755600435e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.98 | consumed tokens: 2324070400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T14:08:52 | step: 283800 | train samples/s: 94.6 | train mfu (16-bit): -1.0 | lr mean: 5.002321756819583e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.56 | consumed tokens: 2324889600.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T14:09:11 | step: 283900 | train samples/s: 94.7 | train mfu (16-bit): -1.0 | lr mean: 5.000075375960478e-08 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.42 | consumed tokens: 2325708800.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
Training done at 2025-12-30 14:09:15.840973.
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/s472389/modalities_test/wandb_storage/wandb/offline-run-20251229_233235-ti3s0hbw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb_storage/wandb/offline-run-20251229_233235-ti3s0hbw/logs[0m
==========================================
Job finished at: Tue Dec 30 02:09:17 PM CET 2025
==========================================
