==========================================
Experiment 3: Fine-tuning GPT-2 on 5 languages
Job ID: 2147260
Node: jn120
Start time: Mon Dec 29 11:35:41 PM CET 2025
==========================================
Mon Dec 29 23:35:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40                     On  |   00000000:25:00.0 Off |                  Off |
| N/A   31C    P8             35W /  300W |       1MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2025-12-29__23-35-55_a91e58afaade00f6
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 124,439,808 parameters): weight_decay = 0.01
=> all (148 modules with 124,439,808 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2025-12-29 23:35:58.747290.



======================== Training Report ========================
Training target: 
	num_target_tokens: 4160987136
	num_target_steps: 507933 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 1
CUDA environment settings: 
	local_rank: 0
	world_size: 1
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (4160993280) does not match the number of target tokens (4160987136). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (507933) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (507933) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (507933) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2025-12-29 23:35:58.747659.
2025-12-29T23:36:20 | step: 100 | train samples/s: 78.7 | train mfu (16-bit): -1.0 | lr mean: 5.043039891461376e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.27 | train loss last: 4.56 | consumed tokens: 819200.0 | grad norm avg: 3.02 | grad norm last: 3.14 | 
2025-12-29T23:36:42 | step: 200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.171995326236356e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.19 | train loss last: 3.98 | consumed tokens: 1638400.0 | grad norm avg: 2.72 | grad norm last: 2.53 | 
2025-12-29T23:37:02 | step: 300 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 5.386372322391253e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.2 | train loss last: 3.7 | consumed tokens: 2457600.0 | grad norm avg: 2.66 | grad norm last: 2.64 | 
2025-12-29T23:37:23 | step: 400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.6853514251997694e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.17 | train loss last: 4.34 | consumed tokens: 3276800.0 | grad norm avg: 2.59 | grad norm last: 2.84 | 
2025-12-29T23:37:43 | step: 500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 6.067788035579724e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.17 | train loss last: 3.58 | consumed tokens: 4096000.0 | grad norm avg: 2.53 | grad norm last: 2.45 | 
2025-12-29T23:38:04 | step: 600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 6.5322196860506665e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.13 | train loss last: 3.92 | consumed tokens: 4915200.0 | grad norm avg: 2.5 | grad norm last: 2.53 | 
2025-12-29T23:38:24 | step: 700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.076869223965332e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.12 | train loss last: 3.59 | consumed tokens: 5734400.0 | grad norm avg: 2.48 | grad norm last: 2.71 | 
2025-12-29T23:38:45 | step: 800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 7.699652996961959e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.09 | train loss last: 4.19 | consumed tokens: 6553600.0 | grad norm avg: 2.44 | grad norm last: 2.54 | 
2025-12-29T23:39:05 | step: 900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.398189493163954e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.09 | train loss last: 4.09 | consumed tokens: 7372800.0 | grad norm avg: 2.45 | grad norm last: 2.57 | 
2025-12-29T23:39:26 | step: 1000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.169803888653405e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.11 | train loss last: 3.8 | consumed tokens: 8192000.0 | grad norm avg: 2.4 | grad norm last: 2.28 | 
2025-12-29T23:39:46 | step: 1100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0011545782617759e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.09 | train loss last: 4.22 | consumed tokens: 9011200.0 | grad norm avg: 2.35 | grad norm last: 2.38 | 
2025-12-29T23:40:07 | step: 1200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0920194654318038e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.07 | train loss last: 4.22 | consumed tokens: 9830400.0 | grad norm avg: 2.35 | grad norm last: 2.34 | 
2025-12-29T23:40:27 | step: 1300 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.1892274415004067e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.06 | train loss last: 3.92 | consumed tokens: 10649600.0 | grad norm avg: 2.33 | grad norm last: 2.23 | 
2025-12-29T23:40:48 | step: 1400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.2924065231345594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.02 | train loss last: 4.22 | consumed tokens: 11468800.0 | grad norm avg: 2.28 | grad norm last: 2.3 | 
2025-12-29T23:41:08 | step: 1500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.4011620805831626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.07 | train loss last: 3.91 | consumed tokens: 12288000.0 | grad norm avg: 2.24 | grad norm last: 2.27 | 
2025-12-29T23:41:29 | step: 1600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.5150780200201552e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.01 | train loss last: 4.22 | consumed tokens: 13107200.0 | grad norm avg: 2.17 | grad norm last: 2.11 | 
2025-12-29T23:41:49 | step: 1700 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.6337184206349775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.99 | train loss last: 4.44 | consumed tokens: 13926400.0 | grad norm avg: 2.16 | grad norm last: 2.15 | 
2025-12-29T23:42:10 | step: 1800 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.756629535520915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.0 | train loss last: 3.59 | consumed tokens: 14745600.0 | grad norm avg: 2.1 | grad norm last: 1.99 | 
2025-12-29T23:42:30 | step: 1900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.8833410649676807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.0 | train loss last: 4.03 | consumed tokens: 15564800.0 | grad norm avg: 2.06 | grad norm last: 2.06 | 
2025-12-29T23:42:51 | step: 2000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0133680664002895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 4.0 | train loss last: 3.59 | consumed tokens: 16384000.0 | grad norm avg: 2.01 | grad norm last: 1.97 | 
2025-12-29T23:43:11 | step: 2100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.1462134100147523e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.94 | train loss last: 4.16 | consumed tokens: 17203200.0 | grad norm avg: 1.95 | grad norm last: 1.78 | 
2025-12-29T23:43:31 | step: 2200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.281368688272778e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.93 | train loss last: 3.52 | consumed tokens: 18022400.0 | grad norm avg: 1.92 | grad norm last: 1.84 | 
2025-12-29T23:43:52 | step: 2300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.4183167624869384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.97 | train loss last: 4.16 | consumed tokens: 18841600.0 | grad norm avg: 1.88 | grad norm last: 1.82 | 
2025-12-29T23:44:12 | step: 2400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.5565339456079528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.95 | train loss last: 4.44 | consumed tokens: 19660800.0 | grad norm avg: 1.82 | grad norm last: 1.8 | 
2025-12-29T23:44:33 | step: 2500 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 2.6954910936183296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.91 | train loss last: 3.44 | consumed tokens: 20480000.0 | grad norm avg: 1.81 | grad norm last: 1.71 | 
2025-12-29T23:44:53 | step: 2600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.834656879713293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.93 | train loss last: 3.98 | consumed tokens: 21299200.0 | grad norm avg: 1.74 | grad norm last: 1.75 | 
2025-12-29T23:45:14 | step: 2700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.9734988856944256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.91 | train loss last: 3.95 | consumed tokens: 22118400.0 | grad norm avg: 1.72 | grad norm last: 1.61 | 
2025-12-29T23:45:34 | step: 2800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.111485784756951e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.88 | train loss last: 4.09 | consumed tokens: 22937600.0 | grad norm avg: 1.66 | grad norm last: 1.77 | 
2025-12-29T23:45:55 | step: 2900 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 3.2480897061759606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.93 | train loss last: 3.88 | consumed tokens: 23756800.0 | grad norm avg: 1.64 | grad norm last: 1.62 | 
2025-12-29T23:46:15 | step: 3000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.382787690497935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.9 | train loss last: 3.95 | consumed tokens: 24576000.0 | grad norm avg: 1.59 | grad norm last: 1.64 | 
2025-12-29T23:46:36 | step: 3100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.515065327519551e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.85 | train loss last: 4.56 | consumed tokens: 25395200.0 | grad norm avg: 1.55 | grad norm last: 1.58 | 
2025-12-29T23:46:56 | step: 3200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.644415846792981e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.89 | train loss last: 4.5 | consumed tokens: 26214400.0 | grad norm avg: 1.54 | grad norm last: 1.56 | 
2025-12-29T23:47:17 | step: 3300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.77034411940258e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.84 | train loss last: 3.58 | consumed tokens: 27033600.0 | grad norm avg: 1.48 | grad norm last: 1.36 | 
2025-12-29T23:47:37 | step: 3400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.89236920455005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.85 | train loss last: 3.44 | consumed tokens: 27852800.0 | grad norm avg: 1.45 | grad norm last: 1.49 | 
2025-12-29T23:47:58 | step: 3500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.010023985756561e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.85 | train loss last: 4.09 | consumed tokens: 28672000.0 | grad norm avg: 1.42 | grad norm last: 1.47 | 
2025-12-29T23:48:18 | step: 3600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.122857717447914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.81 | train loss last: 3.61 | consumed tokens: 29491200.0 | grad norm avg: 1.41 | grad norm last: 1.38 | 
2025-12-29T23:48:39 | step: 3700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.2304396629333496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.83 | train loss last: 3.81 | consumed tokens: 30310400.0 | grad norm avg: 1.38 | grad norm last: 1.46 | 
2025-12-29T23:48:59 | step: 3800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.332357275416143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.84 | train loss last: 3.28 | consumed tokens: 31129600.0 | grad norm avg: 1.34 | grad norm last: 1.42 | 
2025-12-29T23:49:20 | step: 3900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.428221654961817e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.78 | train loss last: 3.91 | consumed tokens: 31948800.0 | grad norm avg: 1.32 | grad norm last: 1.3 | 
2025-12-29T23:49:41 | step: 4000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.5176653657108545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.79 | train loss last: 4.09 | consumed tokens: 32768000.0 | grad norm avg: 1.29 | grad norm last: 1.3 | 
2025-12-29T23:50:01 | step: 4100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.600346437655389e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.8 | train loss last: 3.67 | consumed tokens: 33587200.0 | grad norm avg: 1.28 | grad norm last: 1.23 | 
2025-12-29T23:50:22 | step: 4200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.675948366639204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.81 | train loss last: 3.47 | consumed tokens: 34406400.0 | grad norm avg: 1.26 | grad norm last: 1.33 | 
2025-12-29T23:50:42 | step: 4300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.744181933347136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 3.66 | consumed tokens: 35225600.0 | grad norm avg: 1.22 | grad norm last: 1.2 | 
2025-12-29T23:51:03 | step: 4400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.804786294698715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 3.27 | consumed tokens: 36044800.0 | grad norm avg: 1.19 | grad norm last: 1.06 | 
2025-12-29T23:51:23 | step: 4500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.857529711443931e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 3.94 | consumed tokens: 36864000.0 | grad norm avg: 1.19 | grad norm last: 1.09 | 
2025-12-29T23:51:44 | step: 4600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9022099119611084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.75 | train loss last: 3.81 | consumed tokens: 37683200.0 | grad norm avg: 1.17 | grad norm last: 1.11 | 
2025-12-29T23:52:05 | step: 4700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.938656638842076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 3.44 | consumed tokens: 38502400.0 | grad norm avg: 1.15 | grad norm last: 1.09 | 
2025-12-29T23:52:25 | step: 4800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.966729829902761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.74 | train loss last: 3.44 | consumed tokens: 39321600.0 | grad norm avg: 1.13 | grad norm last: 1.06 | 
2025-12-29T23:52:46 | step: 4900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.986322164768353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.77 | train loss last: 3.83 | consumed tokens: 40140800.0 | grad norm avg: 1.13 | grad norm last: 1.15 | 
2025-12-29T23:53:06 | step: 5000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.997359064873308e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.73 | train loss last: 3.97 | consumed tokens: 40960000.0 | grad norm avg: 1.11 | grad norm last: 1.09 | 
2025-12-29T23:53:29 | step: 5100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.999999873689376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.75 | train loss last: 3.33 | consumed tokens: 41779200.0 | grad norm avg: 1.09 | grad norm last: 1.06 | 
2025-12-29T23:53:49 | step: 5200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9999991460936144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.74 | train loss last: 2.72 | consumed tokens: 42598400.0 | grad norm avg: 1.06 | grad norm last: 1.01 | 
2025-12-29T23:54:10 | step: 5300 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.9999976909020916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.74 | train loss last: 3.59 | consumed tokens: 43417600.0 | grad norm avg: 1.06 | grad norm last: 0.98 | 
2025-12-29T23:54:31 | step: 5400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.999994780519046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.72 | train loss last: 4.09 | consumed tokens: 44236800.0 | grad norm avg: 1.05 | grad norm last: 1.07 | 
2025-12-29T23:54:51 | step: 5500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999915063381195e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.73 | train loss last: 4.06 | consumed tokens: 45056000.0 | grad norm avg: 1.03 | grad norm last: 1.06 | 
2025-12-29T23:55:12 | step: 5600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.99998677696567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.71 | train loss last: 3.53 | consumed tokens: 45875200.0 | grad norm avg: 1.01 | grad norm last: 0.95 | 
2025-12-29T23:55:32 | step: 5700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999813199974597e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.17 | consumed tokens: 46694400.0 | grad norm avg: 1.01 | grad norm last: 0.99 | 
2025-12-29T23:55:53 | step: 5800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.999974771635607e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.77 | consumed tokens: 47513600.0 | grad norm avg: 1.01 | grad norm last: 0.95 | 
2025-12-29T23:56:14 | step: 5900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.999967131880112e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.71 | train loss last: 3.78 | consumed tokens: 48332800.0 | grad norm avg: 0.99 | grad norm last: 0.94 | 
2025-12-29T23:56:34 | step: 6000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.999958764528856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.69 | train loss last: 4.44 | consumed tokens: 49152000.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-29T23:56:55 | step: 6100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.999949305783957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.59 | consumed tokens: 49971200.0 | grad norm avg: 0.97 | grad norm last: 0.88 | 
2025-12-29T23:57:15 | step: 6200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.999938755645417e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.7 | train loss last: 3.69 | consumed tokens: 50790400.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-29T23:57:36 | step: 6300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.999927114113234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.68 | train loss last: 3.86 | consumed tokens: 51609600.0 | grad norm avg: 0.97 | grad norm last: 0.85 | 
2025-12-29T23:57:56 | step: 6400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.99991474498529e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.69 | train loss last: 3.77 | consumed tokens: 52428800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-29T23:58:17 | step: 6500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9999016482615843e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.68 | train loss last: 3.81 | consumed tokens: 53248000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-29T23:58:37 | step: 6600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.999887096346356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.69 | consumed tokens: 54067200.0 | grad norm avg: 0.94 | grad norm last: 1.02 | 
2025-12-29T23:58:58 | step: 6700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.999871816835366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.66 | train loss last: 3.73 | consumed tokens: 54886400.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-29T23:59:18 | step: 6800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.999855445930734e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.42 | consumed tokens: 55705600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-29T23:59:39 | step: 6900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.999838347430341e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.66 | train loss last: 3.31 | consumed tokens: 56524800.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T00:00:00 | step: 7000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9998201575363055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.09 | consumed tokens: 57344000.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T00:00:20 | step: 7100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.999800876248628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.19 | consumed tokens: 58163200.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T00:00:41 | step: 7200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.999780503567308e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.67 | train loss last: 3.73 | consumed tokens: 58982400.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T00:01:01 | step: 7300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.999759403290227e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.48 | consumed tokens: 59801600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T00:01:22 | step: 7400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.999737211619504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.65 | train loss last: 3.33 | consumed tokens: 60620800.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T00:01:42 | step: 7500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.999714292353019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.36 | consumed tokens: 61440000.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T00:02:03 | step: 7600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9996899178950116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.41 | consumed tokens: 62259200.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T00:02:23 | step: 7700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9996651796391234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.16 | consumed tokens: 63078400.0 | grad norm avg: 0.9 | grad norm last: 0.83 | 
2025-12-30T00:02:44 | step: 7800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.999638986191712e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.64 | train loss last: 3.81 | consumed tokens: 63897600.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T00:03:04 | step: 7900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.99961206514854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.69 | consumed tokens: 64716800.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T00:03:25 | step: 8000 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.999584052711725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.39 | consumed tokens: 65536000.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T00:03:46 | step: 8100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9995549488812685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.78 | consumed tokens: 66355200.0 | grad norm avg: 0.88 | grad norm last: 1.01 | 
2025-12-30T00:04:06 | step: 8200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9995251174550503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.94 | consumed tokens: 67174400.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T00:04:27 | step: 8300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.99949419463519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 4.06 | consumed tokens: 67993600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T00:04:48 | step: 8400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.999462180421688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.67 | consumed tokens: 68812800.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T00:05:08 | step: 8500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.999429438612424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.97 | consumed tokens: 69632000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T00:05:29 | step: 8600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.999395605409518e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.45 | consumed tokens: 70451200.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T00:05:49 | step: 8700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.99936068081297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.22 | consumed tokens: 71270400.0 | grad norm avg: 0.87 | grad norm last: 0.94 | 
2025-12-30T00:06:10 | step: 8800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.99932502862066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.83 | consumed tokens: 72089600.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T00:06:31 | step: 8900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.999288285034709e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.38 | consumed tokens: 72908800.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T00:06:51 | step: 9000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.999250450055115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 3.61 | consumed tokens: 73728000.0 | grad norm avg: 0.87 | grad norm last: 0.95 | 
2025-12-30T00:07:12 | step: 9100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.99921188747976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.17 | consumed tokens: 74547200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T00:07:32 | step: 9200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.999171869712882e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.3 | consumed tokens: 75366400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T00:07:53 | step: 9300 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.999131488148123e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.56 | consumed tokens: 76185600.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T00:08:14 | step: 9400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9990896513918415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.62 | train loss last: 3.42 | consumed tokens: 77004800.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T00:08:35 | step: 9500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.9990470870397985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.63 | train loss last: 3.34 | consumed tokens: 77824000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T00:08:55 | step: 9600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9990034312941134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.08 | consumed tokens: 78643200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T00:09:16 | step: 9700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.998959047952667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.6 | train loss last: 3.62 | consumed tokens: 79462400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T00:09:37 | step: 9800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.998913573217578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 4.09 | consumed tokens: 80281600.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T00:09:57 | step: 9900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9988670070888475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.44 | consumed tokens: 81100800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T00:10:18 | step: 10000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9988193495664746e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.66 | consumed tokens: 81920000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T00:10:40 | step: 10100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.99877096444834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.45 | consumed tokens: 82739200.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T00:11:01 | step: 10200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.998721487936564e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.61 | train loss last: 4.19 | consumed tokens: 83558400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T00:11:21 | step: 10300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.998671283829026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.62 | consumed tokens: 84377600.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T00:11:42 | step: 10400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.998619624529965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.59 | train loss last: 3.56 | consumed tokens: 85196800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T00:12:03 | step: 10500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.998567601433024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.8 | consumed tokens: 86016000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T00:12:23 | step: 10600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.9985141231445596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.97 | consumed tokens: 86835200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T00:12:44 | step: 10700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.998459917260334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.95 | consumed tokens: 87654400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T00:13:05 | step: 10800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.998404619982466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.45 | consumed tokens: 88473600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T00:13:25 | step: 10900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.998348231310956e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.22 | consumed tokens: 89292800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T00:13:46 | step: 11000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.998291115043685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.23 | consumed tokens: 90112000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T00:14:06 | step: 11100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9982329073827714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.25 | consumed tokens: 90931200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T00:14:27 | step: 11200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.998173608328216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.75 | consumed tokens: 91750400.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T00:14:47 | step: 11300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.998113581677899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.61 | consumed tokens: 92569600.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T00:15:08 | step: 11400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.9980524636339396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.97 | consumed tokens: 93388800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T00:15:28 | step: 11500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9979902541963384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.0 | consumed tokens: 94208000.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T00:15:49 | step: 11600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.997927317162976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.45 | consumed tokens: 95027200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T00:16:09 | step: 11700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.997863288735971e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.28 | consumed tokens: 95846400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T00:16:30 | step: 11800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.997798168915324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.56 | consumed tokens: 96665600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T00:16:51 | step: 11900 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.9977323214989156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.12 | consumed tokens: 97484800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T00:17:11 | step: 12000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.997665382688865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.58 | train loss last: 3.64 | consumed tokens: 98304000.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T00:17:32 | step: 12100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9975973524851725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.53 | consumed tokens: 99123200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:17:53 | step: 12200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.997528230887838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.83 | consumed tokens: 99942400.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T00:18:14 | step: 12300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.9974583816947415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.33 | consumed tokens: 100761600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T00:18:34 | step: 12400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.997387441108003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.41 | consumed tokens: 101580800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:18:55 | step: 12500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9973157729255036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.97 | consumed tokens: 102400000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T00:19:15 | step: 12600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.997243013349362e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.57 | train loss last: 3.78 | consumed tokens: 103219200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T00:19:36 | step: 12700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.997169162379578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.44 | consumed tokens: 104038400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T00:19:56 | step: 12800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9970945838140324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 4.22 | consumed tokens: 104857600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:20:17 | step: 12900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.997018550056964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.67 | consumed tokens: 105676800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:20:37 | step: 13000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.996942152502015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.56 | train loss last: 3.41 | consumed tokens: 106496000.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T00:20:58 | step: 13100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9968642997555435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.48 | consumed tokens: 107315200.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T00:21:19 | step: 13200 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.99678571941331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 4.03 | consumed tokens: 108134400.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T00:21:39 | step: 13300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.996706047677435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.38 | consumed tokens: 108953600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T00:22:00 | step: 13400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9966252845479175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.58 | consumed tokens: 109772800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T00:22:21 | step: 13500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.996543793822639e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.27 | consumed tokens: 110592000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T00:22:41 | step: 13600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.996461211703718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.69 | consumed tokens: 111411200.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T00:23:02 | step: 13700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.996377901989035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 2.8 | consumed tokens: 112230400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T00:23:23 | step: 13800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.996293500880711e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.69 | consumed tokens: 113049600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T00:23:43 | step: 13900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.996208008378744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 3.78 | consumed tokens: 113868800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T00:24:04 | step: 14000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9961214244831353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.55 | consumed tokens: 114688000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:24:24 | step: 14100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.996034112991765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.06 | consumed tokens: 115507200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T00:24:45 | step: 14200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.995945710106753e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.3 | consumed tokens: 116326400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T00:25:06 | step: 14300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.9958562158280984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.34 | consumed tokens: 117145600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T00:25:26 | step: 14400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9957659939536825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.8 | consumed tokens: 117964800.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T00:25:47 | step: 14500 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.9956746806856245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.27 | consumed tokens: 118784000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:26:08 | step: 14600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9955822760239244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.97 | consumed tokens: 119603200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:26:28 | step: 14700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.995489143766463e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.73 | consumed tokens: 120422400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T00:26:49 | step: 14800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.995394920115359e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.59 | consumed tokens: 121241600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:27:10 | step: 14900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.995299605070613e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.19 | consumed tokens: 122060800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:27:30 | step: 15000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.995203562430106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.34 | consumed tokens: 122880000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:27:52 | step: 15100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.995106428395957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.5 | consumed tokens: 123699200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:28:13 | step: 15200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.995008202968165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.36 | consumed tokens: 124518400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:28:34 | step: 15300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9949092499446124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.3 | consumed tokens: 125337600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T00:28:54 | step: 15400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9948092055274174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.28 | consumed tokens: 126156800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:29:15 | step: 15500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.99470806971658e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 2.81 | consumed tokens: 126976000.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T00:29:36 | step: 15600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9946062063099816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.95 | consumed tokens: 127795200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T00:29:56 | step: 15700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.994503251509741e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.67 | consumed tokens: 128614400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T00:30:17 | step: 15800 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.994399205315858e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.69 | consumed tokens: 129433600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:30:38 | step: 15900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.994294067728333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.42 | consumed tokens: 130252800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:30:58 | step: 16000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.994188202545047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.51 | train loss last: 3.44 | consumed tokens: 131072000.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:31:19 | step: 16100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.994081609765999e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.86 | consumed tokens: 131891200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:31:40 | step: 16200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.9939735617954284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.45 | consumed tokens: 132710400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T00:32:00 | step: 16300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9938647862290964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.75 | consumed tokens: 133529600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:32:21 | step: 16400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.993754919269122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.53 | consumed tokens: 134348800.0 | grad norm avg: 0.82 | grad norm last: 0.73 | 
2025-12-30T00:32:41 | step: 16500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9936443247133866e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.03 | consumed tokens: 135168000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:33:02 | step: 16600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.993532638764009e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.41 | consumed tokens: 135987200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:33:23 | step: 16700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.993419861420989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.12 | consumed tokens: 136806400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:33:43 | step: 16800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.993305992684327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.54 | train loss last: 3.47 | consumed tokens: 137625600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:34:04 | step: 16900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.993191396351904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.52 | consumed tokens: 138444800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:34:24 | step: 17000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.993075708625838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 2.8 | consumed tokens: 139264000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T00:34:45 | step: 17100 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.992959293304011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.39 | consumed tokens: 140083200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:35:06 | step: 17200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.992841786588542e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 3.73 | consumed tokens: 140902400.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:35:26 | step: 17300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.992723188479431e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.64 | consumed tokens: 141721600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:35:47 | step: 17400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.9926034989766777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.55 | train loss last: 4.19 | consumed tokens: 142540800.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T00:36:08 | step: 17500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.992483081878163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.64 | consumed tokens: 143360000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:36:28 | step: 17600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.992361573386006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.83 | consumed tokens: 144179200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T00:36:49 | step: 17700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.992239337298088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.42 | consumed tokens: 144998400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T00:37:10 | step: 17800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9921160098165274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.72 | consumed tokens: 145817600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:37:30 | step: 17900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.991991590941325e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.03 | consumed tokens: 146636800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:37:51 | step: 18000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.99186608067248e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.23 | consumed tokens: 147456000.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T00:38:12 | step: 18100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.991739842807874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.53 | train loss last: 3.61 | consumed tokens: 148275200.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T00:38:32 | step: 18200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.991612513549626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.52 | consumed tokens: 149094400.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T00:38:53 | step: 18300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.991484456695616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.88 | consumed tokens: 149913600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T00:39:13 | step: 18400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.9913553084479645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.75 | consumed tokens: 150732800.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:39:34 | step: 18500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9912250688066706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.47 | consumed tokens: 151552000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:39:55 | step: 18600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9910937377717346e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.03 | consumed tokens: 152371200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:40:15 | step: 18700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.990961679141037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.94 | consumed tokens: 153190400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:40:36 | step: 18800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9908285291166976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.23 | consumed tokens: 154009600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T00:40:57 | step: 18900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.990694287698716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.5 | train loss last: 3.42 | consumed tokens: 154828800.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:41:17 | step: 19000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.990559318684973e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.36 | consumed tokens: 155648000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:41:38 | step: 19100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9904232582775876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 4.09 | consumed tokens: 156467200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:41:58 | step: 19200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.990286470274441e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.52 | train loss last: 4.62 | consumed tokens: 157286400.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:42:19 | step: 19300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9901482270797715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.48 | consumed tokens: 158105600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:42:39 | step: 19400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.990009620087221e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.11 | consumed tokens: 158924800.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T00:43:00 | step: 19500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.989869557903148e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.23 | consumed tokens: 159744000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:43:20 | step: 19600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.989728768123314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.55 | consumed tokens: 160563200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:43:41 | step: 19700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.989586886949837e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.12 | consumed tokens: 161382400.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T00:44:01 | step: 19800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9894439143827185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.14 | consumed tokens: 162201600.0 | grad norm avg: 0.81 | grad norm last: 0.95 | 
2025-12-30T00:44:22 | step: 19900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9893002142198384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.75 | consumed tokens: 163020800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:44:43 | step: 20000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.989155422663316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.44 | consumed tokens: 163840000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:45:05 | step: 20100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.989009539713152e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.83 | consumed tokens: 164659200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:45:25 | step: 20200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.988862929167226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.16 | consumed tokens: 165478400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:45:46 | step: 20300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.988715227227658e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.19 | consumed tokens: 166297600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:46:06 | step: 20400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.988566797692329e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.8 | consumed tokens: 167116800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T00:46:27 | step: 20500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9884169129654765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.75 | consumed tokens: 167936000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T00:46:47 | step: 20600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.988266300642863e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.25 | consumed tokens: 168755200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T00:47:08 | step: 20700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.988114960724488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.77 | consumed tokens: 169574400.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T00:47:28 | step: 20800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.987962529412471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.36 | consumed tokens: 170393600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T00:47:49 | step: 20900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9878090067068115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.56 | consumed tokens: 171212800.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T00:48:10 | step: 21000 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.98765439260751e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.49 | train loss last: 3.77 | consumed tokens: 172032000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:48:31 | step: 21100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.987499050912447e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.25 | consumed tokens: 172851200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:48:51 | step: 21200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.987342617823742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.86 | consumed tokens: 173670400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T00:49:12 | step: 21300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.987185093341395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 4.12 | consumed tokens: 174489600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:49:32 | step: 21400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.987026841263287e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 4.09 | consumed tokens: 175308800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T00:49:53 | step: 21500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.986867497791536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.59 | consumed tokens: 176128000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T00:50:13 | step: 21600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.986707426724024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.2 | consumed tokens: 176947200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:50:34 | step: 21700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.986545900464989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 4.16 | consumed tokens: 177766400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:50:54 | step: 21800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.986383646610193e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.42 | consumed tokens: 178585600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:51:15 | step: 21900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.986220665159635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.73 | consumed tokens: 179404800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T00:51:35 | step: 22000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9860565923154354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 2.91 | consumed tokens: 180224000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:51:56 | step: 22100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9858914280775934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.89 | consumed tokens: 181043200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:52:17 | step: 22200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9857251724461094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.31 | consumed tokens: 181862400.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T00:52:38 | step: 22300 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.985558189218864e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.55 | consumed tokens: 182681600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T00:52:58 | step: 22400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.985390114597976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.0 | consumed tokens: 183500800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T00:53:19 | step: 22500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9852209485834464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.36 | consumed tokens: 184320000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:53:39 | step: 22600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.985051054973155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.22 | consumed tokens: 185139200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:54:00 | step: 22700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.984880069969222e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.28 | consumed tokens: 185958400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:54:21 | step: 22800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.984708357369527e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.53 | consumed tokens: 186777600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T00:54:41 | step: 22900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9845351895783097e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.5 | consumed tokens: 187596800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T00:55:02 | step: 23000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9843616579892114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.5 | consumed tokens: 188416000.0 | grad norm avg: 0.8 | grad norm last: 0.87 | 
2025-12-30T00:55:23 | step: 23100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.98418667120859e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.7 | consumed tokens: 189235200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T00:55:43 | step: 23200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.984010956832208e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.77 | consumed tokens: 190054400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:56:04 | step: 23300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.983834151062183e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.53 | consumed tokens: 190873600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T00:56:24 | step: 23400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.983656253898516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.64 | consumed tokens: 191692800.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T00:56:45 | step: 23500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.983477629139088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.17 | consumed tokens: 192512000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:57:06 | step: 23600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.983297912986018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 3.52 | consumed tokens: 193331200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T00:57:27 | step: 23700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.983117469237186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.84 | consumed tokens: 194150400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T00:57:47 | step: 23800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.982935934094712e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.41 | consumed tokens: 194969600.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T00:58:08 | step: 23900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.982753307558596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.46 | train loss last: 3.45 | consumed tokens: 195788800.0 | grad norm avg: 0.8 | grad norm last: 1.1 | 
2025-12-30T00:58:29 | step: 24000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.982569589628838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.69 | consumed tokens: 196608000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T00:58:49 | step: 24100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9823851441033185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.12 | consumed tokens: 197427200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:59:10 | step: 24200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.982199607184157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.33 | consumed tokens: 198246400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T00:59:30 | step: 24300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.982013342669234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.19 | consumed tokens: 199065600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T00:59:50 | step: 24400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.9818259867606685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 199884800.0 | grad norm avg: 0.8 | grad norm last: 0.73 | 
2025-12-30T01:00:11 | step: 24500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.981637539458461e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.48 | train loss last: 3.36 | consumed tokens: 200704000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:00:31 | step: 24600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9814480007626116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.98 | consumed tokens: 201523200.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:00:52 | step: 24700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.981257734471001e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.48 | consumed tokens: 202342400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:01:13 | step: 24800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.981066376785748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.08 | consumed tokens: 203161600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:01:33 | step: 24900 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.980874291504733e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.45 | train loss last: 3.27 | consumed tokens: 203980800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:01:54 | step: 25000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.980681114830077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.67 | consumed tokens: 204800000.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:02:16 | step: 25100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.980486846761778e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.53 | consumed tokens: 205619200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:02:37 | step: 25200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.980291851097718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 206438400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:02:58 | step: 25300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9800957640400156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.89 | consumed tokens: 207257600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:03:18 | step: 25400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.979898585588671e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.41 | consumed tokens: 208076800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:03:39 | step: 25500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9797006795415655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.47 | consumed tokens: 208896000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:03:59 | step: 25600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.979501318302937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.81 | consumed tokens: 209715200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:04:20 | step: 25700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.9793015932664275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.98 | consumed tokens: 210534400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:04:41 | step: 25800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9791004130383953e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.47 | train loss last: 4.25 | consumed tokens: 211353600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:05:01 | step: 25900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.978898505214602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 212172800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:05:22 | step: 26000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.978695869795047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.52 | consumed tokens: 212992000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T01:05:42 | step: 26100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.978491779183969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.77 | consumed tokens: 213811200.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T01:06:03 | step: 26200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.9782869609771296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.28 | consumed tokens: 214630400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:06:24 | step: 26300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.978081415174529e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.59 | consumed tokens: 215449600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:06:44 | step: 26400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9778744141804054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.59 | consumed tokens: 216268800.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:07:05 | step: 26500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9776666855905205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.75 | consumed tokens: 217088000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:07:26 | step: 26600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.977458229404874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 3.02 | consumed tokens: 217907200.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:07:46 | step: 26700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.977248681825586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.5 | consumed tokens: 218726400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:08:07 | step: 26800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.977038042852655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.47 | consumed tokens: 219545600.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:08:27 | step: 26900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.976826312486082e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.97 | consumed tokens: 220364800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:08:48 | step: 27000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.976613854523748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.27 | consumed tokens: 221184000.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T01:09:09 | step: 27100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.976400305167772e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.41 | consumed tokens: 222003200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:09:30 | step: 27200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9761856644181535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.28 | consumed tokens: 222822400.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T01:09:50 | step: 27300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9759702960727736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.45 | consumed tokens: 223641600.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:10:11 | step: 27400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.975753836333752e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.55 | consumed tokens: 224460800.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:10:31 | step: 27500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.975536648998968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.55 | consumed tokens: 225280000.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T01:10:52 | step: 27600 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.975318370270543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 2.78 | consumed tokens: 226099200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:11:13 | step: 27700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.975099000148475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.47 | consumed tokens: 226918400.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:11:33 | step: 27800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9748785386327654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.56 | consumed tokens: 227737600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:11:54 | step: 27900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.974657349521294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.47 | consumed tokens: 228556800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:12:15 | step: 28000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.974435069016181e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.48 | consumed tokens: 229376000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:12:35 | step: 28100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.974212060915306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.05 | consumed tokens: 230195200.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:12:56 | step: 28200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9739879614207894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.08 | consumed tokens: 231014400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:13:17 | step: 28300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9737627705326304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 231833600.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:13:37 | step: 28400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.97353685204871e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.81 | consumed tokens: 232652800.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:13:58 | step: 28500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9733098421711475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.22 | consumed tokens: 233472000.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T01:14:18 | step: 28600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.973081740899943e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.8 | consumed tokens: 234291200.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T01:14:39 | step: 28700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.972852912032977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.94 | consumed tokens: 235110400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:15:00 | step: 28800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9726229917723686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.73 | consumed tokens: 235929600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:15:20 | step: 28900 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.972391980118118e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.2 | consumed tokens: 236748800.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:15:41 | step: 29000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9721602408681065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.31 | consumed tokens: 237568000.0 | grad norm avg: 0.79 | grad norm last: 0.83 | 
2025-12-30T01:16:02 | step: 29100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9719274102244526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.33 | consumed tokens: 238387200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:16:22 | step: 29200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9716934881871566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 2.77 | consumed tokens: 239206400.0 | grad norm avg: 0.8 | grad norm last: 0.87 | 
2025-12-30T01:16:43 | step: 29300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.971458838554099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.11 | consumed tokens: 240025600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:17:03 | step: 29400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9712230975273997e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.44 | train loss last: 4.62 | consumed tokens: 240844800.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:17:24 | step: 29500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.970986628904939e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 241664000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:17:44 | step: 29600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.970748705090955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.09 | consumed tokens: 242483200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:18:05 | step: 29700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9705104174790904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.41 | consumed tokens: 243302400.0 | grad norm avg: 0.8 | grad norm last: 0.73 | 
2025-12-30T01:18:26 | step: 29800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.970270674675703e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.42 | consumed tokens: 244121600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T01:18:46 | step: 29900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.970030204276554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.2 | consumed tokens: 244940800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:19:07 | step: 30000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9697886424837634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.98 | consumed tokens: 245760000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:19:29 | step: 30100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.969546353095211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 2.97 | consumed tokens: 246579200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:19:50 | step: 30200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9693029723130167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.67 | consumed tokens: 247398400.0 | grad norm avg: 0.8 | grad norm last: 0.88 | 
2025-12-30T01:20:10 | step: 30300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.96905850013718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.52 | consumed tokens: 248217600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T01:20:31 | step: 30400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.968813300365582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.94 | consumed tokens: 249036800.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:20:52 | step: 30500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.968567009200342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.42 | train loss last: 3.27 | consumed tokens: 249856000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:21:12 | step: 30600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.96831962664146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.86 | consumed tokens: 250675200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:21:33 | step: 30700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.968071516486816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.61 | consumed tokens: 251494400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:21:53 | step: 30800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.96782231493853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.12 | consumed tokens: 252313600.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:22:14 | step: 30900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9675720219966024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 253132800.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:22:35 | step: 31000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.967321001458913e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.31 | consumed tokens: 253952000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:22:55 | step: 31100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9670688895275816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.58 | consumed tokens: 254771200.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:23:16 | step: 31200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.966816050000489e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.41 | consumed tokens: 255590400.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:23:36 | step: 31300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.966561755281873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.06 | consumed tokens: 256409600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:23:57 | step: 31400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9663070967653766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.38 | consumed tokens: 257228800.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:24:18 | step: 31500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.9660509830573574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.66 | consumed tokens: 258048000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:24:38 | step: 31600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.965794141753577e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.89 | consumed tokens: 258867200.0 | grad norm avg: 0.79 | grad norm last: 0.82 | 
2025-12-30T01:24:59 | step: 31700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.965536209056154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.06 | consumed tokens: 259686400.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:25:19 | step: 31800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.96527754876297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.69 | consumed tokens: 260505600.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:25:40 | step: 31900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.965017797076143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.41 | consumed tokens: 261324800.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:26:01 | step: 32000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.964756953995675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.7 | consumed tokens: 262144000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:26:21 | step: 32100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.964495383319445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.78 | consumed tokens: 262963200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:26:42 | step: 32200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.964232721249573e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.25 | consumed tokens: 263782400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:27:02 | step: 32300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9639693315839395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 264601600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:27:23 | step: 32400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.963704486726783e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.83 | consumed tokens: 265420800.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:27:43 | step: 32500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9634389142738655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.69 | consumed tokens: 266240000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:28:04 | step: 32600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9631726142251864e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.27 | consumed tokens: 267059200.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2025-12-30T01:28:25 | step: 32700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.962905222782865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.97 | consumed tokens: 267878400.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T01:28:46 | step: 32800 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 4.962636739946902e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.31 | consumed tokens: 268697600.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2025-12-30T01:29:06 | step: 32900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.962367529515177e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.3 | consumed tokens: 269516800.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:29:27 | step: 33000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9620968638919294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.03 | consumed tokens: 270336000.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:29:47 | step: 33100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.961825834470801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.19 | consumed tokens: 271155200.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:30:08 | step: 33200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.96155334985815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.7 | consumed tokens: 271974400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:30:29 | step: 33300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.961280137649737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.43 | train loss last: 3.38 | consumed tokens: 272793600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:30:49 | step: 33400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.961006197845563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.16 | consumed tokens: 273612800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:31:10 | step: 33500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.960731166647747e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.75 | consumed tokens: 274432000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:31:30 | step: 33600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.960455044056289e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.09 | consumed tokens: 275251200.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:31:51 | step: 33700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9601778300711885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.67 | consumed tokens: 276070400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T01:32:11 | step: 33800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.959899888490327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.53 | consumed tokens: 276889600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:32:32 | step: 33900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.959620855515823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.56 | consumed tokens: 277708800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:32:53 | step: 34000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9593410949455574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.53 | consumed tokens: 278528000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:33:14 | step: 34100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.95906024298165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.42 | consumed tokens: 279347200.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2025-12-30T01:33:34 | step: 34200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9587782996241e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.11 | consumed tokens: 280166400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:33:55 | step: 34300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.958495628670789e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.47 | consumed tokens: 280985600.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:34:16 | step: 34400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.958211866323836e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.11 | consumed tokens: 281804800.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:34:36 | step: 34500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.957927012583241e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 4.75 | consumed tokens: 282624000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T01:34:57 | step: 34600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.957641431246884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 2.8 | consumed tokens: 283443200.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:35:17 | step: 34700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9573547585168853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.38 | consumed tokens: 284262400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:35:38 | step: 34800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.9570669943932444e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.58 | consumed tokens: 285081600.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T01:35:59 | step: 34900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.956778502673842e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 285900800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:36:19 | step: 35000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9564889195607975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.36 | consumed tokens: 286720000.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T01:36:41 | step: 35100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9561986088519916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.19 | consumed tokens: 287539200.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:37:02 | step: 35200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.9559072067495435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.41 | train loss last: 3.25 | consumed tokens: 288358400.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2025-12-30T01:37:23 | step: 35300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.9556147132534534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.5 | consumed tokens: 289177600.0 | grad norm avg: 0.8 | grad norm last: 0.88 | 
2025-12-30T01:37:44 | step: 35400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.955321492161602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.62 | consumed tokens: 289996800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T01:38:04 | step: 35500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.955027179676108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 290816000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:38:25 | step: 35600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.954731775796972e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.56 | consumed tokens: 291635200.0 | grad norm avg: 0.8 | grad norm last: 0.7 | 
2025-12-30T01:38:46 | step: 35700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.954435644322075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.2 | consumed tokens: 292454400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:39:06 | step: 35800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9541384214535356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.69 | consumed tokens: 293273600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:39:27 | step: 35900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.953840470989235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.55 | consumed tokens: 294092800.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T01:39:47 | step: 36000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.953541429131292e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.94 | consumed tokens: 294912000.0 | grad norm avg: 0.79 | grad norm last: 0.7 | 
2025-12-30T01:40:08 | step: 36100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.953241295879707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.03 | consumed tokens: 295731200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:40:29 | step: 36200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.95294043503236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 296550400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:40:49 | step: 36300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9526384827913716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.0 | consumed tokens: 297369600.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T01:41:10 | step: 36400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.952335439156741e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.94 | consumed tokens: 298188800.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T01:41:31 | step: 36500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.952031667926349e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.94 | consumed tokens: 299008000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T01:41:51 | step: 36600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9517268053023145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 299827200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:42:12 | step: 36700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.951420851284638e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.38 | consumed tokens: 300646400.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T01:42:33 | step: 36800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9511141696712e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.06 | consumed tokens: 301465600.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T01:42:53 | step: 36900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.95080639666412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.41 | consumed tokens: 302284800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:43:14 | step: 37000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.950497896061279e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.02 | consumed tokens: 303104000.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:43:35 | step: 37100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.9501883040647954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.0 | consumed tokens: 303923200.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:43:55 | step: 37200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.94987762067467e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.77 | consumed tokens: 304742400.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:44:16 | step: 37300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.949566209688783e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.69 | consumed tokens: 305561600.0 | grad norm avg: 0.79 | grad norm last: 0.82 | 
2025-12-30T01:44:36 | step: 37400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9492537073092535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.31 | consumed tokens: 306380800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:44:57 | step: 37500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.948940477333963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.42 | consumed tokens: 307200000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:45:18 | step: 37600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.94862615596503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.84 | consumed tokens: 308019200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:45:38 | step: 37700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9483107432024553e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 4.38 | consumed tokens: 308838400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:45:59 | step: 37800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.947994602844119e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.92 | consumed tokens: 309657600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:46:19 | step: 37900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.94767700729426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.31 | consumed tokens: 310476800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:46:40 | step: 38000 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.94735904794652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.88 | consumed tokens: 311296000.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:47:01 | step: 38100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.947039997205138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.91 | consumed tokens: 312115200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:47:21 | step: 38200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.946719855070114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 4.19 | consumed tokens: 312934400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:47:42 | step: 38300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.946398621541448e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.2 | consumed tokens: 313753600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:48:02 | step: 38400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.94607666041702e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.5 | consumed tokens: 314572800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:48:23 | step: 38500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9457536078989506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 3.22 | consumed tokens: 315392000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:48:44 | step: 38600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9454298277851194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.44 | consumed tokens: 316211200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:49:04 | step: 38700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.945104956277646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.42 | consumed tokens: 317030400.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T01:49:25 | step: 38800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.944778993376531e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.4 | train loss last: 2.84 | consumed tokens: 317849600.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:49:46 | step: 38900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.944452302879654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.61 | consumed tokens: 318668800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:50:06 | step: 39000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.944124520989135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.69 | consumed tokens: 319488000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T01:50:27 | step: 39100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.9437960115028545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.47 | consumed tokens: 320307200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:50:48 | step: 39200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.943466410622932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.45 | consumed tokens: 321126400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:51:08 | step: 39300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.9431357183493674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.78 | consumed tokens: 321945600.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:51:29 | step: 39400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.942804298480041e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 322764800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:51:50 | step: 39500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.942471787217073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 323584000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T01:52:10 | step: 39600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.942138184560463e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.83 | consumed tokens: 324403200.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T01:52:31 | step: 39700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.941803854308091e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.7 | consumed tokens: 325222400.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2025-12-30T01:52:52 | step: 39800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.941468432662077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.22 | consumed tokens: 326041600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:53:12 | step: 39900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.941132283420302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 326860800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T01:53:33 | step: 40000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.9407950427848846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.64 | consumed tokens: 327680000.0 | grad norm avg: 0.79 | grad norm last: 0.85 | 
2025-12-30T01:53:55 | step: 40100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.940456710755825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.64 | consumed tokens: 328499200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T01:54:16 | step: 40200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.940117651131004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 329318400.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2025-12-30T01:54:36 | step: 40300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.939777500112541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.19 | consumed tokens: 330137600.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T01:54:57 | step: 40400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9394366214983165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.75 | consumed tokens: 330956800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T01:55:18 | step: 40500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.939094287692569e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.88 | consumed tokens: 331776000.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T01:55:38 | step: 40600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.938751590088941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.42 | consumed tokens: 332595200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T01:55:59 | step: 40700 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.93840743729379e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.39 | train loss last: 3.28 | consumed tokens: 333414400.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T01:56:19 | step: 40800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.938062556902878e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.81 | consumed tokens: 334233600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:56:40 | step: 40900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.937716948916204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.53 | consumed tokens: 335052800.0 | grad norm avg: 0.79 | grad norm last: 0.81 | 
2025-12-30T01:57:00 | step: 41000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9373702495358884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.06 | consumed tokens: 335872000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:57:21 | step: 41100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9370224587619305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 336691200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T01:57:42 | step: 41200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9366735765943304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.5 | consumed tokens: 337510400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T01:58:02 | step: 41300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.936323966830969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.98 | consumed tokens: 338329600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T01:58:23 | step: 41400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.935973629471846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.11 | consumed tokens: 339148800.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T01:58:43 | step: 41500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.935622200719081e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.45 | consumed tokens: 339968000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:59:04 | step: 41600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.935269680572674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 340787200.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2025-12-30T01:59:25 | step: 41700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9349160690326244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.58 | consumed tokens: 341606400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T01:59:45 | step: 41800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9345617298968136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.47 | consumed tokens: 342425600.0 | grad norm avg: 0.79 | grad norm last: 0.73 | 
2025-12-30T02:00:06 | step: 41900 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.9342066631652415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.52 | consumed tokens: 343244800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:00:27 | step: 42000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.9338501412421465e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 344064000.0 | grad norm avg: 0.8 | grad norm last: 0.9 | 
2025-12-30T02:00:48 | step: 42100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.93349289172329e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.66 | consumed tokens: 344883200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T02:01:08 | step: 42200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.933134914608672e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.48 | consumed tokens: 345702400.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T02:01:29 | step: 42300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.932775846100412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.17 | consumed tokens: 346521600.0 | grad norm avg: 0.8 | grad norm last: 0.88 | 
2025-12-30T02:01:49 | step: 42400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.93241568619851e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.48 | consumed tokens: 347340800.0 | grad norm avg: 0.8 | grad norm last: 0.88 | 
2025-12-30T02:02:10 | step: 42500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.932054798700847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.81 | consumed tokens: 348160000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:02:31 | step: 42600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.931692819809541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.11 | consumed tokens: 348979200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:02:51 | step: 42700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9313297495245934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.8 | consumed tokens: 349798400.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:03:12 | step: 42800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.930965951643884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 350617600.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:03:32 | step: 42900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.930601062369533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.19 | consumed tokens: 351436800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:03:53 | step: 43000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.93023544549942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.02 | consumed tokens: 352256000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:04:14 | step: 43100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.929868737235665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.52 | consumed tokens: 353075200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:04:34 | step: 43200 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.929501301376149e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 353894400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T02:04:55 | step: 43300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.92913241032511e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.33 | consumed tokens: 354713600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:05:16 | step: 43400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.92876315547619e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.38 | consumed tokens: 355532800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T02:05:37 | step: 43500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9283924454357475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 2.75 | consumed tokens: 356352000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T02:05:57 | step: 43600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9280210077995434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.94 | consumed tokens: 357171200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:06:18 | step: 43700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.927648842567578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.0 | consumed tokens: 357990400.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:06:38 | step: 43800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9272755859419703e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.25 | consumed tokens: 358809600.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T02:06:59 | step: 43900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.9269012379227206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.53 | consumed tokens: 359628800.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2025-12-30T02:07:20 | step: 44000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.926525798509829e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.83 | consumed tokens: 360448000.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:07:40 | step: 44100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9261496315011755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.23 | consumed tokens: 361267200.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T02:08:01 | step: 44200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.925772736896761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 362086400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:08:21 | step: 44300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.925394750898704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.84 | consumed tokens: 362905600.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:08:42 | step: 44400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.925015673507005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.44 | consumed tokens: 363724800.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:09:03 | step: 44500 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.9246358685195446e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 2.72 | consumed tokens: 364544000.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2025-12-30T02:09:23 | step: 44600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.924254972138442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.25 | consumed tokens: 365363200.0 | grad norm avg: 0.79 | grad norm last: 0.79 | 
2025-12-30T02:09:44 | step: 44700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9238729843636975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.64 | consumed tokens: 366182400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:10:05 | step: 44800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9234902689931914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.77 | consumed tokens: 367001600.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T02:10:25 | step: 44900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.923106462229043e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.89 | consumed tokens: 367820800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T02:10:46 | step: 45000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9227219278691337e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 368640000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T02:11:08 | step: 45100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.922336302115582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.72 | consumed tokens: 369459200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:11:29 | step: 45200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.921949584968388e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 370278400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:11:49 | step: 45300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.921562140225433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.55 | consumed tokens: 371097600.0 | grad norm avg: 0.8 | grad norm last: 0.91 | 
2025-12-30T02:12:10 | step: 45400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9211736040888354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.91 | consumed tokens: 371916800.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:12:31 | step: 45500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9207843403564766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 372736000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:12:51 | step: 45600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.920393985230476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.5 | consumed tokens: 373555200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T02:13:12 | step: 45700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9200025387108326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 374374400.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T02:13:33 | step: 45800 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.919610364595428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.2 | consumed tokens: 375193600.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2025-12-30T02:13:53 | step: 45900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.919217462884262e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.41 | consumed tokens: 376012800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:14:14 | step: 46000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9188231059815735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.95 | consumed tokens: 376832000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T02:14:34 | step: 46100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.918428021483123e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.36 | consumed tokens: 377651200.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T02:14:55 | step: 46200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.918032209388912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.42 | consumed tokens: 378470400.0 | grad norm avg: 0.79 | grad norm last: 0.79 | 
2025-12-30T02:15:16 | step: 46300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.917635305901058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.22 | consumed tokens: 379289600.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:15:36 | step: 46400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.917237311019562e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.25 | consumed tokens: 380108800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:15:57 | step: 46500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.916838588542305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 380928000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:16:17 | step: 46600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.9164387746714056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.47 | consumed tokens: 381747200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:16:38 | step: 46700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.916038233204745e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.59 | consumed tokens: 382566400.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:16:59 | step: 46800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.915636236546561e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.77 | consumed tokens: 383385600.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T02:17:19 | step: 46900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.915233876090497e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.8 | consumed tokens: 384204800.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2025-12-30T02:17:40 | step: 47000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.91483042424079e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.14 | consumed tokens: 385024000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T02:18:01 | step: 47100 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.914425880997442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.08 | consumed tokens: 385843200.0 | grad norm avg: 0.79 | grad norm last: 0.85 | 
2025-12-30T02:18:21 | step: 47200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.914020246360451e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 386662400.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2025-12-30T02:18:42 | step: 47300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.913613884127699e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.37 | train loss last: 3.44 | consumed tokens: 387481600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:19:03 | step: 47400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.913206794299185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.34 | consumed tokens: 388300800.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2025-12-30T02:19:23 | step: 47500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9127986130770296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 389120000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:19:44 | step: 47600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.912389340461232e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 2.98 | consumed tokens: 389939200.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:20:05 | step: 47700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9119793402496725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.41 | consumed tokens: 390758400.0 | grad norm avg: 0.79 | grad norm last: 0.74 | 
2025-12-30T02:20:25 | step: 47800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.911568248644471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.14 | consumed tokens: 391577600.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:20:46 | step: 47900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.911156065645628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.11 | consumed tokens: 392396800.0 | grad norm avg: 0.8 | grad norm last: 0.87 | 
2025-12-30T02:21:06 | step: 48000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.910743155051023e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.97 | consumed tokens: 393216000.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:21:27 | step: 48100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.910329153062776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.22 | consumed tokens: 394035200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:21:47 | step: 48200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.909914423478767e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.44 | consumed tokens: 394854400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:22:08 | step: 48300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.909498602501117e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.59 | consumed tokens: 395673600.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T02:22:29 | step: 48400 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.909082053927705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.17 | consumed tokens: 396492800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:22:50 | step: 48500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.9086644139606506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.03 | consumed tokens: 397312000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:23:10 | step: 48600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.908245682599954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 4.0 | consumed tokens: 398131200.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:23:31 | step: 48700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9078262236434966e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.64 | consumed tokens: 398950400.0 | grad norm avg: 0.8 | grad norm last: 0.89 | 
2025-12-30T02:23:52 | step: 48800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9074060370912775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.78 | consumed tokens: 399769600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:24:12 | step: 48900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9069843953475356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 4.34 | consumed tokens: 400588800.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:24:33 | step: 49000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.906562026008032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.41 | consumed tokens: 401408000.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T02:24:53 | step: 49100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9061389290727675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.34 | consumed tokens: 402227200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:25:14 | step: 49200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.9057147407438606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.45 | consumed tokens: 403046400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:25:35 | step: 49300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9052894610213116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.38 | train loss last: 3.12 | consumed tokens: 403865600.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T02:25:55 | step: 49400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.904863453703001e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.38 | consumed tokens: 404684800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:26:16 | step: 49500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.9044363549910486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.0 | consumed tokens: 405504000.0 | grad norm avg: 0.79 | grad norm last: 0.74 | 
2025-12-30T02:26:36 | step: 49600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.9040085286833346e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.28 | consumed tokens: 406323200.0 | grad norm avg: 0.79 | grad norm last: 0.84 | 
2025-12-30T02:26:57 | step: 49700 | train samples/s: 81.8 | train mfu (16-bit): -1.0 | lr mean: 4.9035796109819785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.3 | consumed tokens: 407142400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:27:18 | step: 49800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.90314960188698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.09 | consumed tokens: 407961600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:27:39 | step: 49900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.9027188651962206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.09 | consumed tokens: 408780800.0 | grad norm avg: 0.79 | grad norm last: 0.83 | 
2025-12-30T02:27:59 | step: 50000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.9022874009096995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.09 | consumed tokens: 409600000.0 | grad norm avg: 0.79 | grad norm last: 0.81 | 
2025-12-30T02:28:21 | step: 50100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.9018544814316556e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 410419200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:28:42 | step: 50200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.90142083435785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.31 | consumed tokens: 411238400.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:29:03 | step: 50300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.9009864596882835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.11 | consumed tokens: 412057600.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2025-12-30T02:29:23 | step: 50400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.9005509936250746e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.97 | consumed tokens: 412876800.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:29:44 | step: 50500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.900114799966104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.86 | consumed tokens: 413696000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:30:04 | step: 50600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.899677151115611e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.34 | consumed tokens: 414515200.0 | grad norm avg: 0.79 | grad norm last: 0.82 | 
2025-12-30T02:30:25 | step: 50700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.8992391384672374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.86 | consumed tokens: 415334400.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:30:46 | step: 50800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.898799670627341e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.23 | consumed tokens: 416153600.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T02:31:06 | step: 50900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.898359838989563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.09 | consumed tokens: 416972800.0 | grad norm avg: 0.79 | grad norm last: 0.81 | 
2025-12-30T02:31:27 | step: 51000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.897918552160263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.38 | consumed tokens: 417792000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:31:48 | step: 51100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.8974765377352014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 418611200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:32:08 | step: 51200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.8970337957143784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.17 | consumed tokens: 419430400.0 | grad norm avg: 0.79 | grad norm last: 0.83 | 
2025-12-30T02:32:29 | step: 51300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.896589962299913e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.16 | consumed tokens: 420249600.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T02:32:49 | step: 51400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.896145037491806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.41 | consumed tokens: 421068800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:33:10 | step: 51500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.895699385087937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.52 | consumed tokens: 421888000.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:33:30 | step: 51600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.895252641290426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.94 | consumed tokens: 422707200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:33:51 | step: 51700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.894805169897154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.7 | consumed tokens: 423526400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T02:34:12 | step: 51800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.8943566071102396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.31 | consumed tokens: 424345600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:34:32 | step: 51900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.893906952929683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.73 | consumed tokens: 425164800.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T02:34:52 | step: 52000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.893456571153365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.84 | consumed tokens: 425984000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:35:13 | step: 52100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.893005097983405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 426803200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:35:34 | step: 52200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.8925528972176835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.09 | consumed tokens: 427622400.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T02:35:55 | step: 52300 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.89209960505832e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.11 | consumed tokens: 428441600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:36:15 | step: 52400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.891645585303195e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.17 | consumed tokens: 429260800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T02:36:36 | step: 52500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.8911904741544276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.31 | consumed tokens: 430080000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:36:56 | step: 52600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.890734635409899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.2 | consumed tokens: 430899200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:37:17 | step: 52700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.890277705271728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.14 | consumed tokens: 431718400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:37:37 | step: 52800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.8898196837399155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.78 | consumed tokens: 432537600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:37:58 | step: 52900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.889360934612341e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.91 | consumed tokens: 433356800.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:38:19 | step: 53000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.888901094091125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.44 | consumed tokens: 434176000.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:38:39 | step: 53100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.888440525974147e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.67 | consumed tokens: 434995200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:39:00 | step: 53200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.887978866463527e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 2.89 | consumed tokens: 435814400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T02:39:20 | step: 53300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.887516479357146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.23 | consumed tokens: 436633600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:39:41 | step: 53400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.887053000857122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.36 | train loss last: 3.42 | consumed tokens: 437452800.0 | grad norm avg: 0.79 | grad norm last: 0.81 | 
2025-12-30T02:40:02 | step: 53500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8865884309634566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.31 | consumed tokens: 438272000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:40:22 | step: 53600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8861231334740296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 4.06 | consumed tokens: 439091200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T02:40:43 | step: 53700 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.885657108388841e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.56 | consumed tokens: 439910400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T02:41:04 | step: 53800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.88518962811213e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.17 | consumed tokens: 440729600.0 | grad norm avg: 0.8 | grad norm last: 0.87 | 
2025-12-30T02:41:25 | step: 53900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.884721784037538e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.56 | consumed tokens: 441548800.0 | grad norm avg: 0.8 | grad norm last: 0.89 | 
2025-12-30T02:41:45 | step: 54000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.884252484771423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.36 | consumed tokens: 442368000.0 | grad norm avg: 0.8 | grad norm last: 0.87 | 
2025-12-30T02:42:06 | step: 54100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.8837828217074275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.17 | consumed tokens: 443187200.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T02:42:26 | step: 54200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.883311703451909e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.42 | consumed tokens: 444006400.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:42:47 | step: 54300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.882839857600629e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.52 | consumed tokens: 444825600.0 | grad norm avg: 0.8 | grad norm last: 0.9 | 
2025-12-30T02:43:07 | step: 54400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.882367284153588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.03 | consumed tokens: 445644800.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T02:43:28 | step: 54500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.881893619312905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.45 | consumed tokens: 446464000.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T02:43:49 | step: 54600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.881418863078579e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.67 | consumed tokens: 447283200.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2025-12-30T02:44:09 | step: 54700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.8809433792484924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.56 | consumed tokens: 448102400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:44:30 | step: 54800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.8804668040247634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.89 | consumed tokens: 448921600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:44:51 | step: 54900 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.879989501205273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.66 | consumed tokens: 449740800.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:45:11 | step: 55000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.8795111069921404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.5 | consumed tokens: 450560000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:45:34 | step: 55100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.8790319851832464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.8 | consumed tokens: 451379200.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:45:54 | step: 55200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.87855177198071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.97 | consumed tokens: 452198400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:46:15 | step: 55300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.878070467384532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.84 | consumed tokens: 453017600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:46:36 | step: 55400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.8775884351925924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.66 | consumed tokens: 453836800.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T02:46:56 | step: 55500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8771056754048914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.0 | consumed tokens: 454656000.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T02:47:17 | step: 55600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.876621824223548e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.22 | consumed tokens: 455475200.0 | grad norm avg: 0.8 | grad norm last: 0.87 | 
2025-12-30T02:47:38 | step: 55700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.876136881648563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.73 | consumed tokens: 456294400.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T02:47:58 | step: 55800 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.875651211477816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.95 | consumed tokens: 457113600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T02:48:19 | step: 55900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.875164449913427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.45 | consumed tokens: 457932800.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:48:39 | step: 56000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.874676960753277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.31 | consumed tokens: 458752000.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T02:49:00 | step: 56100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8741883801994845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.38 | consumed tokens: 459571200.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T02:49:21 | step: 56200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.87369870825205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.39 | consumed tokens: 460390400.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:49:42 | step: 56300 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 4.873208672506735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.95 | consumed tokens: 461209600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:50:02 | step: 56400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8727171815698966e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.42 | consumed tokens: 462028800.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:50:23 | step: 56500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.872224963037297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.35 | train loss last: 2.91 | consumed tokens: 462848000.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2025-12-30T02:50:43 | step: 56600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.8717316531110555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.34 | consumed tokens: 463667200.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T02:51:04 | step: 56700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.8712376155890524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.0 | consumed tokens: 464486400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:51:25 | step: 56800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.870742850471288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.14 | consumed tokens: 465305600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T02:51:45 | step: 56900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8702469939598814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 4.06 | consumed tokens: 466124800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T02:52:06 | step: 57000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.8697500460548326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.73 | consumed tokens: 466944000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T02:52:27 | step: 57100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8692523705540225e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.52 | consumed tokens: 467763200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:52:47 | step: 57200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.86875360365957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 2.83 | consumed tokens: 468582400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:53:08 | step: 57300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.868253745371476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.5 | consumed tokens: 469401600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T02:53:29 | step: 57400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.867753523285501e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.39 | consumed tokens: 470220800.0 | grad norm avg: 0.8 | grad norm last: 0.75 | 
2025-12-30T02:53:49 | step: 57500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.867251846008003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.17 | consumed tokens: 471040000.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T02:54:10 | step: 57600 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 4.8667494411347434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.98 | consumed tokens: 471859200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T02:54:31 | step: 57700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.8662463086657226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.64 | consumed tokens: 472678400.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T02:54:52 | step: 57800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.86574208480306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.73 | consumed tokens: 473497600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T02:55:12 | step: 57900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.8652367695467547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.09 | consumed tokens: 474316800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T02:55:33 | step: 58000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.864730726694688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.34 | consumed tokens: 475136000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:55:54 | step: 58100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8642235924489796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 475955200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T02:56:14 | step: 58200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8637157306075096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.38 | consumed tokens: 476774400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:56:35 | step: 58300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.8632067773723975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 4.09 | consumed tokens: 477593600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T02:56:56 | step: 58400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.862697096541524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.97 | consumed tokens: 478412800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T02:57:16 | step: 58500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.862186324317008e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.19 | consumed tokens: 479232000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T02:57:37 | step: 58600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.861674824496731e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.77 | consumed tokens: 480051200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T02:57:57 | step: 58700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.861162233282812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.08 | consumed tokens: 480870400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T02:58:18 | step: 58800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.860648914473131e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.11 | consumed tokens: 481689600.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T02:58:39 | step: 58900 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.8601345042698085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.92 | consumed tokens: 482508800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T02:59:00 | step: 59000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.8596193664707243e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.86 | consumed tokens: 483328000.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T02:59:20 | step: 59100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.859103137277998e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.98 | consumed tokens: 484147200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T02:59:41 | step: 59200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.8585858166916296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.98 | consumed tokens: 484966400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T03:00:01 | step: 59300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.8580677685095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.78 | consumed tokens: 485785600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:00:22 | step: 59400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.8575489927316085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.44 | consumed tokens: 486604800.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2025-12-30T03:00:42 | step: 59500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.857029125560075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 4.5 | consumed tokens: 487424000.0 | grad norm avg: 0.81 | grad norm last: 0.73 | 
2025-12-30T03:01:03 | step: 59600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.8565081669948995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.16 | consumed tokens: 488243200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:01:24 | step: 59700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8559864808339626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.06 | consumed tokens: 489062400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:01:44 | step: 59800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8554637032793835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.3 | consumed tokens: 489881600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:02:05 | step: 59900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.854940198129043e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.16 | consumed tokens: 490700800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:02:26 | step: 60000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.8544156015850604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.97 | consumed tokens: 491520000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T03:02:48 | step: 60100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.853890277445316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 2.59 | consumed tokens: 492339200.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T03:03:09 | step: 60200 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 4.853364225709811e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.33 | train loss last: 3.72 | consumed tokens: 493158400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T03:03:30 | step: 60300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8528367187827826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.72 | consumed tokens: 493977600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:03:51 | step: 60400 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.8523088480578735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.19 | consumed tokens: 494796800.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T03:04:11 | step: 60500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.851779522141442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.11 | consumed tokens: 495616000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T03:04:32 | step: 60600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.8512494686292484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.11 | consumed tokens: 496435200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T03:04:52 | step: 60700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.850718687521294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.53 | consumed tokens: 497254400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:05:13 | step: 60800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.850186815019697e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.17 | consumed tokens: 498073600.0 | grad norm avg: 0.8 | grad norm last: 0.86 | 
2025-12-30T03:05:34 | step: 60900 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.849654214922339e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.55 | consumed tokens: 498892800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T03:05:55 | step: 61000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.8491205234313384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.8 | consumed tokens: 499712000.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:06:15 | step: 61100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.8485861043445766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.06 | consumed tokens: 500531200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:06:36 | step: 61200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.848050593864173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.86 | consumed tokens: 501350400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:06:57 | step: 61300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.847513991990127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 4.06 | consumed tokens: 502169600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T03:07:17 | step: 61400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.846976662520319e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 502988800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:07:38 | step: 61500 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.8464386054547504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.98 | consumed tokens: 503808000.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T03:07:59 | step: 61600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.8458994569955394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.55 | consumed tokens: 504627200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:08:19 | step: 61700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.845359217142686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.44 | consumed tokens: 505446400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:08:40 | step: 61800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.844818249694072e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.38 | consumed tokens: 506265600.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T03:09:01 | step: 61900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.844276554649696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.36 | consumed tokens: 507084800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:09:21 | step: 62000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.843733768211678e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.42 | consumed tokens: 507904000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:09:42 | step: 62100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.8431898903800175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.41 | consumed tokens: 508723200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:10:02 | step: 62200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.842645284952596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 5.28 | consumed tokens: 509542400.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T03:10:23 | step: 62300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.842099588131532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.27 | consumed tokens: 510361600.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T03:10:43 | step: 62400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.841553163714707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.23 | consumed tokens: 511180800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:11:04 | step: 62500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.84100601170212e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.09 | consumed tokens: 512000000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:11:25 | step: 62600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.8404577682958916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.22 | consumed tokens: 512819200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:11:45 | step: 62700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.839908433496021e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.45 | consumed tokens: 513638400.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T03:12:06 | step: 62800 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.8393583711003885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.28 | consumed tokens: 514457600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:12:27 | step: 62900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.838807217311114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.34 | consumed tokens: 515276800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:12:47 | step: 63000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.838255335926078e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.41 | consumed tokens: 516096000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2025-12-30T03:13:08 | step: 63100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.837702726945281e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.86 | consumed tokens: 516915200.0 | grad norm avg: 0.82 | grad norm last: 0.74 | 
2025-12-30T03:13:29 | step: 63200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.837148662772961e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.47 | consumed tokens: 517734400.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T03:13:49 | step: 63300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.83659423480276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.64 | consumed tokens: 518553600.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2025-12-30T03:14:10 | step: 63400 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.836038715438917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.62 | consumed tokens: 519372800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:14:31 | step: 63500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.835482104681432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.2 | consumed tokens: 520192000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:14:51 | step: 63600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.834924766328186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.53 | consumed tokens: 521011200.0 | grad norm avg: 0.8 | grad norm last: 1.14 | 
2025-12-30T03:15:12 | step: 63700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.834366336581297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.14 | consumed tokens: 521830400.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T03:15:33 | step: 63800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.833807179238647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.33 | consumed tokens: 522649600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2025-12-30T03:15:53 | step: 63900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.833246930502355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.0 | consumed tokens: 523468800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T03:16:14 | step: 64000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.8326859541703016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.25 | consumed tokens: 524288000.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:16:35 | step: 64100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.832123886444606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.11 | consumed tokens: 525107200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:16:55 | step: 64200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.831561091123149e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.06 | consumed tokens: 525926400.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2025-12-30T03:17:16 | step: 64300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.83099756820593e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.53 | consumed tokens: 526745600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:17:37 | step: 64400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.830432590097189e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.0 | consumed tokens: 527564800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T03:17:57 | step: 64500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.829867248190567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.09 | consumed tokens: 528384000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T03:18:18 | step: 64600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.829300814890303e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.2 | consumed tokens: 529203200.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T03:18:38 | step: 64700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8287332901963964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.34 | train loss last: 3.19 | consumed tokens: 530022400.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T03:18:59 | step: 64800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.828165037906729e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.11 | consumed tokens: 530841600.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T03:19:20 | step: 64900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.827595694223419e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.12 | consumed tokens: 531660800.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:19:40 | step: 65000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8270256229443476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.55 | consumed tokens: 532480000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:20:03 | step: 65100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.826454460271634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.73 | consumed tokens: 533299200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:20:23 | step: 65200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.8258825700031593e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.33 | consumed tokens: 534118400.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T03:20:44 | step: 65300 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.825309952138923e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.36 | consumed tokens: 534937600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:21:05 | step: 65400 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.824735879083164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.77 | consumed tokens: 535756800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:21:26 | step: 65500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.824161442229524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.56 | consumed tokens: 536576000.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T03:21:46 | step: 65600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.8235859139822423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.05 | consumed tokens: 537395200.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:22:07 | step: 65700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.823009294341318e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.38 | consumed tokens: 538214400.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:22:28 | step: 65800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.822431947104633e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.67 | consumed tokens: 539033600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:22:48 | step: 65900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.821853872272186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.89 | consumed tokens: 539852800.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:23:09 | step: 66000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.821274706046097e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.34 | consumed tokens: 540672000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T03:23:30 | step: 66100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.820694448426366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.23 | consumed tokens: 541491200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:23:50 | step: 66200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.820113463210873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.08 | consumed tokens: 542310400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:24:11 | step: 66300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.8195313866017386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.66 | consumed tokens: 543129600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:24:32 | step: 66400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.8189485823968425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.97 | consumed tokens: 543948800.0 | grad norm avg: 0.8 | grad norm last: 0.84 | 
2025-12-30T03:24:52 | step: 66500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.818365050596185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.45 | consumed tokens: 544768000.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2025-12-30T03:25:13 | step: 66600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.8177804274018854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.64 | consumed tokens: 545587200.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T03:25:34 | step: 66700 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.817195076611824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.61 | consumed tokens: 546406400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T03:25:55 | step: 66800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.816608634428121e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.27 | consumed tokens: 547225600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:26:16 | step: 66900 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.816021100850776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.48 | consumed tokens: 548044800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:26:36 | step: 67000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.815432839677669e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.08 | consumed tokens: 548864000.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T03:26:57 | step: 67100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.814843850908801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.03 | consumed tokens: 549683200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:27:18 | step: 67200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.814253770746291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.81 | consumed tokens: 550502400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T03:27:38 | step: 67300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.813662962988019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.0 | consumed tokens: 551321600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:27:59 | step: 67400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.813071063836105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.97 | consumed tokens: 552140800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T03:28:20 | step: 67500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.81247843708843e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.27 | consumed tokens: 552960000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:28:40 | step: 67600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.8118847189471126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.03 | consumed tokens: 553779200.0 | grad norm avg: 0.8 | grad norm last: 0.9 | 
2025-12-30T03:29:01 | step: 67700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.811289909412153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.95 | consumed tokens: 554598400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:29:22 | step: 67800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.810694736079313e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.56 | consumed tokens: 555417600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:29:42 | step: 67900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.8100984713528305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.5 | consumed tokens: 556236800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:30:03 | step: 68000 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.809501115232706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.5 | consumed tokens: 557056000.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T03:30:24 | step: 68100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.80890303151682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.25 | consumed tokens: 557875200.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2025-12-30T03:30:44 | step: 68200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.808303856407292e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.53 | consumed tokens: 558694400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:31:05 | step: 68300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.807703953702003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.77 | consumed tokens: 559513600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T03:31:26 | step: 68400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.807102959603071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.0 | consumed tokens: 560332800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:31:47 | step: 68500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.806501237908378e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.84 | consumed tokens: 561152000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:32:07 | step: 68600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.805898788617924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.12 | consumed tokens: 561971200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:32:28 | step: 68700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.8052952479338273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.27 | consumed tokens: 562790400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:32:49 | step: 68800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.804690615856089e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.11 | consumed tokens: 563609600.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:33:09 | step: 68900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.8040852561825886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.16 | consumed tokens: 564428800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:33:30 | step: 69000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.803479168913327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.72 | consumed tokens: 565248000.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T03:33:51 | step: 69100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.8028719902504236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.06 | consumed tokens: 566067200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T03:34:11 | step: 69200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.8022640839917585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.7 | consumed tokens: 566886400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T03:34:32 | step: 69300 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 4.8016550863394514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.7 | consumed tokens: 567705600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:34:53 | step: 69400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.801045361091383e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 2.98 | consumed tokens: 568524800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T03:35:14 | step: 69500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.800434544449672e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 569344000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T03:35:34 | step: 69600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.7998230002122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.33 | consumed tokens: 570163200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:35:55 | step: 69700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.799210364581086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.2 | consumed tokens: 570982400.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T03:36:15 | step: 69800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.79859700135421e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.59 | consumed tokens: 571801600.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:36:36 | step: 69900 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.797982546733692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.73 | consumed tokens: 572620800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:36:57 | step: 70000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.797367364517413e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.8 | consumed tokens: 573440000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:37:19 | step: 70100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.7967514547053725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.66 | consumed tokens: 574259200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T03:37:40 | step: 70200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.79613445349969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.08 | consumed tokens: 575078400.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:38:01 | step: 70300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.795516360900365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.11 | consumed tokens: 575897600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:38:21 | step: 70400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.7948975407052785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.98 | consumed tokens: 576716800.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T03:38:42 | step: 70500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.794277992914431e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.59 | consumed tokens: 577536000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:39:02 | step: 70600 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.793657353729941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.91 | consumed tokens: 578355200.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:39:23 | step: 70700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.79303598694969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.16 | consumed tokens: 579174400.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T03:39:44 | step: 70800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.792413528775796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 579993600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:40:05 | step: 70900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.7917903430061415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.92 | consumed tokens: 580812800.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:40:25 | step: 71000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.7911660658428445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.69 | consumed tokens: 581632000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:40:45 | step: 71100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.790541061083786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.19 | consumed tokens: 582451200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:41:06 | step: 71200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.789914964931086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.28 | consumed tokens: 583270400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:41:27 | step: 71300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.789288141182624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.36 | consumed tokens: 584089600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T03:41:47 | step: 71400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.7886605898384005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.3 | consumed tokens: 584908800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T03:42:08 | step: 71500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.788031947100535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.45 | consumed tokens: 585728000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:42:29 | step: 71600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.787402576766908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.88 | consumed tokens: 586547200.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T03:42:49 | step: 71700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.786772115039639e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.25 | consumed tokens: 587366400.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T03:43:10 | step: 71800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.786140561918728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.27 | consumed tokens: 588185600.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:43:31 | step: 71900 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.785508644999936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.98 | consumed tokens: 589004800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:43:52 | step: 72000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.784875636687502e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.41 | consumed tokens: 589824000.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T03:44:12 | step: 72100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.784241536981426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.5 | consumed tokens: 590643200.0 | grad norm avg: 0.81 | grad norm last: 0.73 | 
2025-12-30T03:44:33 | step: 72200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.783606709679589e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.05 | consumed tokens: 591462400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:44:54 | step: 72300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.782970790984109e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.78 | consumed tokens: 592281600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:45:14 | step: 72400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.782334144692868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.56 | consumed tokens: 593100800.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T03:45:35 | step: 72500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.7816967708058655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.23 | consumed tokens: 593920000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:45:56 | step: 72600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.781058305525221e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.45 | consumed tokens: 594739200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:46:16 | step: 72700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.780419112648815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.19 | consumed tokens: 595558400.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:46:37 | step: 72800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.779778828378767e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.39 | consumed tokens: 596377600.0 | grad norm avg: 0.8 | grad norm last: 0.89 | 
2025-12-30T03:46:58 | step: 72900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.779137816512957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.86 | consumed tokens: 597196800.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:47:19 | step: 73000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.7784957132535055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 598016000.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:47:39 | step: 73100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.7778528823982924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.11 | consumed tokens: 598835200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:48:00 | step: 73200 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.777208960149437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.56 | consumed tokens: 599654400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2025-12-30T03:48:21 | step: 73300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.776564674102701e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.59 | consumed tokens: 600473600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T03:48:41 | step: 73400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.7759189328644425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.66 | consumed tokens: 601292800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:49:02 | step: 73500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.775272464030422e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.22 | consumed tokens: 602112000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:49:23 | step: 73600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.7746252676006407e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.66 | consumed tokens: 602931200.0 | grad norm avg: 0.81 | grad norm last: 0.73 | 
2025-12-30T03:49:43 | step: 73700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.773976979777217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.27 | consumed tokens: 603750400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:50:04 | step: 73800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.773327964358032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.59 | consumed tokens: 604569600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:50:25 | step: 73900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.7726778575452045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 4.38 | consumed tokens: 605388800.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T03:50:45 | step: 74000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.772027023136616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.31 | consumed tokens: 606208000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:51:06 | step: 74100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.7713754611322656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.12 | consumed tokens: 607027200.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:51:27 | step: 74200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.7707228077342734e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.0 | consumed tokens: 607846400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:51:47 | step: 74300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.77006942674052e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.66 | consumed tokens: 608665600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T03:52:08 | step: 74400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.769414954353124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.47 | consumed tokens: 609484800.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T03:52:29 | step: 74500 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.768759754369967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.91 | consumed tokens: 610304000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T03:52:50 | step: 74600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.7681034629931673e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.11 | consumed tokens: 611123200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T03:53:10 | step: 74700 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.7674464440206066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.05 | consumed tokens: 611942400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T03:53:31 | step: 74800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.7667886974522844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 3.27 | consumed tokens: 612761600.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:53:52 | step: 74900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.76612985949032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.8 | consumed tokens: 613580800.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T03:54:12 | step: 75000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.7654699301347136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.06 | consumed tokens: 614400000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:54:34 | step: 75100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.7648096369812265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.12 | consumed tokens: 615219200.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2025-12-30T03:54:55 | step: 75200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.7641478886362165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.23 | consumed tokens: 616038400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:55:16 | step: 75300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.763485776493326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.08 | consumed tokens: 616857600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:55:37 | step: 75400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.762822572956793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.41 | consumed tokens: 617676800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T03:55:57 | step: 75500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.762158278026618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.0 | consumed tokens: 618496000.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:56:18 | step: 75600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.761493255500682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.64 | consumed tokens: 619315200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:56:39 | step: 75700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.760827505378984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 3.56 | consumed tokens: 620134400.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:57:00 | step: 75800 | train samples/s: 81.7 | train mfu (16-bit): -1.0 | lr mean: 4.760160663863644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.32 | train loss last: 4.41 | consumed tokens: 620953600.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T03:57:20 | step: 75900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.759493094752543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.94 | consumed tokens: 621772800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T03:57:41 | step: 76000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.758824434247799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.16 | consumed tokens: 622592000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T03:58:01 | step: 76100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.7581550461472943e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.03 | consumed tokens: 623411200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T03:58:22 | step: 76200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.757484930451028e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 4.16 | consumed tokens: 624230400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T03:58:43 | step: 76300 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.7568137233611196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.3 | consumed tokens: 625049600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:59:04 | step: 76400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.75614178867545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.78 | consumed tokens: 625868800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T03:59:24 | step: 76500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.755468762596138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.97 | consumed tokens: 626688000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T03:59:45 | step: 76600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.7547950089210644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.84 | consumed tokens: 627507200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T04:00:05 | step: 76700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.7541205276502296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.05 | consumed tokens: 628326400.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:00:26 | step: 76800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.753444954985753e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.28 | consumed tokens: 629145600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T04:00:47 | step: 76900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.7527682909276336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.3 | consumed tokens: 629964800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T04:01:07 | step: 77000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.752090899273753e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.05 | consumed tokens: 630784000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T04:01:29 | step: 77100 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 4.751412780024111e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.28 | consumed tokens: 631603200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:01:49 | step: 77200 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.750733933178708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.0 | consumed tokens: 632422400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T04:02:10 | step: 77300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.7500539949396625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.23 | consumed tokens: 633241600.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T04:02:31 | step: 77400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.749372965306975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.52 | consumed tokens: 634060800.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:02:51 | step: 77500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.748691208078526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.81 | consumed tokens: 634880000.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T04:03:12 | step: 77600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.7480087232543156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.33 | consumed tokens: 635699200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:03:33 | step: 77700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.747325147036463e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.31 | train loss last: 3.06 | consumed tokens: 636518400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:03:53 | step: 77800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.746640843222849e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.27 | consumed tokens: 637337600.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:04:14 | step: 77900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.745955811813474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.77 | consumed tokens: 638156800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T04:04:35 | step: 78000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.745269689010456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.59 | consumed tokens: 638976000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:04:56 | step: 78100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.744582838611677e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.53 | consumed tokens: 639795200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T04:05:16 | step: 78200 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.743894896819256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.7 | consumed tokens: 640614400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:05:37 | step: 78300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.743206227431074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.05 | consumed tokens: 641433600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T04:05:58 | step: 78400 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.74251683044713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.42 | consumed tokens: 642252800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:06:19 | step: 78500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.741826342069544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.23 | consumed tokens: 643072000.0 | grad norm avg: 0.82 | grad norm last: 0.75 | 
2025-12-30T04:06:40 | step: 78600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.741134762298316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.27 | consumed tokens: 643891200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2025-12-30T04:07:00 | step: 78700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.740442818729207e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.45 | consumed tokens: 644710400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T04:07:21 | step: 78800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.739749783766456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.69 | consumed tokens: 645529600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T04:07:42 | step: 78900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.739055657410063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.81 | consumed tokens: 646348800.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T04:08:02 | step: 79000 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.738360803457908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.7 | consumed tokens: 647168000.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:08:23 | step: 79100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.7376652219099924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.97 | consumed tokens: 647987200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:08:44 | step: 79200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.736968548968434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.59 | consumed tokens: 648806400.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:09:04 | step: 79300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.736271148431115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.83 | consumed tokens: 649625600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:09:25 | step: 79400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.735573020298034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.19 | consumed tokens: 650444800.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2025-12-30T04:09:46 | step: 79500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.734873800771311e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.44 | consumed tokens: 651264000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:10:06 | step: 79600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.734173489850946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.55 | consumed tokens: 652083200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T04:10:27 | step: 79700 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.7334728151327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.69 | consumed tokens: 652902400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T04:10:48 | step: 79800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.732771049020812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.64 | consumed tokens: 653721600.0 | grad norm avg: 0.81 | grad norm last: 0.74 | 
2025-12-30T04:11:09 | step: 79900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.732068191515282e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.91 | consumed tokens: 654540800.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T04:11:29 | step: 80000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.73136460641399e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.23 | consumed tokens: 655360000.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T04:11:52 | step: 80100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.730660293716937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.83 | consumed tokens: 656179200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T04:12:12 | step: 80200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.729954889626242e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.39 | consumed tokens: 656998400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:12:33 | step: 80300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.729248757939786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.39 | consumed tokens: 657817600.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:12:54 | step: 80400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.728541898657568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.98 | consumed tokens: 658636800.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T04:13:14 | step: 80500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.727833947981708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.41 | consumed tokens: 659456000.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T04:13:35 | step: 80600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.727125269710086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.19 | consumed tokens: 660275200.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T04:13:55 | step: 80700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.726415500044823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.56 | consumed tokens: 661094400.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T04:14:16 | step: 80800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.725705002783798e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 2.58 | consumed tokens: 661913600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T04:14:37 | step: 80900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.724993777927011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.34 | consumed tokens: 662732800.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T04:14:57 | step: 81000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.724281461676583e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.23 | consumed tokens: 663552000.0 | grad norm avg: 0.81 | grad norm last: 0.9 | 
2025-12-30T04:15:18 | step: 81100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.723568417830393e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.53 | consumed tokens: 664371200.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:15:38 | step: 81200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.722854646388441e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.78 | consumed tokens: 665190400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:15:59 | step: 81300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.722139783552848e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.58 | consumed tokens: 666009600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T04:16:20 | step: 81400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.721423829323612e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.55 | consumed tokens: 666828800.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:16:40 | step: 81500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.720707511296496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.66 | consumed tokens: 667648000.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T04:17:01 | step: 81600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.719990101875737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.69 | consumed tokens: 668467200.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T04:17:21 | step: 81700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.719271601061337e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.92 | consumed tokens: 669286400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:17:42 | step: 81800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.7185527364490554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.38 | consumed tokens: 670105600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T04:18:02 | step: 81900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.717832416645251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.19 | consumed tokens: 670924800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:18:23 | step: 82000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.7171117330435663e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.3 | train loss last: 4.0 | consumed tokens: 671744000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:18:43 | step: 82100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.7163899580482394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 4.0 | consumed tokens: 672563200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T04:19:04 | step: 82200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.715667455457151e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.5 | consumed tokens: 673382400.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T04:19:25 | step: 82300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.7149438614724204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 4.03 | consumed tokens: 674201600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T04:19:45 | step: 82400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.7142195398919284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.98 | consumed tokens: 675020800.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T04:20:06 | step: 82500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.713494490715675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.52 | consumed tokens: 675840000.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T04:20:26 | step: 82600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.7127683501457796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.69 | consumed tokens: 676659200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:20:47 | step: 82700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.7120414819801226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.98 | consumed tokens: 677478400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T04:21:07 | step: 82800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.7113135224208236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.5 | consumed tokens: 678297600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:21:28 | step: 82900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.710584835265763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.3 | consumed tokens: 679116800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:21:48 | step: 83000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.709855420514941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.14 | consumed tokens: 679936000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:22:09 | step: 83100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.709124914370477e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.58 | consumed tokens: 680755200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:22:31 | step: 83200 | train samples/s: 79.2 | train mfu (16-bit): -1.0 | lr mean: 4.708393680630252e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.66 | consumed tokens: 681574400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:22:52 | step: 83300 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.707661719294265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.88 | consumed tokens: 682393600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:23:12 | step: 83400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.706928666564636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.16 | consumed tokens: 683212800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:23:33 | step: 83500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.7061948862392455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.38 | consumed tokens: 684032000.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T04:23:54 | step: 83600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.705460378318094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.19 | consumed tokens: 684851200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:24:15 | step: 83700 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.7047247790033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.23 | consumed tokens: 685670400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T04:24:35 | step: 83800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.7039884520927444e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.33 | consumed tokens: 686489600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:24:56 | step: 83900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.703251033788547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.86 | consumed tokens: 687308800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T04:25:16 | step: 84000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.702512887888588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.28 | consumed tokens: 688128000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T04:25:37 | step: 84100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.701774014392868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.02 | consumed tokens: 688947200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:25:57 | step: 84200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.701034049503505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 4.03 | consumed tokens: 689766400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:26:18 | step: 84300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.7002933570183814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.64 | consumed tokens: 690585600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:26:39 | step: 84400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.699551936937496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.17 | consumed tokens: 691404800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:26:59 | step: 84500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.6988094254629686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.09 | consumed tokens: 692224000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:27:20 | step: 84600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.69806618639268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.36 | consumed tokens: 693043200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:27:40 | step: 84700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.6973222197266296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 3.61 | consumed tokens: 693862400.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T04:28:01 | step: 84800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.696577161666937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.3 | consumed tokens: 694681600.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T04:28:21 | step: 84900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.6958313760114834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.22 | consumed tokens: 695500800.0 | grad norm avg: 0.81 | grad norm last: 0.83 | 
2025-12-30T04:28:42 | step: 85000 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.6950844989623874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.17 | consumed tokens: 696320000.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T04:29:04 | step: 85100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.694337258115411e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.97 | consumed tokens: 697139200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:29:24 | step: 85200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.693588562076911e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.45 | consumed tokens: 697958400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:29:45 | step: 85300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.692839502240531e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.88 | consumed tokens: 698777600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:30:05 | step: 85400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.692089351010509e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.56 | consumed tokens: 699596800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:30:26 | step: 85500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.691338472184725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.98 | consumed tokens: 700416000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:30:47 | step: 85600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.690586501965299e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.03 | consumed tokens: 701235200.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T04:31:07 | step: 85700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.689833804150112e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.14 | consumed tokens: 702054400.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T04:31:28 | step: 85800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.689080378739163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.14 | consumed tokens: 702873600.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:31:48 | step: 85900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.6883258619345725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.8 | consumed tokens: 703692800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:32:09 | step: 86000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.68757061753422e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.92 | consumed tokens: 704512000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:32:29 | step: 86100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.6868146455381066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.14 | consumed tokens: 705331200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:32:50 | step: 86200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.686057582148351e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.29 | train loss last: 2.91 | consumed tokens: 706150400.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T04:33:11 | step: 86300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.6852997911628336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.28 | consumed tokens: 706969600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:33:31 | step: 86400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.684541272581555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.44 | consumed tokens: 707788800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2025-12-30T04:33:52 | step: 86500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.683781662606634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.75 | consumed tokens: 708608000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:34:12 | step: 86600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.683021325035952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.09 | consumed tokens: 709427200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:34:33 | step: 86700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.6822602598695084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.95 | consumed tokens: 710246400.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2025-12-30T04:34:53 | step: 86800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.681498103309423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.03 | consumed tokens: 711065600.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2025-12-30T04:35:14 | step: 86900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.6807352191535756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.36 | consumed tokens: 711884800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:35:34 | step: 87000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.679971607401967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.91 | consumed tokens: 712704000.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:35:55 | step: 87100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.6792069042567164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.12 | consumed tokens: 713523200.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:36:15 | step: 87200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.678441473515704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.8 | consumed tokens: 714342400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:36:36 | step: 87300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.677675315178931e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.0 | consumed tokens: 715161600.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2025-12-30T04:36:56 | step: 87400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.676908065448515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.5 | consumed tokens: 715980800.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T04:37:17 | step: 87500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.676140088122338e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.08 | consumed tokens: 716800000.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T04:37:37 | step: 87600 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.675371019402519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.69 | consumed tokens: 717619200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T04:37:58 | step: 87700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.674601586884819e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.41 | consumed tokens: 718438400.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T04:38:18 | step: 87800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.673831062973477e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.8 | consumed tokens: 719257600.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T04:38:39 | step: 87900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.673059447668493e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.92 | consumed tokens: 720076800.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:38:59 | step: 88000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.672287468565628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.62 | consumed tokens: 720896000.0 | grad norm avg: 0.82 | grad norm last: 0.94 | 
2025-12-30T04:39:20 | step: 88100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.671514398069121e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.86 | consumed tokens: 721715200.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T04:39:40 | step: 88200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.670740236178972e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.33 | consumed tokens: 722534400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:40:01 | step: 88300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.669965710490942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.02 | consumed tokens: 723353600.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2025-12-30T04:40:21 | step: 88400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.66919009340927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.62 | consumed tokens: 724172800.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T04:40:42 | step: 88500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.668413384933956e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.38 | consumed tokens: 724992000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:41:03 | step: 88600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.667636312660761e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 725811200.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:41:23 | step: 88700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.666858148993924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.98 | consumed tokens: 726630400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T04:41:44 | step: 88800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.666078893933445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.75 | consumed tokens: 727449600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:42:04 | step: 88900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.6652992750750855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.23 | consumed tokens: 728268800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:42:25 | step: 89000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.6645185648230836e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.78 | consumed tokens: 729088000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:42:45 | step: 89100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.6637367631774396e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 729907200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:43:06 | step: 89200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.662954597733915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 4.06 | consumed tokens: 730726400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:43:26 | step: 89300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.662171340896748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.5 | consumed tokens: 731545600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T04:43:47 | step: 89400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.66138735646382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.97 | consumed tokens: 732364800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T04:44:07 | step: 89500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.6606022806372494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.28 | consumed tokens: 733184000.0 | grad norm avg: 0.82 | grad norm last: 0.91 | 
2025-12-30T04:44:28 | step: 89600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.6598164772149175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.42 | consumed tokens: 734003200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:44:48 | step: 89700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.659029946196824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.33 | consumed tokens: 734822400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:45:09 | step: 89800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.65824268758297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.73 | consumed tokens: 735641600.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T04:45:30 | step: 89900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.657454337575473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.94 | consumed tokens: 736460800.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T04:45:50 | step: 90000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.656665259972215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.88 | consumed tokens: 737280000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:46:12 | step: 90100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.6558750909753144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 738099200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:46:33 | step: 90200 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.6550845581805333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.92 | consumed tokens: 738918400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:46:54 | step: 90300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.65429293399211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.73 | consumed tokens: 739737600.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T04:47:14 | step: 90400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.653500218410045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.33 | consumed tokens: 740556800.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:47:35 | step: 90500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.652707139030099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.05 | consumed tokens: 741376000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T04:47:56 | step: 90600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.651912968256511e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.67 | consumed tokens: 742195200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T04:48:16 | step: 90700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.651118069887161e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.77 | consumed tokens: 743014400.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T04:48:37 | step: 90800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.6503220801241696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 4.03 | consumed tokens: 743833600.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:48:57 | step: 90900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.6495253627654165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.75 | consumed tokens: 744652800.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2025-12-30T04:49:18 | step: 91000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.648727917810902e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.52 | consumed tokens: 745472000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2025-12-30T04:49:39 | step: 91100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.6479293814627454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.98 | consumed tokens: 746291200.0 | grad norm avg: 0.81 | grad norm last: 0.86 | 
2025-12-30T04:49:59 | step: 91200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.647130481316708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.56 | consumed tokens: 747110400.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T04:50:20 | step: 91300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.6463304897770286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.92 | consumed tokens: 747929600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:50:40 | step: 91400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.645529406843707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.92 | consumed tokens: 748748800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T04:51:01 | step: 91500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.644727596314624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.31 | consumed tokens: 749568000.0 | grad norm avg: 0.81 | grad norm last: 0.87 | 
2025-12-30T04:51:22 | step: 91600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.64392542198766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.0 | consumed tokens: 750387200.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T04:51:42 | step: 91700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.643121792469174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.83 | consumed tokens: 751206400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:52:03 | step: 91800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.6423177991528064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.73 | consumed tokens: 752025600.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T04:52:23 | step: 91900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.641512714442797e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 4.09 | consumed tokens: 752844800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:52:44 | step: 92000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.640706902137026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.25 | consumed tokens: 753664000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:53:04 | step: 92100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.639899998437613e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.5 | consumed tokens: 754483200.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:53:25 | step: 92200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.639092367142439e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.22 | consumed tokens: 755302400.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:53:45 | step: 92300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.638284008251503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.23 | consumed tokens: 756121600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:54:06 | step: 92400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.637474921764806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.75 | consumed tokens: 756940800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T04:54:27 | step: 92500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.6366647438844666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.92 | consumed tokens: 757760000.0 | grad norm avg: 0.82 | grad norm last: 0.73 | 
2025-12-30T04:54:47 | step: 92600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.6358542022062466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.19 | consumed tokens: 758579200.0 | grad norm avg: 0.83 | grad norm last: 0.71 | 
2025-12-30T04:55:08 | step: 92700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.635042205336504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.03 | consumed tokens: 759398400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T04:55:28 | step: 92800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.63422984466888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.09 | consumed tokens: 760217600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T04:55:49 | step: 92900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.6334163926076144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.33 | consumed tokens: 761036800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:56:09 | step: 93000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.632602212950587e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 4.0 | consumed tokens: 761856000.0 | grad norm avg: 0.82 | grad norm last: 0.74 | 
2025-12-30T04:56:30 | step: 93100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.631787305697799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.34 | consumed tokens: 762675200.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T04:56:50 | step: 93200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.630971307051368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.83 | consumed tokens: 763494400.0 | grad norm avg: 0.82 | grad norm last: 0.72 | 
2025-12-30T04:57:11 | step: 93300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.630154580809176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.81 | consumed tokens: 764313600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:57:31 | step: 93400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.6293371269712225e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.2 | consumed tokens: 765132800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T04:57:52 | step: 93500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.628518581739627e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 2.66 | consumed tokens: 765952000.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T04:58:12 | step: 93600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.6276996727101505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.97 | consumed tokens: 766771200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T04:58:33 | step: 93700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.626879672287032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.89 | consumed tokens: 767590400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T04:58:53 | step: 93800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.6260585804702714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.53 | consumed tokens: 768409600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T04:59:14 | step: 93900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.62523712485563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.03 | consumed tokens: 769228800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T04:59:34 | step: 94000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.624414577847347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.44 | consumed tokens: 770048000.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T04:59:55 | step: 94100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.623591303243302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.77 | consumed tokens: 770867200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:00:16 | step: 94200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.622766937245615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.94 | consumed tokens: 771686400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:00:36 | step: 94300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.6219418436521664e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.23 | consumed tokens: 772505600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:00:57 | step: 94400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.6211160224629566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.12 | consumed tokens: 773324800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:01:17 | step: 94500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.6202894736779854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.84 | consumed tokens: 774144000.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2025-12-30T05:01:38 | step: 94600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.619462197297253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.94 | consumed tokens: 774963200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:01:59 | step: 94700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.618633829522878e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.62 | consumed tokens: 775782400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T05:02:19 | step: 94800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.617804734152742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.19 | consumed tokens: 776601600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T05:02:39 | step: 94900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.616974911186844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.64 | consumed tokens: 777420800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:03:00 | step: 95000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.6161439968273044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.81 | consumed tokens: 778240000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:03:22 | step: 95100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.615312354872003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.12 | consumed tokens: 779059200.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T05:03:43 | step: 95200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.6144799853209406e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.95 | consumed tokens: 779878400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:04:03 | step: 95300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.613646524376236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.98 | consumed tokens: 780697600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:04:24 | step: 95400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.6128126996336505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.3 | consumed tokens: 781516800.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T05:04:45 | step: 95500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.611977783497423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.86 | consumed tokens: 782336000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:05:05 | step: 95600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.611142139765434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.16 | consumed tokens: 783155200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T05:05:26 | step: 95700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.610305404639803e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.28 | consumed tokens: 783974400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:05:46 | step: 95800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.609468305716291e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.17 | consumed tokens: 784793600.0 | grad norm avg: 0.81 | grad norm last: 0.85 | 
2025-12-30T05:06:07 | step: 95900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.608630115399137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.55 | consumed tokens: 785612800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:06:27 | step: 96000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.607790833688341e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.86 | consumed tokens: 786432000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:06:48 | step: 96100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.606951188179664e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.59 | consumed tokens: 787251200.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2025-12-30T05:07:08 | step: 96200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.6061104512773454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.92 | consumed tokens: 788070400.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T05:07:29 | step: 96300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.605268986779265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.06 | consumed tokens: 788889600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:07:49 | step: 96400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.6044267946854234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.89 | consumed tokens: 789708800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T05:08:10 | step: 96500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.6035835111979395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.52 | consumed tokens: 790528000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:08:31 | step: 96600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.602739500114694e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.39 | consumed tokens: 791347200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:08:51 | step: 96700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.6018947614356875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.91 | consumed tokens: 792166400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:09:12 | step: 96800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.6010492951609194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.22 | consumed tokens: 792985600.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:09:33 | step: 96900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.60020310129039e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.28 | consumed tokens: 793804800.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T05:09:53 | step: 97000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.599355816026218e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 4.12 | consumed tokens: 794624000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:10:14 | step: 97100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.598507803166285e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.41 | consumed tokens: 795443200.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T05:10:34 | step: 97200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.59765869891271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.25 | consumed tokens: 796262400.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T05:10:55 | step: 97300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.596809230861254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.94 | consumed tokens: 797081600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:11:16 | step: 97400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.595958671416156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 797900800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:11:36 | step: 97500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.5951073843752965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.81 | consumed tokens: 798720000.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T05:11:57 | step: 97600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.5942553697386757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 799539200.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T05:12:17 | step: 97700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.5934022637084126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.86 | consumed tokens: 800358400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:12:38 | step: 97800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.592548793880269e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.06 | consumed tokens: 801177600.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:12:59 | step: 97900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.591694232658483e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.38 | consumed tokens: 801996800.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T05:13:19 | step: 98000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.590838580043055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.67 | consumed tokens: 802816000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:13:40 | step: 98100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.5899825636297464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.7 | consumed tokens: 803635200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:14:00 | step: 98200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.5891254558227956e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.88 | consumed tokens: 804454400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T05:14:21 | step: 98300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5882676204200834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.72 | consumed tokens: 805273600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:14:41 | step: 98400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.58740905742161e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.94 | consumed tokens: 806092800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T05:15:02 | step: 98500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.586549403029494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.98 | consumed tokens: 806912000.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T05:15:22 | step: 98600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.5856893848394975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.05 | consumed tokens: 807731200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T05:15:43 | step: 98700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.584828275255859e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.72 | consumed tokens: 808550400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:16:03 | step: 98800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.583966438076459e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.78 | consumed tokens: 809369600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:16:24 | step: 98900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.583103509503417e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 4.09 | consumed tokens: 810188800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2025-12-30T05:16:45 | step: 99000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.582240217132494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.52 | consumed tokens: 811008000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:17:05 | step: 99100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.581375833367929e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.28 | train loss last: 3.56 | consumed tokens: 811827200.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T05:17:26 | step: 99200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.5805107220076025e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.95 | consumed tokens: 812646400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:17:46 | step: 99300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.5796448830515146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.88 | consumed tokens: 813465600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T05:18:07 | step: 99400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.578777952701785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.8 | consumed tokens: 814284800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T05:18:28 | step: 99500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.577910294756293e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.67 | consumed tokens: 815104000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:18:48 | step: 99600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.5770419092150405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.83 | consumed tokens: 815923200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:19:09 | step: 99700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.576172796078026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.42 | consumed tokens: 816742400.0 | grad norm avg: 0.82 | grad norm last: 0.77 | 
2025-12-30T05:19:29 | step: 99800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.575302955345251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.97 | consumed tokens: 817561600.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:19:50 | step: 99900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.574432023218833e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.12 | consumed tokens: 818380800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:20:10 | step: 100000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.573560363496654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.69 | consumed tokens: 819200000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:20:33 | step: 100100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.572687976178713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.39 | consumed tokens: 820019200.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T05:20:53 | step: 100200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.571814861265011e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.77 | consumed tokens: 820838400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:21:14 | step: 100300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.570940654957667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.75 | consumed tokens: 821657600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T05:21:34 | step: 100400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.5700657210545614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.19 | consumed tokens: 822476800.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T05:21:55 | step: 100500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.5691900595556945e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.89 | consumed tokens: 823296000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T05:22:16 | step: 100600 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.568313670461066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.66 | consumed tokens: 824115200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:22:36 | step: 100700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.567436553770676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.75 | consumed tokens: 824934400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:22:57 | step: 100800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.566558345686644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.28 | consumed tokens: 825753600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T05:23:17 | step: 100900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.565679410006851e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 826572800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:23:38 | step: 101000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.564799746731296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.53 | consumed tokens: 827392000.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T05:23:59 | step: 101100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.56391935585998e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 828211200.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T05:24:20 | step: 101200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.5630382373929024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.52 | consumed tokens: 829030400.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T05:24:40 | step: 101300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.5621560275321826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.5 | consumed tokens: 829849600.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T05:25:01 | step: 101400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.5612730900757015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.81 | consumed tokens: 830668800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:25:21 | step: 101500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.560389425023459e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.56 | consumed tokens: 831488000.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T05:25:42 | step: 101600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.559504668577574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.41 | consumed tokens: 832307200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:26:02 | step: 101700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.558619548333809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.72 | consumed tokens: 833126400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:26:23 | step: 101800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.557733336696401e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.52 | consumed tokens: 833945600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:26:44 | step: 101900 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 4.556846397463232e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.17 | consumed tokens: 834764800.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T05:27:05 | step: 102000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.555958730634302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.91 | consumed tokens: 835584000.0 | grad norm avg: 0.82 | grad norm last: 0.9 | 
2025-12-30T05:27:25 | step: 102100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.55507033620961e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.73 | consumed tokens: 836403200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T05:27:45 | step: 102200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.554180850391276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.77 | consumed tokens: 837222400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:28:06 | step: 102300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.553290636977181e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.97 | consumed tokens: 838041600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T05:28:27 | step: 102400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.552400059765205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 4.12 | consumed tokens: 838860800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T05:28:47 | step: 102500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.551508027361706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.47 | consumed tokens: 839680000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:29:08 | step: 102600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.550615631160326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.97 | consumed tokens: 840499200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:29:28 | step: 102700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.549722507363185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.03 | consumed tokens: 841318400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:29:49 | step: 102800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.548828292172402e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.06 | consumed tokens: 842137600.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:30:09 | step: 102900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.5479333493858576e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.17 | consumed tokens: 842956800.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T05:30:30 | step: 103000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.5470376790035516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 843776000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:30:50 | step: 103100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.5461409172276035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.5 | consumed tokens: 844595200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T05:31:11 | step: 103200 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.545243791653775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.41 | consumed tokens: 845414400.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:31:32 | step: 103300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.544345574686304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.97 | consumed tokens: 846233600.0 | grad norm avg: 0.81 | grad norm last: 0.88 | 
2025-12-30T05:31:52 | step: 103400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.5434466301230714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.36 | consumed tokens: 847052800.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T05:32:13 | step: 103500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.5425469579640776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.64 | consumed tokens: 847872000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:32:34 | step: 103600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.5416465582093224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.27 | consumed tokens: 848691200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:32:54 | step: 103700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.540745067060925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.23 | consumed tokens: 849510400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:33:15 | step: 103800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.539843212114647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.39 | consumed tokens: 850329600.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T05:33:35 | step: 103900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.538940265774727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.98 | consumed tokens: 851148800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:33:56 | step: 104000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.538036591839045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.36 | consumed tokens: 851968000.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:34:16 | step: 104100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.537132190307602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.62 | consumed tokens: 852787200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:34:37 | step: 104200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.536226697382517e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.3 | consumed tokens: 853606400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:34:58 | step: 104300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.535320840659551e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.25 | consumed tokens: 854425600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:35:18 | step: 104400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.5344138925429434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.23 | consumed tokens: 855244800.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T05:35:39 | step: 104500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.533506216830574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.58 | consumed tokens: 856064000.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:35:59 | step: 104600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.532597813522443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.53 | consumed tokens: 856883200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:36:20 | step: 104700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.531688682618551e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.94 | consumed tokens: 857702400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T05:36:41 | step: 104800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5307784603210166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.41 | consumed tokens: 858521600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:37:01 | step: 104900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.5298678742256016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.38 | consumed tokens: 859340800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:37:21 | step: 105000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.5289561967365444e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.14 | consumed tokens: 860160000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T05:37:44 | step: 105100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.528043791651726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.02 | consumed tokens: 860979200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T05:38:04 | step: 105200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.527130658971146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.25 | consumed tokens: 861798400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T05:38:25 | step: 105300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.5262164348969236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.77 | consumed tokens: 862617600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:38:45 | step: 105400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.525301847024821e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.25 | consumed tokens: 863436800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:39:06 | step: 105500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.524386167759076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.17 | consumed tokens: 864256000.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T05:39:26 | step: 105600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.5234697608975694e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.67 | consumed tokens: 865075200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:39:47 | step: 105700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.5225526264403015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.36 | consumed tokens: 865894400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:40:08 | step: 105800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.521634764387272e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.81 | consumed tokens: 866713600.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T05:40:28 | step: 105900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.5207161747384816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.52 | consumed tokens: 867532800.0 | grad norm avg: 0.83 | grad norm last: 0.76 | 
2025-12-30T05:40:49 | step: 106000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.519796493696049e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.34 | consumed tokens: 868352000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:41:09 | step: 106100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.5188760850578547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.89 | consumed tokens: 869171200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:41:30 | step: 106200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.51795531262178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.14 | consumed tokens: 869990400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:41:51 | step: 106300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.517033448792063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.72 | consumed tokens: 870809600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:42:11 | step: 106400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.5161104935687035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.64 | consumed tokens: 871628800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T05:42:32 | step: 106500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.5151871745474637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.39 | consumed tokens: 872448000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:42:52 | step: 106600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.5142631279304624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.47 | consumed tokens: 873267200.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T05:43:13 | step: 106700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.513337989919819e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.2 | consumed tokens: 874086400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:43:34 | step: 106800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.512412124313414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 4.5 | consumed tokens: 874905600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T05:43:54 | step: 106900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.511485531111248e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.05 | consumed tokens: 875724800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:44:15 | step: 107000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.51055821031332e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.3 | consumed tokens: 876544000.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T05:44:36 | step: 107100 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 4.509630161919631e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.33 | consumed tokens: 877363200.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:44:57 | step: 107200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.5087010221323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.22 | consumed tokens: 878182400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:45:17 | step: 107300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.507771518547088e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.69 | consumed tokens: 879001600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:45:38 | step: 107400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.506840923568234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.12 | consumed tokens: 879820800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:45:58 | step: 107500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.5059096009936184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.06 | consumed tokens: 880640000.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T05:46:19 | step: 107600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.5049775508232415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.28 | consumed tokens: 881459200.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2025-12-30T05:46:39 | step: 107700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.504044773057103e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.44 | consumed tokens: 882278400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:47:00 | step: 107800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.5031112676952034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.55 | consumed tokens: 883097600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T05:47:20 | step: 107900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.5021766709396616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.11 | consumed tokens: 883916800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:47:41 | step: 108000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.501241710386239e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.41 | consumed tokens: 884736000.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T05:48:01 | step: 108100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.500305658439174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.58 | consumed tokens: 885555200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:48:22 | step: 108200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.499368878896348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.77 | consumed tokens: 886374400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:48:42 | step: 108300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.4984313717577606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.98 | consumed tokens: 887193600.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2025-12-30T05:49:03 | step: 108400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.497493137023412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.53 | consumed tokens: 888012800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:49:24 | step: 108500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.4965538108954206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.72 | consumed tokens: 888832000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T05:49:44 | step: 108600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.495614120969549e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.34 | consumed tokens: 889651200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T05:50:05 | step: 108700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.494673339650035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.59 | consumed tokens: 890470400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T05:50:25 | step: 108800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.49373219453264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.94 | consumed tokens: 891289600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T05:50:46 | step: 108900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.4927899580216035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.48 | consumed tokens: 892108800.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T05:51:06 | step: 109000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.4918469939148054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.66 | consumed tokens: 892928000.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T05:51:27 | step: 109100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.490903302212246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.7 | consumed tokens: 893747200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:51:47 | step: 109200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.489958519116044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.95 | consumed tokens: 894566400.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T05:52:08 | step: 109300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.4890133722219616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.41 | consumed tokens: 895385600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:52:28 | step: 109400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.488067133934237e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.55 | consumed tokens: 896204800.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T05:52:49 | step: 109500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.487120531848632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 897024000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:53:10 | step: 109600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.4861728383693844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.44 | consumed tokens: 897843200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:53:30 | step: 109700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.4852244172943756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.95 | consumed tokens: 898662400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T05:53:51 | step: 109800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.4842752686236054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.3 | consumed tokens: 899481600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:54:11 | step: 109900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.483325392357074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.28 | consumed tokens: 900300800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:54:32 | step: 110000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.4823744246969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.39 | consumed tokens: 901120000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:54:54 | step: 110100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.4814230932388455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.83 | consumed tokens: 901939200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:55:15 | step: 110200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.480470670387149e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 902758400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:55:35 | step: 110300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.4795178837375715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.06 | consumed tokens: 903577600.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T05:55:56 | step: 110400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.478564005694352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.38 | consumed tokens: 904396800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:56:16 | step: 110500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.477609400055371e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.64 | consumed tokens: 905216000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T05:56:37 | step: 110600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.476654066820629e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.03 | consumed tokens: 906035200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T05:56:58 | step: 110700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.475698005990125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.83 | consumed tokens: 906854400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T05:57:18 | step: 110800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.4747408537659794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.53 | consumed tokens: 907673600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:57:39 | step: 110900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.473783337743953e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.41 | consumed tokens: 908492800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T05:58:00 | step: 111000 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.472825094126165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.66 | consumed tokens: 909312000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T05:58:20 | step: 111100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.471865759114735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 4.0 | consumed tokens: 910131200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T05:58:41 | step: 111200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.470905696507543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.3 | consumed tokens: 910950400.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T05:59:02 | step: 111300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.4699449063045904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.59 | consumed tokens: 911769600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T05:59:22 | step: 111400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.468983388505876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.48 | consumed tokens: 912588800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T05:59:43 | step: 111500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.4680211431114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.84 | consumed tokens: 913408000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:00:04 | step: 111600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.467058170121163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.22 | consumed tokens: 914227200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:00:24 | step: 111700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.4660944695351645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.47 | consumed tokens: 915046400.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:00:45 | step: 111800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.465129677555524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 2.86 | consumed tokens: 915865600.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T06:01:06 | step: 111900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.4641645217780024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.27 | consumed tokens: 916684800.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T06:01:26 | step: 112000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.463198274606839e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.81 | consumed tokens: 917504000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:01:47 | step: 112100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.4622316636377946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.06 | consumed tokens: 918323200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:02:07 | step: 112200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.461263961275108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.14 | consumed tokens: 919142400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2025-12-30T06:02:28 | step: 112300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.4602955313166603e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.25 | consumed tokens: 919961600.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2025-12-30T06:02:49 | step: 112400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.459326373762451e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.89 | consumed tokens: 920780800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:03:09 | step: 112500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.4583564886124805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.67 | consumed tokens: 921600000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:03:30 | step: 112600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.4573858758667484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 922419200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T06:03:50 | step: 112700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.456414171727374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.03 | consumed tokens: 923238400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:04:11 | step: 112800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.455442103790119e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.03 | consumed tokens: 924057600.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T06:04:31 | step: 112900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.454468944459222e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.08 | consumed tokens: 924876800.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T06:04:52 | step: 113000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.4534954213304445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.91 | consumed tokens: 925696000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T06:05:12 | step: 113100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.4525208068080246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.45 | consumed tokens: 926515200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:05:33 | step: 113200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.4515454646898434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 927334400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:05:53 | step: 113300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.4505697587737814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.27 | train loss last: 3.41 | consumed tokens: 928153600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:06:14 | step: 113400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.449592961464077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.17 | consumed tokens: 928972800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:06:35 | step: 113500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.448615436558612e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.97 | consumed tokens: 929792000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T06:06:56 | step: 113600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.447636820259504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.58 | consumed tokens: 930611200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:07:16 | step: 113700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.4466578401625156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.64 | consumed tokens: 931430400.0 | grad norm avg: 0.82 | grad norm last: 0.74 | 
2025-12-30T06:07:37 | step: 113800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.445678132469766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.98 | consumed tokens: 932249600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:07:57 | step: 113900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.444697333383374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.25 | consumed tokens: 933068800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:08:18 | step: 114000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.443716170499101e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.89 | consumed tokens: 933888000.0 | grad norm avg: 0.82 | grad norm last: 0.85 | 
2025-12-30T06:08:39 | step: 114100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.4427339162211865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.05 | consumed tokens: 934707200.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T06:08:59 | step: 114200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.441751298145391e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.95 | consumed tokens: 935526400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:09:20 | step: 114300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.4407675886759534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.08 | consumed tokens: 936345600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:09:40 | step: 114400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.4397831516107544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.47 | consumed tokens: 937164800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:10:01 | step: 114500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.438797986949794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.36 | consumed tokens: 937984000.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T06:10:21 | step: 114600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.437812094693072e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.73 | consumed tokens: 938803200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:10:42 | step: 114700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.436825474840589e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.67 | consumed tokens: 939622400.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T06:11:02 | step: 114800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.435838127392344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.56 | consumed tokens: 940441600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:11:23 | step: 114900 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.434850052348338e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.66 | consumed tokens: 941260800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:11:44 | step: 115000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.4338612497085705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.36 | consumed tokens: 942080000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T06:12:06 | step: 115100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.4328717194730416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.39 | consumed tokens: 942899200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T06:12:27 | step: 115200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.4318810978438705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.28 | consumed tokens: 943718400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:12:47 | step: 115300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.430890112416819e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.72 | consumed tokens: 944537600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:13:08 | step: 115400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.429898035596125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.86 | consumed tokens: 945356800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T06:13:28 | step: 115500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.42890559497755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.08 | consumed tokens: 946176000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:13:49 | step: 115600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.4279120629653335e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.44 | consumed tokens: 946995200.0 | grad norm avg: 0.84 | grad norm last: 0.75 | 
2025-12-30T06:14:10 | step: 115700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.426917803357355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.06 | consumed tokens: 947814400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T06:14:30 | step: 115800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.425922816153616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.25 | consumed tokens: 948633600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:14:51 | step: 115900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.4249274651519954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.97 | consumed tokens: 949452800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:15:11 | step: 116000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.423931022756733e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.81 | consumed tokens: 950272000.0 | grad norm avg: 0.83 | grad norm last: 0.93 | 
2025-12-30T06:15:32 | step: 116100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.422933852765709e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.75 | consumed tokens: 951091200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:15:53 | step: 116200 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.421935955178924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.95 | consumed tokens: 951910400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2025-12-30T06:16:13 | step: 116300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.4209369661984965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 4.03 | consumed tokens: 952729600.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T06:16:34 | step: 116400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.4199376134201884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.47 | consumed tokens: 953548800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:16:54 | step: 116500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.418937533046119e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.16 | consumed tokens: 954368000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:17:15 | step: 116600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.417936725076288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.73 | consumed tokens: 955187200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:17:35 | step: 116700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.416934825712815e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.03 | consumed tokens: 956006400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:17:56 | step: 116800 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 4.415932562551461e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.34 | consumed tokens: 956825600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T06:18:16 | step: 116900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.414929571794346e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.7 | consumed tokens: 957644800.0 | grad norm avg: 0.83 | grad norm last: 0.91 | 
2025-12-30T06:18:37 | step: 117000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.413925489643589e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.97 | consumed tokens: 958464000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T06:18:58 | step: 117100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.4129210436949506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.28 | consumed tokens: 959283200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:19:18 | step: 117200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.4119155063526705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.84 | consumed tokens: 960102400.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T06:19:39 | step: 117300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.410909241414629e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 3.2 | consumed tokens: 960921600.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2025-12-30T06:20:00 | step: 117400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.4099026126787066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.56 | consumed tokens: 961740800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:20:20 | step: 117500 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.408894892549142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.86 | consumed tokens: 962560000.0 | grad norm avg: 0.83 | grad norm last: 0.76 | 
2025-12-30T06:20:41 | step: 117600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.4078864448238164e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 963379200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:21:02 | step: 117700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.406877269502729e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.03 | consumed tokens: 964198400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:21:22 | step: 117800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.4058673665858805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.59 | consumed tokens: 965017600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:21:43 | step: 117900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.4048567360732704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.28 | consumed tokens: 965836800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T06:22:04 | step: 118000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.4038457417627797e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.52 | consumed tokens: 966656000.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T06:22:24 | step: 118100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.402833656058647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.66 | consumed tokens: 967475200.0 | grad norm avg: 0.83 | grad norm last: 0.69 | 
2025-12-30T06:22:45 | step: 118200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.4018208427587524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.03 | consumed tokens: 968294400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:23:05 | step: 118300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.400806938065216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.94 | consumed tokens: 969113600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:23:26 | step: 118400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.399792669573799e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.19 | consumed tokens: 969932800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:23:47 | step: 118500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.39877767348662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.48 | consumed tokens: 970752000.0 | grad norm avg: 0.83 | grad norm last: 0.94 | 
2025-12-30T06:24:07 | step: 118600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.39776194980368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.22 | consumed tokens: 971571200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T06:24:28 | step: 118700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.396745498524979e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.62 | consumed tokens: 972390400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:24:49 | step: 118800 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.395728319650516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.08 | consumed tokens: 973209600.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2025-12-30T06:25:09 | step: 118900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.394710049382411e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.92 | consumed tokens: 974028800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:25:30 | step: 119000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.393691415316425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.06 | consumed tokens: 974848000.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T06:25:51 | step: 119100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.392672053654678e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.98 | consumed tokens: 975667200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:26:11 | step: 119200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.391651600599289e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.58 | consumed tokens: 976486400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:26:32 | step: 119300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.390630783746019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.78 | consumed tokens: 977305600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:26:52 | step: 119400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.389608875499107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.2 | consumed tokens: 978124800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T06:27:13 | step: 119500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.388586603454314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.81 | consumed tokens: 978944000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:27:33 | step: 119600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.38756360381376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.05 | consumed tokens: 979763200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:27:54 | step: 119700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.386539512779564e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.19 | consumed tokens: 980582400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:28:15 | step: 119800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.3855150579474866e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.44 | consumed tokens: 981401600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:28:35 | step: 119900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.3844895117217675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.64 | consumed tokens: 982220800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:28:56 | step: 120000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.3834636016981676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.95 | consumed tokens: 983040000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:29:18 | step: 120100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.3824366002809256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.05 | consumed tokens: 983859200.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T06:29:39 | step: 120200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.381408871267922e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.33 | consumed tokens: 984678400.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:29:59 | step: 120300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.380380778457038e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.08 | consumed tokens: 985497600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T06:30:20 | step: 120400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.379351594252512e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.95 | consumed tokens: 986316800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:30:40 | step: 120500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.378322046250105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.61 | consumed tokens: 987136000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:31:01 | step: 120600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.377291406854056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.05 | consumed tokens: 987955200.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T06:31:22 | step: 120700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.376260039862245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.7 | consumed tokens: 988774400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T06:31:42 | step: 120800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.375228309072554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.56 | consumed tokens: 989593600.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T06:32:03 | step: 120900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.374195486889221e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.44 | consumed tokens: 990412800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:32:23 | step: 121000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.373161937110126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.42 | consumed tokens: 991232000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:32:44 | step: 121100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.3721280235331506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.55 | consumed tokens: 992051200.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:33:05 | step: 121200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.371093018562533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.44 | consumed tokens: 992870400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:33:25 | step: 121300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.370057285996154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.89 | consumed tokens: 993689600.0 | grad norm avg: 0.83 | grad norm last: 0.91 | 
2025-12-30T06:33:46 | step: 121400 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.369021189631894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.14 | consumed tokens: 994508800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:34:07 | step: 121500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.3679840018739924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.19 | consumed tokens: 995328000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T06:34:27 | step: 121600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.366946086520329e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.62 | consumed tokens: 996147200.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2025-12-30T06:34:48 | step: 121700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.3659074435709044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.53 | consumed tokens: 996966400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:35:09 | step: 121800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.364868436823599e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.06 | consumed tokens: 997785600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T06:35:29 | step: 121900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.3638283386826515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.81 | consumed tokens: 998604800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:35:50 | step: 122000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.3627875129459426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.62 | consumed tokens: 999424000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T06:36:10 | step: 122100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.361746323411353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.77 | consumed tokens: 1000243200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:36:31 | step: 122200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.360704042483121e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 1001062400.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T06:36:51 | step: 122300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.359661033959128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.69 | consumed tokens: 1001881600.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T06:37:12 | step: 122400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.358617661637254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.64 | consumed tokens: 1002700800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T06:37:32 | step: 122500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.357573197921738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.61 | consumed tokens: 1003520000.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T06:37:53 | step: 122600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.3565280066104606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.98 | consumed tokens: 1004339200.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T06:38:14 | step: 122700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.3554824515013024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.39 | consumed tokens: 1005158400.0 | grad norm avg: 0.82 | grad norm last: 0.84 | 
2025-12-30T06:38:34 | step: 122800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.354435804998502e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.42 | consumed tokens: 1005977600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:38:55 | step: 122900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.3533884308999404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.5 | consumed tokens: 1006796800.0 | grad norm avg: 0.83 | grad norm last: 0.75 | 
2025-12-30T06:39:15 | step: 123000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.352340693003498e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.05 | consumed tokens: 1007616000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T06:39:36 | step: 123100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.3512918637134135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 1008435200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T06:39:56 | step: 123200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.350242670625448e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.8 | consumed tokens: 1009254400.0 | grad norm avg: 0.83 | grad norm last: 1.02 | 
2025-12-30T06:40:17 | step: 123300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.349192386143841e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 1010073600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T06:40:37 | step: 123400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.348141737864353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.09 | consumed tokens: 1010892800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:40:58 | step: 123500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.3470899981912225e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.39 | consumed tokens: 1011712000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:41:19 | step: 123600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.346037530922331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.36 | consumed tokens: 1012531200.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T06:41:39 | step: 123700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.3449846998555586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.02 | consumed tokens: 1013350400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:41:59 | step: 123800 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 4.343931141193025e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.11 | consumed tokens: 1014169600.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T06:42:20 | step: 123900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.342876491136849e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.55 | consumed tokens: 1014988800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:42:41 | step: 124000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.341821477282792e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.05 | consumed tokens: 1015808000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:43:01 | step: 124100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.3407653720350936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.77 | consumed tokens: 1016627200.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T06:43:22 | step: 124200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.339708902989514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.72 | consumed tokens: 1017446400.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T06:43:42 | step: 124300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.338651706348173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.11 | consumed tokens: 1018265600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:44:03 | step: 124400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.3375934183131903e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 1019084800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:44:23 | step: 124500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.3365347664803267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.02 | consumed tokens: 1019904000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:44:44 | step: 124600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.3354753870517015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.17 | consumed tokens: 1020723200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:45:05 | step: 124700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.334415280027315e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.38 | consumed tokens: 1021542400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:45:25 | step: 124800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.333354445407167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1022361600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T06:45:46 | step: 124900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.332292519393377e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.09 | consumed tokens: 1023180800.0 | grad norm avg: 0.84 | grad norm last: 0.96 | 
2025-12-30T06:46:06 | step: 125000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.331230229581706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.47 | consumed tokens: 1024000000.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T06:46:28 | step: 125100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.330167212174274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.12 | consumed tokens: 1024819200.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T06:46:49 | step: 125200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.3291034671710804e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.56 | consumed tokens: 1025638400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T06:47:09 | step: 125300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.3280389945721254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.44 | consumed tokens: 1026457600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:47:30 | step: 125400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.326973794377409e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.48 | consumed tokens: 1027276800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:47:50 | step: 125500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.325908230384812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.7 | consumed tokens: 1028096000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:48:11 | step: 125600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.3248415749985725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.55 | consumed tokens: 1028915200.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:48:31 | step: 125700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.323774192016572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 2.81 | consumed tokens: 1029734400.0 | grad norm avg: 0.83 | grad norm last: 0.91 | 
2025-12-30T06:48:52 | step: 125800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.3227060814388096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.58 | consumed tokens: 1030553600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T06:49:12 | step: 125900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.321637243265286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.05 | consumed tokens: 1031372800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T06:49:33 | step: 126000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.320568041293882e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.69 | consumed tokens: 1032192000.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T06:49:54 | step: 126100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.3194977479288355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.11 | consumed tokens: 1033011200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T06:50:14 | step: 126200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.3184270907659084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.86 | consumed tokens: 1033830400.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T06:50:34 | step: 126300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.317355342209339e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.47 | consumed tokens: 1034649600.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T06:50:55 | step: 126400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.316283229854889e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.03 | consumed tokens: 1035468800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T06:51:16 | step: 126500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.315210026106797e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.19 | consumed tokens: 1036288000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:51:36 | step: 126600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.3141364585608244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.0 | consumed tokens: 1037107200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T06:51:57 | step: 126700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.31306216341909e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.84 | consumed tokens: 1037926400.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T06:52:17 | step: 126800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.311986776883714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 2.08 | consumed tokens: 1038745600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:52:38 | step: 126900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.310911026550457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.47 | consumed tokens: 1039564800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:52:58 | step: 127000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.3098345486214384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.34 | consumed tokens: 1040384000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T06:53:19 | step: 127100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.3087573430966586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.3 | consumed tokens: 1041203200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T06:53:39 | step: 127200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.307679409976117e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.73 | consumed tokens: 1042022400.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T06:54:00 | step: 127300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.3066007492598146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.2 | consumed tokens: 1042841600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T06:54:20 | step: 127400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.3055213609477505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.06 | consumed tokens: 1043660800.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T06:54:41 | step: 127500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.304441608837806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.09 | consumed tokens: 1044480000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T06:55:02 | step: 127600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.303360765334219e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.8 | consumed tokens: 1045299200.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2025-12-30T06:55:22 | step: 127700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.3022791942348704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.28 | consumed tokens: 1046118400.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T06:55:43 | step: 127800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.301197259337641e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.17 | consumed tokens: 1046937600.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2025-12-30T06:56:03 | step: 127900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.30011423304677e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.62 | consumed tokens: 1047756800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T06:56:24 | step: 128000 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.299030842958018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.45 | consumed tokens: 1048576000.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T06:56:45 | step: 128100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.297946361475624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.0 | consumed tokens: 1049395200.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:57:05 | step: 128200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.2968615161953494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.59 | consumed tokens: 1050214400.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T06:57:26 | step: 128300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.295775943319313e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.88 | consumed tokens: 1051033600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T06:57:47 | step: 128400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.2946896428475156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.12 | consumed tokens: 1051852800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T06:58:07 | step: 128500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.2936026147799566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.44 | consumed tokens: 1052672000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T06:58:28 | step: 128600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.292514859116636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.48 | consumed tokens: 1053491200.0 | grad norm avg: 0.83 | grad norm last: 0.76 | 
2025-12-30T06:58:48 | step: 128700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.2914263758575544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.19 | consumed tokens: 1054310400.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T06:59:09 | step: 128800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.290337165002711e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.41 | consumed tokens: 1055129600.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T06:59:29 | step: 128900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.2892472265521064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.09 | consumed tokens: 1055948800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T06:59:50 | step: 129000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.2881565605057403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.53 | consumed tokens: 1056768000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:00:11 | step: 129100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.2870655306614935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.88 | consumed tokens: 1057587200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:00:31 | step: 129200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.2859734094236046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.75 | consumed tokens: 1058406400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:00:52 | step: 129300 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.284880924387835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.05 | consumed tokens: 1059225600.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T07:01:13 | step: 129400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.283787711756304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.69 | consumed tokens: 1060044800.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T07:01:33 | step: 129500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.282693407731131e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.56 | consumed tokens: 1060864000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:01:54 | step: 129600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.281598739908077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.8 | consumed tokens: 1061683200.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:02:14 | step: 129700 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.2805033444892615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.86 | consumed tokens: 1062502400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T07:02:35 | step: 129800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.279407221474685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.5 | consumed tokens: 1063321600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:02:55 | step: 129900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.2783103708643466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.94 | consumed tokens: 1064140800.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T07:03:16 | step: 130000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.277213156456128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.33 | consumed tokens: 1064960000.0 | grad norm avg: 0.84 | grad norm last: 0.72 | 
2025-12-30T07:03:38 | step: 130100 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 4.276114850654267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 1065779200.0 | grad norm avg: 0.84 | grad norm last: 0.98 | 
2025-12-30T07:03:58 | step: 130200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.2750158172566444e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.59 | consumed tokens: 1066598400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:04:19 | step: 130300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.273916420061141e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 4.22 | consumed tokens: 1067417600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:04:40 | step: 130400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.272815931471996e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.19 | consumed tokens: 1068236800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:05:00 | step: 130500 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 4.27171507908497e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.3 | consumed tokens: 1069056000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:05:21 | step: 130600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.270613499102183e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.95 | consumed tokens: 1069875200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:05:41 | step: 130700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.269511191523634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.17 | consumed tokens: 1070694400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:06:02 | step: 130800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.268408156349324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.45 | consumed tokens: 1071513600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:06:22 | step: 130900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.267304393579252e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 1072332800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T07:06:43 | step: 131000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.266199903213419e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.56 | consumed tokens: 1073152000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T07:07:04 | step: 131100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.265095049049705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.2 | consumed tokens: 1073971200.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T07:07:24 | step: 131200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.2639891034923494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 1074790400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:07:45 | step: 131300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.262882794137113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.11 | consumed tokens: 1075609600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:08:05 | step: 131400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.261775393388234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.09 | consumed tokens: 1076428800.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T07:08:26 | step: 131500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.2606676288414747e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.75 | consumed tokens: 1077248000.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T07:08:46 | step: 131600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.259559136698954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.02 | consumed tokens: 1078067200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:09:07 | step: 131700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.2584499169606715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.28 | consumed tokens: 1078886400.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T07:09:27 | step: 131800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.257339969626628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.98 | consumed tokens: 1079705600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:09:48 | step: 131900 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 4.256229294696823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.86 | consumed tokens: 1080524800.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T07:10:09 | step: 132000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.255118255969137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.97 | consumed tokens: 1081344000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:10:29 | step: 132100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.254006125847809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.7 | consumed tokens: 1082163200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:10:50 | step: 132200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.2528936319286004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.09 | consumed tokens: 1082982400.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:11:10 | step: 132300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.25178041041363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.94 | consumed tokens: 1083801600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T07:11:31 | step: 132400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.250666461302899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.17 | consumed tokens: 1084620800.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T07:11:52 | step: 132500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.249551420798525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.27 | consumed tokens: 1085440000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T07:12:12 | step: 132600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.2484363802941516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.83 | consumed tokens: 1086259200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:12:33 | step: 132700 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 4.247320248396136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.02 | consumed tokens: 1087078400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:12:53 | step: 132800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.246203388902359e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.91 | consumed tokens: 1087897600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:13:14 | step: 132900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.245086165610701e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.34 | consumed tokens: 1088716800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:13:34 | step: 133000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.243967850925401e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.95 | consumed tokens: 1089536000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:13:55 | step: 133100 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.24284917244222e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.86 | consumed tokens: 1090355200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:14:15 | step: 133200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.241729766363278e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.17 | consumed tokens: 1091174400.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2025-12-30T07:14:36 | step: 133300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.2406096326885745e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.31 | consumed tokens: 1091993600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T07:14:57 | step: 133400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.2394887714181095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.24 | train loss last: 3.98 | consumed tokens: 1092812800.0 | grad norm avg: 0.83 | grad norm last: 0.74 | 
2025-12-30T07:15:17 | step: 133500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.238367182551883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.59 | consumed tokens: 1093632000.0 | grad norm avg: 0.84 | grad norm last: 0.78 | 
2025-12-30T07:15:38 | step: 133600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.237245229887776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.78 | consumed tokens: 1094451200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:15:58 | step: 133700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.236122185830027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.95 | consumed tokens: 1095270400.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T07:16:19 | step: 133800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.234998777974397e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.03 | consumed tokens: 1096089600.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T07:16:40 | step: 133900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.2338746425230056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.19 | consumed tokens: 1096908800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T07:17:00 | step: 134000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.232749779475853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.16 | consumed tokens: 1097728000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:17:21 | step: 134100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.231624188832939e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.38 | consumed tokens: 1098547200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:17:42 | step: 134200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.230497870594263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.66 | consumed tokens: 1099366400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:18:02 | step: 134300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.229371188557707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.39 | consumed tokens: 1100185600.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T07:18:23 | step: 134400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.2282434151275083e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.12 | consumed tokens: 1101004800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:18:43 | step: 134500 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.227115277899429e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 1101824000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T07:19:04 | step: 134600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.2259864130755886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.42 | consumed tokens: 1102643200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T07:19:25 | step: 134700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.224856820655987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.53 | consumed tokens: 1103462400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:19:45 | step: 134800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.223726500640623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.75 | consumed tokens: 1104281600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:20:06 | step: 134900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.2225954530294985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.16 | consumed tokens: 1105100800.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:20:27 | step: 135000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.221464041620493e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.28 | consumed tokens: 1105920000.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T07:20:49 | step: 135100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.220331902615726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.12 | consumed tokens: 1106739200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:21:09 | step: 135200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.219198672217317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.88 | consumed tokens: 1107558400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:21:30 | step: 135300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.218065078021027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 1108377600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:21:51 | step: 135400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.216930756228976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.06 | consumed tokens: 1109196800.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T07:22:11 | step: 135500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.215796070639044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.0 | consumed tokens: 1110016000.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:22:32 | step: 135600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.21466029365547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.09 | consumed tokens: 1110835200.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T07:22:52 | step: 135700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.213524152874015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 1111654400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:23:13 | step: 135800 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.2123869206989184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.03 | consumed tokens: 1112473600.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:23:34 | step: 135900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.211249324725941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.66 | consumed tokens: 1113292800.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T07:23:55 | step: 136000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.2101113649550825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.03 | consumed tokens: 1114112000.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T07:24:15 | step: 136100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.208972313790582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.23 | consumed tokens: 1114931200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T07:24:36 | step: 136200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.20783253503032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.05 | consumed tokens: 1115750400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:24:56 | step: 136300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.206692392472178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.02 | consumed tokens: 1116569600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:25:17 | step: 136400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.205551522318274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.14 | consumed tokens: 1117388800.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-30T07:25:38 | step: 136500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.2044099245686084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.44 | consumed tokens: 1118208000.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:25:58 | step: 136600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.2032675992231816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.98 | consumed tokens: 1119027200.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T07:26:19 | step: 136700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.2021245462819934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.33 | consumed tokens: 1119846400.0 | grad norm avg: 0.83 | grad norm last: 1.14 | 
2025-12-30T07:26:39 | step: 136800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.200980765745044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.58 | consumed tokens: 1120665600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:27:00 | step: 136900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.1998366214102134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.38 | consumed tokens: 1121484800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:27:21 | step: 137000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.1986917494796216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.2 | consumed tokens: 1122304000.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T07:27:41 | step: 137100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.1975461499532685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 1123123200.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T07:28:02 | step: 137200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.196399822831154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.56 | consumed tokens: 1123942400.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T07:28:23 | step: 137300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.195252768113278e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.88 | consumed tokens: 1124761600.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:28:43 | step: 137400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.194105349597521e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 1125580800.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:29:04 | step: 137500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.192957203486003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.25 | train loss last: 3.41 | consumed tokens: 1126400000.0 | grad norm avg: 0.83 | grad norm last: 0.78 | 
2025-12-30T07:29:25 | step: 137600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.1918083297787234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.36 | consumed tokens: 1127219200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:29:45 | step: 137700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.1906587284756824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.09 | consumed tokens: 1128038400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:30:06 | step: 137800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.18950839957688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.59 | consumed tokens: 1128857600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:30:27 | step: 137900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.188357706880197e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.94 | consumed tokens: 1129676800.0 | grad norm avg: 0.83 | grad norm last: 0.9 | 
2025-12-30T07:30:47 | step: 138000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.187205922789872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.08 | consumed tokens: 1130496000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T07:31:08 | step: 138100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.186053774901666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.14 | consumed tokens: 1131315200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2025-12-30T07:31:28 | step: 138200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.1849008994176984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.5 | consumed tokens: 1132134400.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:31:49 | step: 138300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.1837472963379696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 1132953600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:32:10 | step: 138400 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.18259332946036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.08 | consumed tokens: 1133772800.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T07:32:31 | step: 138500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.181438634986989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.86 | consumed tokens: 1134592000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:32:51 | step: 138600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.180282849119976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.25 | consumed tokens: 1135411200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:33:12 | step: 138700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.179127063252963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.26 | train loss last: 2.77 | consumed tokens: 1136230400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T07:33:32 | step: 138800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.177970185992308e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.84 | consumed tokens: 1137049600.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T07:33:53 | step: 138900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.1768125811358914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.33 | consumed tokens: 1137868800.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:34:13 | step: 139000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.175654612481594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 1138688000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:34:34 | step: 139100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.1744959162315354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 4.03 | consumed tokens: 1139507200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:34:54 | step: 139200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.173336492385715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.97 | consumed tokens: 1140326400.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:35:15 | step: 139300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.172176340944134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.28 | consumed tokens: 1141145600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:35:36 | step: 139400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.1710158257046714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.97 | consumed tokens: 1141964800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:35:57 | step: 139500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.169854219071567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.23 | consumed tokens: 1142784000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:36:17 | step: 139600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.168692248640582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.83 | consumed tokens: 1143603200.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T07:36:38 | step: 139700 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.1675295506138355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.73 | consumed tokens: 1144422400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:36:59 | step: 139800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.166366488789208e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.22 | consumed tokens: 1145241600.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T07:37:20 | step: 139900 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.165202335570939e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.06 | consumed tokens: 1146060800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T07:37:40 | step: 140000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.164037818554789e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.34 | consumed tokens: 1146880000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T07:38:03 | step: 140100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.1628725739428774e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.48 | consumed tokens: 1147699200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:38:23 | step: 140200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.1617066017352045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.33 | consumed tokens: 1148518400.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2025-12-30T07:38:44 | step: 140300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.160540265729651e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.19 | consumed tokens: 1149337600.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2025-12-30T07:39:04 | step: 140400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.159372838330455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.66 | consumed tokens: 1150156800.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:39:25 | step: 140500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.158205047133379e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.34 | consumed tokens: 1150976000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:39:46 | step: 140600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.157036528340541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 4.19 | consumed tokens: 1151795200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T07:40:06 | step: 140700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.155867645749822e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.48 | consumed tokens: 1152614400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:40:27 | step: 140800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.1546976717654616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.19 | consumed tokens: 1153433600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T07:40:47 | step: 140900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.15352733398322e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.48 | consumed tokens: 1154252800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:41:08 | step: 141000 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.152356268605217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 1155072000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:41:29 | step: 141100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.151184475631453e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.0 | consumed tokens: 1155891200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T07:41:49 | step: 141200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.150012318859808e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.7 | consumed tokens: 1156710400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T07:42:10 | step: 141300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.148839070694521e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.09 | consumed tokens: 1157529600.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:42:31 | step: 141400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.147665458731353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 4.28 | consumed tokens: 1158348800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:42:51 | step: 141500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.146491119172424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.28 | consumed tokens: 1159168000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T07:43:12 | step: 141600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.145316415815614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.27 | consumed tokens: 1159987200.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:43:33 | step: 141700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.144140621065162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.98 | consumed tokens: 1160806400.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2025-12-30T07:43:53 | step: 141800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.1429644625168294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.36 | consumed tokens: 1161625600.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T07:44:14 | step: 141900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.141787576372735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.8 | consumed tokens: 1162444800.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T07:44:35 | step: 142000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.14061032643076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.5 | consumed tokens: 1163264000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T07:44:55 | step: 142100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.139431985095143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.59 | consumed tokens: 1164083200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:45:16 | step: 142200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.1382532799616456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.66 | consumed tokens: 1164902400.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T07:45:37 | step: 142300 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 4.1370738472323865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.31 | consumed tokens: 1165721600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T07:45:58 | step: 142400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.1358940507052466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.2 | consumed tokens: 1166540800.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:46:18 | step: 142500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.1347131627844647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.55 | consumed tokens: 1167360000.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:46:39 | step: 142600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.133531911065802e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.12 | consumed tokens: 1168179200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:46:59 | step: 142700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.132349931751378e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.2 | consumed tokens: 1168998400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:47:20 | step: 142800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.1311672248411924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.58 | consumed tokens: 1169817600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:47:41 | step: 142900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.129984154133126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.33 | consumed tokens: 1170636800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:48:01 | step: 143000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.1288003558292985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.84 | consumed tokens: 1171456000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:48:22 | step: 143100 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.1276158299297094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1172275200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:48:42 | step: 143200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.126430576434359e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.84 | consumed tokens: 1173094400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T07:49:03 | step: 143300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.125244959141128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.91 | consumed tokens: 1173913600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:49:24 | step: 143400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.124058614252135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.77 | consumed tokens: 1174732800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2025-12-30T07:49:44 | step: 143500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.122871541767381e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.89 | consumed tokens: 1175552000.0 | grad norm avg: 0.83 | grad norm last: 0.86 | 
2025-12-30T07:50:05 | step: 143600 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.121683741686866e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.73 | consumed tokens: 1176371200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:50:26 | step: 143700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.1204955778084695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.03 | consumed tokens: 1177190400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T07:50:47 | step: 143800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.119306686334312e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 1178009600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:51:07 | step: 143900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.118117067264393e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.02 | consumed tokens: 1178828800.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T07:51:28 | step: 144000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.1169267205987126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.17 | consumed tokens: 1179648000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2025-12-30T07:51:49 | step: 144100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.1157360101351514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.08 | consumed tokens: 1180467200.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T07:52:09 | step: 144200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.114544572075829e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.0 | consumed tokens: 1181286400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T07:52:30 | step: 144300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.113352406420745e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.53 | consumed tokens: 1182105600.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:52:51 | step: 144400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.11215987696778e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.36 | consumed tokens: 1182924800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:53:11 | step: 144500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.110966619919054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.19 | consumed tokens: 1183744000.0 | grad norm avg: 0.84 | grad norm last: 0.93 | 
2025-12-30T07:53:32 | step: 144600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.109772635274567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 4.22 | consumed tokens: 1184563200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T07:53:52 | step: 144700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.108577923034318e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.52 | consumed tokens: 1185382400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:54:13 | step: 144800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.107382846996188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.27 | consumed tokens: 1186201600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:54:34 | step: 144900 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 4.106187043362297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.5 | consumed tokens: 1187020800.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T07:54:54 | step: 145000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.1049905121326447e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.77 | consumed tokens: 1187840000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T07:55:17 | step: 145100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.103793253307231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.39 | consumed tokens: 1188659200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T07:55:37 | step: 145200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.102595630683936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.28 | consumed tokens: 1189478400.0 | grad norm avg: 0.84 | grad norm last: 0.96 | 
2025-12-30T07:55:58 | step: 145300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.10139728046488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.22 | consumed tokens: 1190297600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T07:56:19 | step: 145400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.100198202650063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 1191116800.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T07:56:39 | step: 145500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.0989987610373646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.36 | consumed tokens: 1191936000.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T07:57:00 | step: 145600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.097798591828905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.66 | consumed tokens: 1192755200.0 | grad norm avg: 0.83 | grad norm last: 0.77 | 
2025-12-30T07:57:21 | step: 145700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.096597695024684e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.8 | consumed tokens: 1193574400.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T07:57:41 | step: 145800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.0953964344225824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.05 | consumed tokens: 1194393600.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-30T07:58:02 | step: 145900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.0941940824268386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.06 | consumed tokens: 1195212800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T07:58:22 | step: 146000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.092991366633214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.2 | consumed tokens: 1196032000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T07:58:43 | step: 146100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.091788287041709e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.22 | consumed tokens: 1196851200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T07:59:04 | step: 146200 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 4.0905841160565615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.44 | consumed tokens: 1197670400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T07:59:25 | step: 146300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.0893795812735334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.12 | consumed tokens: 1198489600.0 | grad norm avg: 0.84 | grad norm last: 0.92 | 
2025-12-30T07:59:45 | step: 146400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.088174318894744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.97 | consumed tokens: 1199308800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:00:06 | step: 146500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.086968692718074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.3 | consumed tokens: 1200128000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:00:27 | step: 146600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.085762338945642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1200947200.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T08:00:47 | step: 146700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.084555257577449e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.12 | consumed tokens: 1201766400.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T08:01:08 | step: 146800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.0833474486134946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.77 | consumed tokens: 1202585600.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T08:01:29 | step: 146900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.0821392758516595e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.03 | consumed tokens: 1203404800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T08:01:49 | step: 147000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.080930375494063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.73 | consumed tokens: 1204224000.0 | grad norm avg: 0.84 | grad norm last: 0.75 | 
2025-12-30T08:02:10 | step: 147100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.079720747540705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.2 | consumed tokens: 1205043200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:02:31 | step: 147200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.078510755789466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.25 | consumed tokens: 1205862400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:02:51 | step: 147300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.077300036442466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.03 | consumed tokens: 1206681600.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-30T08:03:12 | step: 147400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.0760885894997045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.05 | consumed tokens: 1207500800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:03:33 | step: 147500 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 4.074876778759062e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.17 | consumed tokens: 1208320000.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:03:54 | step: 147600 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.0736642404226586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.22 | consumed tokens: 1209139200.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T08:04:14 | step: 147700 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.0724509744904935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 1209958400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:04:35 | step: 147800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.071237344760448e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 1210777600.0 | grad norm avg: 0.83 | grad norm last: 0.82 | 
2025-12-30T08:04:56 | step: 147900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.07002262363676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 1211596800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:05:16 | step: 148000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.068807902513072e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.23 | consumed tokens: 1212416000.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2025-12-30T08:05:37 | step: 148100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.067592089995742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.34 | consumed tokens: 1213235200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:05:58 | step: 148200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.066375913680531e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.11 | consumed tokens: 1214054400.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:06:18 | step: 148300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.065159009769559e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 4.47 | consumed tokens: 1214873600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:06:39 | step: 148400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.063941378262825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.38 | consumed tokens: 1215692800.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T08:06:59 | step: 148500 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 4.062723382958211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.58 | consumed tokens: 1216512000.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:07:20 | step: 148600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.061504660057835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.02 | consumed tokens: 1217331200.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T08:07:41 | step: 148700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.060285573359579e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.61 | consumed tokens: 1218150400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:08:02 | step: 148800 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 4.059065759065561e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.38 | consumed tokens: 1218969600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:08:22 | step: 148900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.057845217175782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.89 | consumed tokens: 1219788800.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T08:08:43 | step: 149000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.056623947690241e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.52 | consumed tokens: 1220608000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:09:03 | step: 149100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.0554023144068196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.62 | consumed tokens: 1221427200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:09:24 | step: 149200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.054179953527637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.83 | consumed tokens: 1222246400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:09:45 | step: 149300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.0529568650526926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.23 | consumed tokens: 1223065600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:10:05 | step: 149400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.0517334127798676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.69 | consumed tokens: 1223884800.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2025-12-30T08:10:26 | step: 149500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 4.050509232911281e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1224704000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:10:46 | step: 149600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.049284689244814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.2 | consumed tokens: 1225523200.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T08:11:07 | step: 149700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.048059054184705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.38 | consumed tokens: 1226342400.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:11:28 | step: 149800 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.046833055326715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.45 | consumed tokens: 1227161600.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T08:11:49 | step: 149900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.0456066926708445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.89 | consumed tokens: 1227980800.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T08:12:09 | step: 150000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.0443796024192125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.31 | consumed tokens: 1228800000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T08:12:32 | step: 150100 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 4.043151784571819e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.64 | consumed tokens: 1229619200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T08:12:53 | step: 150200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.041923239128664e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.11 | consumed tokens: 1230438400.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T08:13:13 | step: 150300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.0406943298876286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.45 | consumed tokens: 1231257600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:13:34 | step: 150400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.0394646930508316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.48 | consumed tokens: 1232076800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:13:54 | step: 150500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.038234692416154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.72 | consumed tokens: 1232896000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:14:15 | step: 150600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.037003964185715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.84 | consumed tokens: 1233715200.0 | grad norm avg: 0.84 | grad norm last: 0.94 | 
2025-12-30T08:14:36 | step: 150700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.035772508359514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.06 | consumed tokens: 1234534400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:14:56 | step: 150800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.034540688735433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.97 | consumed tokens: 1235353600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:15:17 | step: 150900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.03330814151559e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.02 | consumed tokens: 1236172800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:15:37 | step: 151000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.032074866699986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.53 | consumed tokens: 1236992000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:15:58 | step: 151100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.0308412280865014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.38 | consumed tokens: 1237811200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:16:19 | step: 151200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.029606861877255e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.08 | consumed tokens: 1238630400.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T08:16:39 | step: 151300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.0283717680722475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.72 | consumed tokens: 1239449600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:17:00 | step: 151400 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.027136310469359e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1240268800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:17:21 | step: 151500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.0259001252707094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.97 | consumed tokens: 1241088000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:17:42 | step: 151600 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.024663576274179e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.28 | consumed tokens: 1241907200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T08:18:02 | step: 151700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.023425935884006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.27 | consumed tokens: 1242726400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:18:23 | step: 151800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.022188295493834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.22 | consumed tokens: 1243545600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:18:43 | step: 151900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.020949563710019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.91 | consumed tokens: 1244364800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:19:04 | step: 152000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.0197104681283236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.31 | consumed tokens: 1245184000.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:19:25 | step: 152100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.0184710087487474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.91 | consumed tokens: 1246003200.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T08:19:45 | step: 152200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.017230457975529e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.62 | consumed tokens: 1246822400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:20:06 | step: 152300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.01598954340443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 4.22 | consumed tokens: 1247641600.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T08:20:26 | step: 152400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.0147482650354505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.75 | consumed tokens: 1248460800.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T08:20:47 | step: 152500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.0135062590707093e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.78 | consumed tokens: 1249280000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T08:21:08 | step: 152600 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.012263525510207e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.38 | consumed tokens: 1250099200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:21:29 | step: 152700 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.011020064353943e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.53 | consumed tokens: 1250918400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T08:21:49 | step: 152800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.009776239399798e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.38 | consumed tokens: 1251737600.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:22:10 | step: 152900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.008532050647773e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.36 | consumed tokens: 1252556800.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:22:31 | step: 153000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.007287134299986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.58 | consumed tokens: 1253376000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:22:52 | step: 153100 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 4.006041490356438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.08 | consumed tokens: 1254195200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:23:12 | step: 153200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.004795118817128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.77 | consumed tokens: 1255014400.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:23:33 | step: 153300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.003548383479938e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.28 | consumed tokens: 1255833600.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T08:23:54 | step: 153400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.002300920546986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.95 | consumed tokens: 1256652800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:24:14 | step: 153500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.001053093816154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.62 | consumed tokens: 1257472000.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T08:24:35 | step: 153600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.99980453948956e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 4.19 | consumed tokens: 1258291200.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T08:24:56 | step: 153700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.998555621365085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.92 | consumed tokens: 1259110400.0 | grad norm avg: 0.85 | grad norm last: 0.95 | 
2025-12-30T08:25:16 | step: 153800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.997305975644849e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.72 | consumed tokens: 1259929600.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T08:25:37 | step: 153900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.996055602328852e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.3 | consumed tokens: 1260748800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:25:58 | step: 154000 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.994804865214974e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.22 | consumed tokens: 1261568000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T08:26:18 | step: 154100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.993553400505334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.61 | consumed tokens: 1262387200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:26:39 | step: 154200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.992301208199933e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1263206400.0 | grad norm avg: 0.83 | grad norm last: 0.84 | 
2025-12-30T08:26:59 | step: 154300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.9910486520966515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.98 | consumed tokens: 1264025600.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T08:27:20 | step: 154400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.9897953683976084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 2.98 | consumed tokens: 1264844800.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T08:27:41 | step: 154500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.9885417209006846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.39 | consumed tokens: 1265664000.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:28:01 | step: 154600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.9872873458079994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.23 | consumed tokens: 1266483200.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T08:28:22 | step: 154700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.986032243119553e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.47 | consumed tokens: 1267302400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:28:42 | step: 154800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.9847767766332254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.72 | consumed tokens: 1268121600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:29:03 | step: 154900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.983520946349017e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.31 | consumed tokens: 1268940800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:29:24 | step: 155000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.982264024671167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.84 | consumed tokens: 1269760000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:29:46 | step: 155100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.981006739195436e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.83 | consumed tokens: 1270579200.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T08:30:07 | step: 155200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.9797490899218246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.67 | consumed tokens: 1271398400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:30:27 | step: 155300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.9784907130524516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.25 | consumed tokens: 1272217600.0 | grad norm avg: 0.84 | grad norm last: 0.93 | 
2025-12-30T08:30:48 | step: 155400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.977231608587317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.34 | consumed tokens: 1273036800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:31:09 | step: 155500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.975972140324302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.62 | consumed tokens: 1273856000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T08:31:30 | step: 155600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.9747119444655254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.23 | train loss last: 3.41 | consumed tokens: 1274675200.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:31:50 | step: 155700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.973451384808868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.45 | consumed tokens: 1275494400.0 | grad norm avg: 0.84 | grad norm last: 0.91 | 
2025-12-30T08:32:11 | step: 155800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.9721900975564495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.02 | consumed tokens: 1276313600.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T08:32:31 | step: 155900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.97092844650615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.59 | consumed tokens: 1277132800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:32:52 | step: 156000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.9696657040622085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.17 | consumed tokens: 1277952000.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T08:33:13 | step: 156100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.968402961618267e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.17 | consumed tokens: 1278771200.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:33:34 | step: 156200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.967139127780683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.34 | consumed tokens: 1279590400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T08:33:54 | step: 156300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.9658752939431e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.53 | consumed tokens: 1280409600.0 | grad norm avg: 0.85 | grad norm last: 1.28 | 
2025-12-30T08:34:15 | step: 156400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.964610368711874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.98 | consumed tokens: 1281228800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:34:36 | step: 156500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.9633450796827674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1282048000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T08:34:57 | step: 156600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.96207942685578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 4.31 | consumed tokens: 1282867200.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:35:17 | step: 156700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.9608130464330316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.55 | consumed tokens: 1283686400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T08:35:37 | step: 156800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.9595459384145215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.75 | consumed tokens: 1284505600.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T08:35:58 | step: 156900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.958278466598131e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.23 | consumed tokens: 1285324800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:36:19 | step: 157000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.9570102671859786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.88 | consumed tokens: 1286144000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T08:36:39 | step: 157100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.955741703975946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.28 | consumed tokens: 1286963200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:37:00 | step: 157200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.9544724131701514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 2.94 | consumed tokens: 1287782400.0 | grad norm avg: 0.84 | grad norm last: 1.55 | 
2025-12-30T08:37:20 | step: 157300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.953202394768596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.61 | consumed tokens: 1288601600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T08:37:41 | step: 157400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.951932012569159e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.83 | consumed tokens: 1289420800.0 | grad norm avg: 0.84 | grad norm last: 0.75 | 
2025-12-30T08:38:02 | step: 157500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.950661266571842e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.25 | consumed tokens: 1290240000.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T08:38:22 | step: 157600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.9493897929787636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.78 | consumed tokens: 1291059200.0 | grad norm avg: 0.84 | grad norm last: 0.93 | 
2025-12-30T08:38:43 | step: 157700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.9481175917899236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.16 | consumed tokens: 1291878400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:39:03 | step: 157800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.946845026803203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.14 | consumed tokens: 1292697600.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:39:24 | step: 157900 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.945571734220721e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1293516800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:39:45 | step: 158000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.944298077840358e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.28 | consumed tokens: 1294336000.0 | grad norm avg: 0.85 | grad norm last: 0.75 | 
2025-12-30T08:40:06 | step: 158100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.943023693864234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.94 | consumed tokens: 1295155200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:40:26 | step: 158200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.941748582292348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.97 | consumed tokens: 1295974400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T08:40:47 | step: 158300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.940473106922582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.39 | consumed tokens: 1296793600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:41:07 | step: 158400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.939197267754935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.48 | consumed tokens: 1297612800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:41:28 | step: 158500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.937920700991526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.3 | consumed tokens: 1298432000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:41:49 | step: 158600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.9366434066323563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.36 | consumed tokens: 1299251200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:42:09 | step: 158700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.935365748475306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.78 | consumed tokens: 1300070400.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T08:42:30 | step: 158800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.934087362722494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.02 | consumed tokens: 1300889600.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:42:50 | step: 158900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.932808613171801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.66 | consumed tokens: 1301708800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:43:11 | step: 159000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.9315294998232275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.72 | consumed tokens: 1302528000.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2025-12-30T08:43:32 | step: 159100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.930249295081012e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.61 | consumed tokens: 1303347200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:43:53 | step: 159200 | train samples/s: 81.8 | train mfu (16-bit): -1.0 | lr mean: 3.9289690903387964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1304166400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:44:14 | step: 159300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.927687794202939e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.2 | consumed tokens: 1304985600.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T08:44:34 | step: 159400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.9264061342692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.25 | consumed tokens: 1305804800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T08:44:55 | step: 159500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.925124110537581e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.09 | consumed tokens: 1306624000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:45:15 | step: 159600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.9238413592102006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.08 | consumed tokens: 1307443200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T08:45:36 | step: 159700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.9225582440849394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.66 | consumed tokens: 1308262400.0 | grad norm avg: 0.85 | grad norm last: 1.0 | 
2025-12-30T08:45:56 | step: 159800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.921274401363917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.45 | consumed tokens: 1309081600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:46:17 | step: 159900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.9199898310471326e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.03 | consumed tokens: 1309900800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:46:37 | step: 160000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.918704896932468e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.45 | consumed tokens: 1310720000.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T08:46:59 | step: 160100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.917419599019922e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.39 | consumed tokens: 1311539200.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T08:47:20 | step: 160200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.9161335735116154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.02 | consumed tokens: 1312358400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:47:41 | step: 160300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.914846820407547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.61 | consumed tokens: 1313177600.0 | grad norm avg: 0.84 | grad norm last: 0.86 | 
2025-12-30T08:48:01 | step: 160400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.913559703505598e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.16 | consumed tokens: 1313996800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:48:22 | step: 160500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.9122718590078875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.34 | consumed tokens: 1314816000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T08:48:43 | step: 160600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.9109836507122964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.47 | consumed tokens: 1315635200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T08:49:03 | step: 160700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.9096950786188245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.22 | consumed tokens: 1316454400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:49:24 | step: 160800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.908405778929591e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.02 | consumed tokens: 1317273600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:49:45 | step: 160900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.9071157516445965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.42 | consumed tokens: 1318092800.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:50:05 | step: 161000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.905825360561721e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.59 | consumed tokens: 1318912000.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2025-12-30T08:50:26 | step: 161100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.904534241883084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.11 | consumed tokens: 1319731200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T08:50:46 | step: 161200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.9032427594065666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.42 | consumed tokens: 1320550400.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T08:51:07 | step: 161300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.9019505493342876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.33 | consumed tokens: 1321369600.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T08:51:27 | step: 161400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.900657975464128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.02 | consumed tokens: 1322188800.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:51:48 | step: 161500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.8993650377960876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.88 | consumed tokens: 1323008000.0 | grad norm avg: 0.83 | grad norm last: 0.87 | 
2025-12-30T08:52:08 | step: 161600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.898071008734405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.38 | consumed tokens: 1323827200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:52:29 | step: 161700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.8967769796727225e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1324646400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T08:52:50 | step: 161800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.8954822230152786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.75 | consumed tokens: 1325465600.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T08:53:10 | step: 161900 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.894186738762073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.98 | consumed tokens: 1326284800.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T08:53:31 | step: 162000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.892890890710987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.28 | consumed tokens: 1327104000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T08:53:51 | step: 162100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.89159431506414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.14 | consumed tokens: 1327923200.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T08:54:12 | step: 162200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.8902973756194115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.42 | consumed tokens: 1328742400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:54:33 | step: 162300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.8890000723768026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.36 | consumed tokens: 1329561600.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T08:54:53 | step: 162400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.8877016777405515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.41 | consumed tokens: 1330380800.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T08:55:14 | step: 162500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.8864032831043005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.11 | consumed tokens: 1331200000.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T08:55:34 | step: 162600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.885104160872288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.27 | consumed tokens: 1332019200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:55:55 | step: 162700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.883804311044514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 4.28 | consumed tokens: 1332838400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T08:56:16 | step: 162800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.8825040974188596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.34 | consumed tokens: 1333657600.0 | grad norm avg: 0.84 | grad norm last: 0.95 | 
2025-12-30T08:56:36 | step: 162900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.881203519995324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.09 | consumed tokens: 1334476800.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:56:57 | step: 163000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.8799022149760276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.62 | consumed tokens: 1335296000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T08:57:17 | step: 163100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.8786001823609695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.09 | consumed tokens: 1336115200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T08:57:38 | step: 163200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.8772977859480307e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.45 | consumed tokens: 1336934400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T08:57:59 | step: 163300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.875995025737211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.45 | consumed tokens: 1337753600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T08:58:19 | step: 163400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.87469153793063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 4.0 | consumed tokens: 1338572800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T08:58:40 | step: 163500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.8733876863261685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.27 | consumed tokens: 1339392000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T08:59:00 | step: 163600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.8720831071259454e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.16 | consumed tokens: 1340211200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T08:59:21 | step: 163700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.8707781641278416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.45 | consumed tokens: 1341030400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T08:59:42 | step: 163800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.8694724935339764e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.3 | consumed tokens: 1341849600.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:00:03 | step: 163900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.8681664591422305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.5 | consumed tokens: 1342668800.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T09:00:23 | step: 164000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.866859697154723e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.98 | consumed tokens: 1343488000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T09:00:44 | step: 164100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.865552571369335e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.42 | consumed tokens: 1344307200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T09:01:04 | step: 164200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.8642447179881856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.36 | consumed tokens: 1345126400.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T09:01:25 | step: 164300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.8629365008091554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.17 | consumed tokens: 1345945600.0 | grad norm avg: 0.85 | grad norm last: 0.77 | 
2025-12-30T09:01:46 | step: 164400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.8616279198322445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.78 | consumed tokens: 1346764800.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:02:06 | step: 164500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.860318611259572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.91 | consumed tokens: 1347584000.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T09:02:27 | step: 164600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.8590085750911385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.16 | consumed tokens: 1348403200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T09:02:48 | step: 164700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.857698175124824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.7 | consumed tokens: 1349222400.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2025-12-30T09:03:08 | step: 164800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.856387411360629e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.42 | consumed tokens: 1350041600.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:03:29 | step: 164900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.8550759200006723e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.06 | consumed tokens: 1350860800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T09:03:49 | step: 165000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.853764064842835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.73 | consumed tokens: 1351680000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:04:12 | step: 165100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.8524514820892364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.91 | consumed tokens: 1352499200.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:04:32 | step: 165200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.851138535537757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.78 | consumed tokens: 1353318400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:04:53 | step: 165300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.849824861390516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.69 | consumed tokens: 1354137600.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T09:05:14 | step: 165400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.8485108234453946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.59 | consumed tokens: 1354956800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:05:34 | step: 165500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.8471964217023924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.19 | consumed tokens: 1355776000.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T09:05:54 | step: 165600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.845881292363629e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.89 | consumed tokens: 1356595200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:06:15 | step: 165700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.844565435429104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.44 | consumed tokens: 1357414400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:06:36 | step: 165800 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.843249214696698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.33 | consumed tokens: 1358233600.0 | grad norm avg: 0.85 | grad norm last: 0.76 | 
2025-12-30T09:06:56 | step: 165900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.8419326301664114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.67 | consumed tokens: 1359052800.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T09:07:17 | step: 166000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.8406153180403635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.17 | consumed tokens: 1359872000.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T09:07:37 | step: 166100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.839297642116435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.17 | consumed tokens: 1360691200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:07:58 | step: 166200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.8379796023946255e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.81 | consumed tokens: 1361510400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:08:19 | step: 166300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.836660835077055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.52 | consumed tokens: 1362329600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:08:39 | step: 166400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.8353413401637226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.73 | consumed tokens: 1363148800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:09:00 | step: 166500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.83402148145251e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.42 | consumed tokens: 1363968000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:09:20 | step: 166600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.832701258943416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.22 | consumed tokens: 1364787200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:09:41 | step: 166700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.831380308838561e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.81 | consumed tokens: 1365606400.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2025-12-30T09:10:02 | step: 166800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.8300589949358255e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1366425600.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:10:22 | step: 166900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.828737317235209e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.36 | consumed tokens: 1367244800.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T09:10:43 | step: 167000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.827414911938831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1368064000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:11:04 | step: 167100 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 3.826091779046692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.09 | consumed tokens: 1368883200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:11:24 | step: 167200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.824768282356672e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1369702400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:11:45 | step: 167300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.823444421868771e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.81 | consumed tokens: 1370521600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:12:05 | step: 167400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.82212019758299e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.41 | consumed tokens: 1371340800.0 | grad norm avg: 0.85 | grad norm last: 1.21 | 
2025-12-30T09:12:26 | step: 167500 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.8207948819035664e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.11 | consumed tokens: 1372160000.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:12:46 | step: 167600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.819469566224143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.36 | consumed tokens: 1372979200.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:13:07 | step: 167700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.818143522948958e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.25 | consumed tokens: 1373798400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:13:28 | step: 167800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.8168171158758923e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1374617600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:13:48 | step: 167900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.815489981207065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.33 | consumed tokens: 1375436800.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T09:14:09 | step: 168000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.8141624827403575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.45 | consumed tokens: 1376256000.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:14:29 | step: 168100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.812834256677888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.09 | consumed tokens: 1377075200.0 | grad norm avg: 0.84 | grad norm last: 0.89 | 
2025-12-30T09:14:50 | step: 168200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.8115056668175384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.91 | consumed tokens: 1377894400.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T09:15:10 | step: 168300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.810176713159308e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.34 | consumed tokens: 1378713600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:15:31 | step: 168400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.808847031905316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.75 | consumed tokens: 1379532800.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:15:52 | step: 168500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.807516986853443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.09 | consumed tokens: 1380352000.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T09:16:12 | step: 168600 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.806186214205809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.12 | consumed tokens: 1381171200.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:16:33 | step: 168700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.804855077760294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.06 | consumed tokens: 1381990400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T09:16:54 | step: 168800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.8035235775168985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.88 | consumed tokens: 1382809600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T09:17:15 | step: 168900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.8021913496777415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.75 | consumed tokens: 1383628800.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T09:17:35 | step: 169000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.800858758040704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.7 | consumed tokens: 1384448000.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T09:17:56 | step: 169100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.7995258026057854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.86 | consumed tokens: 1385267200.0 | grad norm avg: 0.88 | grad norm last: 0.8 | 
2025-12-30T09:18:17 | step: 169200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.7981921195751056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.83 | consumed tokens: 1386086400.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T09:18:37 | step: 169300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.7968577089486644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.88 | consumed tokens: 1386905600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:18:58 | step: 169400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.795523298322223e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.11 | consumed tokens: 1387724800.0 | grad norm avg: 0.85 | grad norm last: 0.94 | 
2025-12-30T09:19:19 | step: 169500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.79418779630214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.33 | consumed tokens: 1388544000.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T09:19:39 | step: 169600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.7928522942820564e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.67 | consumed tokens: 1389363200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:20:00 | step: 169700 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 3.7915160646662116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.66 | consumed tokens: 1390182400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2025-12-30T09:20:21 | step: 169800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.790179471252486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.27 | consumed tokens: 1391001600.0 | grad norm avg: 0.84 | grad norm last: 0.87 | 
2025-12-30T09:20:41 | step: 169900 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.788842150242999e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.05 | consumed tokens: 1391820800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T09:21:02 | step: 170000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.7875044654356316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.98 | consumed tokens: 1392640000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:21:24 | step: 170100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.786166416830383e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1393459200.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:21:45 | step: 170200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.7848276406293735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.62 | consumed tokens: 1394278400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:22:06 | step: 170300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.783488500630483e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.91 | consumed tokens: 1395097600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:22:26 | step: 170400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.782148633035831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.97 | consumed tokens: 1395916800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:22:47 | step: 170500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.7808084016432986e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1396736000.0 | grad norm avg: 0.85 | grad norm last: 1.0 | 
2025-12-30T09:23:08 | step: 170600 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.779467806452885e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.25 | consumed tokens: 1397555200.0 | grad norm avg: 0.85 | grad norm last: 0.91 | 
2025-12-30T09:23:29 | step: 170700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.7781264836667106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 1398374400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T09:23:49 | step: 170800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.776784797082655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 2.88 | consumed tokens: 1399193600.0 | grad norm avg: 0.84 | grad norm last: 0.94 | 
2025-12-30T09:24:10 | step: 170900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.775442746700719e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.55 | consumed tokens: 1400012800.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T09:24:31 | step: 171000 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 3.7740999687230214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.94 | consumed tokens: 1400832000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:24:52 | step: 171100 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.772756826947443e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.84 | consumed tokens: 1401651200.0 | grad norm avg: 0.84 | grad norm last: 0.79 | 
2025-12-30T09:25:12 | step: 171200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.771413321373984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1402470400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:25:33 | step: 171300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.770069088204764e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.86 | consumed tokens: 1403289600.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T09:25:53 | step: 171400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.768724491237663e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.3 | consumed tokens: 1404108800.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:26:14 | step: 171500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.7673791666748e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1404928000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:26:35 | step: 171600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.766033478314057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.44 | consumed tokens: 1405747200.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T09:26:55 | step: 171700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.764687426155433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.8 | consumed tokens: 1406566400.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T09:27:16 | step: 171800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.763340646401048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.39 | consumed tokens: 1407385600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:27:37 | step: 171900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.7619935028487816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.88 | consumed tokens: 1408204800.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:27:57 | step: 172000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.760645995498635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.88 | consumed tokens: 1409024000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:28:18 | step: 172100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.759297760552727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.05 | consumed tokens: 1409843200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:28:39 | step: 172200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.757949161808938e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.45 | consumed tokens: 1410662400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:28:59 | step: 172300 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.756600199267268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.66 | consumed tokens: 1411481600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:29:20 | step: 172400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.755250509129837e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1412300800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T09:29:41 | step: 172500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.7539004551945254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.59 | consumed tokens: 1413120000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:30:01 | step: 172600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.752550037461333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.17 | consumed tokens: 1413939200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:30:22 | step: 172700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.751198892132379e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.03 | consumed tokens: 1414758400.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:30:43 | step: 172800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.7498473830055445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.19 | consumed tokens: 1415577600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:31:03 | step: 172900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.748495510080829e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.67 | consumed tokens: 1416396800.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:31:24 | step: 173000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.7471429095603526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.91 | consumed tokens: 1417216000.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:31:45 | step: 173100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.745789945241995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.03 | consumed tokens: 1418035200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:32:05 | step: 173200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.744436253327876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.62 | consumed tokens: 1418854400.0 | grad norm avg: 0.85 | grad norm last: 0.95 | 
2025-12-30T09:32:26 | step: 173300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.7430825614137575e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.41 | consumed tokens: 1419673600.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:32:46 | step: 173400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.7417277781059965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 1420492800.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T09:33:07 | step: 173500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.7403729947982356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.66 | consumed tokens: 1421312000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:33:28 | step: 173600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.739017483894713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1422131200.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T09:33:48 | step: 173700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.73766160919331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.14 | consumed tokens: 1422950400.0 | grad norm avg: 0.86 | grad norm last: 0.94 | 
2025-12-30T09:34:09 | step: 173800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.7363053706940264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.61 | consumed tokens: 1423769600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:34:30 | step: 173900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.734948404598981e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.48 | consumed tokens: 1424588800.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T09:34:50 | step: 174000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.733591074706055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.75 | consumed tokens: 1425408000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:35:11 | step: 174100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.7322333810152486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.27 | consumed tokens: 1426227200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:35:31 | step: 174200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.7308749597286806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.89 | consumed tokens: 1427046400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:35:52 | step: 174300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.729516174644232e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.94 | consumed tokens: 1427865600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:36:13 | step: 174400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.728157025761902e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.23 | consumed tokens: 1428684800.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:36:34 | step: 174500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.7267971492838115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.12 | consumed tokens: 1429504000.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T09:36:54 | step: 174600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.72543690900784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.94 | consumed tokens: 1430323200.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T09:37:15 | step: 174700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.7240763049339876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 4.03 | consumed tokens: 1431142400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:37:36 | step: 174800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.722714973264374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.11 | consumed tokens: 1431961600.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T09:37:57 | step: 174900 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 3.7213532777968794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1432780800.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T09:38:17 | step: 175000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.719991218531504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.45 | consumed tokens: 1433600000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T09:38:40 | step: 175100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.7186287954682484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.7 | consumed tokens: 1434419200.0 | grad norm avg: 0.84 | grad norm last: 0.77 | 
2025-12-30T09:39:00 | step: 175200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.717265644809231e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.95 | consumed tokens: 1435238400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T09:39:21 | step: 175300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.715902130352333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.27 | consumed tokens: 1436057600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:39:41 | step: 175400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.714537888299674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.19 | consumed tokens: 1436876800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:40:02 | step: 175500 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.7131736462470144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1437696000.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T09:40:23 | step: 175600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.7118086765985936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.22 | consumed tokens: 1438515200.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T09:40:43 | step: 175700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.7104429793544114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.88 | consumed tokens: 1439334400.0 | grad norm avg: 0.84 | grad norm last: 0.85 | 
2025-12-30T09:41:04 | step: 175800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.709077282110229e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.89 | consumed tokens: 1440153600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:41:25 | step: 175900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.7077108572702855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.81 | consumed tokens: 1440972800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T09:41:46 | step: 176000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.706344068632461e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 1441792000.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:42:06 | step: 176100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.7049765523988754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.94 | consumed tokens: 1442611200.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:42:27 | step: 176200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.703608672367409e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 1443430400.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T09:42:48 | step: 176300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.702240428538062e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1444249600.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T09:43:08 | step: 176400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.700871820910834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 1445068800.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:43:29 | step: 176500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.6995024856878445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.77 | consumed tokens: 1445888000.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T09:43:49 | step: 176600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.698133150464855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1446707200.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T09:44:10 | step: 176700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.696762723848224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.64 | consumed tokens: 1447526400.0 | grad norm avg: 0.85 | grad norm last: 0.78 | 
2025-12-30T09:44:30 | step: 176800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.695392297231592e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.59 | consumed tokens: 1448345600.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:44:51 | step: 176900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.6940211430191994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.39 | consumed tokens: 1449164800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T09:45:12 | step: 177000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.692649625008926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.84 | consumed tokens: 1449984000.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T09:45:32 | step: 177100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.6912777432007715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.41 | consumed tokens: 1450803200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:45:53 | step: 177200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.689905133796856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.64 | consumed tokens: 1451622400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T09:46:13 | step: 177300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.6885321605950594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.8 | consumed tokens: 1452441600.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:46:34 | step: 177400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.687158823595382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 1453260800.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T09:46:55 | step: 177500 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 3.6857851227978244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.08 | consumed tokens: 1454080000.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T09:47:16 | step: 177600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.684410694404505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.39 | consumed tokens: 1454899200.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T09:47:36 | step: 177700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.683035902213305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.92 | consumed tokens: 1455718400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2025-12-30T09:47:57 | step: 177800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.6816607462242246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.03 | consumed tokens: 1456537600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:48:17 | step: 177900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.6802848626393825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.5 | consumed tokens: 1457356800.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T09:48:38 | step: 178000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.6789089790545404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.72 | consumed tokens: 1458176000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T09:48:59 | step: 178100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.677532367873937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.23 | consumed tokens: 1458995200.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T09:49:19 | step: 178200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.676155029097572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.81 | consumed tokens: 1459814400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:49:40 | step: 178300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.674777690321207e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.09 | consumed tokens: 1460633600.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:50:01 | step: 178400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.673399623949081e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.56 | consumed tokens: 1461452800.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T09:50:22 | step: 178500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.6720211937790737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.97 | consumed tokens: 1462272000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T09:50:42 | step: 178600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.670642399811186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.27 | consumed tokens: 1463091200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:51:03 | step: 178700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.669262878247537e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.91 | consumed tokens: 1463910400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:51:24 | step: 178800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.6678833566838875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.38 | consumed tokens: 1464729600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T09:51:44 | step: 178900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.666502743726596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.47 | consumed tokens: 1465548800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T09:52:05 | step: 179000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.665122130769305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.28 | consumed tokens: 1466368000.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T09:52:26 | step: 179100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.663741154014133e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.69 | consumed tokens: 1467187200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T09:52:46 | step: 179200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.6623594496631995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.67 | consumed tokens: 1468006400.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T09:53:07 | step: 179300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.6609773815143853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.5 | consumed tokens: 1468825600.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2025-12-30T09:53:28 | step: 179400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.6595949495676905e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.78 | consumed tokens: 1469644800.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T09:53:48 | step: 179500 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.658211790025234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.2 | consumed tokens: 1470464000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:54:09 | step: 179600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.656828266684897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.91 | consumed tokens: 1471283200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:54:30 | step: 179700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.6554443795466796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.59 | consumed tokens: 1472102400.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T09:54:50 | step: 179800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.654060128610581e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.59 | consumed tokens: 1472921600.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T09:55:11 | step: 179900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.652675513876602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.97 | consumed tokens: 1473740800.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T09:55:32 | step: 180000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.6512901715468615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1474560000.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T09:55:54 | step: 180100 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 3.64990446541924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.31 | consumed tokens: 1475379200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:56:15 | step: 180200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.6485183954937384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.48 | consumed tokens: 1476198400.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T09:56:35 | step: 180300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.647131597972475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.61 | consumed tokens: 1477017600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:56:56 | step: 180400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.6457448004512116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.66 | consumed tokens: 1477836800.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T09:57:17 | step: 180500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.644357275334187e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1478656000.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:57:37 | step: 180600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.6429693864192814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1479475200.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T09:57:58 | step: 180700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.641581133706495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.78 | consumed tokens: 1480294400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T09:58:19 | step: 180800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.6401921533979475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.34 | consumed tokens: 1481113600.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T09:58:39 | step: 180900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.638802809291519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.89 | consumed tokens: 1481932800.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T09:59:00 | step: 181000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.63741310138721e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.05 | consumed tokens: 1482752000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T09:59:21 | step: 181100 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.6360230296850204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.89 | consumed tokens: 1483571200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T09:59:42 | step: 181200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.63463259418495e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.7 | consumed tokens: 1484390400.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T10:00:02 | step: 181300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.633241431089118e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.31 | consumed tokens: 1485209600.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T10:00:23 | step: 181400 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 3.6318499041954055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.62 | consumed tokens: 1486028800.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:00:44 | step: 181500 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.630458013503812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.44 | consumed tokens: 1486848000.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T10:01:05 | step: 181600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.629065759014338e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.02 | consumed tokens: 1487667200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T10:01:25 | step: 181700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.6276731407269835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.58 | consumed tokens: 1488486400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:01:46 | step: 181800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.6262797948438674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 1489305600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:02:06 | step: 181900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.6248860851628706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.5 | consumed tokens: 1490124800.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:02:27 | step: 182000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.623492011683993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.02 | consumed tokens: 1490944000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:02:48 | step: 182100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.622097574407235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.39 | consumed tokens: 1491763200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:03:09 | step: 182200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.620702409534715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.92 | consumed tokens: 1492582400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:03:29 | step: 182300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.6193072446621954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.19 | consumed tokens: 1493401600.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:03:50 | step: 182400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.6179113521939144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.17 | consumed tokens: 1494220800.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:04:11 | step: 182500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.6165150959277526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.48 | consumed tokens: 1495040000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:04:31 | step: 182600 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.615118112065829e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.53 | consumed tokens: 1495859200.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T10:04:52 | step: 182700 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 3.613721128203906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.28 | consumed tokens: 1496678400.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:05:13 | step: 182800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.6123234167462215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.06 | consumed tokens: 1497497600.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T10:05:34 | step: 182900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.610925705288537e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.02 | consumed tokens: 1498316800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:05:54 | step: 183000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.60952690243721e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.84 | consumed tokens: 1499136000.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T10:06:15 | step: 183100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.608128099585883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.67 | consumed tokens: 1499955200.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T10:06:36 | step: 183200 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.606728932936676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.86 | consumed tokens: 1500774400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:06:57 | step: 183300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.605329038691707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.66 | consumed tokens: 1501593600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:07:17 | step: 183400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.603929144446738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.86 | consumed tokens: 1502412800.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:07:38 | step: 183500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.602528522606008e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.47 | consumed tokens: 1503232000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:07:58 | step: 183600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.601127173169516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.41 | consumed tokens: 1504051200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T10:08:19 | step: 183700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.599725823733024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1504870400.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T10:08:40 | step: 183800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.598324110498652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.58 | consumed tokens: 1505689600.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T10:09:00 | step: 183900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.596921669668518e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.75 | consumed tokens: 1506508800.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2025-12-30T10:09:21 | step: 184000 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.5955188650405034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.66 | consumed tokens: 1507328000.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T10:09:42 | step: 184100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.594115696614608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1508147200.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T10:10:02 | step: 184200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.592712164390832e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.7 | consumed tokens: 1508966400.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:10:23 | step: 184300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.5913082683691755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.41 | consumed tokens: 1509785600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:10:44 | step: 184400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.5899036447517574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.34 | consumed tokens: 1510604800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:11:04 | step: 184500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.5884986573364586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.7 | consumed tokens: 1511424000.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T10:11:25 | step: 184600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.58709366992116e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.06 | consumed tokens: 1512243200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:11:46 | step: 184700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.5856879549100995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.64 | consumed tokens: 1513062400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:12:06 | step: 184800 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.584281512303278e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.95 | consumed tokens: 1513881600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:12:27 | step: 184900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.582875069696456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.62 | consumed tokens: 1514700800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:12:48 | step: 185000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.581468263291754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.2 | consumed tokens: 1515520000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T10:13:10 | step: 185100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.58006072929129e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.98 | consumed tokens: 1516339200.0 | grad norm avg: 0.86 | grad norm last: 0.97 | 
2025-12-30T10:13:31 | step: 185200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.5786528314929456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.05 | consumed tokens: 1517158400.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T10:13:52 | step: 185300 | train samples/s: 81.9 | train mfu (16-bit): -1.0 | lr mean: 3.5772445698967203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.42 | consumed tokens: 1517977600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:14:12 | step: 185400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.5758359445026144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 1518796800.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:14:33 | step: 185500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.574426955310628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.0 | consumed tokens: 1519616000.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:14:54 | step: 185600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.57301723852288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.59 | consumed tokens: 1520435200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:15:14 | step: 185700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.571607521735132e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.22 | train loss last: 3.53 | consumed tokens: 1521254400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T10:15:35 | step: 185800 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.570197077351622e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.77 | consumed tokens: 1522073600.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:15:56 | step: 185900 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.568786269170232e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.41 | consumed tokens: 1522892800.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T10:16:17 | step: 186000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.567375097190961e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.92 | consumed tokens: 1523712000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:16:37 | step: 186100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.5659635614138097e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.81 | consumed tokens: 1524531200.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T10:16:58 | step: 186200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.5645516618387774e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.22 | consumed tokens: 1525350400.0 | grad norm avg: 0.85 | grad norm last: 0.9 | 
2025-12-30T10:17:19 | step: 186300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.5631390346679837e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.58 | consumed tokens: 1526169600.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:17:39 | step: 186400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.56172640749719e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.88 | consumed tokens: 1526988800.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2025-12-30T10:18:00 | step: 186500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.560313052730635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.69 | consumed tokens: 1527808000.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T10:18:20 | step: 186600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.558899334166199e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.81 | consumed tokens: 1528627200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T10:18:41 | step: 186700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.5574852518038824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.89 | consumed tokens: 1529446400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:19:02 | step: 186800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.556070805643685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 1530265600.0 | grad norm avg: 0.85 | grad norm last: 0.89 | 
2025-12-30T10:19:22 | step: 186900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.554655995685607e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.34 | consumed tokens: 1531084800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:19:43 | step: 187000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.5532408219296485e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.53 | consumed tokens: 1531904000.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:20:03 | step: 187100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.5518249205779284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.69 | consumed tokens: 1532723200.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:20:24 | step: 187200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.5504086554283276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.64 | consumed tokens: 1533542400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:20:45 | step: 187300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.548992390278727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.98 | consumed tokens: 1534361600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:21:06 | step: 187400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.5475753975333646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.3 | consumed tokens: 1535180800.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:21:26 | step: 187500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.546158040990122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.91 | consumed tokens: 1536000000.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:21:47 | step: 187600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.544740320648998e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.31 | consumed tokens: 1536819200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T10:22:08 | step: 187700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.543322236509994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.08 | consumed tokens: 1537638400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:22:28 | step: 187800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.541903424775228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.22 | consumed tokens: 1538457600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T10:22:49 | step: 187900 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.540484613040462e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.88 | consumed tokens: 1539276800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:23:10 | step: 188000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.539065073709935e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.31 | consumed tokens: 1540096000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:23:30 | step: 188100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.537645534379408e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.97 | consumed tokens: 1540915200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:23:51 | step: 188200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.536225267453119e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.22 | consumed tokens: 1541734400.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:24:12 | step: 188300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.53480463672895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.39 | consumed tokens: 1542553600.0 | grad norm avg: 0.85 | grad norm last: 0.93 | 
2025-12-30T10:24:32 | step: 188400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.5333836422069e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.67 | consumed tokens: 1543372800.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:24:53 | step: 188500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.531962283886969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.86 | consumed tokens: 1544192000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2025-12-30T10:25:14 | step: 188600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.530540197971277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 2.73 | consumed tokens: 1545011200.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:25:34 | step: 188700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.529118112055585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.5 | consumed tokens: 1545830400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:25:55 | step: 188800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.527695662342012e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.84 | consumed tokens: 1546649600.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:26:16 | step: 188900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.5262724850326777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.73 | consumed tokens: 1547468800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:26:36 | step: 189000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.5248493077233434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.53 | consumed tokens: 1548288000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:26:57 | step: 189100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.523425402818248e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 1549107200.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:27:18 | step: 189200 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 3.522001134115271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 1549926400.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:27:39 | step: 189300 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.520576501614414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.75 | consumed tokens: 1550745600.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:27:59 | step: 189400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.519151505315676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.03 | consumed tokens: 1551564800.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:28:20 | step: 189500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.517726145219058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.05 | consumed tokens: 1552384000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:28:40 | step: 189600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.5163004213245586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.94 | consumed tokens: 1553203200.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:29:01 | step: 189700 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.5148743336321786e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.98 | consumed tokens: 1554022400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:29:22 | step: 189800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.513447518344037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.8 | consumed tokens: 1554841600.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:29:43 | step: 189900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.512020703055896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.25 | consumed tokens: 1555660800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:30:03 | step: 190000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.510593160171993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.81 | consumed tokens: 1556480000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:30:25 | step: 190100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.50916561728809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.31 | consumed tokens: 1557299200.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T10:30:46 | step: 190200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.507737346808426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.08 | consumed tokens: 1558118400.0 | grad norm avg: 0.86 | grad norm last: 1.02 | 
2025-12-30T10:31:07 | step: 190300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.506308712530881e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.44 | consumed tokens: 1558937600.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:31:27 | step: 190400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.5048797144554555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.03 | consumed tokens: 1559756800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:31:48 | step: 190500 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.50345071638003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.09 | consumed tokens: 1560576000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:32:09 | step: 190600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.502020990708843e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.97 | consumed tokens: 1561395200.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:32:29 | step: 190700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.500590901239775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.41 | consumed tokens: 1562214400.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:32:50 | step: 190800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.499160084174946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.78 | consumed tokens: 1563033600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T10:33:10 | step: 190900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.497729267110117e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.42 | consumed tokens: 1563852800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:33:31 | step: 191000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.496298086247407e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.09 | consumed tokens: 1564672000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:33:52 | step: 191100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.494866541586816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.23 | consumed tokens: 1565491200.0 | grad norm avg: 0.85 | grad norm last: 0.88 | 
2025-12-30T10:34:13 | step: 191200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.493434269330464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.45 | consumed tokens: 1566310400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:34:33 | step: 191300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.492001997074112e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.44 | consumed tokens: 1567129600.0 | grad norm avg: 0.86 | grad norm last: 0.98 | 
2025-12-30T10:34:54 | step: 191400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.490568997221999e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.91 | consumed tokens: 1567948800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T10:35:14 | step: 191500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.4891359973698854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.03 | consumed tokens: 1568768000.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T10:35:35 | step: 191600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.4877022699220106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.11 | consumed tokens: 1569587200.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2025-12-30T10:35:56 | step: 191700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.486268178676255e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.09 | consumed tokens: 1570406400.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:36:17 | step: 191800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.4848340874304995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 1571225600.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T10:36:37 | step: 191900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.4833992685889825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.83 | consumed tokens: 1572044800.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T10:36:58 | step: 192000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.481964085949585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.8 | consumed tokens: 1572864000.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:37:18 | step: 192100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.4805285395123065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.44 | consumed tokens: 1573683200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T10:37:39 | step: 192200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.4790926292771474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.67 | consumed tokens: 1574502400.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:38:00 | step: 192300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.4776563552441075e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.98 | consumed tokens: 1575321600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T10:38:20 | step: 192400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.476219717413187e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.44 | consumed tokens: 1576140800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2025-12-30T10:38:41 | step: 192500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.474782715784386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.47 | consumed tokens: 1576960000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:39:02 | step: 192600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.473345350357704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.06 | consumed tokens: 1577779200.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T10:39:22 | step: 192700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.471907621133141e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.8 | consumed tokens: 1578598400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:39:43 | step: 192800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.470469164312817e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.61 | consumed tokens: 1579417600.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T10:40:03 | step: 192900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.469030707492493e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.34 | consumed tokens: 1580236800.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:40:24 | step: 193000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.467591886874288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.97 | consumed tokens: 1581056000.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T10:40:45 | step: 193100 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 3.466152338660322e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.41 | consumed tokens: 1581875200.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T10:41:05 | step: 193200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.464712790446356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.55 | consumed tokens: 1582694400.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T10:41:26 | step: 193300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.463272878434509e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.56 | consumed tokens: 1583513600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T10:41:47 | step: 193400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.461832238826901e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.64 | consumed tokens: 1584332800.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:42:07 | step: 193500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.4603915992192924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.33 | consumed tokens: 1585152000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:42:28 | step: 193600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.458950232015923e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.8 | consumed tokens: 1585971200.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:42:49 | step: 193700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.457508864812553e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.21 | train loss last: 3.25 | consumed tokens: 1586790400.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:43:09 | step: 193800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.456066770013422e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 1587609600.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:43:30 | step: 193900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.4546246752142906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.25 | consumed tokens: 1588428800.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T10:43:50 | step: 194000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.453181852819398e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.5 | consumed tokens: 1589248000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T10:44:11 | step: 194100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.4517390304245055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.78 | consumed tokens: 1590067200.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:44:31 | step: 194200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.4502954804338515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.7 | consumed tokens: 1590886400.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:44:52 | step: 194300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.448851566645317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 1591705600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:45:12 | step: 194400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.447407652856782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.45 | consumed tokens: 1592524800.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:45:33 | step: 194500 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 3.445963011472486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.22 | consumed tokens: 1593344000.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2025-12-30T10:45:54 | step: 194600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.444518006290309e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.31 | consumed tokens: 1594163200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T10:46:14 | step: 194700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.443073001108132e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.61 | consumed tokens: 1594982400.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:46:35 | step: 194800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.441627268330194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.14 | consumed tokens: 1595801600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2025-12-30T10:46:55 | step: 194900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.440181171754375e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.42 | consumed tokens: 1596620800.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T10:47:16 | step: 195000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.4387347113806754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.56 | consumed tokens: 1597440000.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T10:47:38 | step: 195100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.437288251006976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.39 | consumed tokens: 1598259200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T10:47:59 | step: 195200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.435841063037515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.8 | consumed tokens: 1599078400.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:48:19 | step: 195300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.434393511270173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.48 | consumed tokens: 1599897600.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2025-12-30T10:48:40 | step: 195400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.432945959502831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.02 | consumed tokens: 1600716800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:49:01 | step: 195500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.431497680139728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 1601536000.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T10:49:21 | step: 195600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.430049036978744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 1602355200.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T10:49:42 | step: 195700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.4286000300198793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.8 | consumed tokens: 1603174400.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:50:03 | step: 195800 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.427151023061015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 1603993600.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T10:50:24 | step: 195900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.425701288506389e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.7 | consumed tokens: 1604812800.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2025-12-30T10:50:44 | step: 196000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.424251190153882e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1605632000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T10:51:05 | step: 196100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.422801091801375e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.02 | consumed tokens: 1606451200.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T10:51:26 | step: 196200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.421350265853107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.22 | consumed tokens: 1607270400.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T10:51:46 | step: 196300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.419899076106958e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.62 | consumed tokens: 1608089600.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T10:52:07 | step: 196400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.418447886360809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.95 | consumed tokens: 1608908800.0 | grad norm avg: 0.86 | grad norm last: 0.98 | 
2025-12-30T10:52:27 | step: 196500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.416995969018899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.11 | consumed tokens: 1609728000.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:52:48 | step: 196600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.4155440516769886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 1610547200.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T10:53:09 | step: 196700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.414091406739317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.78 | consumed tokens: 1611366400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T10:53:29 | step: 196800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.4126383980037645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.75 | consumed tokens: 1612185600.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T10:53:50 | step: 196900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.411185389268212e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.91 | consumed tokens: 1613004800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:54:11 | step: 197000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.409731652936898e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.73 | consumed tokens: 1613824000.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:54:32 | step: 197100 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 3.408277916605584e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.05 | consumed tokens: 1614643200.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T10:54:52 | step: 197200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.406823452678509e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 1615462400.0 | grad norm avg: 0.86 | grad norm last: 0.96 | 
2025-12-30T10:55:13 | step: 197300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.405368988751434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.17 | consumed tokens: 1616281600.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T10:55:34 | step: 197400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.403914161026478e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.08 | consumed tokens: 1617100800.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:55:54 | step: 197500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.4024586057057604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.48 | consumed tokens: 1617920000.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:56:15 | step: 197600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.401003050385043e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.19 | consumed tokens: 1618739200.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:56:36 | step: 197700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.399547131266445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.69 | consumed tokens: 1619558400.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T10:56:56 | step: 197800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.3980904845520854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.03 | consumed tokens: 1620377600.0 | grad norm avg: 0.86 | grad norm last: 0.97 | 
2025-12-30T10:57:17 | step: 197900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.396633837837726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.06 | consumed tokens: 1621196800.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T10:57:38 | step: 198000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.3951768273254856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 1622016000.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T10:57:58 | step: 198100 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.393719453015365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.03 | consumed tokens: 1622835200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T10:58:19 | step: 198200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.392261714907363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.14 | consumed tokens: 1623654400.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:58:40 | step: 198300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.390803613001481e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.97 | consumed tokens: 1624473600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T10:59:01 | step: 198400 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 3.3893451472977176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.62 | consumed tokens: 1625292800.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T10:59:22 | step: 198500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.387886317796074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.09 | consumed tokens: 1626112000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T10:59:42 | step: 198600 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.3864271244965494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1626931200.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2025-12-30T11:00:03 | step: 198700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.384967567399144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.91 | consumed tokens: 1627750400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T11:00:24 | step: 198800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.383507646503858e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.44 | consumed tokens: 1628569600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:00:44 | step: 198900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.3820477256085724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.17 | consumed tokens: 1629388800.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T11:01:05 | step: 199000 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.380587077117525e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.25 | consumed tokens: 1630208000.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:01:26 | step: 199100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.379126064828597e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.33 | consumed tokens: 1631027200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T11:01:47 | step: 199200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.377665052539669e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.33 | consumed tokens: 1631846400.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T11:02:07 | step: 199300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.3762033126549795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.78 | consumed tokens: 1632665600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T11:02:28 | step: 199400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.37474157277029e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.95 | consumed tokens: 1633484800.0 | grad norm avg: 0.86 | grad norm last: 1.02 | 
2025-12-30T11:02:49 | step: 199500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.373279105289839e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.19 | consumed tokens: 1634304000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T11:03:10 | step: 199600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.3718166378093883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.38 | consumed tokens: 1635123200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:03:30 | step: 199700 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 3.370353806531057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.0 | consumed tokens: 1635942400.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:03:51 | step: 199800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.3688906114548445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.05 | consumed tokens: 1636761600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T11:04:12 | step: 199900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.3674270525807515e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.58 | consumed tokens: 1637580800.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T11:04:33 | step: 200000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.365963129908778e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 1638400000.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T11:04:55 | step: 200100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.3644988434389234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.86 | consumed tokens: 1639219200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T11:05:15 | step: 200200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.363034193171188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 1640038400.0 | grad norm avg: 0.86 | grad norm last: 0.76 | 
2025-12-30T11:05:36 | step: 200300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.3615691791055724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 1640857600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T11:05:57 | step: 200400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.360103801242076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.78 | consumed tokens: 1641676800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:06:17 | step: 200500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.358638423378579e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.44 | consumed tokens: 1642496000.0 | grad norm avg: 0.87 | grad norm last: 0.79 | 
2025-12-30T11:06:38 | step: 200600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.3571723179193214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.44 | consumed tokens: 1643315200.0 | grad norm avg: 0.87 | grad norm last: 0.78 | 
2025-12-30T11:06:59 | step: 200700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.3557062124600634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.45 | consumed tokens: 1644134400.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T11:07:19 | step: 200800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.354239379405044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.77 | consumed tokens: 1644953600.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T11:07:40 | step: 200900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.3527725463500246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.58 | consumed tokens: 1645772800.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T11:08:01 | step: 201000 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 3.3513053494971246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.61 | consumed tokens: 1646592000.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:08:22 | step: 201100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.349837425048463e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.39 | consumed tokens: 1647411200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:08:42 | step: 201200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.3483695005998015e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.53 | consumed tokens: 1648230400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T11:09:03 | step: 201300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.34690157615114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 1649049600.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T11:09:23 | step: 201400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.345432924106717e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.25 | consumed tokens: 1649868800.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:09:44 | step: 201500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.3439639082644135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.19 | consumed tokens: 1650688000.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T11:10:05 | step: 201600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.342494528624229e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.94 | consumed tokens: 1651507200.0 | grad norm avg: 0.86 | grad norm last: 0.81 | 
2025-12-30T11:10:26 | step: 201700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.341025148984045e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.39 | consumed tokens: 1652326400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T11:10:46 | step: 201800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.339555041748099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.83 | consumed tokens: 1653145600.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:11:07 | step: 201900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.338084934512153e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.7 | consumed tokens: 1653964800.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:11:27 | step: 202000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.336614463478327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 1654784000.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T11:11:48 | step: 202100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.335143264848739e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.69 | consumed tokens: 1655603200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T11:12:09 | step: 202200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.333672066219151e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.83 | consumed tokens: 1656422400.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T11:12:30 | step: 202300 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.3322005037916824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.78 | consumed tokens: 1657241600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T11:12:50 | step: 202400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.330728941364214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.89 | consumed tokens: 1658060800.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T11:13:11 | step: 202500 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.329256651340984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 1658880000.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T11:13:32 | step: 202600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.327783997519873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.41 | consumed tokens: 1659699200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T11:13:53 | step: 202700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.3263113436987624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.88 | consumed tokens: 1660518400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T11:14:13 | step: 202800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.32483796228189e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.91 | consumed tokens: 1661337600.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T11:14:34 | step: 202900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.323364580865018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.11 | consumed tokens: 1662156800.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T11:14:54 | step: 203000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.321890835650265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.62 | consumed tokens: 1662976000.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T11:15:15 | step: 203100 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.3204167266376317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.53 | consumed tokens: 1663795200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:15:36 | step: 203200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.3189422538271174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1664614400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:15:57 | step: 203300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.3174674172187224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.28 | consumed tokens: 1665433600.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T11:16:17 | step: 203400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.3159925806103274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.47 | consumed tokens: 1666252800.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T11:16:38 | step: 203500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.314517016406171e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.17 | consumed tokens: 1667072000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T11:16:59 | step: 203600 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 3.3130414522020146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 1667891200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T11:17:20 | step: 203700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.3115655241999775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1668710400.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T11:17:40 | step: 203800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.310088868602179e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.0 | consumed tokens: 1669529600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:18:01 | step: 203900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.3086122130043805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 1670348800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:18:21 | step: 204000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.307135557406582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.77 | consumed tokens: 1671168000.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T11:18:42 | step: 204100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.305658174213022e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 4.03 | consumed tokens: 1671987200.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T11:19:03 | step: 204200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.3041804272215813e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 1672806400.0 | grad norm avg: 0.86 | grad norm last: 0.82 | 
2025-12-30T11:19:23 | step: 204300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.302702680230141e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.41 | consumed tokens: 1673625600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:19:44 | step: 204400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.301224569440819e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.94 | consumed tokens: 1674444800.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:20:04 | step: 204500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.2997457310557365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.16 | consumed tokens: 1675264000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:20:25 | step: 204600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.298266892670654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.7 | consumed tokens: 1676083200.0 | grad norm avg: 0.86 | grad norm last: 0.94 | 
2025-12-30T11:20:45 | step: 204700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.296788054285571e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.62 | consumed tokens: 1676902400.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:21:06 | step: 204800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.295308488304727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.72 | consumed tokens: 1677721600.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2025-12-30T11:21:27 | step: 204900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.293828558526002e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.59 | consumed tokens: 1678540800.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T11:21:47 | step: 205000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.292348628747277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1679360000.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T11:22:10 | step: 205100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.2908683351706713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.58 | consumed tokens: 1680179200.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2025-12-30T11:22:30 | step: 205200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.289387677796185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.88 | consumed tokens: 1680998400.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T11:22:51 | step: 205300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.287906656623818e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.77 | consumed tokens: 1681817600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:23:11 | step: 205400 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 3.28642527165357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.89 | consumed tokens: 1682636800.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T11:23:31 | step: 205500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.284943522885442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.16 | consumed tokens: 1683456000.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:23:52 | step: 205600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.283461774117313e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.8 | consumed tokens: 1684275200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:24:13 | step: 205700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.2819792977534235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.45 | consumed tokens: 1685094400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:24:33 | step: 205800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.2804968213895336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.28 | consumed tokens: 1685913600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:24:54 | step: 205900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.279013981227763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.25 | consumed tokens: 1686732800.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T11:25:14 | step: 206000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.277530777268112e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.42 | consumed tokens: 1687552000.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2025-12-30T11:25:35 | step: 206100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.2760475733084604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.23 | consumed tokens: 1688371200.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T11:25:56 | step: 206200 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 3.274563641753048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.14 | consumed tokens: 1689190400.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T11:26:17 | step: 206300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.273079710197635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.97 | consumed tokens: 1690009600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:26:37 | step: 206400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.2715954148443416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 1690828800.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T11:26:57 | step: 206500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.2701107556931674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.89 | consumed tokens: 1691648000.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2025-12-30T11:27:18 | step: 206600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.2686257327441126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.3 | consumed tokens: 1692467200.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T11:27:39 | step: 206700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.267140345997177e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.48 | consumed tokens: 1693286400.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T11:27:59 | step: 206800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.2656549592502415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.09 | consumed tokens: 1694105600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:28:20 | step: 206900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.264169208705425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.72 | consumed tokens: 1694924800.0 | grad norm avg: 0.88 | grad norm last: 0.77 | 
2025-12-30T11:28:40 | step: 207000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.262683094362728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.05 | consumed tokens: 1695744000.0 | grad norm avg: 0.86 | grad norm last: 0.93 | 
2025-12-30T11:29:01 | step: 207100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.2611966162221506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.91 | consumed tokens: 1696563200.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T11:29:21 | step: 207200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.259709774283692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.59 | consumed tokens: 1697382400.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T11:29:42 | step: 207300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.258222932345234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.02 | consumed tokens: 1698201600.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T11:30:03 | step: 207400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.256735362811014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.17 | consumed tokens: 1699020800.0 | grad norm avg: 0.86 | grad norm last: 0.83 | 
2025-12-30T11:30:23 | step: 207500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.255247793276794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.45 | consumed tokens: 1699840000.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2025-12-30T11:30:44 | step: 207600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.253759859944694e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.2 | consumed tokens: 1700659200.0 | grad norm avg: 0.86 | grad norm last: 0.86 | 
2025-12-30T11:31:04 | step: 207700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.252271926612593e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.03 | consumed tokens: 1701478400.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T11:31:25 | step: 207800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.250783265684731e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.61 | consumed tokens: 1702297600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:31:45 | step: 207900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.2492946047568694e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 4.09 | consumed tokens: 1703116800.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T11:32:06 | step: 208000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.247805580031127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.73 | consumed tokens: 1703936000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T11:32:27 | step: 208100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.2463161915075034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.52 | consumed tokens: 1704755200.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T11:32:47 | step: 208200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.244826439185999e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 1705574400.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2025-12-30T11:33:08 | step: 208300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.2433363230666146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.58 | consumed tokens: 1706393600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T11:33:28 | step: 208400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.24184620694723e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.94 | consumed tokens: 1707212800.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T11:33:49 | step: 208500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.240355727029964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.45 | consumed tokens: 1708032000.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T11:34:09 | step: 208600 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 3.238864883314818e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.03 | consumed tokens: 1708851200.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T11:34:30 | step: 208700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.237373675801791e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.94 | consumed tokens: 1709670400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:34:51 | step: 208800 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 3.2358824682887644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 1710489600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:35:11 | step: 208900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.234390533179976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.05 | consumed tokens: 1711308800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:35:32 | step: 209000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.232898598071188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.84 | consumed tokens: 1712128000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:35:52 | step: 209100 | train samples/s: 85.3 | train mfu (16-bit): -1.0 | lr mean: 3.231406299164519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.48 | consumed tokens: 1712947200.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T11:36:13 | step: 209200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.22991400025785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.33 | consumed tokens: 1713766400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:36:33 | step: 209300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.228420973755419e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.67 | consumed tokens: 1714585600.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T11:36:54 | step: 209400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.226927947252989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.06 | consumed tokens: 1715404800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:37:15 | step: 209500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.225434556952678e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.55 | consumed tokens: 1716224000.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:37:35 | step: 209600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.2239411666523665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.48 | consumed tokens: 1717043200.0 | grad norm avg: 0.86 | grad norm last: 0.85 | 
2025-12-30T11:37:56 | step: 209700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.222447048756294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.88 | consumed tokens: 1717862400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:38:16 | step: 209800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.2209529308602214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.81 | consumed tokens: 1718681600.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T11:38:37 | step: 209900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.219458449166268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.89 | consumed tokens: 1719500800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T11:38:58 | step: 210000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.217963603674434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.38 | consumed tokens: 1720320000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:39:20 | step: 210100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.2164683943847194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 1721139200.0 | grad norm avg: 0.87 | grad norm last: 0.94 | 
2025-12-30T11:39:40 | step: 210200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.214973185095005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.52 | consumed tokens: 1721958400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:40:01 | step: 210300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.213477612007409e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.64 | consumed tokens: 1722777600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:40:21 | step: 210400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.211981675121933e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.36 | consumed tokens: 1723596800.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T11:40:42 | step: 210500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.2104853744385764e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.62 | consumed tokens: 1724416000.0 | grad norm avg: 0.86 | grad norm last: 0.87 | 
2025-12-30T11:41:03 | step: 210600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.2089890737552196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.89 | consumed tokens: 1725235200.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:41:23 | step: 210700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.207492409273982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.11 | consumed tokens: 1726054400.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T11:41:44 | step: 210800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.205995380994864e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.36 | consumed tokens: 1726873600.0 | grad norm avg: 0.86 | grad norm last: 0.9 | 
2025-12-30T11:42:04 | step: 210900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.204497988917865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.81 | consumed tokens: 1727692800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:42:25 | step: 211000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.203000233042985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 1728512000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T11:42:45 | step: 211100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.2015024771681055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.2 | train loss last: 3.11 | consumed tokens: 1729331200.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T11:43:06 | step: 211200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.200004357495345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 4.0 | consumed tokens: 1730150400.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T11:43:26 | step: 211300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.198506237822585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.03 | consumed tokens: 1730969600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:43:47 | step: 211400 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.197007390554063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 1731788800.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T11:44:08 | step: 211500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.195508543285541e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.75 | consumed tokens: 1732608000.0 | grad norm avg: 0.88 | grad norm last: 0.8 | 
2025-12-30T11:44:28 | step: 211600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.194009332219139e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.81 | consumed tokens: 1733427200.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:44:49 | step: 211700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.1925097573548555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.44 | consumed tokens: 1734246400.0 | grad norm avg: 0.88 | grad norm last: 0.78 | 
2025-12-30T11:45:10 | step: 211800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.191010182490572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.02 | consumed tokens: 1735065600.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:45:30 | step: 211900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.1895102438284084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.77 | consumed tokens: 1735884800.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T11:45:51 | step: 212000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.188009941368364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.34 | consumed tokens: 1736704000.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:46:11 | step: 212100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.1865092751104385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 1737523200.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T11:46:32 | step: 212200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.185008608852513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.64 | consumed tokens: 1738342400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:46:52 | step: 212300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.183507578796707e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.67 | consumed tokens: 1739161600.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2025-12-30T11:47:13 | step: 212400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.1820061849430203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 1739980800.0 | grad norm avg: 0.86 | grad norm last: 0.8 | 
2025-12-30T11:47:34 | step: 212500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.1805047910893336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.55 | consumed tokens: 1740800000.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:47:54 | step: 212600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.1790026696398854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.97 | consumed tokens: 1741619200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:48:15 | step: 212700 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.177500548190437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.77 | consumed tokens: 1742438400.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:48:36 | step: 212800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.175998426740989e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.77 | consumed tokens: 1743257600.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:48:56 | step: 212900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.1744955776957795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 1744076800.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T11:49:17 | step: 213000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.17299272865057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 1744896000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:49:37 | step: 213100 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.1714895158074796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 1745715200.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2025-12-30T11:49:57 | step: 213200 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 3.169986302964389e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.73 | consumed tokens: 1746534400.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T11:50:18 | step: 213300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.1684823625255376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 1747353600.0 | grad norm avg: 0.87 | grad norm last: 0.95 | 
2025-12-30T11:50:39 | step: 213400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.166978422086686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.66 | consumed tokens: 1748172800.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:50:59 | step: 213500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.1654741178499535e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.61 | consumed tokens: 1748992000.0 | grad norm avg: 0.88 | grad norm last: 0.75 | 
2025-12-30T11:51:20 | step: 213600 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 3.163969813613221e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.58 | consumed tokens: 1749811200.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:51:40 | step: 213700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.162465145578608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.75 | consumed tokens: 1750630400.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:52:01 | step: 213800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.160960113746114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.69 | consumed tokens: 1751449600.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T11:52:22 | step: 213900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.1594547181157395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.67 | consumed tokens: 1752268800.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T11:52:42 | step: 214000 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.157949322485365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.66 | consumed tokens: 1753088000.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T11:53:03 | step: 214100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.15644356305711e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 1753907200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T11:53:23 | step: 214200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.154937439830974e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.94 | consumed tokens: 1754726400.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T11:53:44 | step: 214300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.153431316604838e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.14 | consumed tokens: 1755545600.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T11:54:05 | step: 214400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.151924829580821e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.23 | consumed tokens: 1756364800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:54:25 | step: 214500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.150417978758924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.67 | consumed tokens: 1757184000.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:54:46 | step: 214600 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 3.148911127937026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.53 | consumed tokens: 1758003200.0 | grad norm avg: 0.88 | grad norm last: 0.81 | 
2025-12-30T11:55:06 | step: 214700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.1474035495193675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 1.82 | consumed tokens: 1758822400.0 | grad norm avg: 0.88 | grad norm last: 0.8 | 
2025-12-30T11:55:27 | step: 214800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.1458963348995894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.16 | consumed tokens: 1759641600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:55:47 | step: 214900 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.14438839268405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.97 | consumed tokens: 1760460800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:56:08 | step: 215000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.1428804504685104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.23 | consumed tokens: 1761280000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2025-12-30T11:56:30 | step: 215100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.14137214445509e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.75 | consumed tokens: 1762099200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T11:56:51 | step: 215200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.139863474643789e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.23 | consumed tokens: 1762918400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T11:57:12 | step: 215300 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.138354804832488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.23 | consumed tokens: 1763737600.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T11:57:32 | step: 215400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.1368457712233067e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.17 | consumed tokens: 1764556800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:57:53 | step: 215500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.135336373816244e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.33 | consumed tokens: 1765376000.0 | grad norm avg: 0.87 | grad norm last: 0.82 | 
2025-12-30T11:58:13 | step: 215600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.133826976409182e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 1766195200.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T11:58:34 | step: 215700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.132317215204239e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 1767014400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:58:55 | step: 215800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.130807090201415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 4.16 | consumed tokens: 1767833600.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T11:59:15 | step: 215900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.1292969651985914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.92 | consumed tokens: 1768652800.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T11:59:36 | step: 216000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.127786476397887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.86 | consumed tokens: 1769472000.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T11:59:56 | step: 216100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.126275623799302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 1770291200.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:00:17 | step: 216200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.1247647712007165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.86 | consumed tokens: 1771110400.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:00:37 | step: 216300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.1232535548042506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.62 | consumed tokens: 1771929600.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T12:00:58 | step: 216400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.121741974609904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.86 | consumed tokens: 1772748800.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T12:01:19 | step: 216500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.1202303944155574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 1773568000.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T12:01:39 | step: 216600 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 3.11871845042333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 3.45 | consumed tokens: 1774387200.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T12:02:00 | step: 216700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.117206142633222e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.38 | consumed tokens: 1775206400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:02:21 | step: 216800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.115693471045233e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.38 | consumed tokens: 1776025600.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T12:02:42 | step: 216900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.1141807994572446e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.14 | consumed tokens: 1776844800.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T12:03:02 | step: 217000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.112668127869256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 1777664000.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:03:23 | step: 217100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.111154728685506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.94 | consumed tokens: 1778483200.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2025-12-30T12:03:43 | step: 217200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.1096413295017555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.41 | consumed tokens: 1779302400.0 | grad norm avg: 0.88 | grad norm last: 0.97 | 
2025-12-30T12:04:04 | step: 217300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.1081279303180054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.59 | consumed tokens: 1780121600.0 | grad norm avg: 0.86 | grad norm last: 0.79 | 
2025-12-30T12:04:24 | step: 217400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.1066141673363745e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.42 | consumed tokens: 1780940800.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T12:04:45 | step: 217500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.105100040556863e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.0 | consumed tokens: 1781760000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:05:06 | step: 217600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.103585549979471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 1782579200.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T12:05:26 | step: 217700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.1020710594020784e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.56 | consumed tokens: 1783398400.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:05:47 | step: 217800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.1005562050268054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 1784217600.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:06:07 | step: 217900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 3.099040986853652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 4.03 | consumed tokens: 1785036800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:06:28 | step: 218000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.097525768680498e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.66 | consumed tokens: 1785856000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:06:48 | step: 218100 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 3.0960101867094636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.81 | consumed tokens: 1786675200.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T12:07:09 | step: 218200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.094494604738429e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1787494400.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T12:07:29 | step: 218300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.092978658969514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 1788313600.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T12:07:50 | step: 218400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.091462349402718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.88 | consumed tokens: 1789132800.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:08:10 | step: 218500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.0899460398359224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.56 | consumed tokens: 1789952000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:08:31 | step: 218600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.088429366471246e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.08 | consumed tokens: 1790771200.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T12:08:51 | step: 218700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.0869123293086886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.09 | consumed tokens: 1791590400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:09:12 | step: 218800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.0853952921461314e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 1792409600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2025-12-30T12:09:33 | step: 218900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.0838778911856934e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.75 | consumed tokens: 1793228800.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T12:09:53 | step: 219000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.0823604902252555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.03 | consumed tokens: 1794048000.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2025-12-30T12:10:14 | step: 219100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.080842725466937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.66 | consumed tokens: 1794867200.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:10:35 | step: 219200 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.0793245969107375e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.14 | consumed tokens: 1795686400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:10:55 | step: 219300 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 3.077806468354538e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.45 | consumed tokens: 1796505600.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T12:11:16 | step: 219400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.076287976000458e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.58 | consumed tokens: 1797324800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:11:36 | step: 219500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 3.074769119848497e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.69 | consumed tokens: 1798144000.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T12:11:57 | step: 219600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.0732502636965364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.47 | consumed tokens: 1798963200.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:12:17 | step: 219700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.071731043746695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.53 | consumed tokens: 1799782400.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:12:38 | step: 219800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 3.0702118237968534e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.23 | consumed tokens: 1800601600.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:12:59 | step: 219900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.068692240049131e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.78 | consumed tokens: 1801420800.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T12:13:19 | step: 220000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.067172292503528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1802240000.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T12:13:42 | step: 220100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.0656523449579254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.27 | consumed tokens: 1803059200.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:14:02 | step: 220200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.064132033614442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.41 | consumed tokens: 1803878400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:14:23 | step: 220300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.062611722270958e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 1804697600.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T12:14:43 | step: 220400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.061091047129594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.27 | consumed tokens: 1805516800.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:15:04 | step: 220500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.059570008190349e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.67 | consumed tokens: 1806336000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:15:25 | step: 220600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.058048969251104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.56 | consumed tokens: 1807155200.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:15:45 | step: 220700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.056527566513978e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.72 | consumed tokens: 1807974400.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T12:16:06 | step: 220800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.055006163776852e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.39 | consumed tokens: 1808793600.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T12:16:26 | step: 220900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.053484397241846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.84 | consumed tokens: 1809612800.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:16:47 | step: 221000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.0519622669089586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.39 | consumed tokens: 1810432000.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2025-12-30T12:17:08 | step: 221100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.0504401365760714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 1811251200.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T12:17:28 | step: 221200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 3.0489176424453035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.61 | consumed tokens: 1812070400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:17:49 | step: 221300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.0473951483145356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.28 | consumed tokens: 1812889600.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T12:18:10 | step: 221400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.045872290385887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.81 | consumed tokens: 1813708800.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:18:30 | step: 221500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.044349250558298e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 1814528000.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:18:51 | step: 221600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.0428260288317688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.92 | consumed tokens: 1815347200.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:19:12 | step: 221700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.0413024433073588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 4.03 | consumed tokens: 1816166400.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:19:33 | step: 221800 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 3.0397788577829488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 1816985600.0 | grad norm avg: 0.88 | grad norm last: 0.8 | 
2025-12-30T12:19:53 | step: 221900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.038254908460658e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.81 | consumed tokens: 1817804800.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T12:20:14 | step: 222000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.036730777239427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 1818624000.0 | grad norm avg: 0.87 | grad norm last: 0.98 | 
2025-12-30T12:20:34 | step: 222100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 3.0352064641192555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.84 | consumed tokens: 1819443200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:20:55 | step: 222200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.0336819691001438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 1820262400.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T12:21:16 | step: 222300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.0321572921820916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.5 | consumed tokens: 1821081600.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T12:21:36 | step: 222400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 3.0306322514661588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.58 | consumed tokens: 1821900800.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T12:21:57 | step: 222500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.029107210750226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.47 | consumed tokens: 1822720000.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T12:22:17 | step: 222600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.0275818062364124e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.53 | consumed tokens: 1823539200.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:22:38 | step: 222700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 3.0260562198236585e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.58 | consumed tokens: 1824358400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T12:22:59 | step: 222800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.0245304515119642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.91 | consumed tokens: 1825177600.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:23:19 | step: 222900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 3.0230045013013296e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.95 | consumed tokens: 1825996800.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T12:23:40 | step: 223000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.0214783691917546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 1826816000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:24:01 | step: 223100 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 3.0199520551832393e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.88 | consumed tokens: 1827635200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:24:21 | step: 223200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.0184255592757836e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1828454400.0 | grad norm avg: 0.87 | grad norm last: 0.81 | 
2025-12-30T12:24:42 | step: 223300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.0168986995704472e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.11 | consumed tokens: 1829273600.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:25:02 | step: 223400 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 3.0153718398651108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 1830092800.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:25:23 | step: 223500 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 3.0138446163618937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.94 | consumed tokens: 1830912000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:25:43 | step: 223600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.0123172109597363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.44 | consumed tokens: 1831731200.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:26:04 | step: 223700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.0107896236586384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 1832550400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:26:24 | step: 223800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.0092618544586003e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.42 | consumed tokens: 1833369600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T12:26:45 | step: 223900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.0077339033596218e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.14 | consumed tokens: 1834188800.0 | grad norm avg: 0.88 | grad norm last: 1.1 | 
2025-12-30T12:27:05 | step: 224000 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 3.006205770361703e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.06 | consumed tokens: 1835008000.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:27:26 | step: 224100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.0046774554648437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.05 | consumed tokens: 1835827200.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T12:27:46 | step: 224200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.003148958669044e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.81 | consumed tokens: 1836646400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:28:07 | step: 224300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 3.0016200980753638e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.02 | consumed tokens: 1837465600.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T12:28:27 | step: 224400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.0000912374816835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.09 | consumed tokens: 1838284800.0 | grad norm avg: 0.87 | grad norm last: 0.8 | 
2025-12-30T12:28:48 | step: 224500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.9985620130901225e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.97 | consumed tokens: 1839104000.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T12:29:08 | step: 224600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.9970327886985615e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.91 | consumed tokens: 1839923200.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T12:29:29 | step: 224700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.9955032005091198e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.84 | consumed tokens: 1840742400.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:29:49 | step: 224800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.9939734304207377e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.78 | consumed tokens: 1841561600.0 | grad norm avg: 0.87 | grad norm last: 0.85 | 
2025-12-30T12:30:10 | step: 224900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.9924436603323556e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 1842380800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:30:30 | step: 225000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.990913526446093e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 1843200000.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T12:30:53 | step: 225100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.9893832106608897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.66 | consumed tokens: 1844019200.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2025-12-30T12:31:13 | step: 225200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.9878527129767463e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.67 | consumed tokens: 1844838400.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:31:34 | step: 225300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.9863220333936624e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.48 | consumed tokens: 1845657600.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T12:31:54 | step: 225400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.9847911719116382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.22 | consumed tokens: 1846476800.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T12:32:15 | step: 225500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.9832601285306737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 1847296000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T12:32:36 | step: 225600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.9817289032507688e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.94 | consumed tokens: 1848115200.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T12:32:56 | step: 225700 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 2.9801974960719235e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 1848934400.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:33:17 | step: 225800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.978665906994138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.92 | consumed tokens: 1849753600.0 | grad norm avg: 0.87 | grad norm last: 0.99 | 
2025-12-30T12:33:37 | step: 225900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.977134136017412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.86 | consumed tokens: 1850572800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:33:58 | step: 226000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.9756021831417456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.67 | consumed tokens: 1851392000.0 | grad norm avg: 0.87 | grad norm last: 0.94 | 
2025-12-30T12:34:18 | step: 226100 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.974070048367139e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 1852211200.0 | grad norm avg: 0.88 | grad norm last: 0.78 | 
2025-12-30T12:34:39 | step: 226200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.9725375497946516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 1853030400.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:34:59 | step: 226300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.9710050512221642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.45 | consumed tokens: 1853849600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T12:35:20 | step: 226400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.9694723707507364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.7 | consumed tokens: 1854668800.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:35:40 | step: 226500 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.9679395083803684e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1855488000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:36:01 | step: 226600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.96640646411106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.7 | consumed tokens: 1856307200.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:36:21 | step: 226700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.964873237942811e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.97 | consumed tokens: 1857126400.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:36:42 | step: 226800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.9633396479766816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.42 | consumed tokens: 1857945600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:37:02 | step: 226900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.961806058010552e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.3 | consumed tokens: 1858764800.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T12:37:23 | step: 227000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.9602722861454822e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 1859584000.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T12:37:43 | step: 227100 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 2.958738332381472e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.06 | consumed tokens: 1860403200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:38:04 | step: 227200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.9572041967185214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.12 | consumed tokens: 1861222400.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2025-12-30T12:38:24 | step: 227300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.9556698791566305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.44 | consumed tokens: 1862041600.0 | grad norm avg: 0.87 | grad norm last: 0.96 | 
2025-12-30T12:38:44 | step: 227400 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.9541353796957992e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.12 | consumed tokens: 1862860800.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T12:39:05 | step: 227500 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.9526006983360276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.67 | consumed tokens: 1863680000.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:39:25 | step: 227600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.9510658350773156e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.25 | consumed tokens: 1864499200.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T12:39:46 | step: 227700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.9495307899196632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 1865318400.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T12:40:06 | step: 227800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.9479955628630705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.55 | consumed tokens: 1866137600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:40:27 | step: 227900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.9464601539075375e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 1866956800.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T12:40:47 | step: 228000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.944924563053064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.52 | consumed tokens: 1867776000.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T12:41:07 | step: 228100 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.9433887902996503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.33 | consumed tokens: 1868595200.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T12:41:28 | step: 228200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.9418530175462365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.84 | consumed tokens: 1869414400.0 | grad norm avg: 0.87 | grad norm last: 0.95 | 
2025-12-30T12:41:48 | step: 228300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.940316880994942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.2 | consumed tokens: 1870233600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T12:42:09 | step: 228400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.9387807444436476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.0 | consumed tokens: 1871052800.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:42:30 | step: 228500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.9372442440944724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.48 | consumed tokens: 1871872000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:42:50 | step: 228600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.9357077437452972e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 1872691200.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T12:43:11 | step: 228700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.9341708795982413e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.86 | consumed tokens: 1873510400.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T12:43:31 | step: 228800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.9326340154511854e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.06 | consumed tokens: 1874329600.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T12:43:52 | step: 228900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.931096969405189e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.91 | consumed tokens: 1875148800.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:44:13 | step: 229000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.9295595595613122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.61 | consumed tokens: 1875968000.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T12:44:33 | step: 229100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.9280221497174352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.19 | consumed tokens: 1876787200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:44:54 | step: 229200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.926484557974618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.97 | consumed tokens: 1877606400.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:45:14 | step: 229300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.9249467843328603e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.88 | consumed tokens: 1878425600.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:45:35 | step: 229400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.9234090106911026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.2 | consumed tokens: 1879244800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:45:55 | step: 229500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.9218708732514642e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 1880064000.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:46:16 | step: 229600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.9203325539128855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.61 | consumed tokens: 1880883200.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T12:46:37 | step: 229700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.9187942345743068e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.19 | consumed tokens: 1881702400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T12:46:57 | step: 229800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.9172555514378473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.86 | consumed tokens: 1882521600.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T12:47:18 | step: 229900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.915716868301388e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.22 | consumed tokens: 1883340800.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2025-12-30T12:47:38 | step: 230000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.914178003265988e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 1884160000.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T12:48:00 | step: 230100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.912638956331648e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.75 | consumed tokens: 1884979200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T12:48:21 | step: 230200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.9110997274983674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1885798400.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2025-12-30T12:48:41 | step: 230300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.9095603167661466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.92 | consumed tokens: 1886617600.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T12:49:02 | step: 230400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.9080209060339257e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.41 | consumed tokens: 1887436800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:49:22 | step: 230500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.906481131503824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.61 | consumed tokens: 1888256000.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T12:49:43 | step: 230600 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.9049413569737226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.19 | consumed tokens: 1889075200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2025-12-30T12:50:03 | step: 230700 | train samples/s: 85.3 | train mfu (16-bit): -1.0 | lr mean: 2.9034012186457403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.19 | consumed tokens: 1889894400.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T12:50:23 | step: 230800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.901861080317758e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.69 | consumed tokens: 1890713600.0 | grad norm avg: 0.88 | grad norm last: 0.96 | 
2025-12-30T12:50:44 | step: 230900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.9003207600908354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.08 | consumed tokens: 1891532800.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T12:51:05 | step: 231000 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 2.8987802579649724e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.92 | consumed tokens: 1892352000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T12:51:25 | step: 231100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8972397558391094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.23 | consumed tokens: 1893171200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:51:46 | step: 231200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.8956988899153657e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.59 | consumed tokens: 1893990400.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:52:06 | step: 231300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.894158023991622e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.84 | consumed tokens: 1894809600.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:52:27 | step: 231400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8926167942699976e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.08 | consumed tokens: 1895628800.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T12:52:47 | step: 231500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.8910755645483732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.8 | consumed tokens: 1896448000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T12:53:08 | step: 231600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.8895341529278085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.73 | consumed tokens: 1897267200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:53:28 | step: 231700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.8879927413072437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 1898086400.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T12:53:49 | step: 231800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.8864509658887982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 1898905600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T12:54:09 | step: 231900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.8849091904703528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.06 | consumed tokens: 1899724800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T12:54:29 | step: 232000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.8833670512540266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.72 | consumed tokens: 1900544000.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T12:54:50 | step: 232100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.8818249120377004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.05 | consumed tokens: 1901363200.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T12:55:10 | step: 232200 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.8802827728213742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.19 | consumed tokens: 1902182400.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T12:55:31 | step: 232300 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 2.8787402698071674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.06 | consumed tokens: 1903001600.0 | grad norm avg: 0.88 | grad norm last: 0.93 | 
2025-12-30T12:55:52 | step: 232400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8771977667929605e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 1903820800.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T12:56:12 | step: 232500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.875654899980873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.58 | consumed tokens: 1904640000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T12:56:33 | step: 232600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.8741120331687853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.52 | consumed tokens: 1905459200.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T12:56:54 | step: 232700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8725689844577573e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.89 | consumed tokens: 1906278400.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:57:14 | step: 232800 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.8710259357467294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.36 | consumed tokens: 1907097600.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T12:57:34 | step: 232900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8694825232378207e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 1907916800.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:57:55 | step: 233000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.867939110728912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.81 | consumed tokens: 1908736000.0 | grad norm avg: 0.89 | grad norm last: 0.82 | 
2025-12-30T12:58:16 | step: 233100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.866395516321063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 1909555200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T12:58:36 | step: 233200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.8648517400142737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.64 | consumed tokens: 1910374400.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T12:58:57 | step: 233300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.8633079637074843e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.58 | consumed tokens: 1911193600.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T12:59:17 | step: 233400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.8617638236028142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.7 | consumed tokens: 1912012800.0 | grad norm avg: 0.88 | grad norm last: 0.92 | 
2025-12-30T12:59:38 | step: 233500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.860219683498144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.05 | consumed tokens: 1912832000.0 | grad norm avg: 0.89 | grad norm last: 0.98 | 
2025-12-30T12:59:59 | step: 233600 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 2.8586753614945337e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.67 | consumed tokens: 1913651200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:00:20 | step: 233700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.8571310394909233e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.91 | consumed tokens: 1914470400.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:00:40 | step: 233800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.855586353689432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.67 | consumed tokens: 1915289600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:01:01 | step: 233900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.854041667887941e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.84 | consumed tokens: 1916108800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:01:21 | step: 234000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.8524968001875095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 1916928000.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:01:42 | step: 234100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.8509517505881377e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.23 | consumed tokens: 1917747200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:02:02 | step: 234200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.8494067009887658e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.27 | consumed tokens: 1918566400.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T13:02:23 | step: 234300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8478614694904536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 1919385600.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:02:43 | step: 234400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.846316056093201e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1920204800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T13:03:04 | step: 234500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.844770460797008e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.86 | consumed tokens: 1921024000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:03:25 | step: 234600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.8432248655008152e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.67 | consumed tokens: 1921843200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:03:45 | step: 234700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.841679088305682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.39 | consumed tokens: 1922662400.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T13:04:06 | step: 234800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.8401331292116083e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.75 | consumed tokens: 1923481600.0 | grad norm avg: 0.87 | grad norm last: 0.91 | 
2025-12-30T13:04:27 | step: 234900 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 2.8385869882185943e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.42 | consumed tokens: 1924300800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T13:04:47 | step: 235000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.8370408472255804e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.44 | consumed tokens: 1925120000.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T13:05:10 | step: 235100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.835494524333626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.95 | consumed tokens: 1925939200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:05:30 | step: 235200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.8339480195427313e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.94 | consumed tokens: 1926758400.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T13:05:51 | step: 235300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.8324015147518367e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.56 | consumed tokens: 1927577600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:06:11 | step: 235400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.8308548280620016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.95 | consumed tokens: 1928396800.0 | grad norm avg: 0.89 | grad norm last: 0.82 | 
2025-12-30T13:06:32 | step: 235500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.8293079594732262e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 1929216000.0 | grad norm avg: 0.88 | grad norm last: 0.81 | 
2025-12-30T13:06:53 | step: 235600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.8277609089855105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 1930035200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:07:13 | step: 235700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.8262138584977947e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 1930854400.0 | grad norm avg: 0.88 | grad norm last: 1.09 | 
2025-12-30T13:07:34 | step: 235800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.8246666261111386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 4.16 | consumed tokens: 1931673600.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:07:54 | step: 235900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.8231193937244825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.39 | consumed tokens: 1932492800.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T13:08:15 | step: 236000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.8215717975399457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.5 | consumed tokens: 1933312000.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T13:08:35 | step: 236100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.820024201355409e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.94 | consumed tokens: 1934131200.0 | grad norm avg: 0.88 | grad norm last: 0.84 | 
2025-12-30T13:08:56 | step: 236200 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 2.818476605170872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.28 | consumed tokens: 1934950400.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T13:09:17 | step: 236300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.8169286451884545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.17 | consumed tokens: 1935769600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2025-12-30T13:09:37 | step: 236400 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.815380685206037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 1936588800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:09:58 | step: 236500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.813832543324679e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.48 | consumed tokens: 1937408000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:10:18 | step: 236600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.8122844014433213e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.64 | consumed tokens: 1938227200.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:10:39 | step: 236700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.810736077663023e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.77 | consumed tokens: 1939046400.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2025-12-30T13:11:00 | step: 236800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.8091875719837844e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.5 | consumed tokens: 1939865600.0 | grad norm avg: 0.89 | grad norm last: 0.83 | 
2025-12-30T13:11:20 | step: 236900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.807639066304546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.33 | consumed tokens: 1940684800.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:11:41 | step: 237000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.806090378726367e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 1941504000.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T13:12:01 | step: 237100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.8045415092492476e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.97 | consumed tokens: 1942323200.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T13:12:22 | step: 237200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.8029926397721283e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 1943142400.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:12:43 | step: 237300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.8014435883960687e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.34 | consumed tokens: 1943961600.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-30T13:13:03 | step: 237400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.7998943551210687e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 1944780800.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:13:24 | step: 237500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 2.7983451218460687e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.16 | consumed tokens: 1945600000.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:13:44 | step: 237600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.7967957066721283e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 1946419200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:14:05 | step: 237700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.795246291498188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 1947238400.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:14:26 | step: 237800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.7936966944253072e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.61 | consumed tokens: 1948057600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:14:46 | step: 237900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.792146915453486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 1948876800.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:15:07 | step: 238000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.790597136481665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 1949696000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:15:28 | step: 238100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.7890471756109037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.88 | consumed tokens: 1950515200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:15:48 | step: 238200 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 2.787497032841202e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.03 | consumed tokens: 1951334400.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T13:16:09 | step: 238300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.7859468900715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.72 | consumed tokens: 1952153600.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T13:16:29 | step: 238400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.784396565402858e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 1952972800.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T13:16:50 | step: 238500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.7828460588352755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.5 | consumed tokens: 1953792000.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:17:11 | step: 238600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.781295552267693e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.83 | consumed tokens: 1954611200.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:17:32 | step: 238700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.7797450457001105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.92 | consumed tokens: 1955430400.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T13:17:52 | step: 238800 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 2.7781943572335877e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.81 | consumed tokens: 1956249600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:18:13 | step: 238900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7766434868681245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.77 | consumed tokens: 1957068800.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T13:18:34 | step: 239000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.775092434603721e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.56 | consumed tokens: 1957888000.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2025-12-30T13:18:54 | step: 239100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.7735413823393174e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.8 | consumed tokens: 1958707200.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2025-12-30T13:19:15 | step: 239200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.7719903300749138e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.64 | consumed tokens: 1959526400.0 | grad norm avg: 0.89 | grad norm last: 1.0 | 
2025-12-30T13:19:35 | step: 239300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.77043909591157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.19 | consumed tokens: 1960345600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:19:56 | step: 239400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.7688876798492856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.56 | consumed tokens: 1961164800.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T13:20:17 | step: 239500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7673362637870014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.39 | consumed tokens: 1961984000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:20:37 | step: 239600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.7657846658257768e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.78 | consumed tokens: 1962803200.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T13:20:58 | step: 239700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.7642328859656118e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.42 | consumed tokens: 1963622400.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T13:21:18 | step: 239800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7626811061054468e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 1964441600.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:21:39 | step: 239900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.761129326245282e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.14 | consumed tokens: 1965260800.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T13:21:59 | step: 240000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.7595773644861765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.06 | consumed tokens: 1966080000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:22:22 | step: 240100 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 2.758025220828131e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.86 | consumed tokens: 1966899200.0 | grad norm avg: 0.89 | grad norm last: 0.96 | 
2025-12-30T13:22:42 | step: 240200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.756473077170085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.02 | consumed tokens: 1967718400.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:23:03 | step: 240300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.754920751613099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.88 | consumed tokens: 1968537600.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:23:23 | step: 240400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.753368426056113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.2 | consumed tokens: 1969356800.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:23:44 | step: 240500 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 2.7518159186001867e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.88 | consumed tokens: 1970176000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:24:05 | step: 240600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.75026322924532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.2 | consumed tokens: 1970995200.0 | grad norm avg: 0.88 | grad norm last: 0.78 | 
2025-12-30T13:24:25 | step: 240700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.7487107217893936e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 1971814400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:24:46 | step: 240800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7471578505355865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.8 | consumed tokens: 1972633600.0 | grad norm avg: 0.88 | grad norm last: 0.88 | 
2025-12-30T13:25:07 | step: 240900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.7456049792817794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.31 | consumed tokens: 1973452800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:25:27 | step: 241000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7440521080279723e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.22 | consumed tokens: 1974272000.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2025-12-30T13:25:48 | step: 241100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7424990548752248e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.3 | consumed tokens: 1975091200.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T13:26:08 | step: 241200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.740945819823537e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.69 | consumed tokens: 1975910400.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:26:29 | step: 241300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7393925847718492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.92 | consumed tokens: 1976729600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:26:50 | step: 241400 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 2.7378393497201614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.2 | consumed tokens: 1977548800.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T13:27:10 | step: 241500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.7362859327695332e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.22 | consumed tokens: 1978368000.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2025-12-30T13:27:31 | step: 241600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7347323339199647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.36 | consumed tokens: 1979187200.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:27:51 | step: 241700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7331787350703962e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 4.19 | consumed tokens: 1980006400.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:28:12 | step: 241800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.7316249543218873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 1980825600.0 | grad norm avg: 0.89 | grad norm last: 0.96 | 
2025-12-30T13:28:33 | step: 241900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7300711735733785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 4.38 | consumed tokens: 1981644800.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2025-12-30T13:28:53 | step: 242000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.7285173928248696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.09 | consumed tokens: 1982464000.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:29:14 | step: 242100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.7269634301774204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.0 | consumed tokens: 1983283200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:29:34 | step: 242200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7254092856310308e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.19 | train loss last: 2.78 | consumed tokens: 1984102400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:29:55 | step: 242300 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.7238551410846412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.72 | consumed tokens: 1984921600.0 | grad norm avg: 0.89 | grad norm last: 0.82 | 
2025-12-30T13:30:15 | step: 242400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.7223009965382516e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.7 | consumed tokens: 1985740800.0 | grad norm avg: 0.89 | grad norm last: 0.82 | 
2025-12-30T13:30:36 | step: 242500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.7207466700929217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.05 | consumed tokens: 1986560000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:30:56 | step: 242600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.7191923436475918e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 1987379200.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:31:17 | step: 242700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 2.7176378353033215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 1988198400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:31:38 | step: 242800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.7160833269590512e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.02 | consumed tokens: 1989017600.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T13:31:58 | step: 242900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.7145286367158405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.75 | consumed tokens: 1989836800.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T13:32:19 | step: 243000 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 2.71297394647263e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.38 | consumed tokens: 1990656000.0 | grad norm avg: 0.89 | grad norm last: 1.03 | 
2025-12-30T13:32:40 | step: 243100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.711419074330479e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.45 | consumed tokens: 1991475200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:33:00 | step: 243200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.709864202188328e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.94 | consumed tokens: 1992294400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:33:21 | step: 243300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.7083091481472366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.8 | consumed tokens: 1993113600.0 | grad norm avg: 0.88 | grad norm last: 0.94 | 
2025-12-30T13:33:42 | step: 243400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.7067540941061452e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.77 | consumed tokens: 1993932800.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:34:02 | step: 243500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.705199040065054e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.31 | consumed tokens: 1994752000.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T13:34:23 | step: 243600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.703643804125022e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.27 | consumed tokens: 1995571200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:34:44 | step: 243700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.7020885681849904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 1996390400.0 | grad norm avg: 0.9 | grad norm last: 0.81 | 
2025-12-30T13:35:04 | step: 243800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.7005331503460184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.34 | consumed tokens: 1997209600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T13:35:25 | step: 243900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.6989777325070463e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1998028800.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:35:46 | step: 244000 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 2.697422132769134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.52 | consumed tokens: 1998848000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:36:06 | step: 244100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.6958665330312215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 1999667200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:36:27 | step: 244200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.694310933293309e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.7 | consumed tokens: 2000486400.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:36:47 | step: 244300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6927551516564563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.39 | consumed tokens: 2001305600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:37:08 | step: 244400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.6911993700196035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.36 | consumed tokens: 2002124800.0 | grad norm avg: 0.89 | grad norm last: 0.94 | 
2025-12-30T13:37:29 | step: 244500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.6896434064838104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.88 | consumed tokens: 2002944000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:37:49 | step: 244600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.6880874429480173e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2003763200.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:38:10 | step: 244700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.686531479412224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.53 | consumed tokens: 2004582400.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-30T13:38:30 | step: 244800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6849753339774907e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.97 | consumed tokens: 2005401600.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:38:51 | step: 244900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.6834191885427572e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 2006220800.0 | grad norm avg: 0.89 | grad norm last: 1.04 | 
2025-12-30T13:39:11 | step: 245000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.6818628612090833e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.45 | consumed tokens: 2007040000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:39:34 | step: 245100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.6803065338754095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.19 | consumed tokens: 2007859200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:39:54 | step: 245200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.6787502065417357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.2 | consumed tokens: 2008678400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:40:15 | step: 245300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.6771936973091215e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 2009497600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:40:36 | step: 245400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6756371880765073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2010316800.0 | grad norm avg: 0.89 | grad norm last: 0.94 | 
2025-12-30T13:40:56 | step: 245500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.6740804969449528e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.17 | consumed tokens: 2011136000.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:41:17 | step: 245600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6725238058133982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.5 | consumed tokens: 2011955200.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:41:37 | step: 245700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.6709671146818437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.95 | consumed tokens: 2012774400.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T13:41:58 | step: 245800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.669410423550289e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2013593600.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:42:19 | step: 245900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.6678535505197942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.58 | consumed tokens: 2014412800.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2025-12-30T13:42:39 | step: 246000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.666296495590359e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.66 | consumed tokens: 2015232000.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T13:43:00 | step: 246100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.664739622559864e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.06 | consumed tokens: 2016051200.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:43:20 | step: 246200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.6631825676304288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.77 | consumed tokens: 2016870400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:43:41 | step: 246300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.6616253308020532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.5 | consumed tokens: 2017689600.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T13:44:02 | step: 246400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.660068275872618e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2018508800.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:44:22 | step: 246500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.6585110390442424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.25 | consumed tokens: 2019328000.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:44:43 | step: 246600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 2.6569536203169264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.17 | consumed tokens: 2020147200.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T13:45:04 | step: 246700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.6553963834885508e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.5 | consumed tokens: 2020966400.0 | grad norm avg: 0.89 | grad norm last: 0.81 | 
2025-12-30T13:45:25 | step: 246800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.6538389647612348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.53 | consumed tokens: 2021785600.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T13:45:45 | step: 246900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.6522813641349785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.8 | consumed tokens: 2022604800.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T13:46:06 | step: 247000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.6507239454076625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.89 | consumed tokens: 2023424000.0 | grad norm avg: 0.89 | grad norm last: 0.81 | 
2025-12-30T13:46:26 | step: 247100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.6491663447814062e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.58 | consumed tokens: 2024243200.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:46:47 | step: 247200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6476085622562096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.56 | consumed tokens: 2025062400.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2025-12-30T13:47:07 | step: 247300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6460509616299532e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.66 | consumed tokens: 2025881600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:47:28 | step: 247400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.6444931791047566e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.75 | consumed tokens: 2026700800.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T13:47:49 | step: 247500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.6429352146806195e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.72 | consumed tokens: 2027520000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:48:09 | step: 247600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.641377432155423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.02 | consumed tokens: 2028339200.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:48:30 | step: 247700 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.639819467731286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.19 | consumed tokens: 2029158400.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:48:50 | step: 247800 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.6382615033071488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 2029977600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:49:11 | step: 247900 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.6367035388830118e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 2030796800.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T13:49:31 | step: 248000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.6351453925599344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2031616000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:49:52 | step: 248100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.633587246236857e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.72 | consumed tokens: 2032435200.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:50:12 | step: 248200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.6320290999137796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.84 | consumed tokens: 2033254400.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T13:50:33 | step: 248300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.630470771691762e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.3 | consumed tokens: 2034073600.0 | grad norm avg: 0.88 | grad norm last: 0.87 | 
2025-12-30T13:50:53 | step: 248400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6289124434697442e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.41 | consumed tokens: 2034892800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2025-12-30T13:51:14 | step: 248500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.6273541152477264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.25 | consumed tokens: 2035712000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:51:34 | step: 248600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6257957870257087e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.16 | consumed tokens: 2036531200.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:51:55 | step: 248700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.6242372769047506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.7 | consumed tokens: 2037350400.0 | grad norm avg: 0.9 | grad norm last: 0.83 | 
2025-12-30T13:52:15 | step: 248800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.6226787667837925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.45 | consumed tokens: 2038169600.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T13:52:36 | step: 248900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.6211202566628344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 2038988800.0 | grad norm avg: 0.89 | grad norm last: 0.95 | 
2025-12-30T13:52:56 | step: 249000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6195617465418763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.02 | consumed tokens: 2039808000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:53:17 | step: 249100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.618003054521978e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.97 | consumed tokens: 2040627200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:53:38 | step: 249200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.6164443625020795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 2041446400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:53:58 | step: 249300 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.614885670482181e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.83 | consumed tokens: 2042265600.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:54:19 | step: 249400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.6133269784622826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.89 | consumed tokens: 2043084800.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:54:39 | step: 249500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.6117681045434438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.77 | consumed tokens: 2043904000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T13:55:00 | step: 249600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.610209230624605e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2044723200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T13:55:20 | step: 249700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.6086503567057662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.55 | consumed tokens: 2045542400.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T13:55:41 | step: 249800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.6070914827869274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.88 | consumed tokens: 2046361600.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T13:56:01 | step: 249900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.6055324269691482e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2047180800.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T13:56:22 | step: 250000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.6039735530503094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2048000000.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2025-12-30T13:56:44 | step: 250100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.6024144972325303e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.25 | consumed tokens: 2048819200.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T13:57:04 | step: 250200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.6008552595158108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.19 | consumed tokens: 2049638400.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:57:25 | step: 250300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.5992962036980316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.48 | consumed tokens: 2050457600.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T13:57:45 | step: 250400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.597736965981312e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 2051276800.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T13:58:06 | step: 250500 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 2.596177910163533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 2052096000.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T13:58:27 | step: 250600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.5946186724468134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.08 | consumed tokens: 2052915200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T13:58:47 | step: 250700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.5930592528311536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 2053734400.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T13:59:08 | step: 250800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.591500015114434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.59 | consumed tokens: 2054553600.0 | grad norm avg: 0.9 | grad norm last: 1.15 | 
2025-12-30T13:59:28 | step: 250900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.5899405954987742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.48 | consumed tokens: 2055372800.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T13:59:49 | step: 251000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.5883813577820547e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.41 | consumed tokens: 2056192000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:00:09 | step: 251100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.5868219381663948e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.48 | consumed tokens: 2057011200.0 | grad norm avg: 0.9 | grad norm last: 0.81 | 
2025-12-30T14:00:30 | step: 251200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.5852623366517946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.7 | consumed tokens: 2057830400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T14:00:50 | step: 251300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.5837029170361347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.78 | consumed tokens: 2058649600.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:01:11 | step: 251400 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.582143497420475e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.53 | consumed tokens: 2059468800.0 | grad norm avg: 0.89 | grad norm last: 0.98 | 
2025-12-30T14:01:31 | step: 251500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.5805838959058747e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.7 | consumed tokens: 2060288000.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:01:51 | step: 251600 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.5790242943912745e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.83 | consumed tokens: 2061107200.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:02:12 | step: 251700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.5774646928766742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.88 | consumed tokens: 2061926400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T14:02:33 | step: 251800 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 2.575905091362074e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.52 | consumed tokens: 2062745600.0 | grad norm avg: 0.89 | grad norm last: 0.8 | 
2025-12-30T14:02:53 | step: 251900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.5743454898474738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 2063564800.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T14:03:14 | step: 252000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.5727857064339332e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.64 | consumed tokens: 2064384000.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:03:34 | step: 252100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.5712259230203927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.55 | consumed tokens: 2065203200.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T14:03:55 | step: 252200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.5696663215057924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.78 | consumed tokens: 2066022400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T14:04:15 | step: 252300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.568106538092252e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.38 | consumed tokens: 2066841600.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T14:04:36 | step: 252400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.5665467546787113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.94 | consumed tokens: 2067660800.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T14:04:56 | step: 252500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.5649867893662304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.59 | consumed tokens: 2068480000.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:05:17 | step: 252600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.5634270059526898e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.0 | consumed tokens: 2069299200.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:05:37 | step: 252700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.5618672225391492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.83 | consumed tokens: 2070118400.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T14:05:58 | step: 252800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.5603072572266683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.08 | consumed tokens: 2070937600.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T14:06:19 | step: 252900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.5587472919141874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 2071756800.0 | grad norm avg: 0.89 | grad norm last: 0.96 | 
2025-12-30T14:06:39 | step: 253000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.5571875085006468e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 2072576000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T14:07:00 | step: 253100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.555627543188166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 2073395200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:07:20 | step: 253200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.554067577875685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 4.19 | consumed tokens: 2074214400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:07:41 | step: 253300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.5525074306642637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.56 | consumed tokens: 2075033600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:08:02 | step: 253400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.5509474653517827e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.64 | consumed tokens: 2075852800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:08:22 | step: 253500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.5493875000393018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 2076672000.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T14:08:43 | step: 253600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.5478273528278805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.7 | consumed tokens: 2077491200.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T14:09:03 | step: 253700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.5462673875153996e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.34 | consumed tokens: 2078310400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T14:09:24 | step: 253800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.5447072403039783e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.06 | consumed tokens: 2079129600.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:09:44 | step: 253900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.543147093092557e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.02 | consumed tokens: 2079948800.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:10:05 | step: 254000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.5415869458811358e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.42 | consumed tokens: 2080768000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:10:25 | step: 254100 | train samples/s: 85.3 | train mfu (16-bit): -1.0 | lr mean: 2.5400267986697145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.02 | consumed tokens: 2081587200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:10:45 | step: 254200 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 2.5384666514582932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.27 | consumed tokens: 2082406400.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T14:11:06 | step: 254300 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.536906504246872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.47 | consumed tokens: 2083225600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:11:26 | step: 254400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.5353463570354506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.28 | consumed tokens: 2084044800.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T14:11:47 | step: 254500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.5337862098240294e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 2084864000.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T14:12:07 | step: 254600 | train samples/s: 85.3 | train mfu (16-bit): -1.0 | lr mean: 2.532226062612608e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.91 | consumed tokens: 2085683200.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T14:12:27 | step: 254700 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.5306657335022464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.03 | consumed tokens: 2086502400.0 | grad norm avg: 0.9 | grad norm last: 0.82 | 
2025-12-30T14:12:48 | step: 254800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.529105586290825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.42 | consumed tokens: 2087321600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:13:09 | step: 254900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.5275452571804635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.77 | consumed tokens: 2088140800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:13:29 | step: 255000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.5259851099690422e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.69 | consumed tokens: 2088960000.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:13:51 | step: 255100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.5244247808586806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 2089779200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:14:12 | step: 255200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.522864451748319e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.69 | consumed tokens: 2090598400.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:14:32 | step: 255300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.5213043045368977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2091417600.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-30T14:14:53 | step: 255400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.519743975426536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.23 | consumed tokens: 2092236800.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:15:13 | step: 255500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.5181836463161744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 4.0 | consumed tokens: 2093056000.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T14:15:34 | step: 255600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.5166233172058128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.14 | consumed tokens: 2093875200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:15:54 | step: 255700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.5150631699943915e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.31 | consumed tokens: 2094694400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T14:16:15 | step: 255800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.51350284088403e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.41 | consumed tokens: 2095513600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:16:36 | step: 255900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.5119425117736682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.41 | consumed tokens: 2096332800.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T14:16:57 | step: 256000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.5103821826633066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.95 | consumed tokens: 2097152000.0 | grad norm avg: 0.89 | grad norm last: 0.95 | 
2025-12-30T14:17:17 | step: 256100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.508821853552945e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.66 | consumed tokens: 2097971200.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T14:17:38 | step: 256200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.5072615244425833e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.78 | consumed tokens: 2098790400.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T14:17:59 | step: 256300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.5057011953322217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.69 | consumed tokens: 2099609600.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:18:19 | step: 256400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.50414086622186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2100428800.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T14:18:40 | step: 256500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.5025805371114984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.22 | consumed tokens: 2101248000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:19:00 | step: 256600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.5010202080011368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.48 | consumed tokens: 2102067200.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:19:21 | step: 256700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.499459878890775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.8 | consumed tokens: 2102886400.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:19:41 | step: 256800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.4978995497804135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.81 | consumed tokens: 2103705600.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2025-12-30T14:20:02 | step: 256900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.4963394025689922e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.38 | consumed tokens: 2104524800.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T14:20:23 | step: 257000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.4947790734586306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.23 | consumed tokens: 2105344000.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:20:43 | step: 257100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.493218744348269e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.69 | consumed tokens: 2106163200.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T14:21:04 | step: 257200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.4916584152379073e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.03 | consumed tokens: 2106982400.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T14:21:24 | step: 257300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.4900980861275457e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.44 | consumed tokens: 2107801600.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:21:45 | step: 257400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.488537757017184e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.48 | consumed tokens: 2108620800.0 | grad norm avg: 0.89 | grad norm last: 0.81 | 
2025-12-30T14:22:06 | step: 257500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4869774279068224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.44 | consumed tokens: 2109440000.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:22:26 | step: 257600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.485417280695401e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 2110259200.0 | grad norm avg: 0.9 | grad norm last: 0.98 | 
2025-12-30T14:22:46 | step: 257700 | train samples/s: 85.3 | train mfu (16-bit): -1.0 | lr mean: 2.4838569515850395e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.7 | consumed tokens: 2111078400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:23:07 | step: 257800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.482296622474678e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.91 | consumed tokens: 2111897600.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:23:27 | step: 257900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.4807362933643162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.59 | consumed tokens: 2112716800.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T14:23:48 | step: 258000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.479176146152895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2113536000.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:24:08 | step: 258100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.4776158170425333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.33 | consumed tokens: 2114355200.0 | grad norm avg: 0.9 | grad norm last: 0.8 | 
2025-12-30T14:24:29 | step: 258200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.476055669831112e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 2115174400.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T14:24:49 | step: 258300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.4744953407207504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 2115993600.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T14:25:10 | step: 258400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.472935193509329e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.7 | consumed tokens: 2116812800.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T14:25:31 | step: 258500 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.4713750462979078e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.08 | consumed tokens: 2117632000.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T14:25:51 | step: 258600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.4698147171875462e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2118451200.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T14:26:12 | step: 258700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.468254569976125e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.31 | consumed tokens: 2119270400.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T14:26:32 | step: 258800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.4666944227647036e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.75 | consumed tokens: 2120089600.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:26:53 | step: 258900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.4651342755532824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.88 | consumed tokens: 2120908800.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:27:14 | step: 259000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.463574128341861e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.81 | consumed tokens: 2121728000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T14:27:34 | step: 259100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.4620139811304398e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.06 | consumed tokens: 2122547200.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T14:27:55 | step: 259200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.460454015817959e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2123366400.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:28:15 | step: 259300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.4588938686065376e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.06 | consumed tokens: 2124185600.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2025-12-30T14:28:36 | step: 259400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4573337213951163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.91 | consumed tokens: 2125004800.0 | grad norm avg: 0.9 | grad norm last: 0.82 | 
2025-12-30T14:28:56 | step: 259500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4557737560826354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.3 | consumed tokens: 2125824000.0 | grad norm avg: 0.9 | grad norm last: 0.98 | 
2025-12-30T14:29:17 | step: 259600 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 2.454213608871214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2126643200.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2025-12-30T14:29:38 | step: 259700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.452653643558733e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2127462400.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T14:29:58 | step: 259800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.4510936782462522e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.44 | consumed tokens: 2128281600.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T14:30:19 | step: 259900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.4495337129337713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.92 | consumed tokens: 2129100800.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:30:39 | step: 260000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.4479737476212904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.92 | consumed tokens: 2129920000.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T14:31:02 | step: 260100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4464137823088095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.81 | consumed tokens: 2130739200.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:31:22 | step: 260200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.4448538169963285e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.56 | consumed tokens: 2131558400.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:31:42 | step: 260300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.443294033582788e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.11 | consumed tokens: 2132377600.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:32:03 | step: 260400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.441734068270307e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 2133196800.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:32:24 | step: 260500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.4401742848567665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.5 | consumed tokens: 2134016000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:32:44 | step: 260600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.438614501443226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2134835200.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T14:33:05 | step: 260700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.4370547180296853e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.89 | consumed tokens: 2135654400.0 | grad norm avg: 0.89 | grad norm last: 0.9 | 
2025-12-30T14:33:25 | step: 260800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.4354949346161447e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.53 | consumed tokens: 2136473600.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:33:46 | step: 260900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.4339351512026042e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.52 | consumed tokens: 2137292800.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2025-12-30T14:34:07 | step: 261000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4323753677890636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.61 | consumed tokens: 2138112000.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:34:27 | step: 261100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.4308157662744634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 2138931200.0 | grad norm avg: 0.9 | grad norm last: 0.98 | 
2025-12-30T14:34:48 | step: 261200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.4292559828609228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.03 | consumed tokens: 2139750400.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:35:08 | step: 261300 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.4276963813463226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.52 | consumed tokens: 2140569600.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T14:35:28 | step: 261400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.4261367798317224e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 2141388800.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:35:49 | step: 261500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.424577178317122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.86 | consumed tokens: 2142208000.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:36:09 | step: 261600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.4230177587014623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 2143027200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T14:36:30 | step: 261700 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.421458157186862e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2143846400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2025-12-30T14:36:50 | step: 261800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.4198987375712022e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2144665600.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T14:37:11 | step: 261900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.4183393179555424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2145484800.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:37:31 | step: 262000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.4167798983398825e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.7 | consumed tokens: 2146304000.0 | grad norm avg: 0.89 | grad norm last: 0.82 | 
2025-12-30T14:37:52 | step: 262100 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.4152204787242226e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.59 | consumed tokens: 2147123200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:38:12 | step: 262200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.4136610591085628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.2 | consumed tokens: 2147942400.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:38:33 | step: 262300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.4121018213918433e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.55 | consumed tokens: 2148761600.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T14:38:53 | step: 262400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.4105424017761834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.33 | consumed tokens: 2149580800.0 | grad norm avg: 0.89 | grad norm last: 0.93 | 
2025-12-30T14:39:14 | step: 262500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.408983164059464e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.83 | consumed tokens: 2150400000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:39:34 | step: 262600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.4074241082416847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 2151219200.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:39:55 | step: 262700 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.4058648705249652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.86 | consumed tokens: 2152038400.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:40:15 | step: 262800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.404305814707186e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.45 | consumed tokens: 2152857600.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:40:36 | step: 262900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.4027465769904666e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2153676800.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T14:40:56 | step: 263000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.4011875211726874e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2154496000.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T14:41:17 | step: 263100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.3996286472538486e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 2155315200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T14:41:37 | step: 263200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.3980695914360695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.05 | consumed tokens: 2156134400.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:41:58 | step: 263300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.3965107175172307e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 2.73 | consumed tokens: 2156953600.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T14:42:18 | step: 263400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.394951843598392e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 2157772800.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T14:42:39 | step: 263500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.393392969679553e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.44 | consumed tokens: 2158592000.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:42:59 | step: 263600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.3918340957607143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2159411200.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T14:43:20 | step: 263700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.3902754037408158e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 2160230400.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T14:43:40 | step: 263800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.388716529821977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.39 | consumed tokens: 2161049600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T14:44:01 | step: 263900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.387158019701019e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.41 | consumed tokens: 2161868800.0 | grad norm avg: 0.9 | grad norm last: 0.83 | 
2025-12-30T14:44:21 | step: 264000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.3855993276811205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.27 | consumed tokens: 2162688000.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T14:44:42 | step: 264100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.384040635661222e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.8 | consumed tokens: 2163507200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:45:02 | step: 264200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.382482125540264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.33 | consumed tokens: 2164326400.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T14:45:23 | step: 264300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.380923615419306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.61 | consumed tokens: 2165145600.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:45:43 | step: 264400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.379365287197288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.8 | consumed tokens: 2165964800.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T14:46:04 | step: 264500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.37780677707633e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.98 | consumed tokens: 2166784000.0 | grad norm avg: 0.89 | grad norm last: 0.91 | 
2025-12-30T14:46:24 | step: 264600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.3762484488543123e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.8 | consumed tokens: 2167603200.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:46:45 | step: 264700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.3746901206322946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.59 | consumed tokens: 2168422400.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T14:47:06 | step: 264800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.3731319743092172e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.41 | consumed tokens: 2169241600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2025-12-30T14:47:26 | step: 264900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.3715736460871994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.83 | consumed tokens: 2170060800.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T14:47:47 | step: 265000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.370015499764122e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.47 | consumed tokens: 2170880000.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:48:09 | step: 265100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.368457535339985e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.97 | consumed tokens: 2171699200.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T14:48:29 | step: 265200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.3668993890169077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.19 | consumed tokens: 2172518400.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T14:48:49 | step: 265300 | train samples/s: 85.4 | train mfu (16-bit): -1.0 | lr mean: 2.3653414245927706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.83 | consumed tokens: 2173337600.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T14:49:10 | step: 265400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3637834601686336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.25 | consumed tokens: 2174156800.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T14:49:30 | step: 265500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.362225677643437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.97 | consumed tokens: 2174976000.0 | grad norm avg: 0.9 | grad norm last: 1.0 | 
2025-12-30T14:49:51 | step: 265600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.3606677132193e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.97 | consumed tokens: 2175795200.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:50:11 | step: 265700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.3591099306941032e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.97 | consumed tokens: 2176614400.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2025-12-30T14:50:32 | step: 265800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.357552330067847e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.97 | consumed tokens: 2177433600.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:50:52 | step: 265900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.3559947294415906e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.47 | consumed tokens: 2178252800.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:51:13 | step: 266000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.354436946916394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2179072000.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T14:51:34 | step: 266100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.352879528189078e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2179891200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:51:54 | step: 266200 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 2.3513219275628217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 2180710400.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:52:15 | step: 266300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.3497645088355057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2181529600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T14:52:35 | step: 266400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.34820727200713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 2182348800.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T14:52:56 | step: 266500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.346649853279814e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.75 | consumed tokens: 2183168000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:53:17 | step: 266600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.3450926164514385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.38 | consumed tokens: 2183987200.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:53:37 | step: 266700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.343535379623063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.34 | consumed tokens: 2184806400.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T14:53:58 | step: 266800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3419783246936277e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.08 | consumed tokens: 2185625600.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:54:19 | step: 266900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.3404212697641924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.34 | consumed tokens: 2186444800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T14:54:39 | step: 267000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.338864214834757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 2187264000.0 | grad norm avg: 0.9 | grad norm last: 0.94 | 
2025-12-30T14:55:00 | step: 267100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3373073418042623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.17 | consumed tokens: 2188083200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:55:20 | step: 267200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.3357504687737674e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.67 | consumed tokens: 2188902400.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T14:55:41 | step: 267300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.3341935957432725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.17 | consumed tokens: 2189721600.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T14:56:01 | step: 267400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.332636904611718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 2190540800.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T14:56:22 | step: 267500 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 2.3310802134801634e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 2191360000.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T14:56:43 | step: 267600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.3295237042475492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.25 | consumed tokens: 2192179200.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T14:57:03 | step: 267700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.3279670131159946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.17 | consumed tokens: 2192998400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T14:57:24 | step: 267800 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.3264106857823208e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.27 | consumed tokens: 2193817600.0 | grad norm avg: 0.89 | grad norm last: 0.88 | 
2025-12-30T14:57:44 | step: 267900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.3248541765497066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.78 | consumed tokens: 2194636800.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:58:05 | step: 268000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.3232978492160328e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.8 | consumed tokens: 2195456000.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T14:58:25 | step: 268100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.3217417037812993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.45 | consumed tokens: 2196275200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T14:58:46 | step: 268200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.3201853764476255e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.73 | consumed tokens: 2197094400.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T14:59:06 | step: 268300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.318629231012892e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 2197913600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T14:59:27 | step: 268400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.317073267477099e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.28 | consumed tokens: 2198732800.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T14:59:47 | step: 268500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3155173039413057e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.48 | consumed tokens: 2199552000.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T15:00:08 | step: 268600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.3139613404055126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2200371200.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T15:00:28 | step: 268700 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.3124055587686598e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.97 | consumed tokens: 2201190400.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:00:49 | step: 268800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.310849777131807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.67 | consumed tokens: 2202009600.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T15:01:09 | step: 268900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.3092941773938946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 2202828800.0 | grad norm avg: 0.91 | grad norm last: 0.82 | 
2025-12-30T15:01:30 | step: 269000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.3077385776559822e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 4.34 | consumed tokens: 2203648000.0 | grad norm avg: 0.91 | grad norm last: 0.82 | 
2025-12-30T15:01:51 | step: 269100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.3061829779180698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.69 | consumed tokens: 2204467200.0 | grad norm avg: 0.91 | grad norm last: 0.99 | 
2025-12-30T15:02:11 | step: 269200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.3046275600790977e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.92 | consumed tokens: 2205286400.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T15:02:32 | step: 269300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.3030721422401257e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.64 | consumed tokens: 2206105600.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T15:02:52 | step: 269400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.301516906300094e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.47 | consumed tokens: 2206924800.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T15:03:13 | step: 269500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.2999616703600623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.83 | consumed tokens: 2207744000.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T15:03:33 | step: 269600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.2984064344200306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.78 | consumed tokens: 2208563200.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:03:54 | step: 269700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.2968513803789392e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.84 | consumed tokens: 2209382400.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T15:04:15 | step: 269800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.2952965082367882e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.58 | consumed tokens: 2210201600.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T15:04:35 | step: 269900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.293741454195697e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.92 | consumed tokens: 2211020800.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:04:56 | step: 270000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.2921867639524862e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.56 | consumed tokens: 2211840000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T15:05:18 | step: 270100 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 2.2906318918103352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.61 | consumed tokens: 2212659200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T15:05:39 | step: 270200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.289077383466065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.81 | consumed tokens: 2213478400.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:05:59 | step: 270300 | train samples/s: 85.4 | train mfu (16-bit): -1.0 | lr mean: 2.2875226932228543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.41 | consumed tokens: 2214297600.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T15:06:19 | step: 270400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.285968184878584e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.94 | consumed tokens: 2215116800.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T15:06:40 | step: 270500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.284413858433254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 2215936000.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:07:01 | step: 270600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.282859531987924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.38 | consumed tokens: 2216755200.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:07:22 | step: 270700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.2813053874415345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.52 | consumed tokens: 2217574400.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T15:07:42 | step: 270800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.279751242895145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.7 | consumed tokens: 2218393600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:08:02 | step: 270900 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.2781970983487554e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 2219212800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:08:23 | step: 271000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.276643135701306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.81 | consumed tokens: 2220032000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:08:43 | step: 271100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.275089173053857e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.2 | consumed tokens: 2220851200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T15:09:04 | step: 271200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.273535392305348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.28 | consumed tokens: 2221670400.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:09:24 | step: 271300 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.2719817934557796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.84 | consumed tokens: 2222489600.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T15:09:45 | step: 271400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 2.270428194606211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 4.19 | consumed tokens: 2223308800.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T15:10:06 | step: 271500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2688745957566425e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 2224128000.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2025-12-30T15:10:26 | step: 271600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.2673211788060144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2224947200.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:10:47 | step: 271700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.2657677618553862e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 2225766400.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T15:11:08 | step: 271800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.2642145268036984e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.18 | train loss last: 3.73 | consumed tokens: 2226585600.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T15:11:28 | step: 271900 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.262661473650951e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.38 | consumed tokens: 2227404800.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:11:48 | step: 272000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.2611084204982035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.95 | consumed tokens: 2228224000.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:12:09 | step: 272100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.259555367345456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 2229043200.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T15:12:29 | step: 272200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.258002496091649e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 2229862400.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:12:50 | step: 272300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.2564498067367822e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.28 | consumed tokens: 2230681600.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T15:13:10 | step: 272400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2548971173819155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 4.41 | consumed tokens: 2231500800.0 | grad norm avg: 0.9 | grad norm last: 0.95 | 
2025-12-30T15:13:31 | step: 272500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.2533444280270487e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.58 | consumed tokens: 2232320000.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T15:13:51 | step: 272600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.2517919205711223e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 2233139200.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:14:12 | step: 272700 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 2.2502395950141363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.41 | consumed tokens: 2233958400.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2025-12-30T15:14:33 | step: 272800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2486872694571503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.53 | consumed tokens: 2234777600.0 | grad norm avg: 0.9 | grad norm last: 0.85 | 
2025-12-30T15:14:53 | step: 272900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2471351257991046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 2235596800.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T15:15:14 | step: 273000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.245582982141059e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2236416000.0 | grad norm avg: 0.9 | grad norm last: 0.87 | 
2025-12-30T15:15:35 | step: 273100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.2440310203819536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.81 | consumed tokens: 2237235200.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T15:15:55 | step: 273200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.2424792405217886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.73 | consumed tokens: 2238054400.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2025-12-30T15:16:16 | step: 273300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.2409274606616236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.5 | consumed tokens: 2238873600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:16:36 | step: 273400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.2393756808014587e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.45 | consumed tokens: 2239692800.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T15:16:56 | step: 273500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.237824082840234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.98 | consumed tokens: 2240512000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:17:17 | step: 273600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.2362726667779498e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 2241331200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T15:17:38 | step: 273700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.2347212507156655e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.78 | consumed tokens: 2242150400.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2025-12-30T15:17:58 | step: 273800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.2331700165523216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.62 | consumed tokens: 2242969600.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T15:18:18 | step: 273900 | train samples/s: 85.4 | train mfu (16-bit): -1.0 | lr mean: 2.2316187823889777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 2243788800.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T15:18:39 | step: 274000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.230067730124574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.59 | consumed tokens: 2244608000.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T15:18:59 | step: 274100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.228516859759111e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.33 | consumed tokens: 2245427200.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:19:20 | step: 274200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.2269659893936478e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.59 | consumed tokens: 2246246400.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2025-12-30T15:19:41 | step: 274300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.2254151190281846e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.83 | consumed tokens: 2247065600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:20:01 | step: 274400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.223864612460602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.17 | consumed tokens: 2247884800.0 | grad norm avg: 0.9 | grad norm last: 0.93 | 
2025-12-30T15:20:21 | step: 274500 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 2.2223141058930196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.34 | consumed tokens: 2248704000.0 | grad norm avg: 0.9 | grad norm last: 0.98 | 
2025-12-30T15:20:42 | step: 274600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.220763599325437e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2249523200.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:21:02 | step: 274700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.219213274656795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.55 | consumed tokens: 2250342400.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:21:23 | step: 274800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.2176631318870932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 2251161600.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2025-12-30T15:21:43 | step: 274900 | train samples/s: 85.4 | train mfu (16-bit): -1.0 | lr mean: 2.2161129891173914e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.92 | consumed tokens: 2251980800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:22:04 | step: 275000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.21456302824663e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.31 | consumed tokens: 2252800000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:22:26 | step: 275100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.213013249274809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.25 | consumed tokens: 2253619200.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:22:47 | step: 275200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.211463470302988e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 2254438400.0 | grad norm avg: 0.9 | grad norm last: 0.81 | 
2025-12-30T15:23:08 | step: 275300 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 2.209913873230107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.84 | consumed tokens: 2255257600.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T15:23:28 | step: 275400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.2083642761572264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2256076800.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T15:23:49 | step: 275500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.206814860983286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2256896000.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2025-12-30T15:24:09 | step: 275600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.205265627708286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.53 | consumed tokens: 2257715200.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2025-12-30T15:24:30 | step: 275700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.203716394433286e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.55 | consumed tokens: 2258534400.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:24:50 | step: 275800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.2021673430572264e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.91 | consumed tokens: 2259353600.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:25:11 | step: 275900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.200618473580107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.64 | consumed tokens: 2260172800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:25:31 | step: 276000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.1990696041029878e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.84 | consumed tokens: 2260992000.0 | grad norm avg: 0.91 | grad norm last: 0.99 | 
2025-12-30T15:25:52 | step: 276100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.197520916524809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.42 | consumed tokens: 2261811200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:26:13 | step: 276200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.1959724108455703e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.72 | consumed tokens: 2262630400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T15:26:33 | step: 276300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1944239051663317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 2263449600.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:26:54 | step: 276400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.1928755813860334e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.09 | consumed tokens: 2264268800.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T15:27:14 | step: 276500 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.1913272576057352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 2265088000.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T15:27:35 | step: 276600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.1897791157243773e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2265907200.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:27:55 | step: 276700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.1882311557419598e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 2266726400.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T15:28:16 | step: 276800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.1866833776584826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.92 | consumed tokens: 2267545600.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T15:28:37 | step: 276900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.1851355995750055e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 2268364800.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:28:57 | step: 277000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.1835880033904687e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2269184000.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T15:29:18 | step: 277100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.1820405891048722e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.05 | consumed tokens: 2270003200.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:29:38 | step: 277200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1804931748192757e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.84 | consumed tokens: 2270822400.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T15:29:59 | step: 277300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.1789459424326196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.94 | consumed tokens: 2271641600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:30:19 | step: 277400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.177398891944904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.03 | consumed tokens: 2272460800.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:30:40 | step: 277500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.175851841457188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.78 | consumed tokens: 2273280000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:31:00 | step: 277600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.1743049728684127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 2274099200.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:31:21 | step: 277700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.1727582861785777e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.77 | consumed tokens: 2274918400.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:31:41 | step: 277800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.1712115994887426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.5 | consumed tokens: 2275737600.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:32:02 | step: 277900 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 2.1696652765967883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.56 | consumed tokens: 2276556800.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:32:23 | step: 278000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.168118953704834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.42 | consumed tokens: 2277376000.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:32:43 | step: 278100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.1665726308128797e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.0 | consumed tokens: 2278195200.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:33:04 | step: 278200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.165026671718806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.08 | consumed tokens: 2279014400.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T15:33:25 | step: 278300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1634807126247324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.59 | consumed tokens: 2279833600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:33:45 | step: 278400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.1619347535306588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.94 | consumed tokens: 2280652800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:34:06 | step: 278500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.160389158234466e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.02 | consumed tokens: 2281472000.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T15:34:27 | step: 278600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.158843562938273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.55 | consumed tokens: 2282291200.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T15:34:47 | step: 278700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.1572981495410204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.5 | consumed tokens: 2283110400.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2025-12-30T15:35:08 | step: 278800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.1557529180427082e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2283929600.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:35:28 | step: 278900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.1542078684433363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 2284748800.0 | grad norm avg: 0.9 | grad norm last: 0.98 | 
2025-12-30T15:35:49 | step: 279000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.1526628188439645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.83 | consumed tokens: 2285568000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:36:10 | step: 279100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.151117951143533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.34 | consumed tokens: 2286387200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:36:30 | step: 279200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.149573265342042e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.17 | consumed tokens: 2287206400.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T15:36:51 | step: 279300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.1480285795405507e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.8 | consumed tokens: 2288025600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:37:11 | step: 279400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.146484075638e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.61 | consumed tokens: 2288844800.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T15:37:32 | step: 279500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.1449397536343895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.67 | consumed tokens: 2289664000.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T15:37:53 | step: 279600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.1433956135297194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.97 | consumed tokens: 2290483200.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-30T15:38:13 | step: 279700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.1418516553239897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.14 | consumed tokens: 2291302400.0 | grad norm avg: 0.91 | grad norm last: 1.0 | 
2025-12-30T15:38:33 | step: 279800 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.14030769711826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 2292121600.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:38:54 | step: 279900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.1387639208114706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.7 | consumed tokens: 2292940800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:39:15 | step: 280000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.1372203264036216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.36 | consumed tokens: 2293760000.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T15:39:37 | step: 280100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.135676913894713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.77 | consumed tokens: 2294579200.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:39:57 | step: 280200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.1341335013858043e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.53 | consumed tokens: 2295398400.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T15:40:18 | step: 280300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.132590270775836e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.0 | consumed tokens: 2296217600.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:40:39 | step: 280400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1310474039637484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.44 | consumed tokens: 2297036800.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:40:59 | step: 280500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.1295043552527204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.42 | consumed tokens: 2297856000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:41:20 | step: 280600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.1279616703395732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.69 | consumed tokens: 2298675200.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:41:40 | step: 280700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.1264191673253663e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 2299494400.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2025-12-30T15:42:01 | step: 280800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.1248766643111594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.05 | consumed tokens: 2300313600.0 | grad norm avg: 0.91 | grad norm last: 1.12 | 
2025-12-30T15:42:22 | step: 280900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.123334343195893e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 2301132800.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:42:42 | step: 281000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 2.1217922039795667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.88 | consumed tokens: 2301952000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T15:43:03 | step: 281100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 2.1202500647632405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.17 | consumed tokens: 2302771200.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:43:23 | step: 281200 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 2.118708289344795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2303590400.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:43:44 | step: 281300 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.1171665139263496e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.05 | consumed tokens: 2304409600.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:44:04 | step: 281400 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.1156249204068445e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.64 | consumed tokens: 2305228800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:44:24 | step: 281500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.1140835087862797e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.02 | consumed tokens: 2306048000.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:44:45 | step: 281600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.1125422790646553e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.36 | consumed tokens: 2306867200.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:45:05 | step: 281700 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.1110012312419713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.97 | consumed tokens: 2307686400.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:45:26 | step: 281800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1094603653182276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.72 | consumed tokens: 2308505600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:45:46 | step: 281900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.107919499394484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.42 | consumed tokens: 2309324800.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:46:07 | step: 282000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.1063788153696805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 2.78 | consumed tokens: 2310144000.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:46:28 | step: 282100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 2.1048383132438175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.58 | consumed tokens: 2310963200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T15:46:48 | step: 282200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.103297993016895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 2311782400.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T15:47:08 | step: 282300 | train samples/s: 85.5 | train mfu (16-bit): -1.0 | lr mean: 2.1017578546889126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.83 | consumed tokens: 2312601600.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2025-12-30T15:47:29 | step: 282400 | train samples/s: 85.4 | train mfu (16-bit): -1.0 | lr mean: 2.1002177163609304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2313420800.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:47:49 | step: 282500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0986779418308288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.62 | consumed tokens: 2314240000.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:48:10 | step: 282600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.0971381673007272e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.28 | consumed tokens: 2315059200.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:48:31 | step: 282700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.0955987565685064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2315878400.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T15:48:51 | step: 282800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.0940593458362855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.03 | consumed tokens: 2316697600.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:49:11 | step: 282900 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.092520117003005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.25 | consumed tokens: 2317516800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:49:32 | step: 283000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.0909808881697245e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 2318336000.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:49:53 | step: 283100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.0894420231343247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.25 | consumed tokens: 2319155200.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T15:50:13 | step: 283200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.0879033399978653e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.88 | consumed tokens: 2319974400.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:50:33 | step: 283300 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 2.0863646568614058e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.16 | consumed tokens: 2320793600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:50:54 | step: 283400 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 2.084826337522827e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.02 | consumed tokens: 2321612800.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:51:14 | step: 283500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.0832880181842484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.67 | consumed tokens: 2322432000.0 | grad norm avg: 0.9 | grad norm last: 0.84 | 
2025-12-30T15:51:35 | step: 283600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.08174988074461e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 2323251200.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T15:51:55 | step: 283700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.080211925203912e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.47 | consumed tokens: 2324070400.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T15:52:16 | step: 283800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.0786741515621543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.8 | consumed tokens: 2324889600.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T15:52:36 | step: 283900 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.077136559819337e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.67 | consumed tokens: 2325708800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:52:57 | step: 284000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.07559914997546e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.25 | consumed tokens: 2326528000.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T15:53:17 | step: 284100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0740619220305234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.72 | consumed tokens: 2327347200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T15:53:38 | step: 284200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.0725248759845272e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2328166400.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T15:53:58 | step: 284300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.070987829938531e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.25 | consumed tokens: 2328985600.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:54:19 | step: 284400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.0694511476904154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.03 | consumed tokens: 2329804800.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:54:40 | step: 284500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.0679144654423e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.3 | consumed tokens: 2330624000.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T15:55:01 | step: 284600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 2.0663779650931247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.12 | consumed tokens: 2331443200.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:55:21 | step: 284700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.0648418285418302e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.47 | consumed tokens: 2332262400.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T15:55:42 | step: 284800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.0633056919905357e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 2333081600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T15:56:02 | step: 284900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.0617697373381816e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.83 | consumed tokens: 2333900800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T15:56:23 | step: 285000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0602339645847678e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.84 | consumed tokens: 2334720000.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T15:56:45 | step: 285100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.0586983737302944e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.69 | consumed tokens: 2335539200.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T15:57:06 | step: 285200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0571629647747613e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.31 | consumed tokens: 2336358400.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T15:57:26 | step: 285300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.0556277377181686e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 2337177600.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T15:57:46 | step: 285400 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 2.0540926925605163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.39 | consumed tokens: 2337996800.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T15:58:07 | step: 285500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 2.0525578293018043e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.14 | consumed tokens: 2338816000.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:58:27 | step: 285600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.0510231479420327e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.53 | consumed tokens: 2339635200.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T15:58:48 | step: 285700 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 2.0494886484812014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.89 | consumed tokens: 2340454400.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T15:59:09 | step: 285800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.0479543309193105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.73 | consumed tokens: 2341273600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T15:59:29 | step: 285900 | train samples/s: 85.7 | train mfu (16-bit): -1.0 | lr mean: 2.04642019525636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.75 | consumed tokens: 2342092800.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T15:59:49 | step: 286000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0448862414923497e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.47 | consumed tokens: 2342912000.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T16:00:10 | step: 286100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0433522877283394e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.34 | consumed tokens: 2343731200.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T16:00:31 | step: 286200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 2.04181869776221e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 4.09 | consumed tokens: 2344550400.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T16:00:51 | step: 286300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.0402852896950208e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.38 | consumed tokens: 2345369600.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:01:12 | step: 286400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 2.0387518816278316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.62 | consumed tokens: 2346188800.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:01:32 | step: 286500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.037218837358523e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 2347008000.0 | grad norm avg: 0.91 | grad norm last: 0.82 | 
2025-12-30T16:01:53 | step: 286600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.035685974988155e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.5 | consumed tokens: 2347827200.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T16:02:13 | step: 286700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.0341532945167273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.02 | consumed tokens: 2348646400.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T16:02:34 | step: 286800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0326206140452996e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.75 | consumed tokens: 2349465600.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2025-12-30T16:02:54 | step: 286900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.0310882973717526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2350284800.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T16:03:15 | step: 287000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 2.029556162597146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.81 | consumed tokens: 2351104000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:03:36 | step: 287100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.0280242097214796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.05 | consumed tokens: 2351923200.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T16:03:56 | step: 287200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.0264922568458132e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.73 | consumed tokens: 2352742400.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T16:04:17 | step: 287300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.0249606677680276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.64 | consumed tokens: 2353561600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:04:37 | step: 287400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.0234292605891824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2354380800.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:04:58 | step: 287500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 2.0218980353092775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.62 | consumed tokens: 2355200000.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:05:18 | step: 287600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.020366991928313e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.11 | consumed tokens: 2356019200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T16:05:39 | step: 287700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 2.0188361304462887e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2356838400.0 | grad norm avg: 0.91 | grad norm last: 0.96 | 
2025-12-30T16:05:59 | step: 287800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 2.017305450863205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2357657600.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2025-12-30T16:06:20 | step: 287900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 2.0157749531790614e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.73 | consumed tokens: 2358476800.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T16:06:40 | step: 288000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 2.0142446373938583e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 2359296000.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T16:07:01 | step: 288100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0127145035075955e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.7 | consumed tokens: 2360115200.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2025-12-30T16:07:22 | step: 288200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.011184551520273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.05 | consumed tokens: 2360934400.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T16:07:42 | step: 288300 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 2.009654781431891e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.52 | consumed tokens: 2361753600.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:08:03 | step: 288400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 2.0081251932424493e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.86 | consumed tokens: 2362572800.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T16:08:23 | step: 288500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.0065959688508883e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.91 | consumed tokens: 2363392000.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2025-12-30T16:08:44 | step: 288600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 2.0050667444593273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.77 | consumed tokens: 2364211200.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:09:04 | step: 288700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 2.003537883865647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.84 | consumed tokens: 2365030400.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T16:09:25 | step: 288800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 2.0020090232719667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.39 | consumed tokens: 2365849600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:09:46 | step: 288900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 2.000480526476167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.36 | consumed tokens: 2366668800.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:10:06 | step: 289000 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 1.9989520296803676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.22 | consumed tokens: 2367488000.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2025-12-30T16:10:27 | step: 289100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.9974238966824487e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.16 | consumed tokens: 2368307200.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:10:47 | step: 289200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.99589594558347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 2369126400.0 | grad norm avg: 0.92 | grad norm last: 0.84 | 
2025-12-30T16:11:07 | step: 289300 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.994368176383432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.7 | consumed tokens: 2369945600.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T16:11:28 | step: 289400 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.9928405890823342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2370764800.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T16:11:48 | step: 289500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.9913131836801767e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.7 | consumed tokens: 2371584000.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T16:12:09 | step: 289600 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.9897859601769596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2372403200.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T16:12:30 | step: 289700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.988258918572683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.56 | consumed tokens: 2373222400.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T16:12:50 | step: 289800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.986732240766287e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.48 | consumed tokens: 2374041600.0 | grad norm avg: 0.92 | grad norm last: 1.05 | 
2025-12-30T16:13:11 | step: 289900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.9852055629598908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 2374860800.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:13:32 | step: 290000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.9836792489513755e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 2375680000.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:13:54 | step: 290100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.9821531168418005e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.86 | consumed tokens: 2376499200.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:14:14 | step: 290200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.980627166631166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2377318400.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T16:14:35 | step: 290300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.9791013983194716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.7 | consumed tokens: 2378137600.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T16:14:56 | step: 290400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.9775758119067177e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.31 | consumed tokens: 2378956800.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:15:16 | step: 290500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.976050407392904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.06 | consumed tokens: 2379776000.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:15:37 | step: 290600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.974525184778031e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.31 | consumed tokens: 2380595200.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T16:15:58 | step: 290700 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.9730003259610385e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.44 | consumed tokens: 2381414400.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:16:18 | step: 290800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.971475467144046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.86 | consumed tokens: 2382233600.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T16:16:39 | step: 290900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.9699509721249342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.08 | consumed tokens: 2383052800.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:16:59 | step: 291000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.9684266590047628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.14 | consumed tokens: 2383872000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:17:20 | step: 291100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.9669025277835317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.95 | consumed tokens: 2384691200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:17:41 | step: 291200 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.965378578461241e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2385510400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:18:02 | step: 291300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.963854992936831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.53 | consumed tokens: 2386329600.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:18:22 | step: 291400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.962331407412421e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.03 | consumed tokens: 2387148800.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T16:18:43 | step: 291500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.9608081856858917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 2387968000.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T16:19:03 | step: 291600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.9592851458583027e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.33 | consumed tokens: 2388787200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:19:24 | step: 291700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.957762287929654e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.2 | consumed tokens: 2389606400.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2025-12-30T16:19:45 | step: 291800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.956239611899946e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 2390425600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:20:05 | step: 291900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.954717117769178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2391244800.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T16:20:26 | step: 292000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.9531949874362908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.44 | consumed tokens: 2392064000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T16:20:46 | step: 292100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.9516728571034037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.16 | consumed tokens: 2392883200.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T16:21:07 | step: 292200 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 1.9501510905683972e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 2393702400.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2025-12-30T16:21:28 | step: 292300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.948629505932331e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.55 | consumed tokens: 2394521600.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T16:21:48 | step: 292400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.9471081031952053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 2395340800.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:22:09 | step: 292500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.9455870642559603e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.17 | train loss last: 3.47 | consumed tokens: 2396160000.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T16:22:30 | step: 292600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.9440660253167152e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.45 | consumed tokens: 2396979200.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T16:22:50 | step: 292700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.942545350175351e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 2397798400.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:23:11 | step: 292800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.941024856932927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.2 | consumed tokens: 2398617600.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:23:32 | step: 292900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.9395047274883837e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 2399436800.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2025-12-30T16:23:52 | step: 293000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.9379845980438404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 2400256000.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:24:12 | step: 293100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.9364648323971778e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.48 | consumed tokens: 2401075200.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T16:24:33 | step: 293200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.9349452486494556e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.09 | consumed tokens: 2401894400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:24:54 | step: 293300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.9334258468006738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 2402713600.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T16:25:14 | step: 293400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.9319066268508323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2403532800.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:25:35 | step: 293500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.930387588799931e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.42 | consumed tokens: 2404352000.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:25:56 | step: 293600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.9288689145469107e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2405171200.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:26:16 | step: 293700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.9273504221928306e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.52 | consumed tokens: 2405990400.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T16:26:37 | step: 293800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.925832111737691e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 2406809600.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2025-12-30T16:26:57 | step: 293900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.924314165080432e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.0 | consumed tokens: 2407628800.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:27:18 | step: 294000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.9227964003221132e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.83 | consumed tokens: 2408448000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:27:38 | step: 294100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.921278817462735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.3 | consumed tokens: 2409267200.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T16:27:59 | step: 294200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.919761416502297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.36 | consumed tokens: 2410086400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:28:19 | step: 294300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.9182441974407993e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2410905600.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:28:40 | step: 294400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.9167273421771824e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2411724800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T16:29:00 | step: 294500 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 1.915210668812506e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.25 | consumed tokens: 2412544000.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2025-12-30T16:29:21 | step: 294600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.9136941773467697e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.55 | consumed tokens: 2413363200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T16:29:41 | step: 294700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.9121780496789142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.47 | consumed tokens: 2414182400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:30:02 | step: 294800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.9106619220110588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.69 | consumed tokens: 2415001600.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2025-12-30T16:30:23 | step: 294900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.909146158141084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2415820800.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T16:30:43 | step: 295000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.90763075806899e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.98 | consumed tokens: 2416640000.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:31:05 | step: 295100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.906115357996896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.53 | consumed tokens: 2417459200.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:31:26 | step: 295200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.9046003217226826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.17 | consumed tokens: 2418278400.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2025-12-30T16:31:47 | step: 295300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.9030854673474096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.89 | consumed tokens: 2419097600.0 | grad norm avg: 0.92 | grad norm last: 1.0 | 
2025-12-30T16:32:07 | step: 295400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.901570794871077e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.52 | consumed tokens: 2419916800.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T16:32:28 | step: 295500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.900056486192625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 2420736000.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2025-12-30T16:32:48 | step: 295600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.8985423594131134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.89 | consumed tokens: 2421555200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:33:09 | step: 295700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.8970284145325422e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.03 | consumed tokens: 2422374400.0 | grad norm avg: 0.91 | grad norm last: 0.92 | 
2025-12-30T16:33:29 | step: 295800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8955148334498517e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.23 | consumed tokens: 2423193600.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T16:33:50 | step: 295900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.8940014342661016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.56 | consumed tokens: 2424012800.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:34:10 | step: 296000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.8924882169812918e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.81 | consumed tokens: 2424832000.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:34:31 | step: 296100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.8909753634943627e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.61 | consumed tokens: 2425651200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:34:52 | step: 296200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.8894625100074336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.33 | consumed tokens: 2426470400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:35:12 | step: 296300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8879502022173256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.31 | consumed tokens: 2427289600.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T16:35:33 | step: 296400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.8864378944272175e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.62 | consumed tokens: 2428108800.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:35:54 | step: 296500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8849259504349902e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.28 | consumed tokens: 2428928000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:36:14 | step: 296600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.8834141883417033e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.41 | consumed tokens: 2429747200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:36:35 | step: 296700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.8819026081473567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.94 | consumed tokens: 2430566400.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T16:36:55 | step: 296800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.8803913917508908e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.3 | consumed tokens: 2431385600.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T16:37:16 | step: 296900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.8788803572533652e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.61 | consumed tokens: 2432204800.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:37:36 | step: 297000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.8773696865537204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.23 | consumed tokens: 2433024000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:37:57 | step: 297100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.8758590158540756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.44 | consumed tokens: 2433843200.0 | grad norm avg: 0.91 | grad norm last: 0.87 | 
2025-12-30T16:38:17 | step: 297200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.8743488908512518e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.55 | consumed tokens: 2434662400.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:38:38 | step: 297300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.872838765848428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 4.0 | consumed tokens: 2435481600.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:38:59 | step: 297400 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.871329004643485e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 2436300800.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T16:39:19 | step: 297500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8698194253374822e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.8 | consumed tokens: 2437120000.0 | grad norm avg: 0.92 | grad norm last: 1.04 | 
2025-12-30T16:39:40 | step: 297600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.8683102098293602e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.75 | consumed tokens: 2437939200.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2025-12-30T16:40:01 | step: 297700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.8668009943212382e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 2438758400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:40:21 | step: 297800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.8652923245099373e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.38 | consumed tokens: 2439577600.0 | grad norm avg: 0.92 | grad norm last: 0.83 | 
2025-12-30T16:40:42 | step: 297900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.8637836546986364e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.62 | consumed tokens: 2440396800.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:41:02 | step: 298000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.862275348685216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 2441216000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:41:23 | step: 298100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8607674064696766e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 2442035200.0 | grad norm avg: 0.92 | grad norm last: 1.0 | 
2025-12-30T16:41:44 | step: 298200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.859259464254137e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.67 | consumed tokens: 2442854400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:42:04 | step: 298300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.8577520677354187e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 2443673600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:42:25 | step: 298400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.8562446712167002e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.56 | consumed tokens: 2444492800.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:42:45 | step: 298500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8547376384958625e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.97 | consumed tokens: 2445312000.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T16:43:06 | step: 298600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.853230787673965e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.97 | consumed tokens: 2446131200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:43:27 | step: 298700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.8517243006499484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.52 | consumed tokens: 2446950400.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:43:47 | step: 298800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.850217995524872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.12 | consumed tokens: 2447769600.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:44:08 | step: 298900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8487120541976765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2448588800.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T16:44:29 | step: 299000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.847206112870481e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2449408000.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2025-12-30T16:44:49 | step: 299100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.8457007172401063e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.92 | consumed tokens: 2450227200.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T16:45:09 | step: 299200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.8441953216097318e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2451046400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:45:30 | step: 299300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.842690289777238e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.48 | consumed tokens: 2451865600.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T16:45:51 | step: 299400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8411856217426248e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.48 | consumed tokens: 2452684800.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2025-12-30T16:46:11 | step: 299500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.839681135606952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.86 | consumed tokens: 2453504000.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:46:32 | step: 299600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.8381768313702196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.73 | consumed tokens: 2454323200.0 | grad norm avg: 0.91 | grad norm last: 0.98 | 
2025-12-30T16:46:52 | step: 299700 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.836672890931368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.47 | consumed tokens: 2455142400.0 | grad norm avg: 0.92 | grad norm last: 1.02 | 
2025-12-30T16:47:13 | step: 299800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.8351691323914565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.7 | consumed tokens: 2455961600.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2025-12-30T16:47:33 | step: 299900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.833665737649426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.55 | consumed tokens: 2456780800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2025-12-30T16:47:54 | step: 300000 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 1.8321625248063356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.31 | consumed tokens: 2457600000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:48:16 | step: 300100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.830659675761126e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.13 | train loss last: 2.66 | consumed tokens: 2458419200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:48:37 | step: 300200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.8291570086148567e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.09 | train loss last: 2.72 | consumed tokens: 2459238400.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T16:48:58 | step: 300300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.827654523367528e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.13 | train loss last: 3.34 | consumed tokens: 2460057600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:49:18 | step: 300400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.8261524019180797e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.13 | train loss last: 3.22 | consumed tokens: 2460876800.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:49:39 | step: 300500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.824650462367572e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.14 | train loss last: 2.64 | consumed tokens: 2461696000.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T16:49:59 | step: 300600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.8231488866149448e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.1 | train loss last: 2.88 | consumed tokens: 2462515200.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T16:50:20 | step: 300700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.821647492761258e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 2463334400.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T16:50:40 | step: 300800 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 1.820146462705452e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.13 | train loss last: 3.19 | consumed tokens: 2464153600.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T16:51:01 | step: 300900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.8186456145485863e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.12 | train loss last: 3.28 | consumed tokens: 2464972800.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T16:51:22 | step: 301000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.8171451301896013e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.12 | train loss last: 2.5 | consumed tokens: 2465792000.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T16:51:42 | step: 301100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.8156448277295567e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.13 | train loss last: 2.94 | consumed tokens: 2466611200.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T16:52:03 | step: 301200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.8141447071684524e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.14 | train loss last: 3.72 | consumed tokens: 2467430400.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:52:24 | step: 301300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.812644950405229e-05 | peak memory rank 0 (MB): 4524.28 | train loss avg: 3.12 | train loss last: 2.67 | consumed tokens: 2468249600.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T16:52:44 | step: 301400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.811145557439886e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.53 | consumed tokens: 2469068800.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T16:53:05 | step: 301500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.8096463463734835e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.97 | consumed tokens: 2469888000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T16:53:26 | step: 301600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.8081473172060214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 2470707200.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T16:53:46 | step: 301700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.80664865183644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.42 | consumed tokens: 2471526400.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T16:54:07 | step: 301800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.8051503502647392e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2472345600.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T16:54:27 | step: 301900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.803652230591979e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.08 | consumed tokens: 2473164800.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:54:48 | step: 302000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.802154292818159e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.72 | consumed tokens: 2473984000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:55:08 | step: 302100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.8006567188422196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 2474803200.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T16:55:29 | step: 302200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7991593267652206e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.11 | consumed tokens: 2475622400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:55:49 | step: 302300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.7976622984861024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.62 | consumed tokens: 2476441600.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T16:56:10 | step: 302400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.796165634004865e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.67 | consumed tokens: 2477260800.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T16:56:30 | step: 302500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.7946691514225677e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.92 | consumed tokens: 2478080000.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T16:56:51 | step: 302600 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.7931730326381512e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.66 | consumed tokens: 2478899200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:57:12 | step: 302700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.791677095752675e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.53 | consumed tokens: 2479718400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:57:32 | step: 302800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7901813407661393e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 2480537600.0 | grad norm avg: 0.92 | grad norm last: 1.0 | 
2025-12-30T16:57:53 | step: 302900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.7886859495774843e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.89 | consumed tokens: 2481356800.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T16:58:14 | step: 303000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.78719092218671e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.67 | consumed tokens: 2482176000.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T16:58:34 | step: 303100 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.785696076694876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.45 | consumed tokens: 2482995200.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T16:58:54 | step: 303200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.7842015950009227e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.42 | consumed tokens: 2483814400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T16:59:15 | step: 303300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.7827072952059098e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2484633600.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T16:59:36 | step: 303400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.7812133592087775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.86 | consumed tokens: 2485452800.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T16:59:56 | step: 303500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.7797196051105857e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 2486272000.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T17:00:17 | step: 303600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.7782262148102745e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.73 | consumed tokens: 2487091200.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:00:37 | step: 303700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7767330064089037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2487910400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:00:58 | step: 303800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.775240343704354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.0 | consumed tokens: 2488729600.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2025-12-30T17:01:19 | step: 303900 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.7737476809998043e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.47 | consumed tokens: 2489548800.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T17:01:39 | step: 304000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7722553820931353e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.84 | consumed tokens: 2490368000.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T17:02:00 | step: 304100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.770763446984347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.56 | consumed tokens: 2491187200.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T17:02:21 | step: 304200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.769271693774499e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.48 | consumed tokens: 2492006400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T17:02:42 | step: 304300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.7677803043625318e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 2492825600.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T17:03:02 | step: 304400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.766289096849505e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 2493644800.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:03:23 | step: 304500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7647982531343587e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.14 | consumed tokens: 2494464000.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T17:03:43 | step: 304600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.7633077732170932e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.77 | consumed tokens: 2495283200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T17:04:04 | step: 304700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.761817475198768e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 4.0 | consumed tokens: 2496102400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T17:04:24 | step: 304800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7603275409783237e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.86 | consumed tokens: 2496921600.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T17:04:45 | step: 304900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7588377886568196e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.95 | consumed tokens: 2497740800.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T17:05:06 | step: 305000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.7573484001331963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.42 | consumed tokens: 2498560000.0 | grad norm avg: 0.92 | grad norm last: 1.01 | 
2025-12-30T17:05:28 | step: 305100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.7558593754074536e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.52 | consumed tokens: 2499379200.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T17:05:48 | step: 305200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.7543705325806513e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.8 | consumed tokens: 2500198400.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:06:09 | step: 305300 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 1.7528820535517298e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.75 | consumed tokens: 2501017600.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2025-12-30T17:06:30 | step: 305400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.7513937564217485e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.47 | consumed tokens: 2501836800.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T17:06:51 | step: 305500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.749905823089648e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.91 | consumed tokens: 2502656000.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T17:07:11 | step: 305600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.7484182535554282e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.64 | consumed tokens: 2503475200.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T17:07:32 | step: 305700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.7469308659201488e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.22 | consumed tokens: 2504294400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:07:52 | step: 305800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.74544384208275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.91 | consumed tokens: 2505113600.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T17:08:13 | step: 305900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 1.7439570001442917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.45 | consumed tokens: 2505932800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T17:08:34 | step: 306000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.742470522003714e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.36 | consumed tokens: 2506752000.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T17:08:54 | step: 306100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.740984407661017e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2507571200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:09:15 | step: 306200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.7394984752172604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.92 | consumed tokens: 2508390400.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:09:36 | step: 306300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.7380129065713845e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.22 | consumed tokens: 2509209600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T17:09:56 | step: 306400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.7365277017233893e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 2510028800.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T17:10:17 | step: 306500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.7350426787743345e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.14 | consumed tokens: 2510848000.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T17:10:38 | step: 306600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 1.7335580196231604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.91 | consumed tokens: 2511667200.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T17:10:58 | step: 306700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.7320735423709266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.0 | consumed tokens: 2512486400.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:11:19 | step: 306800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.730589610815514e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.19 | consumed tokens: 2513305600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T17:11:40 | step: 306900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.7291058611590415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.78 | consumed tokens: 2514124800.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T17:12:00 | step: 307000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7276222934015095e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 2514944000.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:12:20 | step: 307100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7261390894418582e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.34 | consumed tokens: 2515763200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T17:12:41 | step: 307200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.7246562492800876e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 2516582400.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2025-12-30T17:13:02 | step: 307300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.7231737729161978e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.48 | consumed tokens: 2517401600.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T17:13:22 | step: 307400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.7216914784512483e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.95 | consumed tokens: 2518220800.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:13:43 | step: 307500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.7202095477841794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2519040000.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T17:14:04 | step: 307600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.718727799016051e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.41 | consumed tokens: 2519859200.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T17:14:24 | step: 307700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.7172465959447436e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.44 | consumed tokens: 2520678400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T17:14:45 | step: 307800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.7157653928734362e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.31 | consumed tokens: 2521497600.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T17:15:06 | step: 307900 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 1.71428473549895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.89 | consumed tokens: 2522316800.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:15:27 | step: 308000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.712804260023404e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.64 | consumed tokens: 2523136000.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2025-12-30T17:15:47 | step: 308100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7113241483457386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.5 | consumed tokens: 2523955200.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T17:16:08 | step: 308200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.709844400465954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2524774400.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:16:28 | step: 308300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.70836483448511e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2525593600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:16:49 | step: 308400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.7068858142010868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 2526412800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:17:10 | step: 308500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.7054067939170636e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.03 | consumed tokens: 2527232000.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T17:17:30 | step: 308600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.7039283193298616e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.55 | consumed tokens: 2528051200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T17:17:51 | step: 308700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.7024500266416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.45 | consumed tokens: 2528870400.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:18:11 | step: 308800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.7009720977512188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.72 | consumed tokens: 2529689600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:18:32 | step: 308900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.6994945326587185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 2530508800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:18:53 | step: 309000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.6980171494651586e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.73 | consumed tokens: 2531328000.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2025-12-30T17:19:13 | step: 309100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.6965401300694793e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.75 | consumed tokens: 2532147200.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:19:34 | step: 309200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.6950634744716808e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 4.16 | consumed tokens: 2532966400.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T17:19:54 | step: 309300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.693587182671763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2533785600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:20:15 | step: 309400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.6921110727707855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.42 | consumed tokens: 2534604800.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:20:36 | step: 309500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.6906353266676888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.27 | consumed tokens: 2535424000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T17:20:56 | step: 309600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.6891599443624727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.3 | consumed tokens: 2536243200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T17:21:17 | step: 309700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.687684743956197e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.78 | consumed tokens: 2537062400.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:21:37 | step: 309800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.6862100892467424e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.92 | consumed tokens: 2537881600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:21:58 | step: 309900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.684735616436228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.8 | consumed tokens: 2538700800.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T17:22:19 | step: 310000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.6832613255246542e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2539520000.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T17:22:41 | step: 310100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.6817875803099014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.25 | consumed tokens: 2540339200.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:23:01 | step: 310200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.680314016994089e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.42 | consumed tokens: 2541158400.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T17:23:22 | step: 310300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.678840817476157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2541977600.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T17:23:43 | step: 310400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.677367981756106e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.41 | consumed tokens: 2542796800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:24:04 | step: 310500 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 1.6758955098339356e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.92 | consumed tokens: 2543616000.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:24:24 | step: 310600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.6744232198107056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.77 | consumed tokens: 2544435200.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:24:45 | step: 310700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.6729512935853563e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.69 | consumed tokens: 2545254400.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:25:06 | step: 310800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.6714797311578877e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.91 | consumed tokens: 2546073600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:25:26 | step: 310900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.6700085325283e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.84 | consumed tokens: 2546892800.0 | grad norm avg: 0.94 | grad norm last: 1.0 | 
2025-12-30T17:25:47 | step: 311000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.6685376976965927e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.53 | consumed tokens: 2547712000.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:26:08 | step: 311100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.667067044763826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.33 | consumed tokens: 2548531200.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T17:26:28 | step: 311200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.6655967556289397e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 2549350400.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T17:26:49 | step: 311300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.6641268302919343e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.58 | consumed tokens: 2550169600.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T17:27:10 | step: 311400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.6626572687528096e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.95 | consumed tokens: 2550988800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:27:31 | step: 311500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.6611878891126253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.0 | consumed tokens: 2551808000.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:27:51 | step: 311600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.6597188732703216e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.55 | consumed tokens: 2552627200.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2025-12-30T17:28:12 | step: 311700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.6582502212258987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.42 | consumed tokens: 2553446400.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:28:32 | step: 311800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.6567819329793565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 2554265600.0 | grad norm avg: 0.93 | grad norm last: 1.04 | 
2025-12-30T17:28:53 | step: 311900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.655314008530695e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2555084800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:29:14 | step: 312000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.6538464478799142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.84 | consumed tokens: 2555904000.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:29:34 | step: 312100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.6523790691280738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.78 | consumed tokens: 2556723200.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:29:55 | step: 312200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.650912054174114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.81 | consumed tokens: 2557542400.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:30:16 | step: 312300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.649445403018035e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 3.14 | consumed tokens: 2558361600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T17:30:36 | step: 312400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.6479791156598367e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2559180800.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:30:57 | step: 312500 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.646513192099519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.41 | consumed tokens: 2560000000.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T17:31:18 | step: 312600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.6450474504381418e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.55 | consumed tokens: 2560819200.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:31:38 | step: 312700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.6435822544735856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2561638400.0 | grad norm avg: 0.93 | grad norm last: 0.99 | 
2025-12-30T17:31:59 | step: 312800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.6421172404079698e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.64 | consumed tokens: 2562457600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:32:20 | step: 312900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.6406525901402347e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.84 | consumed tokens: 2563276800.0 | grad norm avg: 0.92 | grad norm last: 0.94 | 
2025-12-30T17:32:40 | step: 313000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.6391883036703803e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.91 | consumed tokens: 2564096000.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:33:01 | step: 313100 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.6377243809984066e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.72 | consumed tokens: 2564915200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2025-12-30T17:33:22 | step: 313200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.6362606402253732e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.62 | consumed tokens: 2565734400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T17:33:42 | step: 313300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.634797445149161e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2566553600.0 | grad norm avg: 0.93 | grad norm last: 1.04 | 
2025-12-30T17:34:03 | step: 313400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.633334431971889e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.17 | consumed tokens: 2567372800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:34:23 | step: 313500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.6318717825924978e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.92 | consumed tokens: 2568192000.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:34:44 | step: 313600 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 1.6304094970109873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.47 | consumed tokens: 2569011200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:35:04 | step: 313700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.6289475752273574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.22 | consumed tokens: 2569830400.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:35:25 | step: 313800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.6274860172416084e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.14 | consumed tokens: 2570649600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:35:45 | step: 313900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.62602482305374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.25 | consumed tokens: 2571468800.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T17:36:06 | step: 314000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.624563810764812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2572288000.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:36:27 | step: 314100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.623103344172705e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.14 | consumed tokens: 2573107200.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:36:47 | step: 314200 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.6216430594795384e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.95 | consumed tokens: 2573926400.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T17:37:07 | step: 314300 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 1.6201831385842524e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 2574745600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:37:28 | step: 314400 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.6187235814868473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.17 | consumed tokens: 2575564800.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T17:37:49 | step: 314500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.6172643881873228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.16 | consumed tokens: 2576384000.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T17:38:09 | step: 314600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.615805558685679e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 2577203200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:38:30 | step: 314700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.614347092981916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.55 | consumed tokens: 2578022400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T17:38:51 | step: 314800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.6128889910760336e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.34 | consumed tokens: 2578841600.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T17:39:11 | step: 314900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.6114310710690916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.06 | consumed tokens: 2579660800.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2025-12-30T17:39:32 | step: 315000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.6099736967589706e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.56 | consumed tokens: 2580480000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T17:39:54 | step: 315100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.60851650434779e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.8 | consumed tokens: 2581299200.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:40:14 | step: 315200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.6070598576334305e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.17 | consumed tokens: 2582118400.0 | grad norm avg: 0.94 | grad norm last: 0.81 | 
2025-12-30T17:40:35 | step: 315300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.6056033928180113e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.59 | consumed tokens: 2582937600.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:40:55 | step: 315400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.604147291800473e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.31 | consumed tokens: 2583756800.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T17:41:16 | step: 315500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.6026917364797555e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.75 | consumed tokens: 2584576000.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:41:36 | step: 315600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.6012363630579785e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.55 | consumed tokens: 2585395200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:41:57 | step: 315700 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.599781353434082e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.23 | consumed tokens: 2586214400.0 | grad norm avg: 0.92 | grad norm last: 0.88 | 
2025-12-30T17:42:18 | step: 315800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.5983267076080665e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.83 | consumed tokens: 2587033600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:42:39 | step: 315900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5968724255799316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2587852800.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T17:42:59 | step: 316000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.595418325450737e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 2588672000.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T17:43:20 | step: 316100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5939647710183635e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.84 | consumed tokens: 2589491200.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T17:43:40 | step: 316200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.5925115803838708e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.7 | consumed tokens: 2590310400.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T17:44:01 | step: 316300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.5910587535472587e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.02 | consumed tokens: 2591129600.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T17:44:21 | step: 316400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.589606108609587e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.88 | consumed tokens: 2591948800.0 | grad norm avg: 0.93 | grad norm last: 1.02 | 
2025-12-30T17:44:42 | step: 316500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5881540093687363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.97 | consumed tokens: 2592768000.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:45:03 | step: 316600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.586702092026826e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.47 | consumed tokens: 2593587200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T17:45:23 | step: 316700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.5852507203817368e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.92 | consumed tokens: 2594406400.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:45:44 | step: 316800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.583799530635588e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.45 | consumed tokens: 2595225600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T17:46:04 | step: 316900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.58234888658626e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.55 | consumed tokens: 2596044800.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2025-12-30T17:46:25 | step: 317000 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 1.5808984244358726e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.52 | consumed tokens: 2596864000.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T17:46:46 | step: 317100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.5794485079823062e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2597683200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T17:47:06 | step: 317200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.57799877342768e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.58 | consumed tokens: 2598502400.0 | grad norm avg: 0.93 | grad norm last: 0.86 | 
2025-12-30T17:47:27 | step: 317300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.576549584569875e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.25 | consumed tokens: 2599321600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:47:48 | step: 317400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.5751005776110105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.36 | consumed tokens: 2600140800.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:48:08 | step: 317500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5736519344500266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.89 | consumed tokens: 2600960000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T17:48:29 | step: 317600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.5722038369858637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.44 | consumed tokens: 2601779200.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:48:50 | step: 317700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.5707559214206412e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.73 | consumed tokens: 2602598400.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2025-12-30T17:49:10 | step: 317800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5693083696532995e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.39 | consumed tokens: 2603417600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:49:31 | step: 317900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.5678613635827787e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.62 | consumed tokens: 2604236800.0 | grad norm avg: 0.93 | grad norm last: 0.83 | 
2025-12-30T17:49:52 | step: 318000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.5664145394111983e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.56 | consumed tokens: 2605056000.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T17:50:12 | step: 318100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.5649680790374987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.47 | consumed tokens: 2605875200.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:50:33 | step: 318200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.56352216436062e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.02 | consumed tokens: 2606694400.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T17:50:54 | step: 318300 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.562076431582682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.59 | consumed tokens: 2607513600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:51:14 | step: 318400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.5606312445015647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.55 | consumed tokens: 2608332800.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T17:51:35 | step: 318500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5591862393193878e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 2609152000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T17:51:56 | step: 318600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.5577415979350917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.33 | consumed tokens: 2609971200.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T17:52:16 | step: 318700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.5562975022476166e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.84 | consumed tokens: 2610790400.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T17:52:37 | step: 318800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.554853588459082e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.38 | consumed tokens: 2611609600.0 | grad norm avg: 0.94 | grad norm last: 1.02 | 
2025-12-30T17:52:57 | step: 318900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5534102203673683e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.03 | consumed tokens: 2612428800.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T17:53:18 | step: 319000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5519672160735354e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2613248000.0 | grad norm avg: 0.93 | grad norm last: 0.87 | 
2025-12-30T17:53:38 | step: 319100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5505243936786428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.25 | consumed tokens: 2614067200.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T17:53:59 | step: 319200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.5490821169805713e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.81 | consumed tokens: 2614886400.0 | grad norm avg: 0.94 | grad norm last: 0.86 | 
2025-12-30T17:54:19 | step: 319300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.5476402040803805e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.75 | consumed tokens: 2615705600.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T17:54:40 | step: 319400 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 1.54619847307913e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.59 | consumed tokens: 2616524800.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T17:55:00 | step: 319500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.5447572877747007e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.95 | consumed tokens: 2617344000.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:55:21 | step: 319600 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 1.543316466268152e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.48 | consumed tokens: 2618163200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T17:55:42 | step: 319700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.541876008559484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.47 | consumed tokens: 2618982400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T17:56:02 | step: 319800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5404359146486968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.89 | consumed tokens: 2619801600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:56:23 | step: 319900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.5389961845357902e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.86 | consumed tokens: 2620620800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T17:56:44 | step: 320000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.5375568182207644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.44 | consumed tokens: 2621440000.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:57:06 | step: 320100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.5361178157036193e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.7 | consumed tokens: 2622259200.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2025-12-30T17:57:27 | step: 320200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.5346793588832952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.73 | consumed tokens: 2623078400.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T17:57:48 | step: 320300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.5332410839619115e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.47 | consumed tokens: 2623897600.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T17:58:08 | step: 320400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5318031728384085e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.58 | consumed tokens: 2624716800.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T17:58:29 | step: 320500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.5303658074117266e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 2625536000.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T17:58:50 | step: 320600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.5289288057829253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.5 | consumed tokens: 2626355200.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T17:59:10 | step: 320700 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 1.5274919860530645e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.66 | consumed tokens: 2627174400.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T17:59:31 | step: 320800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.5260557120200247e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2627993600.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T17:59:52 | step: 320900 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 1.5246198017848656e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 2628812800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:00:13 | step: 321000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.5231842553475872e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.48 | consumed tokens: 2629632000.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:00:33 | step: 321100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.5217490727081895e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.39 | consumed tokens: 2630451200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:00:54 | step: 321200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5203142538666725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.3 | consumed tokens: 2631270400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:01:14 | step: 321300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5188798897725064e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 2632089600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T18:01:35 | step: 321400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.5174457985267509e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.8 | consumed tokens: 2632908800.0 | grad norm avg: 0.94 | grad norm last: 1.03 | 
2025-12-30T18:01:55 | step: 321500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.5160121620283462e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2633728000.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T18:02:16 | step: 321600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5145789802772924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.8 | consumed tokens: 2634547200.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T18:02:36 | step: 321700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.5131460713746492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.8 | consumed tokens: 2635366400.0 | grad norm avg: 0.92 | grad norm last: 0.96 | 
2025-12-30T18:02:57 | step: 321800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.5117136172193568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.09 | consumed tokens: 2636185600.0 | grad norm avg: 0.93 | grad norm last: 0.88 | 
2025-12-30T18:03:17 | step: 321900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.5102815268619452e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.67 | consumed tokens: 2637004800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:03:38 | step: 322000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.5088498003024142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2637824000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T18:03:59 | step: 322100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.5074185284902342e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2638643200.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:04:19 | step: 322200 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.5059875295264646e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.73 | consumed tokens: 2639462400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:04:40 | step: 322300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.504556985310046e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 2640281600.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T18:05:00 | step: 322400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.5031268958409782e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.25 | consumed tokens: 2641100800.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:05:21 | step: 322500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.501697079220321e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.61 | consumed tokens: 2641920000.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T18:05:41 | step: 322600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.5002677173470147e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.31 | consumed tokens: 2642739200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:06:02 | step: 322700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.498838719271589e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.66 | consumed tokens: 2643558400.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:06:22 | step: 322800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4974101759435143e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 2644377600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T18:06:43 | step: 322900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 1.4959819964133203e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.39 | consumed tokens: 2645196800.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T18:07:04 | step: 323000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.494554180681007e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.33 | consumed tokens: 2646016000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T18:07:24 | step: 323100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4931267287465744e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.19 | consumed tokens: 2646835200.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:07:45 | step: 323200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.4916997315594926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 2647654400.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T18:08:05 | step: 323300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4902730981702916e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.64 | consumed tokens: 2648473600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T18:08:26 | step: 323400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.4888469195284415e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.66 | consumed tokens: 2649292800.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T18:08:47 | step: 323500 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 1.487421104684472e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.66 | consumed tokens: 2650112000.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T18:09:08 | step: 323600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.4859956536383834e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 2650931200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:09:28 | step: 323700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.4845706573396455e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.58 | consumed tokens: 2651750400.0 | grad norm avg: 0.94 | grad norm last: 1.02 | 
2025-12-30T18:09:49 | step: 323800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.4831459338893183e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.52 | consumed tokens: 2652569600.0 | grad norm avg: 0.94 | grad norm last: 1.03 | 
2025-12-30T18:10:10 | step: 323900 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 1.481721756135812e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.56 | consumed tokens: 2653388800.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T18:10:30 | step: 324000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.4802978512307163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.77 | consumed tokens: 2654208000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T18:10:51 | step: 324100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.4788744920224417e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.58 | consumed tokens: 2655027200.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T18:11:12 | step: 324200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.4774514056625776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.28 | consumed tokens: 2655846400.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T18:11:32 | step: 324300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4760287740500644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.42 | consumed tokens: 2656665600.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:11:53 | step: 324400 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.4746065062354319e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.84 | consumed tokens: 2657484800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:12:14 | step: 324500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.4731846931681503e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 2658304000.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T18:12:34 | step: 324600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4717632438987494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2659123200.0 | grad norm avg: 0.94 | grad norm last: 0.85 | 
2025-12-30T18:12:55 | step: 324700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.4703421584272292e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.95 | consumed tokens: 2659942400.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T18:13:16 | step: 324800 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.4689215277030598e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.75 | consumed tokens: 2660761600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:13:36 | step: 324900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4675013517262414e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 4.16 | consumed tokens: 2661580800.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T18:13:57 | step: 325000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.4660815395473037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.12 | consumed tokens: 2662400000.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T18:14:19 | step: 325100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4646620911662467e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.56 | consumed tokens: 2663219200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T18:14:40 | step: 325200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.4632430975325406e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.03 | consumed tokens: 2664038400.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:15:00 | step: 325300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.4618244676967151e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 2664857600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T18:15:21 | step: 325400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4604062926082406e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.52 | consumed tokens: 2665676800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:15:41 | step: 325500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4589884813176468e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.45 | consumed tokens: 2666496000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:16:02 | step: 325600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.4575710338249337e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.23 | consumed tokens: 2667315200.0 | grad norm avg: 0.93 | grad norm last: 0.85 | 
2025-12-30T18:16:23 | step: 325700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.4561540410795715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 2668134400.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T18:16:43 | step: 325800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4547375030815601e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.83 | consumed tokens: 2668953600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T18:17:04 | step: 325900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4533213288814295e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.73 | consumed tokens: 2669772800.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T18:17:24 | step: 326000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.4519056094286498e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2670592000.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:17:45 | step: 326100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.4504902537737507e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.42 | consumed tokens: 2671411200.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T18:18:06 | step: 326200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4490752619167324e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.38 | consumed tokens: 2672230400.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T18:18:26 | step: 326300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.447660724807065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.81 | consumed tokens: 2673049600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T18:18:47 | step: 326400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4462466424447484e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.72 | consumed tokens: 2673868800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T18:19:08 | step: 326500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.4448329238803126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.69 | consumed tokens: 2674688000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:19:29 | step: 326600 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.4434196600632276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.73 | consumed tokens: 2675507200.0 | grad norm avg: 0.93 | grad norm last: 0.89 | 
2025-12-30T18:19:49 | step: 326700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.4420067600440234e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.44 | consumed tokens: 2676326400.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T18:20:10 | step: 326800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.44059431477217e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.64 | consumed tokens: 2677145600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:20:30 | step: 326900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.4391822332981974e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.8 | consumed tokens: 2677964800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:20:51 | step: 327000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.4377706065715756e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2678784000.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T18:21:11 | step: 327100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.4363594345923048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2679603200.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:21:32 | step: 327200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.4349486264109146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 2680422400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:21:53 | step: 327300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4335382729768753e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.53 | consumed tokens: 2681241600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:22:13 | step: 327400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.4321282833407167e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 4.03 | consumed tokens: 2682060800.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T18:22:34 | step: 327500 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.430718748451909e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.8 | consumed tokens: 2682880000.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T18:22:55 | step: 327600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.429309577360982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.8 | consumed tokens: 2683699200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:23:15 | step: 327700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.427900861017406e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.05 | consumed tokens: 2684518400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:23:36 | step: 327800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.4264925994211808e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.38 | consumed tokens: 2685337600.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2025-12-30T18:23:56 | step: 327900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.4250847016228363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.06 | consumed tokens: 2686156800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:24:17 | step: 328000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4236772585718427e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.0 | consumed tokens: 2686976000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:24:37 | step: 328100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.4222702702682e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.61 | consumed tokens: 2687795200.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:24:58 | step: 328200 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.420863645762438e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 2688614400.0 | grad norm avg: 0.95 | grad norm last: 0.88 | 
2025-12-30T18:25:19 | step: 328300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.4194574760040268e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 2689433600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T18:25:39 | step: 328400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.4180516700434964e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.78 | consumed tokens: 2690252800.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T18:26:00 | step: 328500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.4166463188303169e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.55 | consumed tokens: 2691072000.0 | grad norm avg: 0.94 | grad norm last: 1.01 | 
2025-12-30T18:26:21 | step: 328600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.4152414223644882e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 2691891200.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:26:41 | step: 328700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4138369806460105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.16 | train loss last: 2.73 | consumed tokens: 2692710400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T18:27:02 | step: 328800 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 1.4124329027254134e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.47 | consumed tokens: 2693529600.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:27:23 | step: 328900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.4110291886026971e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.31 | consumed tokens: 2694348800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:27:43 | step: 329000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4096260201768018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.7 | consumed tokens: 2695168000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:28:04 | step: 329100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.4082232155487873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.48 | consumed tokens: 2695987200.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:28:25 | step: 329200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.4068208656681236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 2696806400.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:28:45 | step: 329300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.4054189705348108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.2 | consumed tokens: 2697625600.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:29:06 | step: 329400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.4040174391993787e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.8 | consumed tokens: 2698444800.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:29:27 | step: 329500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.4026163626112975e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.45 | consumed tokens: 2699264000.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2025-12-30T18:29:47 | step: 329600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.401215649821097e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.14 | consumed tokens: 2700083200.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T18:30:08 | step: 329700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.3998154827277176e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 4.03 | consumed tokens: 2700902400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:30:28 | step: 329800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3984156794322189e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.88 | consumed tokens: 2701721600.0 | grad norm avg: 0.94 | grad norm last: 1.08 | 
2025-12-30T18:30:49 | step: 329900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.397016330884071e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 2702540800.0 | grad norm avg: 0.94 | grad norm last: 0.85 | 
2025-12-30T18:31:09 | step: 330000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3956174370832741e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 2703360000.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T18:31:32 | step: 330100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.3942189070803579e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2704179200.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:31:52 | step: 330200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.3928208318247925e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.3 | consumed tokens: 2704998400.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T18:32:13 | step: 330300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.3914232113165781e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.03 | consumed tokens: 2705817600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T18:32:34 | step: 330400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3900260455557145e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.45 | consumed tokens: 2706636800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T18:32:54 | step: 330500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.3886292435927317e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2707456000.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T18:33:15 | step: 330600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.3872329873265699e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.91 | consumed tokens: 2708275200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T18:33:35 | step: 330700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3858370948582888e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.47 | consumed tokens: 2709094400.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T18:33:56 | step: 330800 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.3844415661878884e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.34 | consumed tokens: 2709913600.0 | grad norm avg: 0.93 | grad norm last: 0.95 | 
2025-12-30T18:34:17 | step: 330900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.383046583214309e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.83 | consumed tokens: 2710732800.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T18:34:37 | step: 331000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3816519640386105e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.62 | consumed tokens: 2711552000.0 | grad norm avg: 0.94 | grad norm last: 1.01 | 
2025-12-30T18:34:58 | step: 331100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.3802578905597329e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.84 | consumed tokens: 2712371200.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:35:19 | step: 331200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.378864180878736e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 2713190400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:35:40 | step: 331300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.3774709259450901e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.05 | consumed tokens: 2714009600.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:36:00 | step: 331400 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 1.3760780348093249e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.05 | consumed tokens: 2714828800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:36:21 | step: 331500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.3746856893703807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2715648000.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:36:42 | step: 331600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.3732937077293172e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.75 | consumed tokens: 2716467200.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:37:02 | step: 331700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.3719021808356047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.77 | consumed tokens: 2717286400.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:37:23 | step: 331800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.3705111996387132e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.86 | consumed tokens: 2718105600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T18:37:44 | step: 331900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.3691204912902322e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 2718924800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:38:04 | step: 332000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.3677303286385722e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.64 | consumed tokens: 2719744000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T18:38:25 | step: 332100 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.3663406207342632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.94 | consumed tokens: 2720563200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T18:38:45 | step: 332200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.3649512766278349e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.08 | consumed tokens: 2721382400.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:39:06 | step: 332300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.3635624782182276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.75 | consumed tokens: 2722201600.0 | grad norm avg: 0.94 | grad norm last: 1.0 | 
2025-12-30T18:39:27 | step: 332400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.362174043606501e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.06 | consumed tokens: 2723020800.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:39:47 | step: 332500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3607860637421254e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.5 | consumed tokens: 2723840000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:40:08 | step: 332600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.3593985386251006e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.86 | consumed tokens: 2724659200.0 | grad norm avg: 0.94 | grad norm last: 1.03 | 
2025-12-30T18:40:29 | step: 332700 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.3580115592048969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.34 | consumed tokens: 2725478400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T18:40:49 | step: 332800 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.3566248526331037e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2726297600.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:41:10 | step: 332900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.3552386917581316e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.7 | consumed tokens: 2727116800.0 | grad norm avg: 0.93 | grad norm last: 0.96 | 
2025-12-30T18:41:31 | step: 333000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3538529856305104e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.89 | consumed tokens: 2727936000.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:41:51 | step: 333100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.35246773425024e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.17 | consumed tokens: 2728755200.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T18:42:12 | step: 333200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.3510828466678504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2729574400.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:42:32 | step: 333300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3496985047822818e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.92 | consumed tokens: 2730393600.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2025-12-30T18:42:53 | step: 333400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.348314526694594e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.67 | consumed tokens: 2731212800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T18:43:13 | step: 333500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.3469310943037271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.97 | consumed tokens: 2732032000.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:43:34 | step: 333600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.345548025710741e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.61 | consumed tokens: 2732851200.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:43:55 | step: 333700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.344165502814576e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.75 | consumed tokens: 2733670400.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:44:15 | step: 333800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.3427833437162917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2734489600.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T18:44:36 | step: 333900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.3414016393653583e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.31 | consumed tokens: 2735308800.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:44:57 | step: 334000 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 1.3400204807112459e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 2736128000.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:45:17 | step: 334100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.3386396858550142e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.27 | consumed tokens: 2736947200.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T18:45:38 | step: 334200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.3372593457461335e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.66 | consumed tokens: 2737766400.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T18:45:59 | step: 334300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.3358795513340738e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 2738585600.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:46:19 | step: 334400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3345001207198948e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 4.38 | consumed tokens: 2739404800.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:46:40 | step: 334500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3331211448530667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.19 | consumed tokens: 2740224000.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:47:01 | step: 334600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3317427146830596e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.42 | consumed tokens: 2741043200.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T18:47:21 | step: 334700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.3303646483109333e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2741862400.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T18:47:42 | step: 334800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3289870366861578e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.47 | consumed tokens: 2742681600.0 | grad norm avg: 0.93 | grad norm last: 0.97 | 
2025-12-30T18:48:02 | step: 334900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.3276099707582034e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.73 | consumed tokens: 2743500800.0 | grad norm avg: 0.94 | grad norm last: 0.86 | 
2025-12-30T18:48:23 | step: 335000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.3262332686281297e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 2744320000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T18:48:45 | step: 335100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3248571121948771e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.03 | consumed tokens: 2745139200.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:49:06 | step: 335200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.3234814105089754e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.88 | consumed tokens: 2745958400.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T18:49:26 | step: 335300 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 1.3221060726209544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.66 | consumed tokens: 2746777600.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:49:47 | step: 335400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.3207312804297544e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.3 | consumed tokens: 2747596800.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:50:08 | step: 335500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.3193569429859053e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.83 | consumed tokens: 2748416000.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T18:50:28 | step: 335600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.317982969339937e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.0 | consumed tokens: 2749235200.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:50:49 | step: 335700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3166095413907897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.39 | consumed tokens: 2750054400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:51:10 | step: 335800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.3152365681889933e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2750873600.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:51:30 | step: 335900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.3138641406840179e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.47 | consumed tokens: 2751692800.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:51:51 | step: 336000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.3124920769769233e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.61 | consumed tokens: 2752512000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T18:52:11 | step: 336100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3111204680171795e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.08 | consumed tokens: 2753331200.0 | grad norm avg: 0.93 | grad norm last: 0.93 | 
2025-12-30T18:52:32 | step: 336200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.3097494047542568e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.88 | consumed tokens: 2754150400.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2025-12-30T18:52:53 | step: 336300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.3083787052892148e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.56 | consumed tokens: 2754969600.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T18:53:13 | step: 336400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.307008551520994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 2755788800.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:53:34 | step: 336500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.3056388525001239e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.56 | consumed tokens: 2756608000.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T18:53:55 | step: 336600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 1.3042696082266048e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2757427200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T18:54:15 | step: 336700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.3029008187004365e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.31 | consumed tokens: 2758246400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:54:36 | step: 336800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.3015324839216191e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.2 | consumed tokens: 2759065600.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:54:56 | step: 336900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.3001646038901526e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 2759884800.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T18:55:17 | step: 337000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.2987972695555072e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.44 | consumed tokens: 2760704000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:55:38 | step: 337100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2974303899682127e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.72 | consumed tokens: 2761523200.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T18:55:58 | step: 337200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.296063965128269e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2762342400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T18:56:19 | step: 337300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2946979950356763e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.72 | consumed tokens: 2763161600.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T18:56:39 | step: 337400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2933324796904344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.73 | consumed tokens: 2763980800.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:57:00 | step: 337500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.2919674190925434e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.09 | consumed tokens: 2764800000.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:57:20 | step: 337600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.2906029041914735e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.3 | consumed tokens: 2765619200.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T18:57:41 | step: 337700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2892388440377545e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2766438400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T18:58:02 | step: 337800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.2878752386313863e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.73 | consumed tokens: 2767257600.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T18:58:22 | step: 337900 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.286512087972369e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 4.0 | consumed tokens: 2768076800.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T18:58:43 | step: 338000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2851494830101728e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.52 | consumed tokens: 2768896000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2025-12-30T18:59:04 | step: 338100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.2837872418458574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.19 | consumed tokens: 2769715200.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T18:59:24 | step: 338200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.282425546378363e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.78 | consumed tokens: 2770534400.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T18:59:45 | step: 338300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2810643056582194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.84 | consumed tokens: 2771353600.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T19:00:05 | step: 338400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.2797036106348969e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.77 | consumed tokens: 2772172800.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T19:00:26 | step: 338500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2783433703589253e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.92 | consumed tokens: 2772992000.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T19:00:46 | step: 338600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2769834938808344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.8 | consumed tokens: 2773811200.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T19:01:07 | step: 338700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.2756242540490348e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.92 | consumed tokens: 2774630400.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T19:01:27 | step: 338800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2742653780151159e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.33 | consumed tokens: 2775449600.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T19:01:48 | step: 338900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.272907047678018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.8 | consumed tokens: 2776268800.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T19:02:08 | step: 339000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.271549172088271e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.19 | consumed tokens: 2777088000.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T19:02:29 | step: 339100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.2701917512458749e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 2777907200.0 | grad norm avg: 0.95 | grad norm last: 0.85 | 
2025-12-30T19:02:50 | step: 339200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.2688348761002999e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.61 | consumed tokens: 2778726400.0 | grad norm avg: 0.94 | grad norm last: 1.03 | 
2025-12-30T19:03:10 | step: 339300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.2674783647526056e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 2779545600.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T19:03:31 | step: 339400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.2661224900512025e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 2780364800.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2025-12-30T19:03:51 | step: 339500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2647669791476801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.61 | consumed tokens: 2781184000.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T19:04:12 | step: 339600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.2634120139409788e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.2 | consumed tokens: 2782003200.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T19:04:32 | step: 339700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.2620575034816284e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.42 | consumed tokens: 2782822400.0 | grad norm avg: 0.94 | grad norm last: 0.88 | 
2025-12-30T19:04:53 | step: 339800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.2607034477696288e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.72 | consumed tokens: 2783641600.0 | grad norm avg: 0.94 | grad norm last: 0.99 | 
2025-12-30T19:05:13 | step: 339900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2593499377544504e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.47 | consumed tokens: 2784460800.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T19:05:34 | step: 340000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.2579968824866228e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.73 | consumed tokens: 2785280000.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:05:56 | step: 340100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2566443729156163e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.77 | consumed tokens: 2786099200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:06:16 | step: 340200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.2552922271424904e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.28 | consumed tokens: 2786918400.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T19:06:37 | step: 340300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.2539406270661857e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.64 | consumed tokens: 2787737600.0 | grad norm avg: 0.94 | grad norm last: 1.0 | 
2025-12-30T19:06:57 | step: 340400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.252589572686702e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2788556800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:07:18 | step: 340500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.2512389730545692e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.44 | consumed tokens: 2789376000.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:07:39 | step: 340600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.2498888281697873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.7 | consumed tokens: 2790195200.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T19:07:59 | step: 340700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2485392289818265e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.75 | consumed tokens: 2791014400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:08:20 | step: 340800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2471900845412165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 4.03 | consumed tokens: 2791833600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:08:40 | step: 340900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2458413948479574e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 2792652800.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:09:01 | step: 341000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.2444932508515194e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.47 | consumed tokens: 2793472000.0 | grad norm avg: 0.93 | grad norm last: 0.91 | 
2025-12-30T19:09:22 | step: 341100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.2431455616024323e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.19 | consumed tokens: 2794291200.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T19:09:42 | step: 341200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.241798327100696e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 2795110400.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T19:10:03 | step: 341300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2404516382957809e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.12 | consumed tokens: 2795929600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:10:23 | step: 341400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.2391054951876868e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.45 | consumed tokens: 2796748800.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T19:10:44 | step: 341500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2377597158774734e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.47 | consumed tokens: 2797568000.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T19:11:04 | step: 341600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2364145732135512e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.58 | consumed tokens: 2798387200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T19:11:25 | step: 341700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.2350697943475097e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.72 | consumed tokens: 2799206400.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T19:11:46 | step: 341800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.2337255611782894e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2800025600.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T19:12:06 | step: 341900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.23238187370589e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.61 | consumed tokens: 2800844800.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T19:12:27 | step: 342000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.2310386409808416e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.81 | consumed tokens: 2801664000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:12:47 | step: 342100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.229695863003144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.78 | consumed tokens: 2802483200.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:13:08 | step: 342200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.2283536307222676e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.2 | consumed tokens: 2803302400.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:13:28 | step: 342300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.2270119441382121e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.66 | consumed tokens: 2804121600.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T19:13:49 | step: 342400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.2256707123015076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.53 | consumed tokens: 2804940800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:14:09 | step: 342500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.224329935212154e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.89 | consumed tokens: 2805760000.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T19:14:30 | step: 342600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2229897038196214e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.05 | consumed tokens: 2806579200.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T19:14:50 | step: 342700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.2216499271744397e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 2807398400.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:15:11 | step: 342800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.220310696226079e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2808217600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:15:31 | step: 342900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.2189719200250693e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.8 | consumed tokens: 2809036800.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:15:52 | step: 343000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2176336895208806e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.91 | consumed tokens: 2809856000.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:16:13 | step: 343100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.2162959137640428e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.97 | consumed tokens: 2810675200.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:16:33 | step: 343200 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 1.214958683704026e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.47 | consumed tokens: 2811494400.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:16:54 | step: 343300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.2136219993408304e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.44 | consumed tokens: 2812313600.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:17:15 | step: 343400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.2122857697249856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.05 | consumed tokens: 2813132800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:17:35 | step: 343500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.2109499948564917e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.67 | consumed tokens: 2813952000.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T19:17:56 | step: 343600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.2096147656848188e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 4.12 | consumed tokens: 2814771200.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:18:16 | step: 343700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.208280082209967e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.17 | consumed tokens: 2815590400.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T19:18:37 | step: 343800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.2069458534824662e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 2816409600.0 | grad norm avg: 0.94 | grad norm last: 1.0 | 
2025-12-30T19:18:57 | step: 343900 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.2056120795023162e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.42 | consumed tokens: 2817228800.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T19:19:18 | step: 344000 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.2042788512189873e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.03 | consumed tokens: 2818048000.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T19:19:39 | step: 344100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.2029461686324794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.48 | consumed tokens: 2818867200.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:19:59 | step: 344200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.2016140317427926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.0 | consumed tokens: 2819686400.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:20:20 | step: 344300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.2002823496004567e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.66 | consumed tokens: 2820505600.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T19:20:40 | step: 344400 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.1989511222054716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 2821324800.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T19:21:01 | step: 344500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1976204405073076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.92 | consumed tokens: 2822144000.0 | grad norm avg: 0.94 | grad norm last: 1.06 | 
2025-12-30T19:21:22 | step: 344600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.1962903045059647e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.62 | consumed tokens: 2822963200.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2025-12-30T19:21:42 | step: 344700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1949606232519727e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.23 | consumed tokens: 2823782400.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:22:03 | step: 344800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.1936314876948018e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2824601600.0 | grad norm avg: 0.94 | grad norm last: 0.87 | 
2025-12-30T19:22:23 | step: 344900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.1923028978344519e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 2825420800.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:22:44 | step: 345000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1909747627214529e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.44 | consumed tokens: 2826240000.0 | grad norm avg: 0.94 | grad norm last: 1.02 | 
2025-12-30T19:23:06 | step: 345100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.189647173305275e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.14 | consumed tokens: 2827059200.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:23:27 | step: 345200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.1883200386364479e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.22 | consumed tokens: 2827878400.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:23:48 | step: 345300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.1869934496644419e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.78 | consumed tokens: 2828697600.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:24:08 | step: 345400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.185667406389257e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.73 | consumed tokens: 2829516800.0 | grad norm avg: 0.94 | grad norm last: 0.96 | 
2025-12-30T19:24:29 | step: 345500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.184341908810893e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.06 | consumed tokens: 2830336000.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T19:24:49 | step: 345600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.1830168659798801e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.69 | consumed tokens: 2831155200.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:25:10 | step: 345700 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.181692277896218e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 4.03 | consumed tokens: 2831974400.0 | grad norm avg: 0.94 | grad norm last: 1.0 | 
2025-12-30T19:25:31 | step: 345800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.1803683264588472e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2832793600.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T19:25:51 | step: 345900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.1790448297688272e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.72 | consumed tokens: 2833612800.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T19:26:12 | step: 346000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.1777217878261581e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.83 | consumed tokens: 2834432000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:26:32 | step: 346100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.1763993825297803e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 2835251200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:26:53 | step: 346200 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.1750774319807533e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.94 | consumed tokens: 2836070400.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:27:13 | step: 346300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1737560271285474e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.58 | consumed tokens: 2836889600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T19:27:34 | step: 346400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1724350770236924e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.16 | consumed tokens: 2837708800.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:27:55 | step: 346500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1711147635651287e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.83 | consumed tokens: 2838528000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:28:15 | step: 346600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1697948139044456e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.72 | consumed tokens: 2839347200.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:28:36 | step: 346700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.1684755008900538e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.78 | consumed tokens: 2840166400.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:28:56 | step: 346800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.1671566426230129e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.28 | consumed tokens: 2840985600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:29:17 | step: 346900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1658384210022632e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.09 | consumed tokens: 2841804800.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T19:29:38 | step: 347000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1645205631793942e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.64 | consumed tokens: 2842624000.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T19:29:58 | step: 347100 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.1632033420028165e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 2843443200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:30:19 | step: 347200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.1618865755735897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.61 | consumed tokens: 2844262400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:30:40 | step: 347300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.1605703548411839e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.69 | consumed tokens: 2845081600.0 | grad norm avg: 0.94 | grad norm last: 0.93 | 
2025-12-30T19:31:01 | step: 347400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 1.1592546798055992e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 2845900800.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T19:31:21 | step: 347500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1579395504668355e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.88 | consumed tokens: 2846720000.0 | grad norm avg: 0.95 | grad norm last: 1.05 | 
2025-12-30T19:31:42 | step: 347600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1566248758754227e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.41 | consumed tokens: 2847539200.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T19:32:02 | step: 347700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.155310746980831e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 2848358400.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T19:32:23 | step: 347800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1539971637830604e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.94 | consumed tokens: 2849177600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:32:44 | step: 347900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.1526841262821108e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 2849996800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:33:04 | step: 348000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1513716344779823e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 2850816000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:33:25 | step: 348100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.1500595974212047e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.83 | consumed tokens: 2851635200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T19:33:46 | step: 348200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.1487481060612481e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.14 | consumed tokens: 2852454400.0 | grad norm avg: 0.94 | grad norm last: 0.98 | 
2025-12-30T19:34:06 | step: 348300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1474371603981126e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.09 | consumed tokens: 2853273600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T19:34:27 | step: 348400 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 1.1461267604317982e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2854092800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:34:48 | step: 348500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1448168152128346e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.73 | consumed tokens: 2854912000.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:35:08 | step: 348600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.1435075066401623e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.41 | consumed tokens: 2855731200.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:35:29 | step: 348700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.1421986528148409e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.11 | consumed tokens: 2856550400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:35:50 | step: 348800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1408903446863405e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.41 | consumed tokens: 2857369600.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:36:10 | step: 348900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.1395825822546612e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 2858188800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:36:31 | step: 349000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.138275365519803e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.59 | consumed tokens: 2859008000.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T19:36:51 | step: 349100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1369686035322957e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.41 | consumed tokens: 2859827200.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:37:12 | step: 349200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1356624781910796e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.75 | consumed tokens: 2860646400.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:37:32 | step: 349300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.1343568075972144e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 2861465600.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:37:53 | step: 349400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1330516827001702e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.75 | consumed tokens: 2862284800.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:38:13 | step: 349500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.1317471034999471e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 2863104000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:38:34 | step: 349600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1304430699965451e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.33 | consumed tokens: 2863923200.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:38:55 | step: 349700 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 1.1291395821899641e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.7 | consumed tokens: 2864742400.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T19:39:15 | step: 349800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1278366400802042e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.62 | consumed tokens: 2865561600.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:39:36 | step: 349900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.1265341527177952e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.06 | consumed tokens: 2866380800.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:39:57 | step: 350000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1252323020016775e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.64 | consumed tokens: 2867200000.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T19:40:19 | step: 350100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.1239309969823807e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.11 | consumed tokens: 2868019200.0 | grad norm avg: 0.94 | grad norm last: 0.97 | 
2025-12-30T19:40:40 | step: 350200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.122630146710435e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.22 | consumed tokens: 2868838400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T19:41:00 | step: 350300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.1213298421353102e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.27 | consumed tokens: 2869657600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:41:20 | step: 350400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.1200300832570065e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.25 | consumed tokens: 2870476800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:41:41 | step: 350500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.118730961024994e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.23 | consumed tokens: 2871296000.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:42:02 | step: 350600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.1174322935403325e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.73 | consumed tokens: 2872115200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:42:22 | step: 350700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.116134171752492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.36 | consumed tokens: 2872934400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:42:43 | step: 350800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.1148365956614725e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.66 | consumed tokens: 2873753600.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T19:43:03 | step: 350900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1135395652672742e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.55 | consumed tokens: 2874572800.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T19:43:24 | step: 351000 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.1122430805698968e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.09 | consumed tokens: 2875392000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:43:45 | step: 351100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.1109470506198704e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 2876211200.0 | grad norm avg: 0.94 | grad norm last: 0.94 | 
2025-12-30T19:44:06 | step: 351200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.1096516573161352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.52 | consumed tokens: 2877030400.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T19:44:26 | step: 351300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1083568097092211e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.88 | consumed tokens: 2877849600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:44:47 | step: 351400 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 1.107062507799128e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.16 | consumed tokens: 2878668800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:45:07 | step: 351500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.105768751585856e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 3.17 | consumed tokens: 2879488000.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T19:45:27 | step: 351600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.1044755410694052e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.2 | consumed tokens: 2880307200.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:45:48 | step: 351700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.1031827853003051e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.47 | consumed tokens: 2881126400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T19:46:09 | step: 351800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.1018906661774963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.73 | consumed tokens: 2881945600.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:46:29 | step: 351900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.1005990927515086e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.52 | consumed tokens: 2882764800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:46:50 | step: 352000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0993080650223419e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2883584000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:47:10 | step: 352100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0980175829899963e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2884403200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T19:47:31 | step: 352200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0967275557050016e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.31 | consumed tokens: 2885222400.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:47:52 | step: 352300 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.0954381650662981e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.12 | consumed tokens: 2886041600.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T19:48:12 | step: 352400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0941493201244157e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.33 | consumed tokens: 2886860800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:48:33 | step: 352500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0928610208793543e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 2887680000.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:48:54 | step: 352600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.091573267331114e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.05 | consumed tokens: 2888499200.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:49:14 | step: 352700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.0902860594796948e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.8 | consumed tokens: 2889318400.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:49:34 | step: 352800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.0889994882745668e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.08 | consumed tokens: 2890137600.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T19:49:55 | step: 352900 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.0877133718167897e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.19 | consumed tokens: 2890956800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:50:15 | step: 353000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0864278010558337e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2891776000.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:50:36 | step: 353100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0851427759916987e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.08 | consumed tokens: 2892595200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:50:57 | step: 353200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.083858387573855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.64 | consumed tokens: 2893414400.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2025-12-30T19:51:17 | step: 353300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.0825744539033622e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.12 | consumed tokens: 2894233600.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:51:37 | step: 353400 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.0812911568791606e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.05 | consumed tokens: 2895052800.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T19:51:58 | step: 353500 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.08000840555178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.66 | consumed tokens: 2895872000.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:52:19 | step: 353600 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 1.0787261999212205e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.92 | consumed tokens: 2896691200.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2025-12-30T19:52:39 | step: 353700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.0774445399874821e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.36 | consumed tokens: 2897510400.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T19:53:00 | step: 353800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0761634257505648e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.72 | consumed tokens: 2898329600.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:53:20 | step: 353900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0748828572104685e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.91 | consumed tokens: 2899148800.0 | grad norm avg: 0.96 | grad norm last: 1.05 | 
2025-12-30T19:53:41 | step: 354000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.0736028343671933e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.58 | consumed tokens: 2899968000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:54:01 | step: 354100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.0723234481702093e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.58 | consumed tokens: 2900787200.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T19:54:22 | step: 354200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.0710445167205762e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.92 | consumed tokens: 2901606400.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2025-12-30T19:54:42 | step: 354300 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.0697662219172344e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 2.91 | consumed tokens: 2902425600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:55:03 | step: 354400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0684884728107136e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.94 | consumed tokens: 2903244800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:55:23 | step: 354500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.0672112694010139e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.62 | consumed tokens: 2904064000.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:55:44 | step: 354600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.0659346116881352e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 2904883200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T19:56:04 | step: 354700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0646584996720776e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.66 | consumed tokens: 2905702400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T19:56:25 | step: 354800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.0633830243023112e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2906521600.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T19:56:46 | step: 354900 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 1.062108094629366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.91 | consumed tokens: 2907340800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T19:57:06 | step: 355000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0608336197037715e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 2908160000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T19:57:28 | step: 355100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.0595598723739386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.92 | consumed tokens: 2908979200.0 | grad norm avg: 0.94 | grad norm last: 0.89 | 
2025-12-30T19:57:49 | step: 355200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0582865797914565e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.78 | consumed tokens: 2909798400.0 | grad norm avg: 0.94 | grad norm last: 1.01 | 
2025-12-30T19:58:10 | step: 355300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0570138329057954e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 4.31 | consumed tokens: 2910617600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:58:30 | step: 355400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0557417226664256e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.55 | consumed tokens: 2911436800.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:58:51 | step: 355500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0544701581238769e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.2 | consumed tokens: 2912256000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T19:59:11 | step: 355600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0531991392781492e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.44 | consumed tokens: 2913075200.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T19:59:32 | step: 355700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0519286661292426e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.66 | consumed tokens: 2913894400.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T19:59:52 | step: 355800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0506588296266273e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 2914713600.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T20:00:13 | step: 355900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0493894478713628e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.33 | consumed tokens: 2915532800.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:00:33 | step: 356000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.0481207027623896e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 2916352000.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T20:00:54 | step: 356100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0468525942997076e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 2917171200.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T20:01:15 | step: 356200 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 1.0455849405843765e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.67 | consumed tokens: 2917990400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:01:36 | step: 356300 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 1.0443179235153366e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.47 | consumed tokens: 2918809600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:01:56 | step: 356400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0430514521431178e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 2919628800.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:02:17 | step: 356500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.04178552646772e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.0 | consumed tokens: 2920448000.0 | grad norm avg: 0.95 | grad norm last: 0.89 | 
2025-12-30T20:02:37 | step: 356600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0405202374386135e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.8 | consumed tokens: 2921267200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T20:02:58 | step: 356700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.039255403156858e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.72 | consumed tokens: 2922086400.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T20:03:19 | step: 356800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0379912964708637e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.36 | consumed tokens: 2922905600.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T20:03:39 | step: 356900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.0367276445322204e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.62 | consumed tokens: 2923724800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T20:03:59 | step: 357000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.0354646292398684e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.34 | consumed tokens: 2924544000.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T20:04:20 | step: 357100 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 1.0342021596443374e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.11 | consumed tokens: 2925363200.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:04:40 | step: 357200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 1.0329402357456274e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.98 | consumed tokens: 2926182400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:05:01 | step: 357300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 1.0316788575437386e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.47 | consumed tokens: 2927001600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:05:21 | step: 357400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.030418115988141e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.62 | consumed tokens: 2927820800.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T20:05:42 | step: 357500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 1.0291579201293644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.06 | consumed tokens: 2928640000.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:06:02 | step: 357600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.027898360916879e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 2929459200.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:06:23 | step: 357700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.0266393474012148e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 2.83 | consumed tokens: 2930278400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:06:44 | step: 357800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 1.0253808795823716e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 2931097600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T20:07:04 | step: 357900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 1.0241229574603494e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.41 | consumed tokens: 2931916800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T20:07:25 | step: 358000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 1.0228656719846185e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 2932736000.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:07:46 | step: 358100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0216090231551789e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.52 | consumed tokens: 2933555200.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T20:08:06 | step: 358200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0203528290730901e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 2934374400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:08:27 | step: 358300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.0190972716372926e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.0 | consumed tokens: 2935193600.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:08:47 | step: 358400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.0178422598983161e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.7 | consumed tokens: 2936012800.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T20:09:08 | step: 358500 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0165878848056309e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.98 | consumed tokens: 2936832000.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T20:09:28 | step: 358600 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 1.0153340554097667e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.75 | consumed tokens: 2937651200.0 | grad norm avg: 0.96 | grad norm last: 1.12 | 
2025-12-30T20:09:49 | step: 358700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 1.0140807717107236e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.52 | consumed tokens: 2938470400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:10:10 | step: 358800 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 1.0128281246579718e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.67 | consumed tokens: 2939289600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:10:30 | step: 358900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.011576023302041e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 2940108800.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T20:10:51 | step: 359000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.0103245585924014e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.52 | consumed tokens: 2940928000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T20:11:12 | step: 359100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 1.009073639579583e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.78 | consumed tokens: 2941747200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T20:11:32 | step: 359200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 1.0078232662635855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.58 | consumed tokens: 2942566400.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T20:11:53 | step: 359300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.0065735295938794e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.34 | consumed tokens: 2943385600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:12:13 | step: 359400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 1.0053244295704644e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.31 | consumed tokens: 2944204800.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T20:12:34 | step: 359500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 1.0040757842944004e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.91 | consumed tokens: 2945024000.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T20:12:54 | step: 359600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 1.0028277756646276e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.34 | consumed tokens: 2945843200.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:13:15 | step: 359700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 1.001580403681146e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.64 | consumed tokens: 2946662400.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T20:13:36 | step: 359800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 1.0003335773944855e-05 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.89 | consumed tokens: 2947481600.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T20:13:56 | step: 359900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.99087296804646e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.81 | consumed tokens: 2948300800.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:14:17 | step: 360000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 9.978416528610978e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2949120000.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2025-12-30T20:14:39 | step: 360100 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 9.965965546143707e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.89 | consumed tokens: 2949939200.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:15:00 | step: 360200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 9.953520930139348e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.12 | consumed tokens: 2950758400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:15:21 | step: 360300 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 9.941082680597901e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.84 | consumed tokens: 2951577600.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:15:41 | step: 360400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 9.928648978529964e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.03 | consumed tokens: 2952396800.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T20:16:02 | step: 360500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 9.916221642924938e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.12 | consumed tokens: 2953216000.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T20:16:22 | step: 360600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 9.903800673782825e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.06 | consumed tokens: 2954035200.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T20:16:43 | step: 360700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.891385161608923e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 2954854400.0 | grad norm avg: 0.96 | grad norm last: 0.85 | 
2025-12-30T20:17:04 | step: 360800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.878976015897933e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.98 | consumed tokens: 2955673600.0 | grad norm avg: 0.95 | grad norm last: 1.02 | 
2025-12-30T20:17:24 | step: 360900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.866572327155154e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.91 | consumed tokens: 2956492800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:17:45 | step: 361000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.854175004875287e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 2957312000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:18:05 | step: 361100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 9.841783139563631e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.94 | consumed tokens: 2958131200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:18:26 | step: 361200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.829397640714888e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 2958950400.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:18:47 | step: 361300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 9.817017598834354e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.53 | consumed tokens: 2959769600.0 | grad norm avg: 0.95 | grad norm last: 0.88 | 
2025-12-30T20:19:07 | step: 361400 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 9.804643923416734e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.44 | consumed tokens: 2960588800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T20:19:28 | step: 361500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 9.792275704967324e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.48 | consumed tokens: 2961408000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:19:48 | step: 361600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.779913852980826e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2962227200.0 | grad norm avg: 0.95 | grad norm last: 1.04 | 
2025-12-30T20:20:09 | step: 361700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 9.767557457962539e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.62 | consumed tokens: 2963046400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:20:29 | step: 361800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 9.755207429407164e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.52 | consumed tokens: 2963865600.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T20:20:50 | step: 361900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 9.742863767314702e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.17 | consumed tokens: 2964684800.0 | grad norm avg: 0.95 | grad norm last: 1.03 | 
2025-12-30T20:21:10 | step: 362000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 9.73052556219045e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.0 | consumed tokens: 2965504000.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:21:31 | step: 362100 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 9.71819281403441e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.98 | consumed tokens: 2966323200.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T20:21:51 | step: 362200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 9.705867341835983e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.16 | consumed tokens: 2967142400.0 | grad norm avg: 0.95 | grad norm last: 1.02 | 
2025-12-30T20:22:12 | step: 362300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.693546417111065e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.83 | consumed tokens: 2967961600.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:22:32 | step: 362400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 9.681232768343762e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.8 | consumed tokens: 2968780800.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:22:53 | step: 362500 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 9.668924576544669e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.61 | consumed tokens: 2969600000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T20:23:13 | step: 362600 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 9.656621841713786e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.05 | consumed tokens: 2970419200.0 | grad norm avg: 0.95 | grad norm last: 0.88 | 
2025-12-30T20:23:34 | step: 362700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 9.644325473345816e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.67 | consumed tokens: 2971238400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:23:54 | step: 362800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.632035471440759e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.2 | consumed tokens: 2972057600.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:24:15 | step: 362900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 9.619750926503912e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 2972876800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:24:35 | step: 363000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 9.607472748029977e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.64 | consumed tokens: 2973696000.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T20:24:56 | step: 363100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 9.595200936018955e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 2974515200.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T20:25:16 | step: 363200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.582934580976143e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.91 | consumed tokens: 2975334400.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T20:25:37 | step: 363300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 9.570674592396244e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.75 | consumed tokens: 2976153600.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:25:58 | step: 363400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 9.558420060784556e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.7 | consumed tokens: 2976972800.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:26:18 | step: 363500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 9.54617189563578e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 2977792000.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T20:26:39 | step: 363600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 9.533930096949916e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.5 | consumed tokens: 2978611200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T20:26:59 | step: 363700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.521693755232263e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.84 | consumed tokens: 2979430400.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T20:27:20 | step: 363800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 9.509463779977523e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.52 | consumed tokens: 2980249600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T20:27:40 | step: 363900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.497240171185695e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.06 | consumed tokens: 2981068800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:28:01 | step: 364000 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 9.485022019362077e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.48 | consumed tokens: 2981888000.0 | grad norm avg: 0.95 | grad norm last: 0.85 | 
2025-12-30T20:28:22 | step: 364100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 9.472810234001372e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.92 | consumed tokens: 2982707200.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:28:43 | step: 364200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 9.46060481510358e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.2 | consumed tokens: 2983526400.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T20:29:03 | step: 364300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.448404853173997e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.08 | consumed tokens: 2984345600.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T20:29:24 | step: 364400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 9.436211257707328e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.53 | consumed tokens: 2985164800.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T20:29:44 | step: 364500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.42402402870357e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 2985984000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T20:30:05 | step: 364600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 9.411842256668024e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.42 | consumed tokens: 2986803200.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T20:30:25 | step: 364700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 9.39966685109539e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.33 | consumed tokens: 2987622400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:30:46 | step: 364800 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 9.387497811985668e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.45 | consumed tokens: 2988441600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:31:06 | step: 364900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.375335139338858e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.58 | consumed tokens: 2989260800.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:31:27 | step: 365000 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 9.36317792366026e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.38 | consumed tokens: 2990080000.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T20:31:49 | step: 365100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.351027074444573e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.61 | consumed tokens: 2990899200.0 | grad norm avg: 0.95 | grad norm last: 1.02 | 
2025-12-30T20:32:10 | step: 365200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 9.3388825916918e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.84 | consumed tokens: 2991718400.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T20:32:31 | step: 365300 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 9.326743565907236e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.41 | consumed tokens: 2992537600.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T20:32:51 | step: 365400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 9.314610906585585e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.31 | consumed tokens: 2993356800.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:33:12 | step: 365500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 9.302484613726847e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.62 | consumed tokens: 2994176000.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:33:33 | step: 365600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.29036468733102e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.39 | consumed tokens: 2994995200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:33:53 | step: 365700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 9.278250217903405e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.14 | consumed tokens: 2995814400.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T20:34:14 | step: 365800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 9.266142114938702e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.44 | consumed tokens: 2996633600.0 | grad norm avg: 0.95 | grad norm last: 0.97 | 
2025-12-30T20:34:34 | step: 365900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 9.254040378436912e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 4.12 | consumed tokens: 2997452800.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:34:55 | step: 366000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.241945008398034e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.05 | consumed tokens: 2998272000.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:35:16 | step: 366100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.229855095327366e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.03 | consumed tokens: 2999091200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T20:35:36 | step: 366200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 9.217772458214313e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 2999910400.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T20:35:57 | step: 366300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 9.20569527806947e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 3000729600.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2025-12-30T20:36:17 | step: 366400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.193623554892838e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.89 | consumed tokens: 3001548800.0 | grad norm avg: 0.95 | grad norm last: 1.04 | 
2025-12-30T20:36:38 | step: 366500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.18155910767382e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.27 | consumed tokens: 3002368000.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T20:36:59 | step: 366600 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 9.169500117423013e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.67 | consumed tokens: 3003187200.0 | grad norm avg: 0.95 | grad norm last: 1.02 | 
2025-12-30T20:37:19 | step: 366700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 9.15744840312982e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.44 | consumed tokens: 3004006400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:37:40 | step: 366800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 9.145402145804837e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.34 | consumed tokens: 3004825600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:38:00 | step: 366900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.133361345448066e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.88 | consumed tokens: 3005644800.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T20:38:21 | step: 367000 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 9.121327821048908e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.28 | consumed tokens: 3006464000.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T20:38:42 | step: 367100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 9.109300663112663e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.15 | train loss last: 4.12 | consumed tokens: 3007283200.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T20:39:02 | step: 367200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.097278962144628e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.67 | consumed tokens: 3008102400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:39:23 | step: 367300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.085263627639506e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.81 | consumed tokens: 3008921600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T20:39:43 | step: 367400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 9.073254659597296e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.17 | consumed tokens: 3009740800.0 | grad norm avg: 0.95 | grad norm last: 0.87 | 
2025-12-30T20:40:04 | step: 367500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 9.061252058017999e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.34 | consumed tokens: 3010560000.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T20:40:25 | step: 367600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 9.049255822901614e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 3011379200.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2025-12-30T20:40:45 | step: 367700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 9.03726504475344e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.33 | consumed tokens: 3012198400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:41:06 | step: 367800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 9.02528154256288e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 3013017600.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:41:27 | step: 367900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 9.01330349734053e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 3013836800.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:41:47 | step: 368000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 9.001331818581093e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.27 | consumed tokens: 3014656000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:42:08 | step: 368100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.989366506284568e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.41 | consumed tokens: 3015475200.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:42:28 | step: 368200 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 8.977407560450956e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.45 | consumed tokens: 3016294400.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T20:42:49 | step: 368300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.965454981080256e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.42 | consumed tokens: 3017113600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T20:43:09 | step: 368400 | train samples/s: 85.3 | train mfu (16-bit): -1.0 | lr mean: 8.953507858677767e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.2 | consumed tokens: 3017932800.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:43:30 | step: 368500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.941568012232892e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.66 | consumed tokens: 3018752000.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:43:50 | step: 368600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.929633622756228e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.78 | consumed tokens: 3019571200.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T20:44:11 | step: 368700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.917706509237178e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.09 | consumed tokens: 3020390400.0 | grad norm avg: 0.96 | grad norm last: 0.89 | 
2025-12-30T20:44:31 | step: 368800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.905784852686338e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.19 | consumed tokens: 3021209600.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:44:52 | step: 368900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.893869562598411e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.16 | consumed tokens: 3022028800.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:45:12 | step: 369000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.881960638973396e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.38 | consumed tokens: 3022848000.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:45:33 | step: 369100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.870058081811294e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.86 | consumed tokens: 3023667200.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:45:53 | step: 369200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 8.858161891112104e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.27 | consumed tokens: 3024486400.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T20:46:14 | step: 369300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.846272066875827e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.2 | consumed tokens: 3025305600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:46:34 | step: 369400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.834388609102461e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.25 | consumed tokens: 3026124800.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:46:55 | step: 369500 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.822511517792009e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 3026944000.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:47:16 | step: 369600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 8.810640792944469e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 3027763200.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T20:47:36 | step: 369700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.79877643455984e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.47 | consumed tokens: 3028582400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:47:57 | step: 369800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 8.786917533143423e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.25 | consumed tokens: 3029401600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:48:17 | step: 369900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.77506590768462e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 3030220800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:48:37 | step: 370000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 8.76322064868873e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.69 | consumed tokens: 3031040000.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T20:49:00 | step: 370100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.75138084666105e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 3031859200.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:49:20 | step: 370200 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 8.739548320590984e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.83 | consumed tokens: 3032678400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:49:41 | step: 370300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 8.72772216098383e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.19 | consumed tokens: 3033497600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:50:01 | step: 370400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 8.715901458344888e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.83 | consumed tokens: 3034316800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:50:22 | step: 370500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 8.70408803166356e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.28 | consumed tokens: 3035136000.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T20:50:42 | step: 370600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.692280061950441e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.95 | consumed tokens: 3035955200.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T20:51:03 | step: 370700 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.680479368194938e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 3036774400.0 | grad norm avg: 0.95 | grad norm last: 0.93 | 
2025-12-30T20:51:24 | step: 370800 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 8.668685040902346e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 3037593600.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:51:44 | step: 370900 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 8.656896170577966e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.34 | consumed tokens: 3038412800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:52:05 | step: 371000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.6451145762112e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.55 | consumed tokens: 3039232000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:52:25 | step: 371100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.633339348307345e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.48 | consumed tokens: 3040051200.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:52:46 | step: 371200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 8.621569577371702e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 3040870400.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T20:53:06 | step: 371300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 8.609807082393672e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.58 | consumed tokens: 3041689600.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T20:53:27 | step: 371400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 8.598050953878555e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.5 | consumed tokens: 3042508800.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:53:47 | step: 371500 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 8.586301191826351e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 3043328000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T20:54:08 | step: 371600 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 8.574557796237059e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.73 | consumed tokens: 3044147200.0 | grad norm avg: 0.95 | grad norm last: 1.0 | 
2025-12-30T20:54:28 | step: 371700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.56282076711068e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 4.38 | consumed tokens: 3044966400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:54:49 | step: 371800 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 8.551090104447212e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.14 | consumed tokens: 3045785600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:55:10 | step: 371900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 8.539365808246657e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.33 | consumed tokens: 3046604800.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T20:55:31 | step: 372000 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 8.527647878509015e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 3047424000.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:55:51 | step: 372100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.515936315234285e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.58 | consumed tokens: 3048243200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T20:56:12 | step: 372200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.504232027917169e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.7 | consumed tokens: 3049062400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:56:32 | step: 372300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 8.492533197568264e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 3049881600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T20:56:53 | step: 372400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.480841643176973e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 3050700800.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:57:13 | step: 372500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.469155545753893e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.98 | consumed tokens: 3051520000.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T20:57:34 | step: 372600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.457476724288426e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.69 | consumed tokens: 3052339200.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T20:57:54 | step: 372700 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 8.445804269285873e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.02 | consumed tokens: 3053158400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:58:15 | step: 372800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 8.434138180746231e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 3053977600.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2025-12-30T20:58:36 | step: 372900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 8.422478458669502e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.12 | consumed tokens: 3054796800.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T20:58:56 | step: 373000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 8.410825103055686e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.77 | consumed tokens: 3055616000.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T20:59:17 | step: 373100 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 8.399178113904782e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.17 | consumed tokens: 3056435200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T20:59:38 | step: 373200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 8.387538400711492e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.42 | consumed tokens: 3057254400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T20:59:58 | step: 373300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 8.375904144486412e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.53 | consumed tokens: 3058073600.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:00:19 | step: 373400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 8.364277164218947e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.09 | consumed tokens: 3058892800.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2025-12-30T21:00:40 | step: 373500 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 8.352656550414395e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 3059712000.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T21:01:00 | step: 373600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 8.341042303072754e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.06 | consumed tokens: 3060531200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:01:21 | step: 373700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.329434422194026e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.78 | consumed tokens: 3061350400.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T21:01:42 | step: 373800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 8.317833817272913e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.03 | consumed tokens: 3062169600.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T21:02:02 | step: 373900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 8.30623866932001e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.23 | consumed tokens: 3062988800.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T21:02:23 | step: 374000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.29465079732472e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.53 | consumed tokens: 3063808000.0 | grad norm avg: 0.96 | grad norm last: 1.02 | 
2025-12-30T21:02:43 | step: 374100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.283069291792344e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.61 | consumed tokens: 3064627200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:03:04 | step: 374200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.27149415272288e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.66 | consumed tokens: 3065446400.0 | grad norm avg: 0.95 | grad norm last: 0.91 | 
2025-12-30T21:03:24 | step: 374300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 8.259925380116329e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.17 | consumed tokens: 3066265600.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:03:45 | step: 374400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 8.248363883467391e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 3067084800.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:04:06 | step: 374500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 8.236808753281366e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 3067904000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:04:26 | step: 374600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 8.225259989558253e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.22 | consumed tokens: 3068723200.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T21:04:47 | step: 374700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 8.213717592298053e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.42 | consumed tokens: 3069542400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T21:05:07 | step: 374800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.202181561500765e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.69 | consumed tokens: 3070361600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:05:28 | step: 374900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.19065189716639e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.77 | consumed tokens: 3071180800.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:05:49 | step: 375000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 8.179129508789629e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.05 | consumed tokens: 3072000000.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T21:06:11 | step: 375100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.16761348687578e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 4.06 | consumed tokens: 3072819200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:06:31 | step: 375200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.156103831424844e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 3073638400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:06:52 | step: 375300 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.144601451931521e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.3 | consumed tokens: 3074457600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T21:07:12 | step: 375400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.133105438901111e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.95 | consumed tokens: 3075276800.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:07:33 | step: 375500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 8.121615792333614e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.16 | consumed tokens: 3076096000.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:07:54 | step: 375600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 8.110132512229029e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.19 | consumed tokens: 3076915200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:08:14 | step: 375700 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 8.098655598587357e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.19 | consumed tokens: 3077734400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:08:35 | step: 375800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 8.087185960903298e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.73 | consumed tokens: 3078553600.0 | grad norm avg: 0.95 | grad norm last: 0.9 | 
2025-12-30T21:08:56 | step: 375900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 8.075722689682152e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.58 | consumed tokens: 3079372800.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:09:16 | step: 376000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 8.064265784923919e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.95 | consumed tokens: 3080192000.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:09:37 | step: 376100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.052816156123299e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 3081011200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:09:57 | step: 376200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 8.041372893785592e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.38 | consumed tokens: 3081830400.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T21:10:18 | step: 376300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 8.029935997910798e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.12 | consumed tokens: 3082649600.0 | grad norm avg: 0.96 | grad norm last: 1.03 | 
2025-12-30T21:10:38 | step: 376400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 8.018505468498915e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 3083468800.0 | grad norm avg: 0.95 | grad norm last: 0.95 | 
2025-12-30T21:10:59 | step: 376500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 8.007082215044647e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.03 | consumed tokens: 3084288000.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T21:11:19 | step: 376600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 7.995665328053292e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.81 | consumed tokens: 3085107200.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:11:40 | step: 376700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.984254807524849e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 3085926400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T21:12:01 | step: 376800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.97285156295402e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.55 | consumed tokens: 3086745600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:12:21 | step: 376900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.961454684846103e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.59 | consumed tokens: 3087564800.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:12:42 | step: 377000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.950064173201099e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 3088384000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:13:03 | step: 377100 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 7.938680937513709e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.89 | consumed tokens: 3089203200.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:13:23 | step: 377200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.927304068289232e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.53 | consumed tokens: 3090022400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:13:44 | step: 377300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 7.915933565527666e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.55 | consumed tokens: 3090841600.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:14:04 | step: 377400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 7.904570338723715e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.91 | consumed tokens: 3091660800.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T21:14:25 | step: 377500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.893213478382677e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 3092480000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:14:45 | step: 377600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 7.88186298450455e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.06 | consumed tokens: 3093299200.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:15:06 | step: 377700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.870519766584039e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.2 | consumed tokens: 3094118400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:15:27 | step: 377800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.85918291512644e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.22 | consumed tokens: 3094937600.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:15:47 | step: 377900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 7.847852430131752e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.94 | consumed tokens: 3095756800.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T21:16:08 | step: 378000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.836529221094679e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 3096576000.0 | grad norm avg: 0.96 | grad norm last: 0.88 | 
2025-12-30T21:16:28 | step: 378100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.825212378520519e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 3097395200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T21:16:49 | step: 378200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 7.813902811903972e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.2 | consumed tokens: 3098214400.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:17:10 | step: 378300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.802599611750338e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.52 | consumed tokens: 3099033600.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:17:30 | step: 378400 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 7.791302778059617e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.86 | consumed tokens: 3099852800.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T21:17:51 | step: 378500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.78001322032651e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 3100672000.0 | grad norm avg: 0.95 | grad norm last: 1.04 | 
2025-12-30T21:18:11 | step: 378600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.768730029056314e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.95 | consumed tokens: 3101491200.0 | grad norm avg: 0.95 | grad norm last: 0.98 | 
2025-12-30T21:18:32 | step: 378700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.757453204249032e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.2 | consumed tokens: 3102310400.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T21:18:53 | step: 378800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.746183655399363e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.34 | consumed tokens: 3103129600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:19:13 | step: 378900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.734920473012608e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.59 | consumed tokens: 3103948800.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T21:19:34 | step: 379000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.723664566583466e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.39 | consumed tokens: 3104768000.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:19:54 | step: 379100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 7.712415026617236e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 3105587200.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T21:20:15 | step: 379200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.701172762608621e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.55 | consumed tokens: 3106406400.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:20:35 | step: 379300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 7.689936865062919e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.31 | consumed tokens: 3107225600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:20:56 | step: 379400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.67870824347483e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.97 | consumed tokens: 3108044800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T21:21:16 | step: 379500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 7.667485988349654e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.62 | consumed tokens: 3108864000.0 | grad norm avg: 0.95 | grad norm last: 0.94 | 
2025-12-30T21:21:37 | step: 379600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 7.65627009968739e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.69 | consumed tokens: 3109683200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:21:58 | step: 379700 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 7.64506148698274e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.83 | consumed tokens: 3110502400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:22:19 | step: 379800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.633859240741003e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.78 | consumed tokens: 3111321600.0 | grad norm avg: 0.96 | grad norm last: 1.05 | 
2025-12-30T21:22:39 | step: 379900 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 7.62266427045688e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.55 | consumed tokens: 3112140800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:22:59 | step: 380000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.61147566663567e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 3112960000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T21:23:22 | step: 380100 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.600294338772073e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.45 | consumed tokens: 3113779200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:23:42 | step: 380200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.589119377371389e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.52 | consumed tokens: 3114598400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:24:03 | step: 380300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.577951237180969e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.69 | consumed tokens: 3115417600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:24:23 | step: 380400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.566790372948162e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.48 | consumed tokens: 3116236800.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:24:44 | step: 380500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.555635875178268e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.28 | consumed tokens: 3117056000.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:25:04 | step: 380600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.544488198618637e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.42 | consumed tokens: 3117875200.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T21:25:25 | step: 380700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.533347798016621e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.72 | consumed tokens: 3118694400.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:25:46 | step: 380800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.522213763877517e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.38 | consumed tokens: 3119513600.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:26:06 | step: 380900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 7.511086550948676e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.17 | consumed tokens: 3120332800.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:26:27 | step: 381000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.499966613977449e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 3121152000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:26:48 | step: 381100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.488853043469135e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.52 | consumed tokens: 3121971200.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:27:08 | step: 381200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 7.477746294171084e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.05 | consumed tokens: 3122790400.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T21:27:29 | step: 381300 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.466646820830647e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.84 | consumed tokens: 3123609600.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T21:27:49 | step: 381400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.455554168700473e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.22 | consumed tokens: 3124428800.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T21:28:10 | step: 381500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 7.444467883033212e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.33 | consumed tokens: 3125248000.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:28:31 | step: 381600 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 7.433388873323565e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.88 | consumed tokens: 3126067200.0 | grad norm avg: 0.96 | grad norm last: 0.88 | 
2025-12-30T21:28:51 | step: 381700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.422316684824182e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.55 | consumed tokens: 3126886400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:29:12 | step: 381800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 7.411251317535061e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.69 | consumed tokens: 3127705600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T21:29:32 | step: 381900 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 7.400192771456204e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.45 | consumed tokens: 3128524800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T21:29:53 | step: 382000 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.389141046587611e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.86 | consumed tokens: 3129344000.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:30:14 | step: 382100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.37809614292928e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.28 | consumed tokens: 3130163200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:30:34 | step: 382200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.367058515228564e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.41 | consumed tokens: 3130982400.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T21:30:55 | step: 382300 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 7.356027708738111e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.92 | consumed tokens: 3131801600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:31:16 | step: 382400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.34500326871057e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 3132620800.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:31:36 | step: 382500 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 7.333986104640644e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.05 | train loss last: 3.16 | consumed tokens: 3133440000.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:31:56 | step: 382600 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 7.322976216528332e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 3134259200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:32:17 | step: 382700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.311972694878932e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.02 | consumed tokens: 3135078400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:32:38 | step: 382800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.3009764491871465e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.17 | consumed tokens: 3135897600.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2025-12-30T21:32:58 | step: 382900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 7.289986569958273e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.12 | consumed tokens: 3136716800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T21:33:19 | step: 383000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.279003966687014e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.61 | consumed tokens: 3137536000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:33:39 | step: 383100 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.2680286393733695e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.98 | consumed tokens: 3138355200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:34:00 | step: 383200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 7.257059678522637e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.38 | consumed tokens: 3139174400.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T21:34:21 | step: 383300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.246097993629519e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.77 | consumed tokens: 3139993600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:34:41 | step: 383400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.235143129946664e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.56 | consumed tokens: 3140812800.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T21:35:02 | step: 383500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.224195087474072e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.39 | consumed tokens: 3141632000.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T21:35:23 | step: 383600 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 7.213254320959095e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.09 | consumed tokens: 3142451200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:35:43 | step: 383700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.20231992090703e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.05 | train loss last: 2.58 | consumed tokens: 3143270400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T21:36:04 | step: 383800 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 7.19139325155993e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.31 | consumed tokens: 3144089600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T21:36:25 | step: 383900 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 7.180472948675742e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.61 | consumed tokens: 3144908800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:36:45 | step: 384000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 7.169559921749169e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.64 | consumed tokens: 3145728000.0 | grad norm avg: 0.96 | grad norm last: 0.93 | 
2025-12-30T21:37:06 | step: 384100 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 7.158653716032859e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.27 | consumed tokens: 3146547200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T21:37:26 | step: 384200 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 7.147754331526812e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 3147366400.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T21:37:47 | step: 384300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 7.13686222297838e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.02 | consumed tokens: 3148185600.0 | grad norm avg: 0.96 | grad norm last: 1.01 | 
2025-12-30T21:38:07 | step: 384400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 7.12597693564021e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.72 | consumed tokens: 3149004800.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:38:28 | step: 384500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.115098924259655e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.11 | consumed tokens: 3149824000.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T21:38:48 | step: 384600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 7.1042272793420125e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.05 | consumed tokens: 3150643200.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T21:39:09 | step: 384700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 7.093363365129335e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.58 | consumed tokens: 3151462400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T21:39:29 | step: 384800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 7.08250581737957e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.11 | consumed tokens: 3152281600.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T21:39:50 | step: 384900 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 7.071655545587419e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.81 | consumed tokens: 3153100800.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T21:40:10 | step: 385000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 7.060812549752882e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.83 | consumed tokens: 3153920000.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T21:40:32 | step: 385100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 7.049976375128608e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.47 | consumed tokens: 3154739200.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:40:53 | step: 385200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 7.039147021714598e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.64 | consumed tokens: 3155558400.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T21:41:14 | step: 385300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 7.028324944258202e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.89 | consumed tokens: 3156377600.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T21:41:34 | step: 385400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 7.017509688012069e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.53 | consumed tokens: 3157196800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T21:41:55 | step: 385500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 7.0067012529762e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 3158016000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T21:42:15 | step: 385600 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.995900093897944e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.94 | consumed tokens: 3158835200.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T21:42:36 | step: 385700 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.985106210777303e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 3159654400.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:42:56 | step: 385800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.974319148866925e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.81 | consumed tokens: 3160473600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T21:43:17 | step: 385900 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 6.963538908166811e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.12 | consumed tokens: 3161292800.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T21:43:37 | step: 386000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 6.952766398171661e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.78 | consumed tokens: 3162112000.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T21:43:58 | step: 386100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.942000254639424e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 3162931200.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T21:44:19 | step: 386200 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 6.9312413870648015e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.88 | consumed tokens: 3163750400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T21:44:39 | step: 386300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.920489795447793e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.97 | consumed tokens: 3164569600.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T21:45:00 | step: 386400 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.9097450250410475e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.88 | consumed tokens: 3165388800.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T21:45:21 | step: 386500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.899007530591916e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 3166208000.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T21:45:41 | step: 386600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.888276857353048e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.22 | consumed tokens: 3167027200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T21:46:02 | step: 386700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 6.877553460071795e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.83 | consumed tokens: 3167846400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:46:22 | step: 386800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 6.866836884000804e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.78 | consumed tokens: 3168665600.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T21:46:42 | step: 386900 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 6.856127583887428e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.62 | consumed tokens: 3169484800.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:47:03 | step: 387000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.845425559731666e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.77 | consumed tokens: 3170304000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:47:24 | step: 387100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.834730356786167e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.72 | consumed tokens: 3171123200.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T21:47:44 | step: 387200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.824041975050932e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.19 | consumed tokens: 3171942400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:48:05 | step: 387300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.813361324020661e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.64 | consumed tokens: 3172761600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:48:25 | step: 387400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.802687494200654e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 3173580800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:48:46 | step: 387500 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 6.7920209403382614e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.12 | consumed tokens: 3174400000.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T21:49:07 | step: 387600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.781361207686132e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.17 | consumed tokens: 3175219200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T21:49:28 | step: 387700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.770708750991616e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.52 | consumed tokens: 3176038400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T21:49:48 | step: 387800 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 6.760063570254715e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.58 | consumed tokens: 3176857600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T21:50:09 | step: 387900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.749425210728077e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.14 | consumed tokens: 3177676800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:50:29 | step: 388000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.7387941271590535e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 3178496000.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2025-12-30T21:50:50 | step: 388100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.728170319547644e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.25 | consumed tokens: 3179315200.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
2025-12-30T21:51:10 | step: 388200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.717553333146498e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 3180134400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T21:51:31 | step: 388300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.7069436227029655e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 4.09 | consumed tokens: 3180953600.0 | grad norm avg: 0.97 | grad norm last: 1.08 | 
2025-12-30T21:51:52 | step: 388400 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.696341188217048e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.05 | train loss last: 2.75 | consumed tokens: 3181772800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:52:12 | step: 388500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.685746029688744e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.84 | consumed tokens: 3182592000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T21:52:33 | step: 388600 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 6.6751576923707034e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.73 | consumed tokens: 3183411200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T21:52:54 | step: 388700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.664576631010277e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.28 | consumed tokens: 3184230400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:53:15 | step: 388800 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 6.654002845607465e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.03 | consumed tokens: 3185049600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T21:53:35 | step: 388900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 6.643435881414916e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.02 | consumed tokens: 3185868800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:53:56 | step: 389000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.632876647927333e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.02 | consumed tokens: 3186688000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:54:16 | step: 389100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.622324235650012e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.45 | consumed tokens: 3187507200.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2025-12-30T21:54:37 | step: 389200 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.611779099330306e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.16 | consumed tokens: 3188326400.0 | grad norm avg: 0.96 | grad norm last: 0.88 | 
2025-12-30T21:54:57 | step: 389300 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 6.601241238968214e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.41 | consumed tokens: 3189145600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T21:55:18 | step: 389400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.590710199816385e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.94 | consumed tokens: 3189964800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T21:55:39 | step: 389500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.580186891369522e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.3 | consumed tokens: 3190784000.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T21:55:59 | step: 389600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.569670404132921e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.05 | train loss last: 2.94 | consumed tokens: 3191603200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T21:56:20 | step: 389700 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.559161192853935e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.19 | consumed tokens: 3192422400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:56:40 | step: 389800 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 6.548659257532563e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 4.06 | consumed tokens: 3193241600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:57:01 | step: 389900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.538164598168805e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.38 | consumed tokens: 3194060800.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T21:57:21 | step: 390000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.527676760015311e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 3194880000.0 | grad norm avg: 0.96 | grad norm last: 1.04 | 
2025-12-30T21:57:44 | step: 390100 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 6.517196652566781e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 3195699200.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T21:58:04 | step: 390200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.506723366328515e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.78 | consumed tokens: 3196518400.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T21:58:25 | step: 390300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.496257810795214e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.16 | consumed tokens: 3197337600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T21:58:45 | step: 390400 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 6.485799076472176e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 3198156800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:59:06 | step: 390500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.475347618106753e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.3 | consumed tokens: 3198976000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:59:27 | step: 390600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.464903435698943e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 3199795200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T21:59:47 | step: 390700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.454466529248748e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 3200614400.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T22:00:08 | step: 390800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 6.444036898756167e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.84 | consumed tokens: 3201433600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:00:29 | step: 390900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.4336145442212e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 3202252800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T22:00:49 | step: 391000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.4231990108964965e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.2 | consumed tokens: 3203072000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:01:10 | step: 391100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.412791208276758e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.92 | consumed tokens: 3203891200.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T22:01:31 | step: 391200 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 6.402390681614634e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 3204710400.0 | grad norm avg: 0.96 | grad norm last: 0.95 | 
2025-12-30T22:01:51 | step: 391300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.3919974309101235e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.53 | consumed tokens: 3205529600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:02:12 | step: 391400 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 6.381611001415877e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.3 | consumed tokens: 3206348800.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T22:02:33 | step: 391500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.371232302626595e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.56 | consumed tokens: 3207168000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:02:54 | step: 391600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 6.360860879794927e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.23 | consumed tokens: 3207987200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:03:14 | step: 391700 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 6.350496732920874e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.67 | consumed tokens: 3208806400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:03:35 | step: 391800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.340139407257084e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.42 | consumed tokens: 3209625600.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T22:03:55 | step: 391900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.329789812298259e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.77 | consumed tokens: 3210444800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:04:16 | step: 392000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.319447493297048e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.84 | consumed tokens: 3211264000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:04:37 | step: 392100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 6.309112450253451e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.2 | consumed tokens: 3212083200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:04:57 | step: 392200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.298784683167469e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.09 | consumed tokens: 3212902400.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T22:05:18 | step: 392300 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.2884641920391005e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.42 | consumed tokens: 3213721600.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:05:39 | step: 392400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 6.278150976868346e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.27 | consumed tokens: 3214540800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:05:59 | step: 392500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.267845492402557e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.75 | consumed tokens: 3215360000.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T22:06:20 | step: 392600 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.2575468291470315e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.23 | consumed tokens: 3216179200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T22:06:41 | step: 392700 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 6.24725544184912e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.25 | consumed tokens: 3216998400.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T22:07:01 | step: 392800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.2369717852561735e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 3217817600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:07:22 | step: 392900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 6.226695404620841e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 3218636800.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T22:07:42 | step: 393000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 6.216425845195772e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 3219456000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:08:03 | step: 393100 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 6.206164016475668e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.55 | consumed tokens: 3220275200.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2025-12-30T22:08:23 | step: 393200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.195909918460529e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 3221094400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:08:44 | step: 393300 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.185662641655654e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.33 | consumed tokens: 3221913600.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T22:09:05 | step: 393400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.175422640808392e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.69 | consumed tokens: 3222732800.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T22:09:25 | step: 393500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 6.165190370666096e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.7 | consumed tokens: 3223552000.0 | grad norm avg: 0.97 | grad norm last: 1.07 | 
2025-12-30T22:09:46 | step: 393600 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 6.154965376481414e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.33 | consumed tokens: 3224371200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:10:06 | step: 393700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 6.144747658254346e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.25 | consumed tokens: 3225190400.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T22:10:27 | step: 393800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 6.134537215984892e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 2.92 | consumed tokens: 3226009600.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T22:10:48 | step: 393900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.1243340496730525e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.73 | consumed tokens: 3226828800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:11:08 | step: 394000 | train samples/s: 82.7 | train mfu (16-bit): -1.0 | lr mean: 6.114138614066178e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.12 | consumed tokens: 3227648000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:11:29 | step: 394100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.103950454416918e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.94 | consumed tokens: 3228467200.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T22:11:50 | step: 394200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 6.0937695707252715e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.69 | consumed tokens: 3229286400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:12:10 | step: 394300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 6.08359641773859e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.64 | consumed tokens: 3230105600.0 | grad norm avg: 0.96 | grad norm last: 0.97 | 
2025-12-30T22:12:31 | step: 394400 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.073430085962173e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.34 | consumed tokens: 3230924800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:12:51 | step: 394500 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 6.06327148489072e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.78 | consumed tokens: 3231744000.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T22:13:12 | step: 394600 | train samples/s: 85.0 | train mfu (16-bit): -1.0 | lr mean: 6.053120159776881e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.62 | consumed tokens: 3232563200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:13:32 | step: 394700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 6.042976565368008e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.14 | consumed tokens: 3233382400.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T22:13:53 | step: 394800 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 6.032840246916749e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.89 | consumed tokens: 3234201600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T22:14:13 | step: 394900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 6.0227112044231035e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.61 | consumed tokens: 3235020800.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T22:14:34 | step: 395000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 6.0125894378870726e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.8 | consumed tokens: 3235840000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:14:56 | step: 395100 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 6.002475402056007e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.45 | consumed tokens: 3236659200.0 | grad norm avg: 0.96 | grad norm last: 1.0 | 
2025-12-30T22:15:17 | step: 395200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.992368642182555e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.03 | train loss last: 3.17 | consumed tokens: 3237478400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:15:38 | step: 395300 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 5.9822691582667176e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.88 | consumed tokens: 3238297600.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T22:15:58 | step: 395400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 5.972177405055845e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.62 | consumed tokens: 3239116800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:16:19 | step: 395500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.962092927802587e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 3239936000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:16:39 | step: 395600 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.952015726506943e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.06 | consumed tokens: 3240755200.0 | grad norm avg: 0.97 | grad norm last: 1.09 | 
2025-12-30T22:17:00 | step: 395700 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 5.941946255916264e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.25 | consumed tokens: 3241574400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:17:20 | step: 395800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 5.931884061283199e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.56 | consumed tokens: 3242393600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:17:41 | step: 395900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.921829597355099e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.56 | consumed tokens: 3243212800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:18:01 | step: 396000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.911782409384614e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.59 | consumed tokens: 3244032000.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:18:22 | step: 396100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.9017424973717425e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.31 | consumed tokens: 3244851200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:18:42 | step: 396200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.891710316063836e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.69 | consumed tokens: 3245670400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:19:03 | step: 396300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 5.881685410713544e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.22 | consumed tokens: 3246489600.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:19:23 | step: 396400 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.871668236068217e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.33 | consumed tokens: 3247308800.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T22:19:44 | step: 396500 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.861658337380504e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.5 | consumed tokens: 3248128000.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T22:20:05 | step: 396600 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 5.8516557146504056e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.73 | consumed tokens: 3248947200.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T22:20:25 | step: 396700 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.841660822625272e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.19 | consumed tokens: 3249766400.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T22:20:46 | step: 396800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.8316732065577526e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.53 | consumed tokens: 3250585600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:21:06 | step: 396900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.821693321195198e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.23 | consumed tokens: 3251404800.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:21:27 | step: 397000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 5.811721166537609e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 3252224000.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T22:21:47 | step: 397100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.801756287837634e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 3253043200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:22:08 | step: 397200 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 5.791798685095273e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.94 | consumed tokens: 3253862400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:22:28 | step: 397300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 5.781848813057877e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 3254681600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:22:48 | step: 397400 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 5.7719062169780955e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.84 | consumed tokens: 3255500800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:23:09 | step: 397500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 5.761971351603279e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.11 | consumed tokens: 3256320000.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T22:23:29 | step: 397600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.752044216933427e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.78 | consumed tokens: 3257139200.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:23:50 | step: 397700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.74212435822119e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.67 | consumed tokens: 3257958400.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T22:24:10 | step: 397800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.732211775466567e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.58 | consumed tokens: 3258777600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:24:31 | step: 397900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 5.722306923416909e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.11 | consumed tokens: 3259596800.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T22:24:52 | step: 398000 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.712409802072216e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.62 | consumed tokens: 3260416000.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T22:25:12 | step: 398100 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 5.702519956685137e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.61 | consumed tokens: 3261235200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:25:33 | step: 398200 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 5.692637842003023e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.0 | consumed tokens: 3262054400.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T22:25:53 | step: 398300 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 5.6827634580258746e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.3 | consumed tokens: 3262873600.0 | grad norm avg: 0.97 | grad norm last: 1.1 | 
2025-12-30T22:26:13 | step: 398400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.67289635000634e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.83 | consumed tokens: 3263692800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T22:26:34 | step: 398500 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 5.66303651794442e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.45 | consumed tokens: 3264512000.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:26:55 | step: 398600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.6531844165874645e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.27 | consumed tokens: 3265331200.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T22:27:15 | step: 398700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 5.643340045935474e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.34 | consumed tokens: 3266150400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:27:36 | step: 398800 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.633503405988449e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.64 | consumed tokens: 3266969600.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T22:27:56 | step: 398900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.623674041999038e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.58 | consumed tokens: 3267788800.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T22:28:17 | step: 399000 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.6138524087145925e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.31 | consumed tokens: 3268608000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:28:37 | step: 399100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.604038051387761e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.66 | consumed tokens: 3269427200.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T22:28:58 | step: 399200 | train samples/s: 82.5 | train mfu (16-bit): -1.0 | lr mean: 5.594231879513245e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.91 | consumed tokens: 3270246400.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:29:19 | step: 399300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.584432528848993e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 3271065600.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T22:29:39 | step: 399400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.574641363637056e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 3271884800.0 | grad norm avg: 0.97 | grad norm last: 1.05 | 
2025-12-30T22:30:00 | step: 399500 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 5.564857474382734e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.36 | consumed tokens: 3272704000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:30:20 | step: 399600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.555081315833377e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 3273523200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:30:41 | step: 399700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.545312887988985e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.12 | consumed tokens: 3274342400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:31:01 | step: 399800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.535551736102207e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.47 | consumed tokens: 3275161600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:31:22 | step: 399900 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.525798314920394e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.3 | consumed tokens: 3275980800.0 | grad norm avg: 0.97 | grad norm last: 0.89 | 
2025-12-30T22:31:42 | step: 400000 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 5.516052624443546e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.77 | consumed tokens: 3276800000.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T22:32:04 | step: 400100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.506314209924312e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.0 | consumed tokens: 3277619200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:32:25 | step: 400200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.496583980857395e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.81 | consumed tokens: 3278438400.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T22:32:45 | step: 400300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.486861027748091e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 3279257600.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T22:33:06 | step: 400400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.477145805343753e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.48 | consumed tokens: 3280076800.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:33:27 | step: 400500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.4674378588970285e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.53 | consumed tokens: 3280896000.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:33:47 | step: 400600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.45773809790262e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 3281715200.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T22:34:08 | step: 400700 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.448045612865826e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.92 | consumed tokens: 3282534400.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T22:34:28 | step: 400800 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 5.438360858533997e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 3.27 | consumed tokens: 3283353600.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T22:34:49 | step: 400900 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 5.428683834907133e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.62 | consumed tokens: 3284172800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:35:09 | step: 401000 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.419014087237883e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.16 | consumed tokens: 3284992000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:35:30 | step: 401100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.4093525250209495e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.06 | consumed tokens: 3285811200.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T22:35:51 | step: 401200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.39969823876163e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.78 | consumed tokens: 3286630400.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T22:36:11 | step: 401300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.390051683207275e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 3287449600.0 | grad norm avg: 0.98 | grad norm last: 0.9 | 
2025-12-30T22:36:32 | step: 401400 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 5.380412858357886e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.42 | consumed tokens: 3288268800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:36:52 | step: 401500 | train samples/s: 85.2 | train mfu (16-bit): -1.0 | lr mean: 5.3707817642134614e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.78 | consumed tokens: 3289088000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:37:12 | step: 401600 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 5.361158400774002e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.3 | consumed tokens: 3289907200.0 | grad norm avg: 0.97 | grad norm last: 1.07 | 
2025-12-30T22:37:33 | step: 401700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 5.351542313292157e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.45 | consumed tokens: 3290726400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:37:54 | step: 401800 | train samples/s: 83.0 | train mfu (16-bit): -1.0 | lr mean: 5.341934411262628e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.12 | consumed tokens: 3291545600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:38:14 | step: 401900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.332333785190713e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.42 | consumed tokens: 3292364800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:38:35 | step: 402000 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.322741344571114e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 3293184000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:38:55 | step: 402100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.313156179909129e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.61 | consumed tokens: 3294003200.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T22:39:16 | step: 402200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.303578745952109e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.8 | consumed tokens: 3294822400.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T22:39:37 | step: 402300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.294009042700054e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 3295641600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:39:57 | step: 402400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.2844470701529644e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.98 | consumed tokens: 3296460800.0 | grad norm avg: 0.97 | grad norm last: 1.03 | 
2025-12-30T22:40:18 | step: 402500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 5.27489282831084e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.31 | consumed tokens: 3297280000.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:40:38 | step: 402600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.26534631717368e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.59 | consumed tokens: 3298099200.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T22:40:59 | step: 402700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.255807081994135e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 3298918400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:41:20 | step: 402800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.2462760322669055e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.56 | consumed tokens: 3299737600.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:41:40 | step: 402900 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 5.236752713244641e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.66 | consumed tokens: 3300556800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:42:01 | step: 403000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 5.227237124927342e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.12 | consumed tokens: 3301376000.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T22:42:22 | step: 403100 | train samples/s: 82.6 | train mfu (16-bit): -1.0 | lr mean: 5.217729267315008e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.3 | consumed tokens: 3302195200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:42:42 | step: 403200 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.208228685660288e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.86 | consumed tokens: 3303014400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:43:03 | step: 403300 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.198736289457884e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 3303833600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:43:24 | step: 403400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.189251623960445e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 3304652800.0 | grad norm avg: 0.96 | grad norm last: 0.91 | 
2025-12-30T22:43:44 | step: 403500 | train samples/s: 85.1 | train mfu (16-bit): -1.0 | lr mean: 5.179774689167971e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.36 | consumed tokens: 3305472000.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T22:44:04 | step: 403600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.170305485080462e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.36 | consumed tokens: 3306291200.0 | grad norm avg: 0.97 | grad norm last: 0.9 | 
2025-12-30T22:44:25 | step: 403700 | train samples/s: 84.9 | train mfu (16-bit): -1.0 | lr mean: 5.160843556950567e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.08 | consumed tokens: 3307110400.0 | grad norm avg: 0.96 | grad norm last: 0.96 | 
2025-12-30T22:44:45 | step: 403800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.151389814272989e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.39 | consumed tokens: 3307929600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:45:06 | step: 403900 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.141943802300375e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.55 | consumed tokens: 3308748800.0 | grad norm avg: 0.96 | grad norm last: 0.99 | 
2025-12-30T22:45:26 | step: 404000 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 5.132505521032726e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.64 | consumed tokens: 3309568000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:45:47 | step: 404100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 5.123075425217394e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.77 | consumed tokens: 3310387200.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T22:46:07 | step: 404200 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.113652605359675e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.69 | consumed tokens: 3311206400.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T22:46:28 | step: 404300 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 5.104237516206922e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.09 | consumed tokens: 3312025600.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:46:49 | step: 404400 | train samples/s: 82.8 | train mfu (16-bit): -1.0 | lr mean: 5.094830157759134e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.72 | consumed tokens: 3312844800.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T22:47:10 | step: 404500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 5.085430984763661e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.11 | consumed tokens: 3313664000.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:47:30 | step: 404600 | train samples/s: 84.7 | train mfu (16-bit): -1.0 | lr mean: 5.076039542473154e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.27 | consumed tokens: 3314483200.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:47:50 | step: 404700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 5.066655376140261e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.88 | consumed tokens: 3315302400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:48:11 | step: 404800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 5.057279395259684e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.03 | consumed tokens: 3316121600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:48:32 | step: 404900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 5.047911145084072e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.91 | consumed tokens: 3316940800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:48:52 | step: 405000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 5.038550625613425e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.73 | consumed tokens: 3317760000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:49:15 | step: 405100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 5.029198291595094e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.53 | consumed tokens: 3318579200.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T22:49:35 | step: 405200 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 5.019853233534377e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.38 | consumed tokens: 3319398400.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:49:56 | step: 405300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.010516360925976e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.52 | consumed tokens: 3320217600.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T22:50:16 | step: 405400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 5.00118721902254e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.64 | consumed tokens: 3321036800.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:50:37 | step: 405500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.99186580782407e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.81 | consumed tokens: 3321856000.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T22:50:58 | step: 405600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.982552127330564e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.0 | consumed tokens: 3322675200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:51:19 | step: 405700 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.973246177542023e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 3323494400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T22:51:39 | step: 405800 | train samples/s: 83.1 | train mfu (16-bit): -1.0 | lr mean: 4.963948413205799e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.19 | consumed tokens: 3324313600.0 | grad norm avg: 0.98 | grad norm last: 1.1 | 
2025-12-30T22:52:00 | step: 405900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.954658379574539e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.64 | consumed tokens: 3325132800.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T22:52:21 | step: 406000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.945376076648245e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.12 | consumed tokens: 3325952000.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T22:52:41 | step: 406100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.936101504426915e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 3326771200.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T22:53:02 | step: 406200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.926835117657902e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.22 | consumed tokens: 3327590400.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T22:53:22 | step: 406300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.917576006846502e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.02 | consumed tokens: 3328409600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T22:53:43 | step: 406400 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.908325081487419e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.81 | consumed tokens: 3329228800.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T22:54:04 | step: 406500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.899082341580652e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.16 | consumed tokens: 3330048000.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T22:54:24 | step: 406600 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.8898468776314985e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.42 | consumed tokens: 3330867200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:54:45 | step: 406700 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.880619599134661e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.3 | consumed tokens: 3331686400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:55:06 | step: 406800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.871400051342789e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.58 | consumed tokens: 3332505600.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:55:26 | step: 406900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.862188689003233e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 3333324800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T22:55:47 | step: 407000 | train samples/s: 82.1 | train mfu (16-bit): -1.0 | lr mean: 4.852985057368642e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.33 | consumed tokens: 3334144000.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T22:56:08 | step: 407100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.843789156439016e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.55 | consumed tokens: 3334963200.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T22:56:29 | step: 407200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.834600986214355e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.41 | consumed tokens: 3335782400.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T22:56:49 | step: 407300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.82542100144201e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.77 | consumed tokens: 3336601600.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T22:57:10 | step: 407400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.81624874737463e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 3337420800.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T22:57:31 | step: 407500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.807084224012215e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.17 | consumed tokens: 3338240000.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T22:57:51 | step: 407600 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.797927886102116e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 1.82 | consumed tokens: 3339059200.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T22:58:12 | step: 407700 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.788779278896982e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.02 | consumed tokens: 3339878400.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T22:58:33 | step: 407800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.779638857144164e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 3340697600.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T22:58:53 | step: 407900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.770506166096311e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.53 | consumed tokens: 3341516800.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T22:59:14 | step: 408000 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.761381205753423e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.06 | consumed tokens: 3342336000.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T22:59:34 | step: 408100 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.7522644308628514e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.31 | consumed tokens: 3343155200.0 | grad norm avg: 0.99 | grad norm last: 0.97 | 
2025-12-30T22:59:55 | step: 408200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.743155386677245e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.39 | consumed tokens: 3343974400.0 | grad norm avg: 0.97 | grad norm last: 0.91 | 
2025-12-30T23:00:15 | step: 408300 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.734054073196603e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.8 | consumed tokens: 3344793600.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T23:00:36 | step: 408400 | train samples/s: 82.3 | train mfu (16-bit): -1.0 | lr mean: 4.724960945168277e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.75 | consumed tokens: 3345612800.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T23:00:57 | step: 408500 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.715875547844917e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.56 | consumed tokens: 3346432000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:01:18 | step: 408600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.706798335973872e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.61 | consumed tokens: 3347251200.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T23:01:38 | step: 408700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.697728854807792e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.77 | consumed tokens: 3348070400.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T23:01:59 | step: 408800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.6886675590940285e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.04 | train loss last: 2.73 | consumed tokens: 3348889600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:02:20 | step: 408900 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.67961399408523e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.84 | consumed tokens: 3349708800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T23:02:40 | step: 409000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.670568159781396e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.11 | consumed tokens: 3350528000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T23:03:01 | step: 409100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.661530510929879e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.02 | consumed tokens: 3351347200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T23:03:22 | step: 409200 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.652501047530677e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.73 | consumed tokens: 3352166400.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T23:03:42 | step: 409300 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.643479314836441e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.83 | consumed tokens: 3352985600.0 | grad norm avg: 0.97 | grad norm last: 1.01 | 
2025-12-30T23:04:02 | step: 409400 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.634465312847169e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.66 | consumed tokens: 3353804800.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T23:04:23 | step: 409500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.625459496310214e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.47 | consumed tokens: 3354624000.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T23:04:43 | step: 409600 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.616461865225574e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 3355443200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T23:05:04 | step: 409700 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.6074719648458995e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.48 | consumed tokens: 3356262400.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T23:05:24 | step: 409800 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.59848979517119e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 3357081600.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:05:45 | step: 409900 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.5895158109487966e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.42 | consumed tokens: 3357900800.0 | grad norm avg: 0.98 | grad norm last: 1.04 | 
2025-12-30T23:06:06 | step: 410000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.580550012178719e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.95 | consumed tokens: 3358720000.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T23:06:28 | step: 410100 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.571591944113607e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.17 | consumed tokens: 3359539200.0 | grad norm avg: 0.97 | grad norm last: 1.1 | 
2025-12-30T23:06:48 | step: 410200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.56264206150081e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.83 | consumed tokens: 3360358400.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T23:07:09 | step: 410300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.553699909592979e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.73 | consumed tokens: 3361177600.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T23:07:30 | step: 410400 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.544765943137463e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.2 | consumed tokens: 3361996800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T23:07:50 | step: 410500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.535839707386913e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.14 | consumed tokens: 3362816000.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T23:08:11 | step: 410600 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.526921657088678e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.73 | consumed tokens: 3363635200.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T23:08:31 | step: 410700 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.51801179224276e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 3364454400.0 | grad norm avg: 0.97 | grad norm last: 0.95 | 
2025-12-30T23:08:52 | step: 410800 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.509109658101806e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.0 | consumed tokens: 3365273600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T23:09:12 | step: 410900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.500215709413169e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 3.06 | consumed tokens: 3366092800.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T23:09:33 | step: 411000 | train samples/s: 82.0 | train mfu (16-bit): -1.0 | lr mean: 4.4913294914294966e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.22 | consumed tokens: 3366912000.0 | grad norm avg: 0.99 | grad norm last: 0.96 | 
2025-12-30T23:09:54 | step: 411100 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.48245145889814e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.72 | consumed tokens: 3367731200.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T23:10:15 | step: 411200 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.4735816118191e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.13 | train loss last: 2.94 | consumed tokens: 3368550400.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T23:10:35 | step: 411300 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.464719495445024e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.75 | consumed tokens: 3369369600.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T23:10:56 | step: 411400 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.455865564523265e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.78 | consumed tokens: 3370188800.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:11:17 | step: 411500 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.447019819053821e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.7 | consumed tokens: 3371008000.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T23:11:37 | step: 411600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.438181804289343e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.31 | consumed tokens: 3371827200.0 | grad norm avg: 0.98 | grad norm last: 1.0 | 
2025-12-30T23:11:58 | step: 411700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.42935197497718e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.66 | consumed tokens: 3372646400.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T23:12:19 | step: 411800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.420530331117334e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.47 | consumed tokens: 3373465600.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T23:12:39 | step: 411900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.411716417962452e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 3374284800.0 | grad norm avg: 0.99 | grad norm last: 1.03 | 
2025-12-30T23:13:00 | step: 412000 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.402910690259887e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.75 | consumed tokens: 3375104000.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T23:13:21 | step: 412100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.394113148009637e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.23 | consumed tokens: 3375923200.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:13:41 | step: 412200 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.385323336464353e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.34 | consumed tokens: 3376742400.0 | grad norm avg: 0.97 | grad norm last: 0.99 | 
2025-12-30T23:14:02 | step: 412300 | train samples/s: 82.4 | train mfu (16-bit): -1.0 | lr mean: 4.376541710371384e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.52 | consumed tokens: 3377561600.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:14:23 | step: 412400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.367768269730732e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.94 | consumed tokens: 3378380800.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T23:14:44 | step: 412500 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.359003014542395e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.75 | consumed tokens: 3379200000.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:15:04 | step: 412600 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.350245490059024e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.53 | consumed tokens: 3380019200.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T23:15:25 | step: 412700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.341496151027968e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 2.39 | consumed tokens: 3380838400.0 | grad norm avg: 0.98 | grad norm last: 1.08 | 
2025-12-30T23:15:46 | step: 412800 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.3327549974492285e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.12 | consumed tokens: 3381657600.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T23:16:06 | step: 412900 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.324022029322805e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.77 | consumed tokens: 3382476800.0 | grad norm avg: 0.97 | grad norm last: 1.07 | 
2025-12-30T23:16:27 | step: 413000 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 4.315296791901346e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.81 | consumed tokens: 3383296000.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T23:16:48 | step: 413100 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 4.306579739932204e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.75 | consumed tokens: 3384115200.0 | grad norm avg: 0.97 | grad norm last: 0.94 | 
2025-12-30T23:17:08 | step: 413200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.297870873415377e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.31 | consumed tokens: 3384934400.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T23:17:29 | step: 413300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.289170192350866e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 3.23 | consumed tokens: 3385753600.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T23:17:49 | step: 413400 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.280477241991321e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.05 | train loss last: 3.08 | consumed tokens: 3386572800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T23:18:10 | step: 413500 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.271792477084091e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.59 | consumed tokens: 3387392000.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:18:31 | step: 413600 | train samples/s: 82.9 | train mfu (16-bit): -1.0 | lr mean: 4.263116352376528e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.3 | consumed tokens: 3388211200.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T23:18:51 | step: 413700 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.254447503626579e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 2.59 | consumed tokens: 3389030400.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T23:19:12 | step: 413800 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 4.245787295076298e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.55 | consumed tokens: 3389849600.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:19:33 | step: 413900 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.237135271978332e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.16 | consumed tokens: 3390668800.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:19:53 | step: 414000 | train samples/s: 84.8 | train mfu (16-bit): -1.0 | lr mean: 4.228490979585331e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.66 | consumed tokens: 3391488000.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T23:20:13 | step: 414100 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.219855327391997e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.72 | consumed tokens: 3392307200.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T23:20:34 | step: 414200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.211227405903628e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 4.25 | consumed tokens: 3393126400.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T23:20:55 | step: 414300 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.202607669867575e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.38 | consumed tokens: 3393945600.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2025-12-30T23:21:15 | step: 414400 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.193996119283838e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.89 | consumed tokens: 3394764800.0 | grad norm avg: 0.99 | grad norm last: 1.04 | 
2025-12-30T23:21:36 | step: 414500 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 4.185392299405066e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.19 | consumed tokens: 3395584000.0 | grad norm avg: 0.99 | grad norm last: 1.05 | 
2025-12-30T23:21:56 | step: 414600 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.176797119725961e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.56 | consumed tokens: 3396403200.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T23:22:17 | step: 414700 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 4.168209670751821e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.03 | consumed tokens: 3397222400.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T23:22:37 | step: 414800 | train samples/s: 84.1 | train mfu (16-bit): -1.0 | lr mean: 4.159630861977348e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.86 | consumed tokens: 3398041600.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:22:58 | step: 414900 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.15105978390784e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.41 | consumed tokens: 3398860800.0 | grad norm avg: 0.99 | grad norm last: 1.0 | 
2025-12-30T23:23:19 | step: 415000 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.142497346037999e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.95 | consumed tokens: 3399680000.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T23:23:41 | step: 415100 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.133942638873123e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.61 | consumed tokens: 3400499200.0 | grad norm avg: 0.98 | grad norm last: 0.92 | 
2025-12-30T23:24:02 | step: 415200 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.1253961171605624e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.62 | consumed tokens: 3401318400.0 | grad norm avg: 0.97 | grad norm last: 1.04 | 
2025-12-30T23:24:22 | step: 415300 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.116857780900318e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 3402137600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T23:24:43 | step: 415400 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 4.10832763009239e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.14 | train loss last: 3.22 | consumed tokens: 3402956800.0 | grad norm avg: 0.99 | grad norm last: 0.98 | 
2025-12-30T23:25:04 | step: 415500 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.0998056647367775e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 3403776000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T23:25:24 | step: 415600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.091291884833481e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.16 | consumed tokens: 3404595200.0 | grad norm avg: 0.97 | grad norm last: 0.92 | 
2025-12-30T23:25:45 | step: 415700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 4.082786290382501e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.69 | consumed tokens: 3405414400.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T23:26:05 | step: 415800 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.074288881383836e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 3.0 | consumed tokens: 3406233600.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T23:26:26 | step: 415900 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.065799203090137e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.06 | train loss last: 2.8 | consumed tokens: 3407052800.0 | grad norm avg: 0.98 | grad norm last: 1.02 | 
2025-12-30T23:26:46 | step: 416000 | train samples/s: 84.2 | train mfu (16-bit): -1.0 | lr mean: 4.057318164996104e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.12 | train loss last: 3.08 | consumed tokens: 3407872000.0 | grad norm avg: 0.97 | grad norm last: 0.93 | 
2025-12-30T23:27:07 | step: 416100 | train samples/s: 84.5 | train mfu (16-bit): -1.0 | lr mean: 4.048845312354388e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.73 | consumed tokens: 3408691200.0 | grad norm avg: 0.97 | grad norm last: 1.06 | 
2025-12-30T23:27:28 | step: 416200 | train samples/s: 83.4 | train mfu (16-bit): -1.0 | lr mean: 4.040380645164987e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.59 | consumed tokens: 3409510400.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2025-12-30T23:27:48 | step: 416300 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 4.031924163427902e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.56 | consumed tokens: 3410329600.0 | grad norm avg: 0.98 | grad norm last: 0.91 | 
2025-12-30T23:28:09 | step: 416400 | train samples/s: 83.3 | train mfu (16-bit): -1.0 | lr mean: 4.023475867143134e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.28 | consumed tokens: 3411148800.0 | grad norm avg: 0.97 | grad norm last: 1.0 | 
2025-12-30T23:28:29 | step: 416500 | train samples/s: 84.3 | train mfu (16-bit): -1.0 | lr mean: 4.01503530156333e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.06 | consumed tokens: 3411968000.0 | grad norm avg: 0.98 | grad norm last: 0.96 | 
2025-12-30T23:28:50 | step: 416600 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 4.006603376183193e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.34 | consumed tokens: 3412787200.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T23:29:10 | step: 416700 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.998179636255372e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.55 | consumed tokens: 3413606400.0 | grad norm avg: 0.97 | grad norm last: 0.96 | 
2025-12-30T23:29:31 | step: 416800 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.989764081779867e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 2.77 | consumed tokens: 3414425600.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T23:29:52 | step: 416900 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.9813567127566785e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 4.19 | consumed tokens: 3415244800.0 | grad norm avg: 0.97 | grad norm last: 0.97 | 
2025-12-30T23:30:13 | step: 417000 | train samples/s: 83.5 | train mfu (16-bit): -1.0 | lr mean: 3.972957983933156e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.77 | consumed tokens: 3416064000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T23:30:33 | step: 417100 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.964566985814599e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.81 | consumed tokens: 3416883200.0 | grad norm avg: 0.98 | grad norm last: 0.99 | 
2025-12-30T23:30:54 | step: 417200 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.956184173148358e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.95 | consumed tokens: 3417702400.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:31:14 | step: 417300 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.947809545934433e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 3.03 | consumed tokens: 3418521600.0 | grad norm avg: 0.98 | grad norm last: 0.93 | 
2025-12-30T23:31:35 | step: 417400 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.939443558920175e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.34 | consumed tokens: 3419340800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T23:31:56 | step: 417500 | train samples/s: 82.2 | train mfu (16-bit): -1.0 | lr mean: 3.931085302610882e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.75 | consumed tokens: 3420160000.0 | grad norm avg: 0.98 | grad norm last: 1.03 | 
2025-12-30T23:32:17 | step: 417600 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.922735686501255e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 3420979200.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:32:37 | step: 417700 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.914394255843945e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.47 | consumed tokens: 3421798400.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:32:58 | step: 417800 | train samples/s: 83.9 | train mfu (16-bit): -1.0 | lr mean: 3.90606101063895e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.1 | train loss last: 2.8 | consumed tokens: 3422617600.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:33:18 | step: 417900 | train samples/s: 83.6 | train mfu (16-bit): -1.0 | lr mean: 3.897735950886272e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.78 | consumed tokens: 3423436800.0 | grad norm avg: 0.98 | grad norm last: 0.97 | 
2025-12-30T23:33:39 | step: 418000 | train samples/s: 83.2 | train mfu (16-bit): -1.0 | lr mean: 3.889419076585909e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.61 | consumed tokens: 3424256000.0 | grad norm avg: 1.0 | grad norm last: 0.97 | 
2025-12-30T23:34:00 | step: 418100 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.881110387737863e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.07 | train loss last: 3.12 | consumed tokens: 3425075200.0 | grad norm avg: 0.98 | grad norm last: 0.95 | 
2025-12-30T23:34:20 | step: 418200 | train samples/s: 84.0 | train mfu (16-bit): -1.0 | lr mean: 3.872810339089483e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 3425894400.0 | grad norm avg: 0.97 | grad norm last: 1.02 | 
2025-12-30T23:34:41 | step: 418300 | train samples/s: 84.6 | train mfu (16-bit): -1.0 | lr mean: 3.864518021146068e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.33 | consumed tokens: 3426713600.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:35:01 | step: 418400 | train samples/s: 84.4 | train mfu (16-bit): -1.0 | lr mean: 3.8562343434023205e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.77 | consumed tokens: 3427532800.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
2025-12-30T23:35:22 | step: 418500 | train samples/s: 83.8 | train mfu (16-bit): -1.0 | lr mean: 3.847958851110889e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.09 | train loss last: 3.12 | consumed tokens: 3428352000.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2025-12-30T23:35:43 | step: 418600 | train samples/s: 83.7 | train mfu (16-bit): -1.0 | lr mean: 3.839691544271773e-06 | peak memory rank 0 (MB): 3893.84 | train loss avg: 3.08 | train loss last: 2.89 | consumed tokens: 3429171200.0 | grad norm avg: 0.98 | grad norm last: 0.98 | 
