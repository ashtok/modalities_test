==========================================
Experiment 3: Fine-tuning GPT-2 on 5 languages
Job ID: 2149928
Node: jnultra01
Start time: Thu Jan  1 05:44:16 PM CET 2026
==========================================
Thu Jan  1 17:44:17 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0             70W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2026-01-01__17-44-31_a91e58afaade00f6
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 124,439,808 parameters): weight_decay = 0.01
=> all (148 modules with 124,439,808 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2026-01-01 17:44:35.396476.



======================== Training Report ========================
Training target: 
	num_target_tokens: 5713174528
	num_target_steps: 697409 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 1
CUDA environment settings: 
	local_rank: 0
	world_size: 1
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (5713177600) does not match the number of target tokens (5713174528). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (697409) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (697409) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (697409) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2026-01-01 17:44:35.396816.
2026-01-01T17:44:52 | step: 100 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 5.0228313739353325e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.32 | train loss last: 4.25 | consumed tokens: 819200.0 | grad norm avg: 3.02 | grad norm last: 2.78 | 
2026-01-01T17:45:08 | step: 200 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 5.0912785809487104e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.27 | train loss last: 4.31 | consumed tokens: 1638400.0 | grad norm avg: 2.73 | grad norm last: 2.54 | 
2026-01-01T17:45:24 | step: 300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 5.205202796787489e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.21 | train loss last: 3.59 | consumed tokens: 2457600.0 | grad norm avg: 2.67 | grad norm last: 2.86 | 
2026-01-01T17:45:39 | step: 400 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 5.3643730097974185e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.18 | train loss last: 4.09 | consumed tokens: 3276800.0 | grad norm avg: 2.61 | grad norm last: 2.66 | 
2026-01-01T17:45:55 | step: 500 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 5.568465894612018e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.13 | train loss last: 4.06 | consumed tokens: 4096000.0 | grad norm avg: 2.6 | grad norm last: 2.78 | 
2026-01-01T17:46:10 | step: 600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 5.817067631141981e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.15 | train loss last: 4.22 | consumed tokens: 4915200.0 | grad norm avg: 2.55 | grad norm last: 2.39 | 
2026-01-01T17:46:25 | step: 700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 6.109673449827824e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.11 | train loss last: 3.77 | consumed tokens: 5734400.0 | grad norm avg: 2.57 | grad norm last: 2.76 | 
2026-01-01T17:46:41 | step: 800 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 6.445689905376639e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.11 | train loss last: 4.22 | consumed tokens: 6553600.0 | grad norm avg: 2.57 | grad norm last: 2.46 | 
2026-01-01T17:46:56 | step: 900 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 6.824434422014747e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.08 | train loss last: 3.78 | consumed tokens: 7372800.0 | grad norm avg: 2.51 | grad norm last: 2.4 | 
2026-01-01T17:47:12 | step: 1000 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 7.245138931466499e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.11 | train loss last: 4.34 | consumed tokens: 8192000.0 | grad norm avg: 2.48 | grad norm last: 2.35 | 
2026-01-01T17:47:27 | step: 1100 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 7.706949872954283e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.07 | train loss last: 4.22 | consumed tokens: 9011200.0 | grad norm avg: 2.47 | grad norm last: 2.48 | 
2026-01-01T17:47:43 | step: 1200 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 8.208928193198517e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.08 | train loss last: 3.55 | consumed tokens: 9830400.0 | grad norm avg: 2.44 | grad norm last: 2.24 | 
2026-01-01T17:47:58 | step: 1300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 8.75005753186997e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.05 | train loss last: 3.86 | consumed tokens: 10649600.0 | grad norm avg: 2.43 | grad norm last: 2.37 | 
2026-01-01T17:48:13 | step: 1400 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 9.329239219368901e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.04 | train loss last: 4.78 | consumed tokens: 11468800.0 | grad norm avg: 2.41 | grad norm last: 2.39 | 
2026-01-01T17:48:29 | step: 1500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 9.945296369551215e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.01 | train loss last: 4.34 | consumed tokens: 12288000.0 | grad norm avg: 2.37 | grad norm last: 2.32 | 
2026-01-01T17:48:44 | step: 1600 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 1.0596980246191379e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.08 | train loss last: 4.47 | consumed tokens: 13107200.0 | grad norm avg: 2.37 | grad norm last: 2.39 | 
2026-01-01T17:49:00 | step: 1700 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 1.1282967534498312e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.02 | train loss last: 4.19 | consumed tokens: 13926400.0 | grad norm avg: 2.36 | grad norm last: 2.33 | 
2026-01-01T17:49:15 | step: 1800 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 1.20018657980836e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.03 | train loss last: 4.0 | consumed tokens: 14745600.0 | grad norm avg: 2.33 | grad norm last: 2.41 | 
2026-01-01T17:49:30 | step: 1900 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 1.2752217116940301e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.01 | train loss last: 3.55 | consumed tokens: 15564800.0 | grad norm avg: 2.26 | grad norm last: 2.18 | 
2026-01-01T17:49:46 | step: 2000 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 1.3532498087442946e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 3.78 | consumed tokens: 16384000.0 | grad norm avg: 2.23 | grad norm last: 2.29 | 
2026-01-01T17:50:01 | step: 2100 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 1.4341125279315747e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 4.16 | consumed tokens: 17203200.0 | grad norm avg: 2.22 | grad norm last: 2.09 | 
2026-01-01T17:50:16 | step: 2200 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.5176457964116707e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.98 | train loss last: 4.31 | consumed tokens: 18022400.0 | grad norm avg: 2.18 | grad norm last: 2.13 | 
2026-01-01T17:50:32 | step: 2300 | train samples/s: 106.8 | train mfu (16-bit): -1.0 | lr mean: 1.603679993422702e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 4.06 | consumed tokens: 18841600.0 | grad norm avg: 2.18 | grad norm last: 1.9 | 
2026-01-01T17:50:47 | step: 2400 | train samples/s: 106.9 | train mfu (16-bit): -1.0 | lr mean: 1.692040495981928e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 3.92 | consumed tokens: 19660800.0 | grad norm avg: 2.13 | grad norm last: 2.05 | 
2026-01-01T17:51:02 | step: 2500 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.78254831553204e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.96 | train loss last: 3.88 | consumed tokens: 20480000.0 | grad norm avg: 2.06 | grad norm last: 1.99 | 
2026-01-01T17:51:18 | step: 2600 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.8750193703453988e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.93 | train loss last: 4.25 | consumed tokens: 21299200.0 | grad norm avg: 2.02 | grad norm last: 2.06 | 
2026-01-01T17:51:33 | step: 2700 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.9692661226144992e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.93 | train loss last: 4.28 | consumed tokens: 22118400.0 | grad norm avg: 2.02 | grad norm last: 2.07 | 
2026-01-01T17:51:49 | step: 2800 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 2.065097214654088e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.96 | train loss last: 3.5 | consumed tokens: 22937600.0 | grad norm avg: 1.98 | grad norm last: 1.87 | 
2026-01-01T17:52:04 | step: 2900 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 2.1623183783958666e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.93 | train loss last: 4.16 | consumed tokens: 23756800.0 | grad norm avg: 1.93 | grad norm last: 1.86 | 
2026-01-01T17:52:19 | step: 3000 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 2.2607322534895502e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.89 | train loss last: 4.19 | consumed tokens: 24576000.0 | grad norm avg: 1.91 | grad norm last: 1.91 | 
2026-01-01T17:52:35 | step: 3100 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 2.3601391148986295e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.87 | train loss last: 4.19 | consumed tokens: 25395200.0 | grad norm avg: 1.86 | grad norm last: 2.13 | 
2026-01-01T17:52:50 | step: 3200 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 2.460337054799311e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.92 | train loss last: 3.75 | consumed tokens: 26214400.0 | grad norm avg: 1.81 | grad norm last: 1.91 | 
2026-01-01T17:53:05 | step: 3300 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 2.5611228920752183e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.88 | train loss last: 4.22 | consumed tokens: 27033600.0 | grad norm avg: 1.77 | grad norm last: 1.63 | 
2026-01-01T17:53:21 | step: 3400 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 2.6622919904184528e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.85 | train loss last: 3.36 | consumed tokens: 27852800.0 | grad norm avg: 1.75 | grad norm last: 1.81 | 
2026-01-01T17:53:36 | step: 3500 | train samples/s: 106.9 | train mfu (16-bit): -1.0 | lr mean: 2.7636391678242944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.87 | train loss last: 4.19 | consumed tokens: 28672000.0 | grad norm avg: 1.75 | grad norm last: 1.58 | 
2026-01-01T17:53:51 | step: 3600 | train samples/s: 106.8 | train mfu (16-bit): -1.0 | lr mean: 2.8649586965912022e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.87 | train loss last: 4.12 | consumed tokens: 29491200.0 | grad norm avg: 1.68 | grad norm last: 1.56 | 
2026-01-01T17:54:07 | step: 3700 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 2.966044849017635e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.85 | train loss last: 3.36 | consumed tokens: 30310400.0 | grad norm avg: 1.64 | grad norm last: 1.67 | 
2026-01-01T17:54:22 | step: 3800 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 3.066692443098873e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.84 | train loss last: 3.81 | consumed tokens: 31129600.0 | grad norm avg: 1.63 | grad norm last: 1.56 | 
2026-01-01T17:54:37 | step: 3900 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 3.1666975701227784e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 4.22 | consumed tokens: 31948800.0 | grad norm avg: 1.6 | grad norm last: 1.46 | 
2026-01-01T17:54:53 | step: 4000 | train samples/s: 106.8 | train mfu (16-bit): -1.0 | lr mean: 3.265856867074035e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 3.83 | consumed tokens: 32768000.0 | grad norm avg: 1.57 | grad norm last: 1.66 | 
2026-01-01T17:55:08 | step: 4100 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 3.363969153724611e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.83 | train loss last: 4.31 | consumed tokens: 33587200.0 | grad norm avg: 1.54 | grad norm last: 1.49 | 
2026-01-01T17:55:24 | step: 4200 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 3.460835796431638e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 4.16 | consumed tokens: 34406400.0 | grad norm avg: 1.53 | grad norm last: 1.6 | 
2026-01-01T17:55:39 | step: 4300 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 3.556259616743773e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 4.0 | consumed tokens: 35225600.0 | grad norm avg: 1.49 | grad norm last: 1.38 | 
2026-01-01T17:55:54 | step: 4400 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 3.650047074188478e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.78 | train loss last: 4.03 | consumed tokens: 36044800.0 | grad norm avg: 1.48 | grad norm last: 1.56 | 
2026-01-01T17:56:10 | step: 4500 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 3.742008266272023e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.8 | train loss last: 3.8 | consumed tokens: 36864000.0 | grad norm avg: 1.45 | grad norm last: 1.42 | 
2026-01-01T17:56:25 | step: 4600 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 3.831955837085843e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.78 | train loss last: 3.75 | consumed tokens: 37683200.0 | grad norm avg: 1.41 | grad norm last: 1.37 | 
2026-01-01T17:56:41 | step: 4700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 3.919707887689583e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.8 | train loss last: 4.09 | consumed tokens: 38502400.0 | grad norm avg: 1.39 | grad norm last: 1.29 | 
2026-01-01T17:56:56 | step: 4800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.0050861571216956e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.75 | train loss last: 3.77 | consumed tokens: 39321600.0 | grad norm avg: 1.36 | grad norm last: 1.36 | 
2026-01-01T17:57:12 | step: 4900 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.0879171137930825e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.79 | train loss last: 4.94 | consumed tokens: 40140800.0 | grad norm avg: 1.34 | grad norm last: 1.29 | 
2026-01-01T17:57:27 | step: 5000 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.168033046880737e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.76 | train loss last: 3.66 | consumed tokens: 40960000.0 | grad norm avg: 1.33 | grad norm last: 1.33 | 
2026-01-01T17:57:44 | step: 5100 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.245270974934101e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.73 | train loss last: 3.88 | consumed tokens: 41779200.0 | grad norm avg: 1.32 | grad norm last: 1.44 | 
2026-01-01T17:57:59 | step: 5200 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.31947446486447e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.75 | train loss last: 4.16 | consumed tokens: 42598400.0 | grad norm avg: 1.29 | grad norm last: 1.34 | 
2026-01-01T17:58:15 | step: 5300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.39049290434923e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.72 | train loss last: 3.64 | consumed tokens: 43417600.0 | grad norm avg: 1.28 | grad norm last: 1.28 | 
2026-01-01T17:58:30 | step: 5400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.45818186562974e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.76 | train loss last: 3.42 | consumed tokens: 44236800.0 | grad norm avg: 1.24 | grad norm last: 1.38 | 
2026-01-01T17:58:46 | step: 5500 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.522404196904972e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.73 | train loss last: 4.25 | consumed tokens: 45056000.0 | grad norm avg: 1.24 | grad norm last: 1.18 | 
2026-01-01T17:59:01 | step: 5600 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.583029658533633e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.71 | train loss last: 4.16 | consumed tokens: 45875200.0 | grad norm avg: 1.2 | grad norm last: 1.18 | 
2026-01-01T17:59:17 | step: 5700 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.639934923034161e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.75 | train loss last: 3.94 | consumed tokens: 46694400.0 | grad norm avg: 1.2 | grad norm last: 1.17 | 
2026-01-01T17:59:32 | step: 5800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.693004666478373e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.72 | train loss last: 3.3 | consumed tokens: 47513600.0 | grad norm avg: 1.18 | grad norm last: 1.16 | 
2026-01-01T17:59:48 | step: 5900 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.742131568491459e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.7 | train loss last: 3.88 | consumed tokens: 48332800.0 | grad norm avg: 1.15 | grad norm last: 1.08 | 
2026-01-01T18:00:03 | step: 6000 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.787215220858343e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.71 | train loss last: 3.88 | consumed tokens: 49152000.0 | grad norm avg: 1.15 | grad norm last: 1.07 | 
2026-01-01T18:00:19 | step: 6100 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.828164674108848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.71 | train loss last: 3.92 | consumed tokens: 49971200.0 | grad norm avg: 1.13 | grad norm last: 1.18 | 
2026-01-01T18:00:34 | step: 6200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.864896254730411e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.72 | train loss last: 3.7 | consumed tokens: 50790400.0 | grad norm avg: 1.1 | grad norm last: 1.05 | 
2026-01-01T18:00:50 | step: 6300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.897336111753248e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.69 | train loss last: 3.33 | consumed tokens: 51609600.0 | grad norm avg: 1.1 | grad norm last: 1.02 | 
2026-01-01T18:01:05 | step: 6400 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.925418033963069e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.78 | consumed tokens: 52428800.0 | grad norm avg: 1.08 | grad norm last: 1.07 | 
2026-01-01T18:01:21 | step: 6500 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.949085268890485e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.39 | consumed tokens: 53248000.0 | grad norm avg: 1.07 | grad norm last: 1.07 | 
2026-01-01T18:01:36 | step: 6600 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.968289431417361e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.5 | consumed tokens: 54067200.0 | grad norm avg: 1.07 | grad norm last: 0.98 | 
2026-01-01T18:01:52 | step: 6700 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.9829915951704606e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.94 | consumed tokens: 54886400.0 | grad norm avg: 1.04 | grad norm last: 1.05 | 
2026-01-01T18:02:07 | step: 6800 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 4.993161928723566e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.58 | consumed tokens: 55705600.0 | grad norm avg: 1.04 | grad norm last: 1.15 | 
2026-01-01T18:02:23 | step: 6900 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.998780059395358e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.68 | train loss last: 3.09 | consumed tokens: 56524800.0 | grad norm avg: 1.03 | grad norm last: 1.04 | 
2026-01-01T18:02:38 | step: 7000 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999999873689376e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.65 | train loss last: 4.03 | consumed tokens: 57344000.0 | grad norm avg: 1.02 | grad norm last: 0.98 | 
2026-01-01T18:02:53 | step: 7100 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999999509891495e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 4.03 | consumed tokens: 58163200.0 | grad norm avg: 1.01 | grad norm last: 0.99 | 
2026-01-01T18:03:09 | step: 7200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.999998782295734e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.8 | consumed tokens: 58982400.0 | grad norm avg: 1.01 | grad norm last: 1.03 | 
2026-01-01T18:03:24 | step: 7300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999997327104211e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.64 | train loss last: 3.61 | consumed tokens: 59801600.0 | grad norm avg: 1.0 | grad norm last: 0.93 | 
2026-01-01T18:03:40 | step: 7400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9999951443169266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.63 | train loss last: 3.0 | consumed tokens: 60620800.0 | grad norm avg: 0.99 | grad norm last: 1.01 | 
2026-01-01T18:03:55 | step: 7500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9999929615296423e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.66 | consumed tokens: 61440000.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2026-01-01T18:04:11 | step: 7600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.999989687348716e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 4.31 | consumed tokens: 62259200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2026-01-01T18:04:26 | step: 7700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9999864131677896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.41 | consumed tokens: 63078400.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2026-01-01T18:04:42 | step: 7800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999982411391102e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.66 | consumed tokens: 63897600.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2026-01-01T18:04:57 | step: 7900 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999776820186526e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.44 | consumed tokens: 64716800.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2026-01-01T18:05:13 | step: 8000 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.9999725888483226e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.64 | train loss last: 3.42 | consumed tokens: 65536000.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2026-01-01T18:05:28 | step: 8100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999967131880112e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.55 | consumed tokens: 66355200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2026-01-01T18:05:43 | step: 8200 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.99996094731614e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.61 | train loss last: 3.28 | consumed tokens: 67174400.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2026-01-01T18:05:59 | step: 8300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999954398954287e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.63 | train loss last: 3.44 | consumed tokens: 67993600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2026-01-01T18:06:14 | step: 8400 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999947486794554e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.67 | consumed tokens: 68812800.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2026-01-01T18:06:30 | step: 8500 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999939847039059e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 4.22 | consumed tokens: 69632000.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2026-01-01T18:06:45 | step: 8600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9999314796878025e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.61 | train loss last: 3.31 | consumed tokens: 70451200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2026-01-01T18:07:01 | step: 8700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999227485386655e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.64 | consumed tokens: 71270400.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2026-01-01T18:07:16 | step: 8800 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999913653591648e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.61 | train loss last: 3.67 | consumed tokens: 72089600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2026-01-01T18:07:31 | step: 8900 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999038310488686e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.75 | consumed tokens: 72908800.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2026-01-01T18:07:47 | step: 9000 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.999893644708209e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.6 | train loss last: 4.06 | consumed tokens: 73728000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2026-01-01T18:08:02 | step: 9100 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999883094569668e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.59 | train loss last: 3.05 | consumed tokens: 74547200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-01T18:08:18 | step: 9200 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.999871816835366e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.77 | consumed tokens: 75366400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2026-01-01T18:08:33 | step: 9300 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.9998601753031835e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.7 | consumed tokens: 76185600.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2026-01-01T18:08:49 | step: 9400 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998478061752394e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.59 | train loss last: 4.28 | consumed tokens: 77004800.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2026-01-01T18:09:04 | step: 9500 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9998350732494146e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.6 | train loss last: 3.66 | consumed tokens: 77824000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-01T18:09:20 | step: 9600 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999821612727828e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.7 | consumed tokens: 78643200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2026-01-01T18:09:35 | step: 9700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998077884083614e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.5 | consumed tokens: 79462400.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2026-01-01T18:09:51 | step: 9800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999793236493133e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.6 | train loss last: 3.81 | consumed tokens: 80281600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2026-01-01T18:10:06 | step: 9900 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9997786845779046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 2.98 | consumed tokens: 81100800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2026-01-01T18:10:21 | step: 10000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.999763041269034e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.2 | consumed tokens: 81920000.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2026-01-01T18:10:38 | step: 10100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.999747034162283e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.5 | consumed tokens: 82739200.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2026-01-01T18:10:54 | step: 10200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.999730663257651e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.59 | train loss last: 3.78 | consumed tokens: 83558400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2026-01-01T18:11:09 | step: 10300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9997139285551384e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.57 | train loss last: 3.38 | consumed tokens: 84377600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2026-01-01T18:11:25 | step: 10400 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9996964662568644e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.67 | consumed tokens: 85196800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2026-01-01T18:11:40 | step: 10500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.999678276362829e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.36 | consumed tokens: 86016000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2026-01-01T18:11:56 | step: 10600 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.9996600864687935e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.47 | consumed tokens: 86835200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2026-01-01T18:12:11 | step: 10700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.999640805181116e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.69 | consumed tokens: 87654400.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2026-01-01T18:12:27 | step: 10800 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 4.999621523893438e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.45 | consumed tokens: 88473600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2026-01-01T18:12:42 | step: 10900 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9996011512121186e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.11 | consumed tokens: 89292800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2026-01-01T18:12:57 | step: 11000 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 4.999580778530799e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.8 | consumed tokens: 90112000.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2026-01-01T18:13:13 | step: 11100 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.999559678253718e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.22 | consumed tokens: 90931200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2026-01-01T18:13:28 | step: 11200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999538214178756e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.57 | train loss last: 3.53 | consumed tokens: 91750400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2026-01-01T18:13:44 | step: 11300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9995160225080326e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.54 | train loss last: 3.67 | consumed tokens: 92569600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2026-01-01T18:13:59 | step: 11400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9994934670394287e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.3 | consumed tokens: 93388800.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2026-01-01T18:14:15 | step: 11500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.999470183975063e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.54 | train loss last: 3.06 | consumed tokens: 94208000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2026-01-01T18:14:30 | step: 11600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999446537112817e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.73 | consumed tokens: 95027200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2026-01-01T18:14:46 | step: 11700 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9994225264526904e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.73 | consumed tokens: 95846400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2026-01-01T18:15:01 | step: 11800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999397788196802e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.31 | consumed tokens: 96665600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-01T18:15:17 | step: 11900 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.9993723223451525e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.54 | train loss last: 3.52 | consumed tokens: 97484800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2026-01-01T18:15:32 | step: 12000 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999346856493503e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 2.92 | consumed tokens: 98304000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2026-01-01T18:15:48 | step: 12100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999320299248211e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.83 | consumed tokens: 99123200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2026-01-01T18:16:03 | step: 12200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999293742002919e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.52 | consumed tokens: 99942400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2026-01-01T18:16:18 | step: 12300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999266457161866e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.05 | consumed tokens: 100761600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2026-01-01T18:16:34 | step: 12400 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9992384447250515e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.44 | consumed tokens: 101580800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2026-01-01T18:16:49 | step: 12500 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999210432288237e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.53 | consumed tokens: 102400000.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2026-01-01T18:17:05 | step: 12600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.99918132845778e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.61 | consumed tokens: 103219200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-01T18:17:20 | step: 12700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9991522246273234e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.16 | consumed tokens: 104038400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2026-01-01T18:17:36 | step: 12800 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999122393201105e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.75 | consumed tokens: 104857600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-01T18:17:51 | step: 12900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.999091834179126e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.25 | consumed tokens: 105676800.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2026-01-01T18:18:07 | step: 13000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9990609113592654e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.7 | consumed tokens: 106496000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2026-01-01T18:18:22 | step: 13100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9990296247415245e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.41 | consumed tokens: 107315200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T18:18:38 | step: 13200 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.998997610528022e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.38 | consumed tokens: 108134400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T18:18:53 | step: 13300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998965232516639e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.47 | consumed tokens: 108953600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2026-01-01T18:19:09 | step: 13400 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9989321269094944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.5 | consumed tokens: 109772800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2026-01-01T18:19:24 | step: 13500 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.998898657504469e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.14 | consumed tokens: 110592000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2026-01-01T18:19:40 | step: 13600 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.998864824301563e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.48 | consumed tokens: 111411200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2026-01-01T18:19:56 | step: 13700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.998830263502896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.31 | consumed tokens: 112230400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-01T18:20:11 | step: 13800 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.998794975108467e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.05 | consumed tokens: 113049600.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-01T18:20:27 | step: 13900 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.998759686714038e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.45 | consumed tokens: 113868800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-01T18:20:43 | step: 14000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.998723670723848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.59 | consumed tokens: 114688000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2026-01-01T18:20:58 | step: 14100 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.9986869271378964e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.91 | consumed tokens: 115507200.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2026-01-01T18:21:14 | step: 14200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.998649819754064e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.11 | consumed tokens: 116326400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2026-01-01T18:21:29 | step: 14300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998612348572351e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.69 | consumed tokens: 117145600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2026-01-01T18:21:45 | step: 14400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9985741497948766e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.81 | consumed tokens: 117964800.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-01T18:22:00 | step: 14500 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.998535223421641e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.53 | consumed tokens: 118784000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:22:16 | step: 14600 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.998496297048405e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.97 | consumed tokens: 119603200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2026-01-01T18:22:31 | step: 14700 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9984566430794075e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.34 | consumed tokens: 120422400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2026-01-01T18:22:47 | step: 14800 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.998416261514649e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.97 | consumed tokens: 121241600.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:23:02 | step: 14900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9983755161520094e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.36 | consumed tokens: 122060800.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:23:18 | step: 15000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.998334406991489e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.05 | consumed tokens: 122880000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:23:34 | step: 15100 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.998292570235208e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.62 | consumed tokens: 123699200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T18:23:50 | step: 15200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9982503696810454e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.02 | consumed tokens: 124518400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2026-01-01T18:24:05 | step: 15300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998207441531122e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.58 | consumed tokens: 125337600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2026-01-01T18:24:21 | step: 15400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9981641495833173e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.7 | consumed tokens: 126156800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2026-01-01T18:24:36 | step: 15500 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.998120493837632e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.53 | consumed tokens: 126976000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2026-01-01T18:24:52 | step: 15600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998076110496186e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.45 | consumed tokens: 127795200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2026-01-01T18:25:07 | step: 15700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9980313633568585e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.55 | consumed tokens: 128614400.0 | grad norm avg: 0.79 | grad norm last: 1.09 | 
2026-01-01T18:25:23 | step: 15800 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.99798588862177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.5 | consumed tokens: 129433600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2026-01-01T18:25:39 | step: 15900 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9979400500888005e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.45 | consumed tokens: 130252800.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-01T18:25:54 | step: 16000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.99789348396007e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.25 | consumed tokens: 131072000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:26:10 | step: 16100 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997846554033458e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.67 | consumed tokens: 131891200.0 | grad norm avg: 0.78 | grad norm last: 0.79 | 
2026-01-01T18:26:25 | step: 16200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.997799260308966e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 4.09 | consumed tokens: 132710400.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-01T18:26:41 | step: 16300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9977512389887124e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.25 | consumed tokens: 133529600.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-01T18:26:56 | step: 16400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.997702853870578e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.22 | consumed tokens: 134348800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2026-01-01T18:27:11 | step: 16500 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9976537411566824e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.52 | consumed tokens: 135168000.0 | grad norm avg: 0.79 | grad norm last: 0.75 | 
2026-01-01T18:27:27 | step: 16600 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.997604264644906e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.58 | consumed tokens: 135987200.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-01T18:27:42 | step: 16700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997554424335249e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.25 | consumed tokens: 136806400.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2026-01-01T18:27:58 | step: 16800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.99750385642983e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.34 | consumed tokens: 137625600.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:28:13 | step: 16900 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.99745256092865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.42 | consumed tokens: 138444800.0 | grad norm avg: 0.78 | grad norm last: 0.86 | 
2026-01-01T18:28:29 | step: 17000 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.99740126542747e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.97 | consumed tokens: 139264000.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:28:44 | step: 17100 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.997348878532648e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.78 | consumed tokens: 140083200.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:29:00 | step: 17200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997296491637826e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.0 | consumed tokens: 140902400.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-01T18:29:15 | step: 17300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9972433771472424e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 2.58 | consumed tokens: 141721600.0 | grad norm avg: 0.78 | grad norm last: 0.75 | 
2026-01-01T18:29:31 | step: 17400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9971895350608975e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 2.78 | consumed tokens: 142540800.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:29:46 | step: 17500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997135329176672e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.12 | consumed tokens: 143360000.0 | grad norm avg: 0.79 | grad norm last: 0.74 | 
2026-01-01T18:30:01 | step: 17600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9970807594945654e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.02 | consumed tokens: 144179200.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:30:17 | step: 17700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9970258260145783e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 4.41 | consumed tokens: 144998400.0 | grad norm avg: 0.78 | grad norm last: 0.76 | 
2026-01-01T18:30:32 | step: 17800 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.996969801140949e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.39 | consumed tokens: 145817600.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:30:48 | step: 17900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.99691377626732e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.73 | consumed tokens: 146636800.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-01T18:31:03 | step: 18000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.996857023797929e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.14 | consumed tokens: 147456000.0 | grad norm avg: 0.78 | grad norm last: 0.73 | 
2026-01-01T18:31:19 | step: 18100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.996799907530658e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.95 | consumed tokens: 148275200.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:31:34 | step: 18200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.996742063667625e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 2.81 | consumed tokens: 149094400.0 | grad norm avg: 0.78 | grad norm last: 0.75 | 
2026-01-01T18:31:50 | step: 18300 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.996683856006712e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.56 | consumed tokens: 149913600.0 | grad norm avg: 0.8 | grad norm last: 0.73 | 
2026-01-01T18:32:05 | step: 18400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.996624920750037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.38 | consumed tokens: 150732800.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-01T18:32:21 | step: 18500 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.996565621695481e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.48 | consumed tokens: 151552000.0 | grad norm avg: 0.78 | grad norm last: 0.75 | 
2026-01-01T18:32:36 | step: 18600 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 4.996505595045164e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.38 | consumed tokens: 152371200.0 | grad norm avg: 0.78 | grad norm last: 0.74 | 
2026-01-01T18:32:52 | step: 18700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9964452045969665e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.33 | consumed tokens: 153190400.0 | grad norm avg: 0.77 | grad norm last: 0.8 | 
2026-01-01T18:33:07 | step: 18800 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.996384450350888e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.38 | consumed tokens: 154009600.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:33:22 | step: 18900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.996322968509048e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.53 | consumed tokens: 154828800.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:33:38 | step: 19000 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.996261122869328e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.25 | consumed tokens: 155648000.0 | grad norm avg: 0.78 | grad norm last: 0.82 | 
2026-01-01T18:33:53 | step: 19100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9961989134317264e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.47 | consumed tokens: 156467200.0 | grad norm avg: 0.78 | grad norm last: 0.73 | 
2026-01-01T18:34:09 | step: 19200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.996135976398364e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.41 | consumed tokens: 157286400.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:34:24 | step: 19300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9960723117692396e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.64 | consumed tokens: 158105600.0 | grad norm avg: 0.78 | grad norm last: 0.81 | 
2026-01-01T18:34:40 | step: 19400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.996008283342235e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.55 | consumed tokens: 158924800.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-01T18:34:55 | step: 19500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.995943891117349e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.97 | consumed tokens: 159744000.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:35:11 | step: 19600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.995879135094583e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.28 | consumed tokens: 160563200.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:35:26 | step: 19700 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.995813287678175e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.05 | consumed tokens: 161382400.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:35:42 | step: 19800 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.995747440261766e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.28 | consumed tokens: 162201600.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-01T18:35:57 | step: 19900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9956808652495965e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.19 | consumed tokens: 163020800.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:36:13 | step: 20000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.995613926439546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.69 | consumed tokens: 163840000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:36:30 | step: 20100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.995546260033734e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.14 | consumed tokens: 164659200.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:36:45 | step: 20200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9954782298300415e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.73 | consumed tokens: 165478400.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:37:01 | step: 20300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9954094720305875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.12 | consumed tokens: 166297600.0 | grad norm avg: 0.78 | grad norm last: 0.74 | 
2026-01-01T18:37:16 | step: 20400 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.995340350433253e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.56 | consumed tokens: 167116800.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:37:31 | step: 20500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.995270865038037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.34 | consumed tokens: 167936000.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:37:47 | step: 20600 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9952006520470604e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.25 | consumed tokens: 168755200.0 | grad norm avg: 0.77 | grad norm last: 0.79 | 
2026-01-01T18:38:02 | step: 20700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.995130075258203e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.19 | consumed tokens: 169574400.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:38:18 | step: 20800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.995058770873584e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.55 | consumed tokens: 170393600.0 | grad norm avg: 0.77 | grad norm last: 0.8 | 
2026-01-01T18:38:34 | step: 20900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.994987102691084e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.67 | consumed tokens: 171212800.0 | grad norm avg: 0.77 | grad norm last: 0.82 | 
2026-01-01T18:38:49 | step: 21000 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.994914706912823e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.33 | consumed tokens: 172032000.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:39:05 | step: 21100 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.994841947336681e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.64 | consumed tokens: 172851200.0 | grad norm avg: 0.77 | grad norm last: 0.73 | 
2026-01-01T18:39:20 | step: 21200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9947688239626586e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.53 | consumed tokens: 173670400.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:39:36 | step: 21300 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.994694972992875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 2.97 | consumed tokens: 174489600.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:39:51 | step: 21400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.99462075822521e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.12 | consumed tokens: 175308800.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:40:07 | step: 21500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.994545815861784e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.38 | consumed tokens: 176128000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:40:22 | step: 21600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.994470509700477e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.31 | consumed tokens: 176947200.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:40:38 | step: 21700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9943948397412896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.62 | consumed tokens: 177766400.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:40:53 | step: 21800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.994318442186341e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 178585600.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-01T18:41:09 | step: 21900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.994241680833511e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 179404800.0 | grad norm avg: 0.77 | grad norm last: 0.81 | 
2026-01-01T18:41:24 | step: 22000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.99416419188492e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.27 | consumed tokens: 180224000.0 | grad norm avg: 0.76 | grad norm last: 0.82 | 
2026-01-01T18:41:40 | step: 22100 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.994086339138448e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 2.98 | consumed tokens: 181043200.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:41:55 | step: 22200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.994007758796215e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.47 | consumed tokens: 181862400.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-01T18:42:11 | step: 22300 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.993928814656101e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 2.84 | consumed tokens: 182681600.0 | grad norm avg: 0.77 | grad norm last: 0.86 | 
2026-01-01T18:42:27 | step: 22400 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9938495067181066e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.45 | consumed tokens: 183500800.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:42:42 | step: 22500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9937694711843506e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.17 | consumed tokens: 184320000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:42:57 | step: 22600 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 4.993689071852714e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.05 | consumed tokens: 185139200.0 | grad norm avg: 0.77 | grad norm last: 0.7 | 
2026-01-01T18:43:13 | step: 22700 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.993607944925316e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.56 | consumed tokens: 185958400.0 | grad norm avg: 0.76 | grad norm last: 0.8 | 
2026-01-01T18:43:28 | step: 22800 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.993526454200037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.05 | consumed tokens: 186777600.0 | grad norm avg: 0.76 | grad norm last: 0.82 | 
2026-01-01T18:43:44 | step: 22900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.993444599676877e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.88 | consumed tokens: 187596800.0 | grad norm avg: 0.76 | grad norm last: 0.76 | 
2026-01-01T18:43:59 | step: 23000 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.993362017557956e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.44 | consumed tokens: 188416000.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:44:15 | step: 23100 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.9932790716411546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.72 | consumed tokens: 189235200.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:44:30 | step: 23200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9931953981285915e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.25 | consumed tokens: 190054400.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:44:46 | step: 23300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9931113608181477e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.09 | consumed tokens: 190873600.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:45:01 | step: 23400 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9930265959119424e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 191692800.0 | grad norm avg: 0.77 | grad norm last: 0.71 | 
2026-01-01T18:45:17 | step: 23500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9929414672078565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.38 | consumed tokens: 192512000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:45:32 | step: 23600 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.99285597470589e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.25 | consumed tokens: 193331200.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:45:48 | step: 23700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.992769754608162e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 194150400.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:46:03 | step: 23800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.992683170712553e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.53 | consumed tokens: 194969600.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:46:19 | step: 23900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.992595859221183e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 2.83 | consumed tokens: 195788800.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:46:34 | step: 24000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.992508183931932e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.28 | consumed tokens: 196608000.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:46:50 | step: 24100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9924201448448e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.45 | consumed tokens: 197427200.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:47:05 | step: 24200 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.992331378161907e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 2.59 | consumed tokens: 198246400.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:47:20 | step: 24300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.992241883883253e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 4.06 | consumed tokens: 199065600.0 | grad norm avg: 0.76 | grad norm last: 0.81 | 
2026-01-01T18:47:36 | step: 24400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.992152389604598e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 199884800.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:47:51 | step: 24500 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9920621677301824e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 200704000.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:48:07 | step: 24600 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.991971218260005e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 201523200.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:48:22 | step: 24700 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.991879904991947e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 202342400.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T18:48:38 | step: 24800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9917882279260084e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.12 | consumed tokens: 203161600.0 | grad norm avg: 0.76 | grad norm last: 0.79 | 
2026-01-01T18:48:53 | step: 24900 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.991695823264308e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 203980800.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:49:09 | step: 25000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9916030548047274e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.34 | consumed tokens: 204800000.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:49:26 | step: 25100 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.991509558749385e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.8 | consumed tokens: 205619200.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:49:41 | step: 25200 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.991415698896162e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.03 | consumed tokens: 206438400.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:49:57 | step: 25300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.991321111447178e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 207257600.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:50:12 | step: 25400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9912265239981934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.97 | consumed tokens: 208076800.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:50:28 | step: 25500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.991130845155567e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.14 | consumed tokens: 208896000.0 | grad norm avg: 0.75 | grad norm last: 0.79 | 
2026-01-01T18:50:43 | step: 25600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.99103480251506e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 209715200.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:50:58 | step: 25700 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.990938396076672e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.64 | consumed tokens: 210534400.0 | grad norm avg: 0.76 | grad norm last: 0.76 | 
2026-01-01T18:51:14 | step: 25800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.990841625840403e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.05 | consumed tokens: 211353600.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:51:29 | step: 25900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.990744128008373e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 212172800.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:51:45 | step: 26000 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9906459025805816e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.41 | consumed tokens: 212992000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T18:52:00 | step: 26100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.99054767715279e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 2.95 | consumed tokens: 213811200.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T18:52:16 | step: 26200 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.9904483603313565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 214630400.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:52:32 | step: 26300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.990349043509923e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.78 | consumed tokens: 215449600.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:52:47 | step: 26400 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.990248999092728e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.64 | consumed tokens: 216268800.0 | grad norm avg: 0.75 | grad norm last: 0.7 | 
2026-01-01T18:53:03 | step: 26500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9901482270797715e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.33 | consumed tokens: 217088000.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:53:18 | step: 26600 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.990047091268934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.33 | consumed tokens: 217907200.0 | grad norm avg: 0.76 | grad norm last: 0.72 | 
2026-01-01T18:53:34 | step: 26700 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9899455916602165e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 218726400.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:53:49 | step: 26800 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.989843364455737e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 2.53 | consumed tokens: 219545600.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:54:04 | step: 26900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.989740773453377e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.69 | consumed tokens: 220364800.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:54:20 | step: 27000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9896378186531365e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.5 | consumed tokens: 221184000.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:54:35 | step: 27100 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9895341362571344e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 222003200.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:54:51 | step: 27200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.989429726265371e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 222822400.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:55:06 | step: 27300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9893249524757266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 223641600.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T18:55:22 | step: 27400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9892198148882017e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 224460800.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T18:55:38 | step: 27500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.989114313502796e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.19 | consumed tokens: 225280000.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T18:55:53 | step: 27600 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.989008084521629e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 226099200.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T18:56:09 | step: 27700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9889011279447004e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 4.25 | consumed tokens: 226918400.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T18:56:25 | step: 27800 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.988793807569891e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 227737600.0 | grad norm avg: 0.75 | grad norm last: 0.85 | 
2026-01-01T18:56:40 | step: 27900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.988686123397201e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.56 | consumed tokens: 228556800.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T18:56:56 | step: 28000 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.98857771162875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 229376000.0 | grad norm avg: 0.76 | grad norm last: 0.76 | 
2026-01-01T18:57:11 | step: 28100 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.988468936062418e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 2.97 | consumed tokens: 230195200.0 | grad norm avg: 0.76 | grad norm last: 0.72 | 
2026-01-01T18:57:26 | step: 28200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.988359796698205e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 231014400.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T18:57:42 | step: 28300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.988249929738231e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.25 | consumed tokens: 231833600.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:57:57 | step: 28400 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9881393351824954e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.86 | consumed tokens: 232652800.0 | grad norm avg: 0.76 | grad norm last: 0.8 | 
2026-01-01T18:58:13 | step: 28500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.98802874062676e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.12 | consumed tokens: 233472000.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T18:58:28 | step: 28600 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.987917054677382e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.53 | consumed tokens: 234291200.0 | grad norm avg: 0.75 | grad norm last: 0.8 | 
2026-01-01T18:58:44 | step: 28700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9878053687280044e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.75 | consumed tokens: 235110400.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:59:00 | step: 28800 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.987692955182865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 235929600.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:59:15 | step: 28900 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.987579814041965e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.44 | consumed tokens: 236748800.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:59:31 | step: 29000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9874663091031834e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 2.78 | consumed tokens: 237568000.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:59:46 | step: 29100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9873524403665215e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.11 | consumed tokens: 238387200.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T19:00:02 | step: 29200 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.987238207831979e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.02 | consumed tokens: 239206400.0 | grad norm avg: 0.75 | grad norm last: 0.85 | 
2026-01-01T19:00:17 | step: 29300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.987122883903794e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.5 | consumed tokens: 240025600.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T19:00:33 | step: 29400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.987007559975609e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 240844800.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T19:00:48 | step: 29500 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.986891508451663e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 241664000.0 | grad norm avg: 0.75 | grad norm last: 0.79 | 
2026-01-01T19:01:04 | step: 29600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.986775093129836e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.19 | consumed tokens: 242483200.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:01:19 | step: 29700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.986657950212248e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.28 | consumed tokens: 243302400.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:01:35 | step: 29800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9865404434967786e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.7 | consumed tokens: 244121600.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:01:50 | step: 29900 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.986422209185548e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 244940800.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:02:05 | step: 30000 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.986303611076437e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 2.81 | consumed tokens: 245760000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:02:23 | step: 30100 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.986184649169445e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 246579200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:02:38 | step: 30200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.986064959666692e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.45 | consumed tokens: 247398400.0 | grad norm avg: 0.75 | grad norm last: 0.79 | 
2026-01-01T19:02:54 | step: 30300 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.985944906366058e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.95 | consumed tokens: 248217600.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:03:09 | step: 30400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.985824125469662e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 2.97 | consumed tokens: 249036800.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:03:25 | step: 30500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.985702980775386e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.7 | consumed tokens: 249856000.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T19:03:40 | step: 30600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.985581472283229e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.55 | consumed tokens: 250675200.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T19:03:56 | step: 30700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.985459236195311e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.06 | consumed tokens: 251494400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:04:11 | step: 30800 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.985336272511631e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 4.06 | consumed tokens: 252313600.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:04:27 | step: 30900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9852133088279516e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.66 | consumed tokens: 253132800.0 | grad norm avg: 0.75 | grad norm last: 0.8 | 
2026-01-01T19:04:42 | step: 31000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9850896175485104e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 2.77 | consumed tokens: 253952000.0 | grad norm avg: 0.75 | grad norm last: 0.8 | 
2026-01-01T19:04:58 | step: 31100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.984965198673308e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.0 | consumed tokens: 254771200.0 | grad norm avg: 0.75 | grad norm last: 0.83 | 
2026-01-01T19:05:13 | step: 31200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9848404160002246e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 2.89 | consumed tokens: 255590400.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:05:29 | step: 31300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.984715269529261e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.73 | consumed tokens: 256409600.0 | grad norm avg: 0.75 | grad norm last: 0.81 | 
2026-01-01T19:05:44 | step: 31400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.984589395462535e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.33 | consumed tokens: 257228800.0 | grad norm avg: 0.74 | grad norm last: 0.8 | 
2026-01-01T19:06:00 | step: 31500 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.984463157597929e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 258048000.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:06:15 | step: 31600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.984336192137562e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.56 | consumed tokens: 258867200.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T19:06:31 | step: 31700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9842088628793135e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.84 | consumed tokens: 259686400.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T19:06:46 | step: 31800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9840811698231846e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.28 | consumed tokens: 260505600.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T19:07:02 | step: 31900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.983952749171294e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 261324800.0 | grad norm avg: 0.74 | grad norm last: 0.85 | 
2026-01-01T19:07:18 | step: 32000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.983823964721523e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 262144000.0 | grad norm avg: 0.75 | grad norm last: 0.68 | 
2026-01-01T19:07:33 | step: 32100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.983694452675991e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 262963200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:07:48 | step: 32200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9835645768325776e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.3 | consumed tokens: 263782400.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:08:04 | step: 32300 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.983434337191284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 2.92 | consumed tokens: 264601600.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:08:19 | step: 32400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9833033699542284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 265420800.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:08:35 | step: 32500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.983171675121412e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.98 | consumed tokens: 266240000.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T19:08:50 | step: 32600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.983039980288595e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.36 | consumed tokens: 267059200.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T19:09:06 | step: 32700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.982907194062136e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 267878400.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:09:22 | step: 32800 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.982774407835677e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.59 | consumed tokens: 268697600.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-01T19:09:37 | step: 32900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.982640894013457e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.7 | consumed tokens: 269516800.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:09:52 | step: 33000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.982507016393356e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 270336000.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:10:08 | step: 33100 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9823724111774936e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.23 | consumed tokens: 271155200.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:10:23 | step: 33200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9822374421637505e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 271974400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:10:39 | step: 33300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.982101745554246e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.83 | consumed tokens: 272793600.0 | grad norm avg: 0.74 | grad norm last: 0.75 | 
2026-01-01T19:10:54 | step: 33400 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.981965685146861e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.75 | consumed tokens: 273612800.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:11:10 | step: 33500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.981829260941595e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 4.06 | consumed tokens: 274432000.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:11:25 | step: 33600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9816921091405675e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.66 | consumed tokens: 275251200.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:11:41 | step: 33700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9815545935416594e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.69 | consumed tokens: 276070400.0 | grad norm avg: 0.74 | grad norm last: 0.83 | 
2026-01-01T19:11:56 | step: 33800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.98141635034699e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 276889600.0 | grad norm avg: 0.73 | grad norm last: 0.68 | 
2026-01-01T19:12:12 | step: 33900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.98127774335444e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 277708800.0 | grad norm avg: 0.74 | grad norm last: 0.79 | 
2026-01-01T19:12:27 | step: 34000 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.981138408766128e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 4.12 | consumed tokens: 278528000.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:12:43 | step: 34100 | train samples/s: 103.3 | train mfu (16-bit): -1.0 | lr mean: 4.980998710379936e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 4.09 | consumed tokens: 279347200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:12:59 | step: 34200 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.980858648195863e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.09 | consumed tokens: 280166400.0 | grad norm avg: 0.74 | grad norm last: 0.75 | 
2026-01-01T19:13:14 | step: 34300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.980717858416028e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 280985600.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:13:29 | step: 34400 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.980576704838313e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.3 | consumed tokens: 281804800.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:13:45 | step: 34500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.980435187462717e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.25 | consumed tokens: 282624000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:14:00 | step: 34600 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.98029294249136e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.48 | consumed tokens: 283443200.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:14:16 | step: 34700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.980150333722122e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.11 | consumed tokens: 284262400.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:14:31 | step: 34800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9800069973571226e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.19 | consumed tokens: 285081600.0 | grad norm avg: 0.74 | grad norm last: 0.78 | 
2026-01-01T19:14:47 | step: 34900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9798632971942425e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.42 | consumed tokens: 285900800.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:15:02 | step: 35000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.979718869435601e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.56 | consumed tokens: 286720000.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-01T19:15:19 | step: 35100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.979574077879079e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.25 | consumed tokens: 287539200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:15:35 | step: 35200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.979428922524676e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 2.94 | consumed tokens: 288358400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:15:50 | step: 35300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9792830395745113e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.67 | consumed tokens: 289177600.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:16:06 | step: 35400 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.979136792826466e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 289996800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:16:21 | step: 35500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.97898981848266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 290816000.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:16:38 | step: 35600 | train samples/s: 98.8 | train mfu (16-bit): -1.0 | lr mean: 4.9788424803409725e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.8 | consumed tokens: 291635200.0 | grad norm avg: 0.74 | grad norm last: 0.75 | 
2026-01-01T19:16:53 | step: 35700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9786947784014046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.14 | consumed tokens: 292454400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:17:09 | step: 35800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.978546348866075e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 293273600.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:17:24 | step: 35900 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9783971917349845e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 294092800.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:17:40 | step: 36000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.978248034603894e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 294912000.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:17:55 | step: 36100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9780981498770416e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.7 | consumed tokens: 295731200.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-01T19:18:11 | step: 36200 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.977947537554428e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.33 | consumed tokens: 296550400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:18:26 | step: 36300 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.977796561433934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.5 | consumed tokens: 297369600.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:18:42 | step: 36400 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9776452215155587e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 298188800.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:18:57 | step: 36500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.977493154001422e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 299008000.0 | grad norm avg: 0.73 | grad norm last: 0.79 | 
2026-01-01T19:19:13 | step: 36600 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.977340722689405e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.52 | consumed tokens: 299827200.0 | grad norm avg: 0.74 | grad norm last: 0.69 | 
2026-01-01T19:19:29 | step: 36700 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.977187927579507e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.94 | consumed tokens: 300646400.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:19:44 | step: 36800 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.977034404873848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 301465600.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:20:00 | step: 36900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.976880154572427e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.44 | consumed tokens: 302284800.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:20:15 | step: 37000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9767259042710066e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.25 | consumed tokens: 303104000.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:20:31 | step: 37100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.976570562575944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.42 | consumed tokens: 303923200.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:20:46 | step: 37200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.976415220880881e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 304742400.0 | grad norm avg: 0.73 | grad norm last: 0.76 | 
2026-01-01T19:21:02 | step: 37300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.976259151590057e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.16 | consumed tokens: 305561600.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-01T19:21:17 | step: 37400 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.976102718501352e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 306380800.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:21:32 | step: 37500 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9759455578168854e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 4.34 | consumed tokens: 307200000.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:21:48 | step: 37600 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9757880333345383e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 308019200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:22:04 | step: 37700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.97562978125643e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 4.16 | consumed tokens: 308838400.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:22:19 | step: 37800 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9754711653804407e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.11 | consumed tokens: 309657600.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-01T19:22:35 | step: 37900 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.97531182190869e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.92 | consumed tokens: 310476800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:22:50 | step: 38000 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.9751524784369394e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.66 | consumed tokens: 311296000.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:23:06 | step: 38100 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.974992043571547e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 4.03 | consumed tokens: 312115200.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:23:21 | step: 38200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.974831608706154e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.34 | consumed tokens: 312934400.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:23:37 | step: 38300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.974670446245e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.31 | consumed tokens: 313753600.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:23:52 | step: 38400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.974508556188084e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 2.94 | consumed tokens: 314572800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:24:08 | step: 38500 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.974346302333288e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.5 | consumed tokens: 315392000.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-01T19:24:24 | step: 38600 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.974183684680611e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.67 | consumed tokens: 316211200.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:24:39 | step: 38700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9740203394321725e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 2.97 | consumed tokens: 317030400.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:24:54 | step: 38800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.9738566303858534e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.61 | consumed tokens: 317849600.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:25:10 | step: 38900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9736925575416535e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.47 | consumed tokens: 318668800.0 | grad norm avg: 0.73 | grad norm last: 0.68 | 
2026-01-01T19:25:25 | step: 39000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.973527757101692e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.69 | consumed tokens: 319488000.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:25:41 | step: 39100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.97336259286385e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.8 | consumed tokens: 320307200.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:25:57 | step: 39200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.973196701030247e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 321126400.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:26:12 | step: 39300 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.973030445398763e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.38 | consumed tokens: 321945600.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:26:28 | step: 39400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.972863462171517e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.86 | consumed tokens: 322764800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:26:43 | step: 39500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.972696115146391e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.09 | consumed tokens: 323584000.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:26:59 | step: 39600 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.972528404323384e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 2.77 | consumed tokens: 324403200.0 | grad norm avg: 0.74 | grad norm last: 1.55 | 
2026-01-01T19:27:14 | step: 39700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.972359965904616e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.19 | consumed tokens: 325222400.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:27:30 | step: 39800 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.972191163687967e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.89 | consumed tokens: 326041600.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
