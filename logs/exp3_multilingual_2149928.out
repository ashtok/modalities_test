==========================================
Experiment 3: Fine-tuning GPT-2 on 5 languages
Job ID: 2149928
Node: jnultra01
Start time: Thu Jan  1 05:44:16 PM CET 2026
==========================================
Thu Jan  1 17:44:17 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0             70W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2026-01-01__17-44-31_a91e58afaade00f6
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 124,439,808 parameters): weight_decay = 0.01
=> all (148 modules with 124,439,808 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2026-01-01 17:44:35.396476.



======================== Training Report ========================
Training target: 
	num_target_tokens: 5713174528
	num_target_steps: 697409 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 1
CUDA environment settings: 
	local_rank: 0
	world_size: 1
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (5713177600) does not match the number of target tokens (5713174528). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (697409) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (697409) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (697409) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2026-01-01 17:44:35.396816.
2026-01-01T17:44:52 | step: 100 | train samples/s: 97.1 | train mfu (16-bit): -1.0 | lr mean: 5.0228313739353325e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.32 | train loss last: 4.25 | consumed tokens: 819200.0 | grad norm avg: 3.02 | grad norm last: 2.78 | 
2026-01-01T17:45:08 | step: 200 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 5.0912785809487104e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.27 | train loss last: 4.31 | consumed tokens: 1638400.0 | grad norm avg: 2.73 | grad norm last: 2.54 | 
2026-01-01T17:45:24 | step: 300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 5.205202796787489e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.21 | train loss last: 3.59 | consumed tokens: 2457600.0 | grad norm avg: 2.67 | grad norm last: 2.86 | 
2026-01-01T17:45:39 | step: 400 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 5.3643730097974185e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.18 | train loss last: 4.09 | consumed tokens: 3276800.0 | grad norm avg: 2.61 | grad norm last: 2.66 | 
2026-01-01T17:45:55 | step: 500 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 5.568465894612018e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.13 | train loss last: 4.06 | consumed tokens: 4096000.0 | grad norm avg: 2.6 | grad norm last: 2.78 | 
2026-01-01T17:46:10 | step: 600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 5.817067631141981e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.15 | train loss last: 4.22 | consumed tokens: 4915200.0 | grad norm avg: 2.55 | grad norm last: 2.39 | 
2026-01-01T17:46:25 | step: 700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 6.109673449827824e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.11 | train loss last: 3.77 | consumed tokens: 5734400.0 | grad norm avg: 2.57 | grad norm last: 2.76 | 
2026-01-01T17:46:41 | step: 800 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 6.445689905376639e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.11 | train loss last: 4.22 | consumed tokens: 6553600.0 | grad norm avg: 2.57 | grad norm last: 2.46 | 
2026-01-01T17:46:56 | step: 900 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 6.824434422014747e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.08 | train loss last: 3.78 | consumed tokens: 7372800.0 | grad norm avg: 2.51 | grad norm last: 2.4 | 
2026-01-01T17:47:12 | step: 1000 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 7.245138931466499e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.11 | train loss last: 4.34 | consumed tokens: 8192000.0 | grad norm avg: 2.48 | grad norm last: 2.35 | 
2026-01-01T17:47:27 | step: 1100 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 7.706949872954283e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.07 | train loss last: 4.22 | consumed tokens: 9011200.0 | grad norm avg: 2.47 | grad norm last: 2.48 | 
2026-01-01T17:47:43 | step: 1200 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 8.208928193198517e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.08 | train loss last: 3.55 | consumed tokens: 9830400.0 | grad norm avg: 2.44 | grad norm last: 2.24 | 
2026-01-01T17:47:58 | step: 1300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 8.75005753186997e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.05 | train loss last: 3.86 | consumed tokens: 10649600.0 | grad norm avg: 2.43 | grad norm last: 2.37 | 
2026-01-01T17:48:13 | step: 1400 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 9.329239219368901e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.04 | train loss last: 4.78 | consumed tokens: 11468800.0 | grad norm avg: 2.41 | grad norm last: 2.39 | 
2026-01-01T17:48:29 | step: 1500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 9.945296369551215e-06 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.01 | train loss last: 4.34 | consumed tokens: 12288000.0 | grad norm avg: 2.37 | grad norm last: 2.32 | 
2026-01-01T17:48:44 | step: 1600 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 1.0596980246191379e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.08 | train loss last: 4.47 | consumed tokens: 13107200.0 | grad norm avg: 2.37 | grad norm last: 2.39 | 
2026-01-01T17:49:00 | step: 1700 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 1.1282967534498312e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.02 | train loss last: 4.19 | consumed tokens: 13926400.0 | grad norm avg: 2.36 | grad norm last: 2.33 | 
2026-01-01T17:49:15 | step: 1800 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 1.20018657980836e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.03 | train loss last: 4.0 | consumed tokens: 14745600.0 | grad norm avg: 2.33 | grad norm last: 2.41 | 
2026-01-01T17:49:30 | step: 1900 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 1.2752217116940301e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 4.01 | train loss last: 3.55 | consumed tokens: 15564800.0 | grad norm avg: 2.26 | grad norm last: 2.18 | 
2026-01-01T17:49:46 | step: 2000 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 1.3532498087442946e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 3.78 | consumed tokens: 16384000.0 | grad norm avg: 2.23 | grad norm last: 2.29 | 
2026-01-01T17:50:01 | step: 2100 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 1.4341125279315747e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 4.16 | consumed tokens: 17203200.0 | grad norm avg: 2.22 | grad norm last: 2.09 | 
2026-01-01T17:50:16 | step: 2200 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.5176457964116707e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.98 | train loss last: 4.31 | consumed tokens: 18022400.0 | grad norm avg: 2.18 | grad norm last: 2.13 | 
2026-01-01T17:50:32 | step: 2300 | train samples/s: 106.8 | train mfu (16-bit): -1.0 | lr mean: 1.603679993422702e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 4.06 | consumed tokens: 18841600.0 | grad norm avg: 2.18 | grad norm last: 1.9 | 
2026-01-01T17:50:47 | step: 2400 | train samples/s: 106.9 | train mfu (16-bit): -1.0 | lr mean: 1.692040495981928e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.99 | train loss last: 3.92 | consumed tokens: 19660800.0 | grad norm avg: 2.13 | grad norm last: 2.05 | 
2026-01-01T17:51:02 | step: 2500 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.78254831553204e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.96 | train loss last: 3.88 | consumed tokens: 20480000.0 | grad norm avg: 2.06 | grad norm last: 1.99 | 
2026-01-01T17:51:18 | step: 2600 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.8750193703453988e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.93 | train loss last: 4.25 | consumed tokens: 21299200.0 | grad norm avg: 2.02 | grad norm last: 2.06 | 
2026-01-01T17:51:33 | step: 2700 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 1.9692661226144992e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.93 | train loss last: 4.28 | consumed tokens: 22118400.0 | grad norm avg: 2.02 | grad norm last: 2.07 | 
2026-01-01T17:51:49 | step: 2800 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 2.065097214654088e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.96 | train loss last: 3.5 | consumed tokens: 22937600.0 | grad norm avg: 1.98 | grad norm last: 1.87 | 
2026-01-01T17:52:04 | step: 2900 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 2.1623183783958666e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.93 | train loss last: 4.16 | consumed tokens: 23756800.0 | grad norm avg: 1.93 | grad norm last: 1.86 | 
2026-01-01T17:52:19 | step: 3000 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 2.2607322534895502e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.89 | train loss last: 4.19 | consumed tokens: 24576000.0 | grad norm avg: 1.91 | grad norm last: 1.91 | 
2026-01-01T17:52:35 | step: 3100 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 2.3601391148986295e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.87 | train loss last: 4.19 | consumed tokens: 25395200.0 | grad norm avg: 1.86 | grad norm last: 2.13 | 
2026-01-01T17:52:50 | step: 3200 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 2.460337054799311e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.92 | train loss last: 3.75 | consumed tokens: 26214400.0 | grad norm avg: 1.81 | grad norm last: 1.91 | 
2026-01-01T17:53:05 | step: 3300 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 2.5611228920752183e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.88 | train loss last: 4.22 | consumed tokens: 27033600.0 | grad norm avg: 1.77 | grad norm last: 1.63 | 
2026-01-01T17:53:21 | step: 3400 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 2.6622919904184528e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.85 | train loss last: 3.36 | consumed tokens: 27852800.0 | grad norm avg: 1.75 | grad norm last: 1.81 | 
2026-01-01T17:53:36 | step: 3500 | train samples/s: 106.9 | train mfu (16-bit): -1.0 | lr mean: 2.7636391678242944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.87 | train loss last: 4.19 | consumed tokens: 28672000.0 | grad norm avg: 1.75 | grad norm last: 1.58 | 
2026-01-01T17:53:51 | step: 3600 | train samples/s: 106.8 | train mfu (16-bit): -1.0 | lr mean: 2.8649586965912022e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.87 | train loss last: 4.12 | consumed tokens: 29491200.0 | grad norm avg: 1.68 | grad norm last: 1.56 | 
2026-01-01T17:54:07 | step: 3700 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 2.966044849017635e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.85 | train loss last: 3.36 | consumed tokens: 30310400.0 | grad norm avg: 1.64 | grad norm last: 1.67 | 
2026-01-01T17:54:22 | step: 3800 | train samples/s: 106.7 | train mfu (16-bit): -1.0 | lr mean: 3.066692443098873e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.84 | train loss last: 3.81 | consumed tokens: 31129600.0 | grad norm avg: 1.63 | grad norm last: 1.56 | 
2026-01-01T17:54:37 | step: 3900 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 3.1666975701227784e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 4.22 | consumed tokens: 31948800.0 | grad norm avg: 1.6 | grad norm last: 1.46 | 
2026-01-01T17:54:53 | step: 4000 | train samples/s: 106.8 | train mfu (16-bit): -1.0 | lr mean: 3.265856867074035e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 3.83 | consumed tokens: 32768000.0 | grad norm avg: 1.57 | grad norm last: 1.66 | 
2026-01-01T17:55:08 | step: 4100 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 3.363969153724611e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.83 | train loss last: 4.31 | consumed tokens: 33587200.0 | grad norm avg: 1.54 | grad norm last: 1.49 | 
2026-01-01T17:55:24 | step: 4200 | train samples/s: 106.6 | train mfu (16-bit): -1.0 | lr mean: 3.460835796431638e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 4.16 | consumed tokens: 34406400.0 | grad norm avg: 1.53 | grad norm last: 1.6 | 
2026-01-01T17:55:39 | step: 4300 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 3.556259616743773e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.81 | train loss last: 4.0 | consumed tokens: 35225600.0 | grad norm avg: 1.49 | grad norm last: 1.38 | 
2026-01-01T17:55:54 | step: 4400 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 3.650047074188478e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.78 | train loss last: 4.03 | consumed tokens: 36044800.0 | grad norm avg: 1.48 | grad norm last: 1.56 | 
2026-01-01T17:56:10 | step: 4500 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 3.742008266272023e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.8 | train loss last: 3.8 | consumed tokens: 36864000.0 | grad norm avg: 1.45 | grad norm last: 1.42 | 
2026-01-01T17:56:25 | step: 4600 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 3.831955837085843e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.78 | train loss last: 3.75 | consumed tokens: 37683200.0 | grad norm avg: 1.41 | grad norm last: 1.37 | 
2026-01-01T17:56:41 | step: 4700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 3.919707887689583e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.8 | train loss last: 4.09 | consumed tokens: 38502400.0 | grad norm avg: 1.39 | grad norm last: 1.29 | 
2026-01-01T17:56:56 | step: 4800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.0050861571216956e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.75 | train loss last: 3.77 | consumed tokens: 39321600.0 | grad norm avg: 1.36 | grad norm last: 1.36 | 
2026-01-01T17:57:12 | step: 4900 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.0879171137930825e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.79 | train loss last: 4.94 | consumed tokens: 40140800.0 | grad norm avg: 1.34 | grad norm last: 1.29 | 
2026-01-01T17:57:27 | step: 5000 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.168033046880737e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.76 | train loss last: 3.66 | consumed tokens: 40960000.0 | grad norm avg: 1.33 | grad norm last: 1.33 | 
2026-01-01T17:57:44 | step: 5100 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.245270974934101e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.73 | train loss last: 3.88 | consumed tokens: 41779200.0 | grad norm avg: 1.32 | grad norm last: 1.44 | 
2026-01-01T17:57:59 | step: 5200 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.31947446486447e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.75 | train loss last: 4.16 | consumed tokens: 42598400.0 | grad norm avg: 1.29 | grad norm last: 1.34 | 
2026-01-01T17:58:15 | step: 5300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.39049290434923e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.72 | train loss last: 3.64 | consumed tokens: 43417600.0 | grad norm avg: 1.28 | grad norm last: 1.28 | 
2026-01-01T17:58:30 | step: 5400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.45818186562974e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.76 | train loss last: 3.42 | consumed tokens: 44236800.0 | grad norm avg: 1.24 | grad norm last: 1.38 | 
2026-01-01T17:58:46 | step: 5500 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.522404196904972e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.73 | train loss last: 4.25 | consumed tokens: 45056000.0 | grad norm avg: 1.24 | grad norm last: 1.18 | 
2026-01-01T17:59:01 | step: 5600 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.583029658533633e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.71 | train loss last: 4.16 | consumed tokens: 45875200.0 | grad norm avg: 1.2 | grad norm last: 1.18 | 
2026-01-01T17:59:17 | step: 5700 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.639934923034161e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.75 | train loss last: 3.94 | consumed tokens: 46694400.0 | grad norm avg: 1.2 | grad norm last: 1.17 | 
2026-01-01T17:59:32 | step: 5800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.693004666478373e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.72 | train loss last: 3.3 | consumed tokens: 47513600.0 | grad norm avg: 1.18 | grad norm last: 1.16 | 
2026-01-01T17:59:48 | step: 5900 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.742131568491459e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.7 | train loss last: 3.88 | consumed tokens: 48332800.0 | grad norm avg: 1.15 | grad norm last: 1.08 | 
2026-01-01T18:00:03 | step: 6000 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.787215220858343e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.71 | train loss last: 3.88 | consumed tokens: 49152000.0 | grad norm avg: 1.15 | grad norm last: 1.07 | 
2026-01-01T18:00:19 | step: 6100 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.828164674108848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.71 | train loss last: 3.92 | consumed tokens: 49971200.0 | grad norm avg: 1.13 | grad norm last: 1.18 | 
2026-01-01T18:00:34 | step: 6200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.864896254730411e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.72 | train loss last: 3.7 | consumed tokens: 50790400.0 | grad norm avg: 1.1 | grad norm last: 1.05 | 
2026-01-01T18:00:50 | step: 6300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.897336111753248e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.69 | train loss last: 3.33 | consumed tokens: 51609600.0 | grad norm avg: 1.1 | grad norm last: 1.02 | 
2026-01-01T18:01:05 | step: 6400 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.925418033963069e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.78 | consumed tokens: 52428800.0 | grad norm avg: 1.08 | grad norm last: 1.07 | 
2026-01-01T18:01:21 | step: 6500 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.949085268890485e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.39 | consumed tokens: 53248000.0 | grad norm avg: 1.07 | grad norm last: 1.07 | 
2026-01-01T18:01:36 | step: 6600 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.968289431417361e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.5 | consumed tokens: 54067200.0 | grad norm avg: 1.07 | grad norm last: 0.98 | 
2026-01-01T18:01:52 | step: 6700 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.9829915951704606e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.94 | consumed tokens: 54886400.0 | grad norm avg: 1.04 | grad norm last: 1.05 | 
2026-01-01T18:02:07 | step: 6800 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 4.993161928723566e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.58 | consumed tokens: 55705600.0 | grad norm avg: 1.04 | grad norm last: 1.15 | 
2026-01-01T18:02:23 | step: 6900 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.998780059395358e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.68 | train loss last: 3.09 | consumed tokens: 56524800.0 | grad norm avg: 1.03 | grad norm last: 1.04 | 
2026-01-01T18:02:38 | step: 7000 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999999873689376e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.65 | train loss last: 4.03 | consumed tokens: 57344000.0 | grad norm avg: 1.02 | grad norm last: 0.98 | 
2026-01-01T18:02:53 | step: 7100 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999999509891495e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 4.03 | consumed tokens: 58163200.0 | grad norm avg: 1.01 | grad norm last: 0.99 | 
2026-01-01T18:03:09 | step: 7200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.999998782295734e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.8 | consumed tokens: 58982400.0 | grad norm avg: 1.01 | grad norm last: 1.03 | 
2026-01-01T18:03:24 | step: 7300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999997327104211e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.64 | train loss last: 3.61 | consumed tokens: 59801600.0 | grad norm avg: 1.0 | grad norm last: 0.93 | 
2026-01-01T18:03:40 | step: 7400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9999951443169266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.63 | train loss last: 3.0 | consumed tokens: 60620800.0 | grad norm avg: 0.99 | grad norm last: 1.01 | 
2026-01-01T18:03:55 | step: 7500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9999929615296423e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.67 | train loss last: 3.66 | consumed tokens: 61440000.0 | grad norm avg: 0.98 | grad norm last: 1.01 | 
2026-01-01T18:04:11 | step: 7600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.999989687348716e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 4.31 | consumed tokens: 62259200.0 | grad norm avg: 0.97 | grad norm last: 0.98 | 
2026-01-01T18:04:26 | step: 7700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9999864131677896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.66 | train loss last: 3.41 | consumed tokens: 63078400.0 | grad norm avg: 0.96 | grad norm last: 0.9 | 
2026-01-01T18:04:42 | step: 7800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999982411391102e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.66 | consumed tokens: 63897600.0 | grad norm avg: 0.96 | grad norm last: 0.94 | 
2026-01-01T18:04:57 | step: 7900 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999776820186526e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.44 | consumed tokens: 64716800.0 | grad norm avg: 0.96 | grad norm last: 0.98 | 
2026-01-01T18:05:13 | step: 8000 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.9999725888483226e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.64 | train loss last: 3.42 | consumed tokens: 65536000.0 | grad norm avg: 0.95 | grad norm last: 0.99 | 
2026-01-01T18:05:28 | step: 8100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999967131880112e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.55 | consumed tokens: 66355200.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2026-01-01T18:05:43 | step: 8200 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.99996094731614e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.61 | train loss last: 3.28 | consumed tokens: 67174400.0 | grad norm avg: 0.94 | grad norm last: 0.92 | 
2026-01-01T18:05:59 | step: 8300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999954398954287e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.63 | train loss last: 3.44 | consumed tokens: 67993600.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2026-01-01T18:06:14 | step: 8400 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999947486794554e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.67 | consumed tokens: 68812800.0 | grad norm avg: 0.92 | grad norm last: 0.87 | 
2026-01-01T18:06:30 | step: 8500 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999939847039059e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 4.22 | consumed tokens: 69632000.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2026-01-01T18:06:45 | step: 8600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9999314796878025e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.61 | train loss last: 3.31 | consumed tokens: 70451200.0 | grad norm avg: 0.91 | grad norm last: 0.89 | 
2026-01-01T18:07:01 | step: 8700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999227485386655e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.64 | consumed tokens: 71270400.0 | grad norm avg: 0.9 | grad norm last: 0.97 | 
2026-01-01T18:07:16 | step: 8800 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999913653591648e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.61 | train loss last: 3.67 | consumed tokens: 72089600.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2026-01-01T18:07:31 | step: 8900 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9999038310488686e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.62 | train loss last: 3.75 | consumed tokens: 72908800.0 | grad norm avg: 0.9 | grad norm last: 0.92 | 
2026-01-01T18:07:47 | step: 9000 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.999893644708209e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.6 | train loss last: 4.06 | consumed tokens: 73728000.0 | grad norm avg: 0.9 | grad norm last: 0.88 | 
2026-01-01T18:08:02 | step: 9100 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999883094569668e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.59 | train loss last: 3.05 | consumed tokens: 74547200.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-01T18:08:18 | step: 9200 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.999871816835366e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.77 | consumed tokens: 75366400.0 | grad norm avg: 0.89 | grad norm last: 0.92 | 
2026-01-01T18:08:33 | step: 9300 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.9998601753031835e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.7 | consumed tokens: 76185600.0 | grad norm avg: 0.88 | grad norm last: 0.82 | 
2026-01-01T18:08:49 | step: 9400 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998478061752394e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.59 | train loss last: 4.28 | consumed tokens: 77004800.0 | grad norm avg: 0.89 | grad norm last: 0.87 | 
2026-01-01T18:09:04 | step: 9500 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9998350732494146e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.6 | train loss last: 3.66 | consumed tokens: 77824000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-01T18:09:20 | step: 9600 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.999821612727828e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.7 | consumed tokens: 78643200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2026-01-01T18:09:35 | step: 9700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998077884083614e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.5 | consumed tokens: 79462400.0 | grad norm avg: 0.87 | grad norm last: 0.83 | 
2026-01-01T18:09:51 | step: 9800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999793236493133e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.6 | train loss last: 3.81 | consumed tokens: 80281600.0 | grad norm avg: 0.87 | grad norm last: 0.87 | 
2026-01-01T18:10:06 | step: 9900 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9997786845779046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 2.98 | consumed tokens: 81100800.0 | grad norm avg: 0.88 | grad norm last: 0.85 | 
2026-01-01T18:10:21 | step: 10000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.999763041269034e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.2 | consumed tokens: 81920000.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2026-01-01T18:10:38 | step: 10100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.999747034162283e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.58 | train loss last: 3.5 | consumed tokens: 82739200.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2026-01-01T18:10:54 | step: 10200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.999730663257651e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.59 | train loss last: 3.78 | consumed tokens: 83558400.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2026-01-01T18:11:09 | step: 10300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9997139285551384e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.57 | train loss last: 3.38 | consumed tokens: 84377600.0 | grad norm avg: 0.87 | grad norm last: 0.84 | 
2026-01-01T18:11:25 | step: 10400 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9996964662568644e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.67 | consumed tokens: 85196800.0 | grad norm avg: 0.85 | grad norm last: 0.85 | 
2026-01-01T18:11:40 | step: 10500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.999678276362829e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.36 | consumed tokens: 86016000.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2026-01-01T18:11:56 | step: 10600 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.9996600864687935e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.47 | consumed tokens: 86835200.0 | grad norm avg: 0.86 | grad norm last: 0.88 | 
2026-01-01T18:12:11 | step: 10700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.999640805181116e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.69 | consumed tokens: 87654400.0 | grad norm avg: 0.85 | grad norm last: 0.8 | 
2026-01-01T18:12:27 | step: 10800 | train samples/s: 106.5 | train mfu (16-bit): -1.0 | lr mean: 4.999621523893438e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.45 | consumed tokens: 88473600.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2026-01-01T18:12:42 | step: 10900 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9996011512121186e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.11 | consumed tokens: 89292800.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2026-01-01T18:12:57 | step: 11000 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 4.999580778530799e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.8 | consumed tokens: 90112000.0 | grad norm avg: 0.85 | grad norm last: 0.79 | 
2026-01-01T18:13:13 | step: 11100 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.999559678253718e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.22 | consumed tokens: 90931200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2026-01-01T18:13:28 | step: 11200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.999538214178756e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.57 | train loss last: 3.53 | consumed tokens: 91750400.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2026-01-01T18:13:44 | step: 11300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9995160225080326e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.54 | train loss last: 3.67 | consumed tokens: 92569600.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2026-01-01T18:13:59 | step: 11400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9994934670394287e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.56 | train loss last: 3.3 | consumed tokens: 93388800.0 | grad norm avg: 0.85 | grad norm last: 0.82 | 
2026-01-01T18:14:15 | step: 11500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.999470183975063e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.54 | train loss last: 3.06 | consumed tokens: 94208000.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2026-01-01T18:14:30 | step: 11600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999446537112817e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.73 | consumed tokens: 95027200.0 | grad norm avg: 0.84 | grad norm last: 0.8 | 
2026-01-01T18:14:46 | step: 11700 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9994225264526904e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.73 | consumed tokens: 95846400.0 | grad norm avg: 0.84 | grad norm last: 0.81 | 
2026-01-01T18:15:01 | step: 11800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999397788196802e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.31 | consumed tokens: 96665600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-01T18:15:17 | step: 11900 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.9993723223451525e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.54 | train loss last: 3.52 | consumed tokens: 97484800.0 | grad norm avg: 0.82 | grad norm last: 0.8 | 
2026-01-01T18:15:32 | step: 12000 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999346856493503e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 2.92 | consumed tokens: 98304000.0 | grad norm avg: 0.83 | grad norm last: 0.81 | 
2026-01-01T18:15:48 | step: 12100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999320299248211e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.83 | consumed tokens: 99123200.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2026-01-01T18:16:03 | step: 12200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999293742002919e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.52 | consumed tokens: 99942400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2026-01-01T18:16:18 | step: 12300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999266457161866e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.05 | consumed tokens: 100761600.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2026-01-01T18:16:34 | step: 12400 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9992384447250515e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.55 | train loss last: 3.44 | consumed tokens: 101580800.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2026-01-01T18:16:49 | step: 12500 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.999210432288237e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.53 | consumed tokens: 102400000.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2026-01-01T18:17:05 | step: 12600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.99918132845778e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.53 | train loss last: 3.61 | consumed tokens: 103219200.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-01T18:17:20 | step: 12700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9991522246273234e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.16 | consumed tokens: 104038400.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2026-01-01T18:17:36 | step: 12800 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.999122393201105e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.75 | consumed tokens: 104857600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-01T18:17:51 | step: 12900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.999091834179126e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.25 | consumed tokens: 105676800.0 | grad norm avg: 0.82 | grad norm last: 0.89 | 
2026-01-01T18:18:07 | step: 13000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9990609113592654e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.7 | consumed tokens: 106496000.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2026-01-01T18:18:22 | step: 13100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9990296247415245e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.41 | consumed tokens: 107315200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T18:18:38 | step: 13200 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.998997610528022e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.38 | consumed tokens: 108134400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T18:18:53 | step: 13300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998965232516639e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.47 | consumed tokens: 108953600.0 | grad norm avg: 0.81 | grad norm last: 0.84 | 
2026-01-01T18:19:09 | step: 13400 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9989321269094944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.5 | consumed tokens: 109772800.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2026-01-01T18:19:24 | step: 13500 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.998898657504469e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.14 | consumed tokens: 110592000.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2026-01-01T18:19:40 | step: 13600 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.998864824301563e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.48 | consumed tokens: 111411200.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2026-01-01T18:19:56 | step: 13700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.998830263502896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.31 | consumed tokens: 112230400.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-01T18:20:11 | step: 13800 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.998794975108467e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.05 | consumed tokens: 113049600.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-01T18:20:27 | step: 13900 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.998759686714038e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.45 | consumed tokens: 113868800.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-01T18:20:43 | step: 14000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.998723670723848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.59 | consumed tokens: 114688000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2026-01-01T18:20:58 | step: 14100 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.9986869271378964e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.91 | consumed tokens: 115507200.0 | grad norm avg: 0.81 | grad norm last: 0.76 | 
2026-01-01T18:21:14 | step: 14200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.998649819754064e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.11 | consumed tokens: 116326400.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2026-01-01T18:21:29 | step: 14300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998612348572351e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.69 | consumed tokens: 117145600.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2026-01-01T18:21:45 | step: 14400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9985741497948766e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.81 | consumed tokens: 117964800.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-01T18:22:00 | step: 14500 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.998535223421641e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.53 | consumed tokens: 118784000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:22:16 | step: 14600 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.998496297048405e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.97 | consumed tokens: 119603200.0 | grad norm avg: 0.8 | grad norm last: 0.77 | 
2026-01-01T18:22:31 | step: 14700 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9984566430794075e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.52 | train loss last: 3.34 | consumed tokens: 120422400.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2026-01-01T18:22:47 | step: 14800 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.998416261514649e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.97 | consumed tokens: 121241600.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:23:02 | step: 14900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9983755161520094e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.36 | consumed tokens: 122060800.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:23:18 | step: 15000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.998334406991489e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.05 | consumed tokens: 122880000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:23:34 | step: 15100 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.998292570235208e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.62 | consumed tokens: 123699200.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T18:23:50 | step: 15200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9982503696810454e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.02 | consumed tokens: 124518400.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2026-01-01T18:24:05 | step: 15300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998207441531122e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.58 | consumed tokens: 125337600.0 | grad norm avg: 0.8 | grad norm last: 0.76 | 
2026-01-01T18:24:21 | step: 15400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9981641495833173e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.51 | train loss last: 3.7 | consumed tokens: 126156800.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2026-01-01T18:24:36 | step: 15500 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.998120493837632e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.53 | consumed tokens: 126976000.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2026-01-01T18:24:52 | step: 15600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.998076110496186e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.45 | consumed tokens: 127795200.0 | grad norm avg: 0.8 | grad norm last: 0.8 | 
2026-01-01T18:25:07 | step: 15700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9980313633568585e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.55 | consumed tokens: 128614400.0 | grad norm avg: 0.79 | grad norm last: 1.09 | 
2026-01-01T18:25:23 | step: 15800 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.99798588862177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.5 | consumed tokens: 129433600.0 | grad norm avg: 0.8 | grad norm last: 0.81 | 
2026-01-01T18:25:39 | step: 15900 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9979400500888005e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.45 | consumed tokens: 130252800.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-01T18:25:54 | step: 16000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.99789348396007e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.25 | consumed tokens: 131072000.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-01T18:26:10 | step: 16100 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997846554033458e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.5 | train loss last: 3.67 | consumed tokens: 131891200.0 | grad norm avg: 0.78 | grad norm last: 0.79 | 
2026-01-01T18:26:25 | step: 16200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.997799260308966e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 4.09 | consumed tokens: 132710400.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-01T18:26:41 | step: 16300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9977512389887124e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.25 | consumed tokens: 133529600.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-01T18:26:56 | step: 16400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.997702853870578e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.22 | consumed tokens: 134348800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2026-01-01T18:27:11 | step: 16500 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9976537411566824e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.52 | consumed tokens: 135168000.0 | grad norm avg: 0.79 | grad norm last: 0.75 | 
2026-01-01T18:27:27 | step: 16600 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.997604264644906e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.58 | consumed tokens: 135987200.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-01T18:27:42 | step: 16700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997554424335249e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.25 | consumed tokens: 136806400.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2026-01-01T18:27:58 | step: 16800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.99750385642983e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.34 | consumed tokens: 137625600.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:28:13 | step: 16900 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.99745256092865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.42 | consumed tokens: 138444800.0 | grad norm avg: 0.78 | grad norm last: 0.86 | 
2026-01-01T18:28:29 | step: 17000 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.99740126542747e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.97 | consumed tokens: 139264000.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:28:44 | step: 17100 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.997348878532648e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.78 | consumed tokens: 140083200.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:29:00 | step: 17200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997296491637826e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.0 | consumed tokens: 140902400.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-01T18:29:15 | step: 17300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9972433771472424e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 2.58 | consumed tokens: 141721600.0 | grad norm avg: 0.78 | grad norm last: 0.75 | 
2026-01-01T18:29:31 | step: 17400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9971895350608975e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 2.78 | consumed tokens: 142540800.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:29:46 | step: 17500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.997135329176672e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.12 | consumed tokens: 143360000.0 | grad norm avg: 0.79 | grad norm last: 0.74 | 
2026-01-01T18:30:01 | step: 17600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9970807594945654e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.02 | consumed tokens: 144179200.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-01T18:30:17 | step: 17700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9970258260145783e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 4.41 | consumed tokens: 144998400.0 | grad norm avg: 0.78 | grad norm last: 0.76 | 
2026-01-01T18:30:32 | step: 17800 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.996969801140949e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.39 | consumed tokens: 145817600.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:30:48 | step: 17900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.99691377626732e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.73 | consumed tokens: 146636800.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-01T18:31:03 | step: 18000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.996857023797929e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.49 | train loss last: 3.14 | consumed tokens: 147456000.0 | grad norm avg: 0.78 | grad norm last: 0.73 | 
2026-01-01T18:31:19 | step: 18100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.996799907530658e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.95 | consumed tokens: 148275200.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:31:34 | step: 18200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.996742063667625e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 2.81 | consumed tokens: 149094400.0 | grad norm avg: 0.78 | grad norm last: 0.75 | 
2026-01-01T18:31:50 | step: 18300 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.996683856006712e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.56 | consumed tokens: 149913600.0 | grad norm avg: 0.8 | grad norm last: 0.73 | 
2026-01-01T18:32:05 | step: 18400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.996624920750037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.38 | consumed tokens: 150732800.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-01T18:32:21 | step: 18500 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.996565621695481e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.48 | consumed tokens: 151552000.0 | grad norm avg: 0.78 | grad norm last: 0.75 | 
2026-01-01T18:32:36 | step: 18600 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 4.996505595045164e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.38 | consumed tokens: 152371200.0 | grad norm avg: 0.78 | grad norm last: 0.74 | 
2026-01-01T18:32:52 | step: 18700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.9964452045969665e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.47 | train loss last: 3.33 | consumed tokens: 153190400.0 | grad norm avg: 0.77 | grad norm last: 0.8 | 
2026-01-01T18:33:07 | step: 18800 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.996384450350888e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.48 | train loss last: 3.38 | consumed tokens: 154009600.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:33:22 | step: 18900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.996322968509048e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.53 | consumed tokens: 154828800.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:33:38 | step: 19000 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.996261122869328e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.25 | consumed tokens: 155648000.0 | grad norm avg: 0.78 | grad norm last: 0.82 | 
2026-01-01T18:33:53 | step: 19100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9961989134317264e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.47 | consumed tokens: 156467200.0 | grad norm avg: 0.78 | grad norm last: 0.73 | 
2026-01-01T18:34:09 | step: 19200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.996135976398364e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.41 | consumed tokens: 157286400.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:34:24 | step: 19300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9960723117692396e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.64 | consumed tokens: 158105600.0 | grad norm avg: 0.78 | grad norm last: 0.81 | 
2026-01-01T18:34:40 | step: 19400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.996008283342235e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.55 | consumed tokens: 158924800.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-01T18:34:55 | step: 19500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.995943891117349e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.97 | consumed tokens: 159744000.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-01T18:35:11 | step: 19600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.995879135094583e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.28 | consumed tokens: 160563200.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:35:26 | step: 19700 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.995813287678175e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.05 | consumed tokens: 161382400.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:35:42 | step: 19800 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.995747440261766e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.28 | consumed tokens: 162201600.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-01T18:35:57 | step: 19900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9956808652495965e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.45 | train loss last: 3.19 | consumed tokens: 163020800.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-01T18:36:13 | step: 20000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.995613926439546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.69 | consumed tokens: 163840000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:36:30 | step: 20100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.995546260033734e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.14 | consumed tokens: 164659200.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:36:45 | step: 20200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9954782298300415e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.73 | consumed tokens: 165478400.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:37:01 | step: 20300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9954094720305875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.12 | consumed tokens: 166297600.0 | grad norm avg: 0.78 | grad norm last: 0.74 | 
2026-01-01T18:37:16 | step: 20400 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.995340350433253e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.56 | consumed tokens: 167116800.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:37:31 | step: 20500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.995270865038037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.34 | consumed tokens: 167936000.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:37:47 | step: 20600 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9952006520470604e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.25 | consumed tokens: 168755200.0 | grad norm avg: 0.77 | grad norm last: 0.79 | 
2026-01-01T18:38:02 | step: 20700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.995130075258203e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.46 | train loss last: 3.19 | consumed tokens: 169574400.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:38:18 | step: 20800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.995058770873584e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.55 | consumed tokens: 170393600.0 | grad norm avg: 0.77 | grad norm last: 0.8 | 
2026-01-01T18:38:34 | step: 20900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.994987102691084e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.67 | consumed tokens: 171212800.0 | grad norm avg: 0.77 | grad norm last: 0.82 | 
2026-01-01T18:38:49 | step: 21000 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.994914706912823e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.33 | consumed tokens: 172032000.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:39:05 | step: 21100 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.994841947336681e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.64 | consumed tokens: 172851200.0 | grad norm avg: 0.77 | grad norm last: 0.73 | 
2026-01-01T18:39:20 | step: 21200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9947688239626586e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.53 | consumed tokens: 173670400.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:39:36 | step: 21300 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.994694972992875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 2.97 | consumed tokens: 174489600.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:39:51 | step: 21400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.99462075822521e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.12 | consumed tokens: 175308800.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:40:07 | step: 21500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.994545815861784e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.38 | consumed tokens: 176128000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:40:22 | step: 21600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.994470509700477e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.31 | consumed tokens: 176947200.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:40:38 | step: 21700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9943948397412896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.62 | consumed tokens: 177766400.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:40:53 | step: 21800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.994318442186341e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.47 | consumed tokens: 178585600.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-01T18:41:09 | step: 21900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.994241680833511e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 179404800.0 | grad norm avg: 0.77 | grad norm last: 0.81 | 
2026-01-01T18:41:24 | step: 22000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.99416419188492e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.27 | consumed tokens: 180224000.0 | grad norm avg: 0.76 | grad norm last: 0.82 | 
2026-01-01T18:41:40 | step: 22100 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.994086339138448e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 2.98 | consumed tokens: 181043200.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T18:41:55 | step: 22200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.994007758796215e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.47 | consumed tokens: 181862400.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-01T18:42:11 | step: 22300 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.993928814656101e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 2.84 | consumed tokens: 182681600.0 | grad norm avg: 0.77 | grad norm last: 0.86 | 
2026-01-01T18:42:27 | step: 22400 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9938495067181066e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.45 | consumed tokens: 183500800.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:42:42 | step: 22500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9937694711843506e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.17 | consumed tokens: 184320000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:42:57 | step: 22600 | train samples/s: 106.4 | train mfu (16-bit): -1.0 | lr mean: 4.993689071852714e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.05 | consumed tokens: 185139200.0 | grad norm avg: 0.77 | grad norm last: 0.7 | 
2026-01-01T18:43:13 | step: 22700 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.993607944925316e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.56 | consumed tokens: 185958400.0 | grad norm avg: 0.76 | grad norm last: 0.8 | 
2026-01-01T18:43:28 | step: 22800 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.993526454200037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.05 | consumed tokens: 186777600.0 | grad norm avg: 0.76 | grad norm last: 0.82 | 
2026-01-01T18:43:44 | step: 22900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.993444599676877e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.88 | consumed tokens: 187596800.0 | grad norm avg: 0.76 | grad norm last: 0.76 | 
2026-01-01T18:43:59 | step: 23000 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.993362017557956e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.44 | train loss last: 3.44 | consumed tokens: 188416000.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:44:15 | step: 23100 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.9932790716411546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.72 | consumed tokens: 189235200.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:44:30 | step: 23200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9931953981285915e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.25 | consumed tokens: 190054400.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:44:46 | step: 23300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9931113608181477e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.09 | consumed tokens: 190873600.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:45:01 | step: 23400 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9930265959119424e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.06 | consumed tokens: 191692800.0 | grad norm avg: 0.77 | grad norm last: 0.71 | 
2026-01-01T18:45:17 | step: 23500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9929414672078565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.38 | consumed tokens: 192512000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:45:32 | step: 23600 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.99285597470589e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.25 | consumed tokens: 193331200.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:45:48 | step: 23700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.992769754608162e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 194150400.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:46:03 | step: 23800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.992683170712553e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.43 | train loss last: 3.53 | consumed tokens: 194969600.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:46:19 | step: 23900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.992595859221183e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 2.83 | consumed tokens: 195788800.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-01T18:46:34 | step: 24000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.992508183931932e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.28 | consumed tokens: 196608000.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:46:50 | step: 24100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9924201448448e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.45 | consumed tokens: 197427200.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:47:05 | step: 24200 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.992331378161907e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 2.59 | consumed tokens: 198246400.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:47:20 | step: 24300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.992241883883253e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 4.06 | consumed tokens: 199065600.0 | grad norm avg: 0.76 | grad norm last: 0.81 | 
2026-01-01T18:47:36 | step: 24400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.992152389604598e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.44 | consumed tokens: 199884800.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-01T18:47:51 | step: 24500 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9920621677301824e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.02 | consumed tokens: 200704000.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:48:07 | step: 24600 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.991971218260005e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 201523200.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:48:22 | step: 24700 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.991879904991947e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 202342400.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T18:48:38 | step: 24800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9917882279260084e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.12 | consumed tokens: 203161600.0 | grad norm avg: 0.76 | grad norm last: 0.79 | 
2026-01-01T18:48:53 | step: 24900 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.991695823264308e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.62 | consumed tokens: 203980800.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:49:09 | step: 25000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9916030548047274e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.34 | consumed tokens: 204800000.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:49:26 | step: 25100 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.991509558749385e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.8 | consumed tokens: 205619200.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:49:41 | step: 25200 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.991415698896162e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.03 | consumed tokens: 206438400.0 | grad norm avg: 0.76 | grad norm last: 0.78 | 
2026-01-01T18:49:57 | step: 25300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.991321111447178e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 207257600.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:50:12 | step: 25400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9912265239981934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.97 | consumed tokens: 208076800.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:50:28 | step: 25500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.991130845155567e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.14 | consumed tokens: 208896000.0 | grad norm avg: 0.75 | grad norm last: 0.79 | 
2026-01-01T18:50:43 | step: 25600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.99103480251506e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 209715200.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:50:58 | step: 25700 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.990938396076672e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.64 | consumed tokens: 210534400.0 | grad norm avg: 0.76 | grad norm last: 0.76 | 
2026-01-01T18:51:14 | step: 25800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.990841625840403e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.05 | consumed tokens: 211353600.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-01T18:51:29 | step: 25900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.990744128008373e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 212172800.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:51:45 | step: 26000 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9906459025805816e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.42 | train loss last: 3.41 | consumed tokens: 212992000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T18:52:00 | step: 26100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.99054767715279e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 2.95 | consumed tokens: 213811200.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T18:52:16 | step: 26200 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.9904483603313565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.27 | consumed tokens: 214630400.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T18:52:32 | step: 26300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.990349043509923e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.78 | consumed tokens: 215449600.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:52:47 | step: 26400 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.990248999092728e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.64 | consumed tokens: 216268800.0 | grad norm avg: 0.75 | grad norm last: 0.7 | 
2026-01-01T18:53:03 | step: 26500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9901482270797715e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.33 | consumed tokens: 217088000.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:53:18 | step: 26600 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.990047091268934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.41 | train loss last: 3.33 | consumed tokens: 217907200.0 | grad norm avg: 0.76 | grad norm last: 0.72 | 
2026-01-01T18:53:34 | step: 26700 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9899455916602165e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 218726400.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:53:49 | step: 26800 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.989843364455737e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 2.53 | consumed tokens: 219545600.0 | grad norm avg: 0.77 | grad norm last: 0.76 | 
2026-01-01T18:54:04 | step: 26900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.989740773453377e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.69 | consumed tokens: 220364800.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:54:20 | step: 27000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9896378186531365e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.5 | consumed tokens: 221184000.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:54:35 | step: 27100 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9895341362571344e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.34 | consumed tokens: 222003200.0 | grad norm avg: 0.76 | grad norm last: 0.75 | 
2026-01-01T18:54:51 | step: 27200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.989429726265371e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.08 | consumed tokens: 222822400.0 | grad norm avg: 0.76 | grad norm last: 0.74 | 
2026-01-01T18:55:06 | step: 27300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9893249524757266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 223641600.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T18:55:22 | step: 27400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9892198148882017e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 224460800.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T18:55:38 | step: 27500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.989114313502796e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.19 | consumed tokens: 225280000.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T18:55:53 | step: 27600 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.989008084521629e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 226099200.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T18:56:09 | step: 27700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9889011279447004e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 4.25 | consumed tokens: 226918400.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T18:56:25 | step: 27800 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.988793807569891e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 227737600.0 | grad norm avg: 0.75 | grad norm last: 0.85 | 
2026-01-01T18:56:40 | step: 27900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.988686123397201e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.56 | consumed tokens: 228556800.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T18:56:56 | step: 28000 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.98857771162875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.39 | consumed tokens: 229376000.0 | grad norm avg: 0.76 | grad norm last: 0.76 | 
2026-01-01T18:57:11 | step: 28100 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.988468936062418e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 2.97 | consumed tokens: 230195200.0 | grad norm avg: 0.76 | grad norm last: 0.72 | 
2026-01-01T18:57:26 | step: 28200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.988359796698205e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 231014400.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T18:57:42 | step: 28300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.988249929738231e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.25 | consumed tokens: 231833600.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:57:57 | step: 28400 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9881393351824954e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.86 | consumed tokens: 232652800.0 | grad norm avg: 0.76 | grad norm last: 0.8 | 
2026-01-01T18:58:13 | step: 28500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.98802874062676e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.12 | consumed tokens: 233472000.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T18:58:28 | step: 28600 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.987917054677382e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.53 | consumed tokens: 234291200.0 | grad norm avg: 0.75 | grad norm last: 0.8 | 
2026-01-01T18:58:44 | step: 28700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9878053687280044e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.75 | consumed tokens: 235110400.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:59:00 | step: 28800 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.987692955182865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.83 | consumed tokens: 235929600.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:59:15 | step: 28900 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.987579814041965e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.44 | consumed tokens: 236748800.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T18:59:31 | step: 29000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9874663091031834e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 2.78 | consumed tokens: 237568000.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T18:59:46 | step: 29100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9873524403665215e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.4 | train loss last: 3.11 | consumed tokens: 238387200.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T19:00:02 | step: 29200 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.987238207831979e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.02 | consumed tokens: 239206400.0 | grad norm avg: 0.75 | grad norm last: 0.85 | 
2026-01-01T19:00:17 | step: 29300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.987122883903794e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.5 | consumed tokens: 240025600.0 | grad norm avg: 0.75 | grad norm last: 0.76 | 
2026-01-01T19:00:33 | step: 29400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.987007559975609e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.28 | consumed tokens: 240844800.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T19:00:48 | step: 29500 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.986891508451663e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 241664000.0 | grad norm avg: 0.75 | grad norm last: 0.79 | 
2026-01-01T19:01:04 | step: 29600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.986775093129836e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.19 | consumed tokens: 242483200.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:01:19 | step: 29700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.986657950212248e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.28 | consumed tokens: 243302400.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:01:35 | step: 29800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9865404434967786e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.7 | consumed tokens: 244121600.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:01:50 | step: 29900 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.986422209185548e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 3.34 | consumed tokens: 244940800.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:02:05 | step: 30000 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.986303611076437e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 2.81 | consumed tokens: 245760000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:02:23 | step: 30100 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.986184649169445e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.48 | consumed tokens: 246579200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:02:38 | step: 30200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.986064959666692e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.45 | consumed tokens: 247398400.0 | grad norm avg: 0.75 | grad norm last: 0.79 | 
2026-01-01T19:02:54 | step: 30300 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.985944906366058e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.95 | consumed tokens: 248217600.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:03:09 | step: 30400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.985824125469662e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 2.97 | consumed tokens: 249036800.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:03:25 | step: 30500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.985702980775386e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.7 | consumed tokens: 249856000.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T19:03:40 | step: 30600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.985581472283229e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.55 | consumed tokens: 250675200.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-01T19:03:56 | step: 30700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.985459236195311e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.06 | consumed tokens: 251494400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:04:11 | step: 30800 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.985336272511631e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 4.06 | consumed tokens: 252313600.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:04:27 | step: 30900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9852133088279516e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.66 | consumed tokens: 253132800.0 | grad norm avg: 0.75 | grad norm last: 0.8 | 
2026-01-01T19:04:42 | step: 31000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9850896175485104e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.38 | train loss last: 2.77 | consumed tokens: 253952000.0 | grad norm avg: 0.75 | grad norm last: 0.8 | 
2026-01-01T19:04:58 | step: 31100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.984965198673308e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.0 | consumed tokens: 254771200.0 | grad norm avg: 0.75 | grad norm last: 0.83 | 
2026-01-01T19:05:13 | step: 31200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9848404160002246e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 2.89 | consumed tokens: 255590400.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:05:29 | step: 31300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.984715269529261e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.73 | consumed tokens: 256409600.0 | grad norm avg: 0.75 | grad norm last: 0.81 | 
2026-01-01T19:05:44 | step: 31400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.984589395462535e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.33 | consumed tokens: 257228800.0 | grad norm avg: 0.74 | grad norm last: 0.8 | 
2026-01-01T19:06:00 | step: 31500 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.984463157597929e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 258048000.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:06:15 | step: 31600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.984336192137562e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.56 | consumed tokens: 258867200.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-01T19:06:31 | step: 31700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9842088628793135e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.84 | consumed tokens: 259686400.0 | grad norm avg: 0.77 | grad norm last: 0.78 | 
2026-01-01T19:06:46 | step: 31800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9840811698231846e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.28 | consumed tokens: 260505600.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T19:07:02 | step: 31900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.983952749171294e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.39 | train loss last: 3.73 | consumed tokens: 261324800.0 | grad norm avg: 0.74 | grad norm last: 0.85 | 
2026-01-01T19:07:18 | step: 32000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.983823964721523e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 262144000.0 | grad norm avg: 0.75 | grad norm last: 0.68 | 
2026-01-01T19:07:33 | step: 32100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.983694452675991e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.48 | consumed tokens: 262963200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:07:48 | step: 32200 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9835645768325776e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.3 | consumed tokens: 263782400.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:08:04 | step: 32300 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.983434337191284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 2.92 | consumed tokens: 264601600.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-01T19:08:19 | step: 32400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9833033699542284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 265420800.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:08:35 | step: 32500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.983171675121412e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 2.98 | consumed tokens: 266240000.0 | grad norm avg: 0.75 | grad norm last: 0.78 | 
2026-01-01T19:08:50 | step: 32600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.983039980288595e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.36 | consumed tokens: 267059200.0 | grad norm avg: 0.75 | grad norm last: 0.72 | 
2026-01-01T19:09:06 | step: 32700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.982907194062136e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.05 | consumed tokens: 267878400.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:09:22 | step: 32800 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.982774407835677e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.37 | train loss last: 3.59 | consumed tokens: 268697600.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-01T19:09:37 | step: 32900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.982640894013457e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.7 | consumed tokens: 269516800.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:09:52 | step: 33000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.982507016393356e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 270336000.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:10:08 | step: 33100 | train samples/s: 106.2 | train mfu (16-bit): -1.0 | lr mean: 4.9823724111774936e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.23 | consumed tokens: 271155200.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:10:23 | step: 33200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9822374421637505e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 271974400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:10:39 | step: 33300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.982101745554246e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.83 | consumed tokens: 272793600.0 | grad norm avg: 0.74 | grad norm last: 0.75 | 
2026-01-01T19:10:54 | step: 33400 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.981965685146861e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.75 | consumed tokens: 273612800.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:11:10 | step: 33500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.981829260941595e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 4.06 | consumed tokens: 274432000.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:11:25 | step: 33600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9816921091405675e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.66 | consumed tokens: 275251200.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:11:41 | step: 33700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9815545935416594e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.69 | consumed tokens: 276070400.0 | grad norm avg: 0.74 | grad norm last: 0.83 | 
2026-01-01T19:11:56 | step: 33800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.98141635034699e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.31 | consumed tokens: 276889600.0 | grad norm avg: 0.73 | grad norm last: 0.68 | 
2026-01-01T19:12:12 | step: 33900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.98127774335444e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 277708800.0 | grad norm avg: 0.74 | grad norm last: 0.79 | 
2026-01-01T19:12:27 | step: 34000 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.981138408766128e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 4.12 | consumed tokens: 278528000.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:12:43 | step: 34100 | train samples/s: 103.3 | train mfu (16-bit): -1.0 | lr mean: 4.980998710379936e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 4.09 | consumed tokens: 279347200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:12:59 | step: 34200 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.980858648195863e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.09 | consumed tokens: 280166400.0 | grad norm avg: 0.74 | grad norm last: 0.75 | 
2026-01-01T19:13:14 | step: 34300 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.980717858416028e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 280985600.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:13:29 | step: 34400 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.980576704838313e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.3 | consumed tokens: 281804800.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:13:45 | step: 34500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.980435187462717e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.25 | consumed tokens: 282624000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-01T19:14:00 | step: 34600 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.98029294249136e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.48 | consumed tokens: 283443200.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:14:16 | step: 34700 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.980150333722122e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.11 | consumed tokens: 284262400.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:14:31 | step: 34800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9800069973571226e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.19 | consumed tokens: 285081600.0 | grad norm avg: 0.74 | grad norm last: 0.78 | 
2026-01-01T19:14:47 | step: 34900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9798632971942425e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.42 | consumed tokens: 285900800.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:15:02 | step: 35000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.979718869435601e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.56 | consumed tokens: 286720000.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-01T19:15:19 | step: 35100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.979574077879079e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.25 | consumed tokens: 287539200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:15:35 | step: 35200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.979428922524676e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 2.94 | consumed tokens: 288358400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:15:50 | step: 35300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9792830395745113e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.67 | consumed tokens: 289177600.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:16:06 | step: 35400 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.979136792826466e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.27 | consumed tokens: 289996800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:16:21 | step: 35500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.97898981848266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 290816000.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:16:38 | step: 35600 | train samples/s: 98.8 | train mfu (16-bit): -1.0 | lr mean: 4.9788424803409725e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.8 | consumed tokens: 291635200.0 | grad norm avg: 0.74 | grad norm last: 0.75 | 
2026-01-01T19:16:53 | step: 35700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9786947784014046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.14 | consumed tokens: 292454400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:17:09 | step: 35800 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.978546348866075e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.2 | consumed tokens: 293273600.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:17:24 | step: 35900 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.9783971917349845e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 294092800.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:17:40 | step: 36000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.978248034603894e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.33 | consumed tokens: 294912000.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:17:55 | step: 36100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9780981498770416e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.7 | consumed tokens: 295731200.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-01T19:18:11 | step: 36200 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.977947537554428e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.33 | consumed tokens: 296550400.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:18:26 | step: 36300 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.977796561433934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.5 | consumed tokens: 297369600.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:18:42 | step: 36400 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9776452215155587e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.28 | consumed tokens: 298188800.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:18:57 | step: 36500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.977493154001422e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.52 | consumed tokens: 299008000.0 | grad norm avg: 0.73 | grad norm last: 0.79 | 
2026-01-01T19:19:13 | step: 36600 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.977340722689405e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.52 | consumed tokens: 299827200.0 | grad norm avg: 0.74 | grad norm last: 0.69 | 
2026-01-01T19:19:29 | step: 36700 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.977187927579507e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.94 | consumed tokens: 300646400.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-01T19:19:44 | step: 36800 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.977034404873848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 301465600.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:20:00 | step: 36900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.976880154572427e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.44 | consumed tokens: 302284800.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-01T19:20:15 | step: 37000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9767259042710066e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.25 | consumed tokens: 303104000.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:20:31 | step: 37100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.976570562575944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.42 | consumed tokens: 303923200.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:20:46 | step: 37200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.976415220880881e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.47 | consumed tokens: 304742400.0 | grad norm avg: 0.73 | grad norm last: 0.76 | 
2026-01-01T19:21:02 | step: 37300 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.976259151590057e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.16 | consumed tokens: 305561600.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-01T19:21:17 | step: 37400 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.976102718501352e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.56 | consumed tokens: 306380800.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:21:32 | step: 37500 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9759455578168854e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 4.34 | consumed tokens: 307200000.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:21:48 | step: 37600 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9757880333345383e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.35 | train loss last: 3.3 | consumed tokens: 308019200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:22:04 | step: 37700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.97562978125643e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 4.16 | consumed tokens: 308838400.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:22:19 | step: 37800 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9754711653804407e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.11 | consumed tokens: 309657600.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-01T19:22:35 | step: 37900 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.97531182190869e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.92 | consumed tokens: 310476800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:22:50 | step: 38000 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.9751524784369394e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.66 | consumed tokens: 311296000.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:23:06 | step: 38100 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.974992043571547e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 4.03 | consumed tokens: 312115200.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:23:21 | step: 38200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.974831608706154e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.34 | consumed tokens: 312934400.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:23:37 | step: 38300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.974670446245e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.31 | consumed tokens: 313753600.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:23:52 | step: 38400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.974508556188084e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 2.94 | consumed tokens: 314572800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:24:08 | step: 38500 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.974346302333288e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.5 | consumed tokens: 315392000.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-01T19:24:24 | step: 38600 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.974183684680611e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.67 | consumed tokens: 316211200.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:24:39 | step: 38700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9740203394321725e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 2.97 | consumed tokens: 317030400.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:24:54 | step: 38800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.9738566303858534e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.61 | consumed tokens: 317849600.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:25:10 | step: 38900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9736925575416535e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.47 | consumed tokens: 318668800.0 | grad norm avg: 0.73 | grad norm last: 0.68 | 
2026-01-01T19:25:25 | step: 39000 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.973527757101692e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.69 | consumed tokens: 319488000.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:25:41 | step: 39100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.97336259286385e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.8 | consumed tokens: 320307200.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-01T19:25:57 | step: 39200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.973196701030247e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.12 | consumed tokens: 321126400.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-01T19:26:12 | step: 39300 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.973030445398763e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.38 | consumed tokens: 321945600.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:26:28 | step: 39400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.972863462171517e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.86 | consumed tokens: 322764800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:26:43 | step: 39500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.972696115146391e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.09 | consumed tokens: 323584000.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:26:59 | step: 39600 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.972528404323384e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 2.77 | consumed tokens: 324403200.0 | grad norm avg: 0.74 | grad norm last: 1.55 | 
2026-01-01T19:27:14 | step: 39700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.972359965904616e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.19 | consumed tokens: 325222400.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:27:30 | step: 39800 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.972191163687967e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.89 | consumed tokens: 326041600.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:27:45 | step: 39900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.972021997673437e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.28 | consumed tokens: 326860800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:28:01 | step: 40000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.971852104063146e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.36 | train loss last: 3.05 | consumed tokens: 327680000.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:28:18 | step: 40100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.971681482857093e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.41 | consumed tokens: 328499200.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:28:33 | step: 40200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.97151049785316e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.16 | consumed tokens: 329318400.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:28:49 | step: 40300 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.971339149051346e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.17 | consumed tokens: 330137600.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:29:04 | step: 40400 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.971167436451651e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.81 | consumed tokens: 330956800.0 | grad norm avg: 0.73 | grad norm last: 0.79 | 
2026-01-01T19:29:20 | step: 40500 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.970994996256195e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.53 | consumed tokens: 331776000.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:29:36 | step: 40600 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.9708218284649774e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 2.84 | consumed tokens: 332595200.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:29:51 | step: 40700 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.97064866067376e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.53 | consumed tokens: 333414400.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:30:07 | step: 40800 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.9704744014889e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.09 | consumed tokens: 334233600.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:30:23 | step: 40900 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9703001423040405e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 335052800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:30:38 | step: 41000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9701251555234194e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.02 | consumed tokens: 335872000.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:30:54 | step: 41100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.969949441147037e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.81 | consumed tokens: 336691200.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:31:09 | step: 41200 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.969773726770654e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.55 | consumed tokens: 337510400.0 | grad norm avg: 0.72 | grad norm last: 0.67 | 
2026-01-01T19:31:25 | step: 41300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.96959692100063e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.03 | consumed tokens: 338329600.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:31:40 | step: 41400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.969420115230605e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.03 | consumed tokens: 339148800.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:31:56 | step: 41500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.969242581864819e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.78 | consumed tokens: 339968000.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:32:11 | step: 41600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9690643209032714e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.06 | consumed tokens: 340787200.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:32:27 | step: 41700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.968885696143843e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.75 | consumed tokens: 341606400.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:32:42 | step: 41800 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.968706707586534e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.97 | consumed tokens: 342425600.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:32:58 | step: 41900 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.968526991433464e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.47 | consumed tokens: 343244800.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:33:14 | step: 42000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.968346911482513e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 2.83 | consumed tokens: 344064000.0 | grad norm avg: 0.73 | grad norm last: 0.76 | 
2026-01-01T19:33:29 | step: 42100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.968166467733681e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.06 | consumed tokens: 344883200.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:33:44 | step: 42200 | train samples/s: 106.1 | train mfu (16-bit): -1.0 | lr mean: 4.967985296389088e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.06 | consumed tokens: 345702400.0 | grad norm avg: 0.73 | grad norm last: 0.67 | 
2026-01-01T19:34:00 | step: 42300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.967803761246614e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.34 | consumed tokens: 346521600.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:34:15 | step: 42400 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.967621498508379e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.27 | consumed tokens: 347340800.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:34:31 | step: 42500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.967438871972263e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.61 | consumed tokens: 348160000.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:34:46 | step: 42600 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.967255881638266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.91 | consumed tokens: 348979200.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:35:02 | step: 42700 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.967072163708508e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.78 | consumed tokens: 349798400.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:35:18 | step: 42800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.966888081980869e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 4.0 | consumed tokens: 350617600.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:35:33 | step: 42900 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.966703272657469e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.8 | consumed tokens: 351436800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:35:48 | step: 43000 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.966518099536188e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.67 | consumed tokens: 352256000.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:36:04 | step: 43100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9663321988191456e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.17 | consumed tokens: 353075200.0 | grad norm avg: 0.73 | grad norm last: 0.67 | 
2026-01-01T19:36:20 | step: 43200 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.9661459343042225e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.66 | consumed tokens: 353894400.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:36:35 | step: 43300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9659593059914187e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.45 | consumed tokens: 354713600.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:36:51 | step: 43400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9657719500828534e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.73 | consumed tokens: 355532800.0 | grad norm avg: 0.73 | grad norm last: 0.81 | 
2026-01-01T19:37:06 | step: 43500 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9655842303764075e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 3.39 | consumed tokens: 356352000.0 | grad norm avg: 0.72 | grad norm last: 0.81 | 
2026-01-01T19:37:22 | step: 43600 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.965396146872081e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.59 | consumed tokens: 357171200.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:37:37 | step: 43700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.965207335771993e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.64 | consumed tokens: 357990400.0 | grad norm avg: 0.73 | grad norm last: 0.71 | 
2026-01-01T19:37:53 | step: 43800 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.965018160874024e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.61 | consumed tokens: 358809600.0 | grad norm avg: 0.73 | grad norm last: 0.77 | 
2026-01-01T19:38:08 | step: 43900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.964828258380294e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.33 | train loss last: 3.0 | consumed tokens: 359628800.0 | grad norm avg: 0.74 | grad norm last: 0.73 | 
2026-01-01T19:38:24 | step: 44000 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.964637992088683e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.32 | train loss last: 4.16 | consumed tokens: 360448000.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:38:39 | step: 44100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9644469982013106e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.59 | consumed tokens: 361267200.0 | grad norm avg: 0.74 | grad norm last: 0.78 | 
2026-01-01T19:38:55 | step: 44200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.964256004313938e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.0 | consumed tokens: 362086400.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:39:10 | step: 44300 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.964063919032924e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.34 | consumed tokens: 362905600.0 | grad norm avg: 0.73 | grad norm last: 0.79 | 
2026-01-01T19:39:26 | step: 44400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9638718337519094e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.5 | consumed tokens: 363724800.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:39:42 | step: 44500 | train samples/s: 103.6 | train mfu (16-bit): -1.0 | lr mean: 4.963678657077253e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.0 | consumed tokens: 364544000.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:39:57 | step: 44600 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.963485480402596e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.27 | consumed tokens: 365363200.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:40:13 | step: 44700 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.963291576132178e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.22 | consumed tokens: 366182400.0 | grad norm avg: 0.72 | grad norm last: 0.67 | 
2026-01-01T19:40:28 | step: 44800 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9630973080638796e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.34 | train loss last: 3.03 | consumed tokens: 367001600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:40:44 | step: 44900 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9629023123998195e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.08 | consumed tokens: 367820800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:40:59 | step: 45000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.962706952937879e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.88 | consumed tokens: 368640000.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:41:16 | step: 45100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9625108658801764e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.48 | consumed tokens: 369459200.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T19:41:32 | step: 45200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.962314778822474e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.28 | consumed tokens: 370278400.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:41:47 | step: 45300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.96211760037113e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.55 | consumed tokens: 371097600.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:42:03 | step: 45400 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9619204219197854e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.17 | consumed tokens: 371916800.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:42:18 | step: 45500 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.961722152074799e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.89 | consumed tokens: 372736000.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:42:34 | step: 45600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9615238822298124e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.41 | consumed tokens: 373555200.0 | grad norm avg: 0.73 | grad norm last: 0.67 | 
2026-01-01T19:42:49 | step: 45700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9613248847890645e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.19 | consumed tokens: 374374400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:43:05 | step: 45800 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.961125523550436e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.86 | consumed tokens: 375193600.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:43:21 | step: 45900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.960925434716046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.25 | consumed tokens: 376012800.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:43:36 | step: 46000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.960724982083775e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.28 | consumed tokens: 376832000.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T19:43:52 | step: 46100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.960523801855743e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.97 | consumed tokens: 377651200.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T19:44:07 | step: 46200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.960322621627711e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.27 | consumed tokens: 378470400.0 | grad norm avg: 0.73 | grad norm last: 0.71 | 
2026-01-01T19:44:23 | step: 46300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9601203500060365e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.23 | consumed tokens: 379289600.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:44:38 | step: 46400 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.959918078384362e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.75 | consumed tokens: 380108800.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:44:54 | step: 46500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.959714715369046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.78 | consumed tokens: 380928000.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:45:09 | step: 46600 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.959511352353729e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.34 | consumed tokens: 381747200.0 | grad norm avg: 0.73 | grad norm last: 0.71 | 
2026-01-01T19:45:25 | step: 46700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9593072617426515e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.84 | consumed tokens: 382566400.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:45:40 | step: 46800 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.959102807333693e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.59 | consumed tokens: 383385600.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:45:56 | step: 46900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.958897625328973e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.3 | consumed tokens: 384204800.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:46:11 | step: 47000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.958692079526372e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 3.69 | consumed tokens: 385024000.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:46:27 | step: 47100 | train samples/s: 102.6 | train mfu (16-bit): -1.0 | lr mean: 4.958486169925891e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.25 | consumed tokens: 385843200.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:46:43 | step: 47200 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.958279532729648e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.72 | consumed tokens: 386662400.0 | grad norm avg: 0.72 | grad norm last: 0.76 | 
2026-01-01T19:46:58 | step: 47300 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.958072167937644e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.45 | consumed tokens: 387481600.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:47:14 | step: 47400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9578648031456396e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.14 | consumed tokens: 388300800.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T19:47:29 | step: 47500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.957656710757874e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.19 | consumed tokens: 389120000.0 | grad norm avg: 0.73 | grad norm last: 0.76 | 
2026-01-01T19:47:45 | step: 47600 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.957447890774347e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.91 | consumed tokens: 389939200.0 | grad norm avg: 0.72 | grad norm last: 0.76 | 
2026-01-01T19:48:00 | step: 47700 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.957238706992939e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.38 | consumed tokens: 390758400.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:48:16 | step: 47800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9570291594136506e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.67 | consumed tokens: 391577600.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:48:31 | step: 47900 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.956818884238601e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.14 | consumed tokens: 392396800.0 | grad norm avg: 0.73 | grad norm last: 0.73 | 
2026-01-01T19:48:47 | step: 48000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.95660824526567e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.94 | consumed tokens: 393216000.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:49:02 | step: 48100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.956397242494859e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.52 | consumed tokens: 394035200.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T19:49:18 | step: 48200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.956185512128286e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.39 | consumed tokens: 394854400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:49:34 | step: 48300 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.9559734179638326e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.3 | consumed tokens: 395673600.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:49:49 | step: 48400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.955760596203618e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.3 | consumed tokens: 396492800.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:50:05 | step: 48500 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.955547410645522e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.61 | consumed tokens: 397312000.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:50:20 | step: 48600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.955333861289546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.08 | consumed tokens: 398131200.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-01T19:50:36 | step: 48700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.955119584337808e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.84 | consumed tokens: 398950400.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:50:51 | step: 48800 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.95490494358819e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.3 | consumed tokens: 399769600.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:51:07 | step: 48900 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.95468957524281e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.11 | consumed tokens: 400588800.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:51:22 | step: 49000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9544738430995494e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.52 | consumed tokens: 401408000.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:51:38 | step: 49100 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.954257747158408e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.94 | consumed tokens: 402227200.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T19:51:53 | step: 49200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9540409236215055e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.55 | consumed tokens: 403046400.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:52:09 | step: 49300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.953823736286722e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.88 | consumed tokens: 403865600.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-01T19:52:25 | step: 49400 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.953605821356177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.72 | consumed tokens: 404684800.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:52:40 | step: 49500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.953387542627752e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.31 | train loss last: 2.91 | consumed tokens: 405504000.0 | grad norm avg: 0.73 | grad norm last: 0.74 | 
2026-01-01T19:52:56 | step: 49600 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9531689001014456e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.55 | consumed tokens: 406323200.0 | grad norm avg: 0.72 | grad norm last: 0.66 | 
2026-01-01T19:53:11 | step: 49700 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.952949529979378e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 4.12 | consumed tokens: 407142400.0 | grad norm avg: 0.72 | grad norm last: 0.77 | 
2026-01-01T19:53:27 | step: 49800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9527297960594296e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 4.41 | consumed tokens: 407961600.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:53:43 | step: 49900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.95250933454372e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.05 | consumed tokens: 408780800.0 | grad norm avg: 0.72 | grad norm last: 0.76 | 
2026-01-01T19:53:58 | step: 50000 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9522885092301294e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.36 | consumed tokens: 409600000.0 | grad norm avg: 0.73 | grad norm last: 0.71 | 
2026-01-01T19:54:15 | step: 50100 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.952067320118658e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.5 | consumed tokens: 410419200.0 | grad norm avg: 0.73 | grad norm last: 0.76 | 
2026-01-01T19:54:31 | step: 50200 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9518454034114257e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.3 | consumed tokens: 411238400.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T19:54:46 | step: 50300 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9516231229063123e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.73 | consumed tokens: 412057600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:55:02 | step: 50400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9514001148054376e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.22 | consumed tokens: 412876800.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:55:17 | step: 50500 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.951176742906682e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.22 | consumed tokens: 413696000.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:55:33 | step: 50600 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.950953007210046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.25 | consumed tokens: 414515200.0 | grad norm avg: 0.73 | grad norm last: 0.71 | 
2026-01-01T19:55:48 | step: 50700 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9507285439176485e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.5 | consumed tokens: 415334400.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T19:56:04 | step: 50800 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.95050371682737e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.97 | consumed tokens: 416153600.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:56:20 | step: 50900 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.950278525939211e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.84 | consumed tokens: 416972800.0 | grad norm avg: 0.72 | grad norm last: 0.76 | 
2026-01-01T19:56:35 | step: 51000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.950052607455291e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.55 | consumed tokens: 417792000.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:56:51 | step: 51100 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.94982632517349e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.8 | consumed tokens: 418611200.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:57:06 | step: 51200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.949599315295927e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.3 | consumed tokens: 419430400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T19:57:22 | step: 51300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.949371941620484e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.14 | consumed tokens: 420249600.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T19:57:37 | step: 51400 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.9491438403492793e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 3.22 | consumed tokens: 421068800.0 | grad norm avg: 0.72 | grad norm last: 0.68 | 
2026-01-01T19:57:53 | step: 51500 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.948915375280194e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 2.64 | consumed tokens: 421888000.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:58:08 | step: 51600 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.948686546413228e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.94 | consumed tokens: 422707200.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T19:58:24 | step: 51700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.948457353748381e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 2.62 | consumed tokens: 423526400.0 | grad norm avg: 0.72 | grad norm last: 0.68 | 
2026-01-01T19:58:39 | step: 51800 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.948227433487773e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.38 | consumed tokens: 424345600.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:58:55 | step: 51900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.947996785631403e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.11 | consumed tokens: 425164800.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T19:59:10 | step: 52000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.947765773977153e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.84 | consumed tokens: 425984000.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T19:59:26 | step: 52100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.947534398525022e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.19 | consumed tokens: 426803200.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T19:59:42 | step: 52200 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9473022954771295e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.17 | consumed tokens: 427622400.0 | grad norm avg: 0.72 | grad norm last: 0.68 | 
2026-01-01T19:59:57 | step: 52300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9470698286313564e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.61 | consumed tokens: 428441600.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T20:00:13 | step: 52400 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.9468369979877025e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.3 | train loss last: 2.7 | consumed tokens: 429260800.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:00:28 | step: 52500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.946603439748287e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.17 | consumed tokens: 430080000.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:00:44 | step: 52600 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.946369517710991e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.42 | consumed tokens: 430899200.0 | grad norm avg: 0.72 | grad norm last: 1.14 | 
2026-01-01T20:01:00 | step: 52700 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9461352318758145e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.48 | consumed tokens: 431718400.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:01:15 | step: 52800 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9459002184448764e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 2.55 | consumed tokens: 432537600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:01:31 | step: 52900 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.945664477418177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.12 | consumed tokens: 433356800.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:01:46 | step: 53000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.945428736391477e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.77 | consumed tokens: 434176000.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-01T20:02:02 | step: 53100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9451919039711356e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.44 | consumed tokens: 434995200.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:02:17 | step: 53200 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.944955071550794e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.92 | consumed tokens: 435814400.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T20:02:33 | step: 53300 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.944717511534691e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.53 | consumed tokens: 436633600.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:02:48 | step: 53400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.944479587720707e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.25 | consumed tokens: 437452800.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T20:03:04 | step: 53500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.944240936310962e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.11 | consumed tokens: 438272000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:03:20 | step: 53600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.944001921103336e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.95 | consumed tokens: 439091200.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:03:35 | step: 53700 | train samples/s: 102.6 | train mfu (16-bit): -1.0 | lr mean: 4.9437621782999486e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.08 | consumed tokens: 439910400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:03:51 | step: 53800 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.943522435496561e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.8 | consumed tokens: 440729600.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:04:07 | step: 53900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.943281601299532e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.41 | consumed tokens: 441548800.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:04:22 | step: 54000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.943040767102502e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.09 | consumed tokens: 442368000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:04:38 | step: 54100 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9427992053097114e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.38 | consumed tokens: 443187200.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:04:53 | step: 54200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.942556915921159e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.39 | consumed tokens: 444006400.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T20:05:09 | step: 54300 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.942314262734726e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.25 | consumed tokens: 444825600.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:05:24 | step: 54400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9420712457504123e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.25 | consumed tokens: 445644800.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:05:40 | step: 54500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.941827864968218e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.94 | consumed tokens: 446464000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:05:56 | step: 54600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.941583756590262e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.55 | consumed tokens: 447283200.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T20:06:11 | step: 54700 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.941338920616545e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.45 | consumed tokens: 448102400.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T20:06:27 | step: 54800 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9410940846428275e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.05 | consumed tokens: 448921600.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:06:42 | step: 54900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.940848521073349e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.28 | consumed tokens: 449740800.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:06:58 | step: 55000 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.940602229908109e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.94 | consumed tokens: 450560000.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:07:15 | step: 55100 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.940355574944988e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.47 | consumed tokens: 451379200.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:07:30 | step: 55200 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9401085561839864e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 452198400.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-01T20:07:46 | step: 55300 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.9398608098272234e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.94 | consumed tokens: 453017600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:08:02 | step: 55400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.93961269967258e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.25 | consumed tokens: 453836800.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:08:17 | step: 55500 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9393642257200554e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.08 | consumed tokens: 454656000.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T20:08:33 | step: 55600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9391150241717696e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.97 | consumed tokens: 455475200.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T20:08:48 | step: 55700 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.938865458825603e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 456294400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:09:04 | step: 55800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.938615165883675e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.62 | consumed tokens: 457113600.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T20:09:19 | step: 55900 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.9383645091438666e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 2.91 | consumed tokens: 457932800.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:09:35 | step: 56000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.938113488606177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.27 | consumed tokens: 458752000.0 | grad norm avg: 0.72 | grad norm last: 0.79 | 
2026-01-01T20:09:51 | step: 56100 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9378617404727265e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.23 | consumed tokens: 459571200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:10:06 | step: 56200 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.937609628541395e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.33 | consumed tokens: 460390400.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:10:22 | step: 56300 | train samples/s: 103.1 | train mfu (16-bit): -1.0 | lr mean: 4.937356789014302e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.38 | consumed tokens: 461209600.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T20:10:38 | step: 56400 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.9371035856893286e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.84 | consumed tokens: 462028800.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-01T20:10:53 | step: 56500 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.936850018566474e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.45 | consumed tokens: 462848000.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:11:09 | step: 56600 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9365957238478586e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.06 | consumed tokens: 463667200.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-01T20:11:24 | step: 56700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.936341065331362e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.22 | consumed tokens: 464486400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:11:40 | step: 56800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.936086043016985e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.73 | consumed tokens: 465305600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:11:56 | step: 56900 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9358302931068465e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.38 | consumed tokens: 466124800.0 | grad norm avg: 0.72 | grad norm last: 0.74 | 
2026-01-01T20:12:11 | step: 57000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.935574179398827e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.78 | consumed tokens: 466944000.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:12:27 | step: 57100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9353173380950466e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.36 | consumed tokens: 467763200.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T20:12:42 | step: 57200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.935060132993385e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.34 | consumed tokens: 468582400.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:12:58 | step: 57300 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.934802564093843e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.42 | consumed tokens: 469401600.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:13:13 | step: 57400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.9345442675985396e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.34 | consumed tokens: 470220800.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:13:29 | step: 57500 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9342856073053554e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.11 | consumed tokens: 471040000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:13:45 | step: 57600 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.93402621941641e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.11 | consumed tokens: 471859200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:14:00 | step: 57700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9337664677295834e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 2.55 | consumed tokens: 472678400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:14:16 | step: 57800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.933506352244876e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.98 | consumed tokens: 473497600.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:14:31 | step: 57900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.933245509164408e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.03 | consumed tokens: 474316800.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:14:47 | step: 58000 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.932984302286059e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.06 | consumed tokens: 475136000.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:15:03 | step: 58100 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.932722731609829e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.34 | consumed tokens: 475955200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:15:18 | step: 58200 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.9324604333378375e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.31 | consumed tokens: 476774400.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T20:15:34 | step: 58300 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9321977712679654e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.45 | consumed tokens: 477593600.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:15:49 | step: 58400 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.931934381602332e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.25 | consumed tokens: 478412800.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:16:05 | step: 58500 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.931670628138818e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.92 | consumed tokens: 479232000.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-01T20:16:21 | step: 58600 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.931406510877423e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.03 | consumed tokens: 480051200.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:16:36 | step: 58700 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.931141666020267e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.75 | consumed tokens: 480870400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:16:52 | step: 58800 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.93087645736523e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.02 | consumed tokens: 481689600.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:17:08 | step: 58900 | train samples/s: 103.1 | train mfu (16-bit): -1.0 | lr mean: 4.930610521114431e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.31 | consumed tokens: 482508800.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:17:23 | step: 59000 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.930344221065752e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.17 | consumed tokens: 483328000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:17:39 | step: 59100 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9300775572191924e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.16 | consumed tokens: 484147200.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:17:55 | step: 59200 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.929810165776871e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.98 | consumed tokens: 484966400.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:18:10 | step: 59300 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.929542410536669e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.38 | consumed tokens: 485785600.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T20:18:26 | step: 59400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9292742914985865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.84 | consumed tokens: 486604800.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T20:18:41 | step: 59500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9290054448647425e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.7 | consumed tokens: 487424000.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:18:57 | step: 59600 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.928736234433018e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.78 | consumed tokens: 488243200.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:19:12 | step: 59700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.9284662964055315e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.05 | consumed tokens: 489062400.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:19:28 | step: 59800 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.9281959945801646e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.98 | consumed tokens: 489881600.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:19:44 | step: 59900 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.927925328956917e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.09 | consumed tokens: 490700800.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:19:59 | step: 60000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.927653935737908e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.06 | consumed tokens: 491520000.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T20:20:16 | step: 60100 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.927382178721018e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.98 | consumed tokens: 492339200.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:20:32 | step: 60200 | train samples/s: 102.7 | train mfu (16-bit): -1.0 | lr mean: 4.927110057906248e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.67 | consumed tokens: 493158400.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:20:48 | step: 60300 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.926837209495716e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 2.95 | consumed tokens: 493977600.0 | grad norm avg: 0.72 | grad norm last: 0.77 | 
2026-01-01T20:21:03 | step: 60400 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.926563997287303e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.08 | consumed tokens: 494796800.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:21:19 | step: 60500 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.926290057483129e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.29 | train loss last: 3.67 | consumed tokens: 495616000.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:21:34 | step: 60600 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9260157538810745e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.08 | consumed tokens: 496435200.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:21:50 | step: 60700 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.925741086481139e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.59 | consumed tokens: 497254400.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:22:06 | step: 60800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.925465691485442e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.34 | consumed tokens: 498073600.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:22:21 | step: 60900 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.925189932691865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.12 | consumed tokens: 498892800.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:22:37 | step: 61000 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.924913446302526e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.75 | consumed tokens: 499712000.0 | grad norm avg: 0.72 | grad norm last: 0.72 | 
2026-01-01T20:22:52 | step: 61100 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.924636596115306e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.42 | consumed tokens: 500531200.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:23:08 | step: 61200 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.9243593821302056e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.25 | consumed tokens: 501350400.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:23:23 | step: 61300 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.924081440549344e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 4.03 | consumed tokens: 502169600.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:23:38 | step: 61400 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.923803135170601e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.86 | consumed tokens: 502988800.0 | grad norm avg: 0.72 | grad norm last: 0.66 | 
2026-01-01T20:23:54 | step: 61500 | train samples/s: 103.4 | train mfu (16-bit): -1.0 | lr mean: 4.923524465993978e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.38 | consumed tokens: 503808000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:24:10 | step: 61600 | train samples/s: 106.0 | train mfu (16-bit): -1.0 | lr mean: 4.9232450692215934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.53 | consumed tokens: 504627200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:24:25 | step: 61700 | train samples/s: 106.3 | train mfu (16-bit): -1.0 | lr mean: 4.922965308651328e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.11 | consumed tokens: 505446400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:24:41 | step: 61800 | train samples/s: 105.9 | train mfu (16-bit): -1.0 | lr mean: 4.922685184283182e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.89 | consumed tokens: 506265600.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:24:56 | step: 61900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9224043323192745e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.89 | consumed tokens: 507084800.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:25:12 | step: 62000 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.9221227527596056e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.42 | consumed tokens: 507904000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T20:25:27 | step: 62100 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.921841173199937e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.8 | consumed tokens: 508723200.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:25:43 | step: 62200 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9215588660445064e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.36 | consumed tokens: 509542400.0 | grad norm avg: 0.73 | grad norm last: 0.7 | 
2026-01-01T20:25:58 | step: 62300 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.921275831293315e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.28 | train loss last: 3.2 | consumed tokens: 510361600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T20:26:14 | step: 62400 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.920992432744242e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.98 | consumed tokens: 511180800.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T20:26:29 | step: 62500 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.920708670397289e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.25 | consumed tokens: 512000000.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   04.6 | train mfu (16-bit): -1.0 | lr mean: 4.916973557556048e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.3 | consumed tokens: 522649600.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:30:07 | step: 63900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.916682519251481e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.34 | consumed tokens: 523468800.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:30:23 | step: 64000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9163914809469134e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.66 | consumed tokens: 524288000.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:30:39 | step: 64100 | train samples/s: 103.6 | train mfu (16-bit): -1.0 | lr mean: 4.916099351248704e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.28 | consumed tokens: 525107200.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:30:54 | step: 64200 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.9158072215504944e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.11 | consumed tokens: 525926400.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-01T20:31:10 | step: 64300 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.9155143642565235e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.0 | consumed tokens: 526745600.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:31:26 | step: 64400 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.915221143164672e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.28 | consumed tokens: 527564800.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:31:41 | step: 64500 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.914927194477059e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.58 | consumed tokens: 528384000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:31:57 | step: 64600 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.914632881991565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.33 | consumed tokens: 529203200.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:32:12 | step: 64700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.914338205708191e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 4.0 | consumed tokens: 530022400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:32:28 | step: 64800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.914042801829055e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.88 | consumed tokens: 530841600.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:32:43 | step: 64900 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.9137470341520384e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.28 | consumed tokens: 531660800.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:32:59 | step: 65000 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.9134505388792604e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.94 | consumed tokens: 532480000.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T20:33:16 | step: 65100 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.913153679808602e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.3 | consumed tokens: 533299200.0 | grad norm avg: 0.72 | grad norm last: 0.67 | 
2026-01-01T20:33:31 | step: 65200 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.9128564569400623e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.86 | consumed tokens: 534118400.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:33:47 | step: 65300 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.9125585064757615e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.0 | consumed tokens: 534937600.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:34:03 | step: 65400 | train samples/s: 102.5 | train mfu (16-bit): -1.0 | lr mean: 4.91226019221358e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.2 | consumed tokens: 535756800.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:34:18 | step: 65500 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.911961514153518e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.28 | consumed tokens: 536576000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:34:34 | step: 65600 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.911662108497694e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.16 | consumed tokens: 537395200.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:34:50 | step: 65700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.91136233904399e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.88 | consumed tokens: 538214400.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:35:05 | step: 65800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.911061841994524e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.3 | consumed tokens: 539033600.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:35:21 | step: 65900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.910761344945058e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.58 | consumed tokens: 539852800.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:35:36 | step: 66000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.91045975650195e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 540672000.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:35:52 | step: 66100 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.9101581680588424e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.64 | consumed tokens: 541491200.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:36:08 | step: 66200 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.909855852019973e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.39 | consumed tokens: 542310400.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T20:36:23 | step: 66300 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9095528083853424e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.25 | consumed tokens: 543129600.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:36:39 | step: 66400 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9092497647507116e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.56 | consumed tokens: 543948800.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:36:54 | step: 66500 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9089459935203195e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.19 | consumed tokens: 544768000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:37:10 | step: 66600 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.908641494694166e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.27 | consumed tokens: 545587200.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:37:26 | step: 66700 | train samples/s: 102.2 | train mfu (16-bit): -1.0 | lr mean: 4.9083366320701316e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.78 | consumed tokens: 546406400.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:37:41 | step: 66800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9080314056482166e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.45 | consumed tokens: 547225600.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T20:37:57 | step: 66900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.907725815428421e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.56 | consumed tokens: 548044800.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:38:13 | step: 67000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.907419497612864e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.03 | consumed tokens: 548864000.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:38:28 | step: 67100 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.907112452201545e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.16 | consumed tokens: 549683200.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:38:44 | step: 67200 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.906805406790227e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.95 | consumed tokens: 550502400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:38:59 | step: 67300 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.906497633783147e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 4.0 | consumed tokens: 551321600.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:39:15 | step: 67400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9061891331803054e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.02 | consumed tokens: 552140800.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-01T20:39:30 | step: 67500 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.905880268779583e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.97 | consumed tokens: 552960000.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:39:46 | step: 67600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.9055710405809805e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.02 | consumed tokens: 553779200.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:40:02 | step: 67700 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.905261448584497e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.36 | consumed tokens: 554598400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:40:17 | step: 67800 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.904951128992252e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.11 | consumed tokens: 555417600.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-01T20:40:33 | step: 67900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9046404456021264e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.94 | consumed tokens: 556236800.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:40:48 | step: 68000 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.9043290346162394e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.67 | consumed tokens: 557056000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:41:04 | step: 68100 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9040172598324716e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 2.84 | consumed tokens: 557875200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:41:20 | step: 68200 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.903705121250823e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.66 | consumed tokens: 558694400.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T20:41:35 | step: 68300 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.903392255073413e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.69 | consumed tokens: 559513600.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:41:51 | step: 68400 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.9030790250981227e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.08 | consumed tokens: 560332800.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:42:06 | step: 68500 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.9027650675270706e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.41 | consumed tokens: 561152000.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:42:22 | step: 68600 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.9024511099560186e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.52 | consumed tokens: 561971200.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:42:37 | step: 68700 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.9021360609913245e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 2.97 | consumed tokens: 562790400.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-01T20:42:53 | step: 68800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.9018210120266303e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.34 | consumed tokens: 563609600.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T20:43:09 | step: 68900 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.901505235466175e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.42 | consumed tokens: 564428800.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:43:24 | step: 69000 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.9011890951078385e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.14 | consumed tokens: 565248000.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:43:40 | step: 69100 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.900872227153741e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.26 | train loss last: 3.5 | consumed tokens: 566067200.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:43:55 | step: 69200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.9005549954017624e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.34 | consumed tokens: 566886400.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:44:11 | step: 69300 | train samples/s: 102.9 | train mfu (16-bit): -1.0 | lr mean: 4.900237399851903e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.67 | consumed tokens: 567705600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T20:44:27 | step: 69400 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.899919076706283e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.12 | consumed tokens: 568524800.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:44:43 | step: 69500 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.8996003897627816e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.38 | consumed tokens: 569344000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T20:44:58 | step: 69600 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.899280975223519e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.12 | consumed tokens: 570163200.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:45:14 | step: 69700 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.898961560684256e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.84 | consumed tokens: 570982400.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:45:29 | step: 69800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.8986410547513515e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.58 | consumed tokens: 571801600.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:45:45 | step: 69900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.898320548818447e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 4.09 | consumed tokens: 572620800.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:46:01 | step: 70000 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8979993152897805e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.69 | consumed tokens: 573440000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:46:18 | step: 70100 | train samples/s: 103.6 | train mfu (16-bit): -1.0 | lr mean: 4.8976777179632336e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 574259200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:46:33 | step: 70200 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.897355393040925e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.61 | consumed tokens: 575078400.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:46:49 | step: 70300 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.897032704320736e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.08 | consumed tokens: 575897600.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:47:05 | step: 70400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.8967096518026665e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.05 | consumed tokens: 576716800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T20:47:20 | step: 70500 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.896385871688835e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.89 | consumed tokens: 577536000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:47:36 | step: 70600 | train samples/s: 102.5 | train mfu (16-bit): -1.0 | lr mean: 4.8960617277771235e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.33 | consumed tokens: 578355200.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:47:52 | step: 70700 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.89573685626965e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 579174400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T20:48:07 | step: 70800 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.895411620964296e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.83 | consumed tokens: 579993600.0 | grad norm avg: 0.71 | grad norm last: 0.79 | 
2026-01-01T20:48:23 | step: 70900 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.8950860218610615e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.23 | consumed tokens: 580812800.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:48:38 | step: 71000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.894760058959946e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.03 | consumed tokens: 581632000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:48:54 | step: 71100 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.894433368463069e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.53 | consumed tokens: 582451200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T20:49:10 | step: 71200 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.8941063141683117e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.02 | consumed tokens: 583270400.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-01T20:49:25 | step: 71300 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.893778532277793e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.5 | consumed tokens: 584089600.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T20:49:41 | step: 71400 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.893450386589393e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.27 | train loss last: 3.28 | consumed tokens: 584908800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T20:49:56 | step: 71500 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.8931218771031126e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.5 | consumed tokens: 585728000.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:50:12 | step: 71600 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.892792640021071e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.73 | consumed tokens: 586547200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:50:27 | step: 71700 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.892463039141148e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.45 | consumed tokens: 587366400.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-01T20:50:43 | step: 71800 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8921327106654644e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.3 | consumed tokens: 588185600.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:50:59 | step: 71900 | train samples/s: 102.9 | train mfu (16-bit): -1.0 | lr mean: 4.8918023821897805e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.69 | consumed tokens: 589004800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T20:51:14 | step: 72000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.8914709623204544e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.98 | consumed tokens: 589824000.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T20:51:30 | step: 72100 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8911395424511284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 3.22 | consumed tokens: 590643200.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T20:51:45 | step: 72200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.890807394986041e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.59 | consumed tokens: 591462400.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T20:52:01 | step: 72300 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.890474883723073e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.28 | consumed tokens: 592281600.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T20:52:17 | step: 72400 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.890141644864343e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.52 | consumed tokens: 593100800.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T20:52:32 | step: 72500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.889808042207733e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.16 | consumed tokens: 593920000.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T20:52:48 | step: 72600 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.889474075753242e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.88 | consumed tokens: 594739200.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:53:04 | step: 72700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8891393817029893e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 2.78 | consumed tokens: 595558400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T20:53:19 | step: 72800 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.888804323854856e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.28 | consumed tokens: 596377600.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T20:53:35 | step: 72900 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.888468902208842e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.19 | consumed tokens: 597196800.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T20:53:50 | step: 73000 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.888132752967067e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.56 | consumed tokens: 598016000.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T20:54:06 | step: 73100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.887796239927411e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.55 | consumed tokens: 598835200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T20:54:22 | step: 73200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.8874593630898744e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.94 | consumed tokens: 599654400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T20:54:38 | step: 73300 | train samples/s: 103.1 | train mfu (16-bit): -1.0 | lr mean: 4.887121758656576e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 2.12 | consumed tokens: 600473600.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T20:54:53 | step: 73400 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.8867837904253975e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.77 | consumed tokens: 601292800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T20:55:09 | step: 73500 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.886445094598457e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.52 | consumed tokens: 602112000.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T20:55:24 | step: 73600 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.886106034973636e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.2 | consumed tokens: 602931200.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T20:55:40 | step: 73700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8857666115509346e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.88 | consumed tokens: 603750400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T20:55:55 | step: 73800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8854264605324715e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.31 | consumed tokens: 604569600.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T20:56:11 | step: 73900 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.8850863095140085e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.44 | consumed tokens: 605388800.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:56:27 | step: 74000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.884745067101903e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 606208000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:56:42 | step: 74100 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.884403824689798e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.88 | consumed tokens: 607027200.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T20:56:58 | step: 74200 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8840618546819314e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.05 | consumed tokens: 607846400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T20:57:13 | step: 74300 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8837191570783034e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.0 | consumed tokens: 608665600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T20:57:29 | step: 74400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8833760956767946e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 4.12 | consumed tokens: 609484800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T20:57:44 | step: 74500 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.883032670477405e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 2.98 | consumed tokens: 610304000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T20:58:00 | step: 74600 | train samples/s: 103.3 | train mfu (16-bit): -1.0 | lr mean: 4.882688881480135e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 611123200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:58:16 | step: 74700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.8823443648871034e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.45 | consumed tokens: 611942400.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-01T20:58:31 | step: 74800 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.881999484496191e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.42 | consumed tokens: 612761600.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T20:58:47 | step: 74900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.8816538765095174e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.33 | consumed tokens: 613580800.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-01T20:59:02 | step: 75000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.881308268522844e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.31 | consumed tokens: 614400000.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-01T20:59:19 | step: 75100 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.880961569142528e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.48 | consumed tokens: 615219200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T20:59:35 | step: 75200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.880614869762212e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.03 | consumed tokens: 616038400.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T20:59:51 | step: 75300 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.880267442786135e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.39 | consumed tokens: 616857600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:00:06 | step: 75400 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.879919652012177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.3 | consumed tokens: 617676800.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T21:00:22 | step: 75500 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.8795711336424574e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.08 | consumed tokens: 618496000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:00:37 | step: 75600 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.879222251474857e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.94 | consumed tokens: 619315200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T21:00:53 | step: 75700 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.8788730055093765e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.03 | consumed tokens: 620134400.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T21:01:08 | step: 75800 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.878523031948134e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.5 | consumed tokens: 620953600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:01:24 | step: 75900 | train samples/s: 102.6 | train mfu (16-bit): -1.0 | lr mean: 4.8781726945890114e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.25 | consumed tokens: 621772800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:01:40 | step: 76000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.877821993432008e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.36 | consumed tokens: 622592000.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T21:01:55 | step: 76100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.877470564679243e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.61 | consumed tokens: 623411200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:02:11 | step: 76200 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.877118772128597e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.73 | consumed tokens: 624230400.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T21:02:26 | step: 76300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.87676625198219e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.47 | consumed tokens: 625049600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:02:42 | step: 76400 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.8764137318357825e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.59 | consumed tokens: 625868800.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:02:58 | step: 76500 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.876060120295733e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.94 | consumed tokens: 626688000.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T21:03:13 | step: 76600 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.875706508755684e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.12 | consumed tokens: 627507200.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T21:03:29 | step: 76700 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.875352169619873e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.67 | consumed tokens: 628326400.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:03:44 | step: 76800 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.874997466686182e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.77 | consumed tokens: 629145600.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T21:04:00 | step: 76900 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.874642036156729e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.09 | consumed tokens: 629964800.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-01T21:04:16 | step: 77000 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.874286605627276e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.69 | consumed tokens: 630784000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:04:31 | step: 77100 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.873930083704181e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 631603200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:04:47 | step: 77200 | train samples/s: 102.9 | train mfu (16-bit): -1.0 | lr mean: 4.873573561781086e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 632422400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:05:03 | step: 77300 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8732163122622296e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.97 | consumed tokens: 633241600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:05:18 | step: 77400 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.872858335147612e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.53 | consumed tokens: 634060800.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-01T21:05:34 | step: 77500 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.872500358032994e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.59 | consumed tokens: 634880000.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:05:49 | step: 77600 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.872141653322615e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.27 | consumed tokens: 635699200.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:06:05 | step: 77700 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.871782221016474e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.39 | consumed tokens: 636518400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:06:21 | step: 77800 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.8714227887103334e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.83 | consumed tokens: 637337600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:06:36 | step: 77900 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.871062628808431e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.06 | consumed tokens: 638156800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:06:52 | step: 78000 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.870701741310768e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.34 | consumed tokens: 638976000.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T21:07:07 | step: 78100 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.870340853813104e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.88 | consumed tokens: 639795200.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T21:07:23 | step: 78200 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8699792387196794e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.25 | train loss last: 2.97 | consumed tokens: 640614400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:07:39 | step: 78300 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.869616896030493e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.11 | consumed tokens: 641433600.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:07:54 | step: 78400 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.869254189543426e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.39 | consumed tokens: 642252800.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T21:08:10 | step: 78500 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.868891119258478e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.69 | consumed tokens: 643072000.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T21:08:26 | step: 78600 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.86852768517565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.42 | consumed tokens: 643891200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:08:41 | step: 78700 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.86816352349706e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.59 | consumed tokens: 644710400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:08:57 | step: 78800 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.8677989980205894e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.34 | consumed tokens: 645529600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:09:12 | step: 78900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.8674337449483573e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.84 | consumed tokens: 646348800.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:09:28 | step: 79000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.8670681280782446e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.62 | consumed tokens: 647168000.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T21:09:43 | step: 79100 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.866702147410251e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.39 | consumed tokens: 647987200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:09:59 | step: 79200 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.866335802944377e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.45 | consumed tokens: 648806400.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:10:15 | step: 79300 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8659687308827415e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.47 | consumed tokens: 649625600.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:10:30 | step: 79400 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.865601295023225e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.23 | consumed tokens: 650444800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:10:46 | step: 79500 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.8652331315679476e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 651264000.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:11:01 | step: 79600 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.864864604314789e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 2.95 | consumed tokens: 652083200.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T21:11:17 | step: 79700 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.86449571326375e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.98 | consumed tokens: 652902400.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T21:11:33 | step: 79800 | train samples/s: 102.6 | train mfu (16-bit): -1.0 | lr mean: 4.8641260946169496e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.12 | consumed tokens: 653721600.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T21:11:49 | step: 79900 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.863756112172268e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 4.16 | consumed tokens: 654540800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:12:04 | step: 80000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.8633857659297064e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.33 | consumed tokens: 655360000.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:12:23 | step: 80100 | train samples/s: 94.3 | train mfu (16-bit): -1.0 | lr mean: 4.863014692091383e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.98 | consumed tokens: 656179200.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T21:12:38 | step: 80200 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.862643254455179e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.23 | consumed tokens: 656998400.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T21:12:54 | step: 80300 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.862271453021094e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.56 | consumed tokens: 657817600.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T21:13:10 | step: 80400 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.861898923991248e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.83 | consumed tokens: 658636800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:13:25 | step: 80500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.861526031163521e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.53 | consumed tokens: 659456000.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:13:40 | step: 80600 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.8611527745379135e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.12 | consumed tokens: 660275200.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:13:56 | step: 80700 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.8607787903165445e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.8 | consumed tokens: 661094400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T21:14:12 | step: 80800 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.860404442297295e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.89 | consumed tokens: 661913600.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T21:14:27 | step: 80900 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.860029730480164e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.36 | consumed tokens: 662732800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:14:43 | step: 81000 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.8596542910672724e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.92 | consumed tokens: 663552000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:14:58 | step: 81100 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.8592784878565e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.39 | consumed tokens: 664371200.0 | grad norm avg: 0.71 | grad norm last: 0.65 | 
2026-01-01T21:15:14 | step: 81200 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8589023208478466e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.48 | consumed tokens: 665190400.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T21:15:29 | step: 81300 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.858525426243432e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.88 | consumed tokens: 666009600.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:15:45 | step: 81400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.8581481678411365e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.2 | consumed tokens: 666828800.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:16:01 | step: 81500 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.8577701818430796e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.31 | consumed tokens: 667648000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:16:16 | step: 81600 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.857392195845023e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 3.14 | consumed tokens: 668467200.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:16:31 | step: 81700 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.8570134822512046e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.97 | consumed tokens: 669286400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:16:47 | step: 81800 | train samples/s: 105.7 | train mfu (16-bit): -1.0 | lr mean: 4.856634041061625e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.98 | consumed tokens: 670105600.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:17:02 | step: 81900 | train samples/s: 105.6 | train mfu (16-bit): -1.0 | lr mean: 4.8562542360741645e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.08 | consumed tokens: 670924800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:17:18 | step: 82000 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.8558740672888234e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.67 | consumed tokens: 671744000.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T21:17:34 | step: 82100 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.8554935347056016e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.36 | consumed tokens: 672563200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:17:49 | step: 82200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.8551122745266184e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.23 | consumed tokens: 673382400.0 | grad norm avg: 0.69 | grad norm last: 0.68 | 
2026-01-01T21:18:05 | step: 82300 | train samples/s: 105.8 | train mfu (16-bit): -1.0 | lr mean: 4.8547306505497545e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.17 | consumed tokens: 674201600.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:18:20 | step: 82400 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.854348298977129e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 675020800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:18:36 | step: 82500 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.853965583606623e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.95 | consumed tokens: 675840000.0 | grad norm avg: 0.7 | grad norm last: 0.76 | 
2026-01-01T21:18:52 | step: 82600 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.8535825044382364e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 4.09 | consumed tokens: 676659200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:19:07 | step: 82700 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.853199061471969e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.62 | consumed tokens: 677478400.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-01T21:19:23 | step: 82800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.85281489090994e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 678297600.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-01T21:19:38 | step: 82900 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8524303565500304e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.97 | consumed tokens: 679116800.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:19:54 | step: 83000 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8520450945943594e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.67 | consumed tokens: 679936000.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T21:20:10 | step: 83100 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.851659468840808e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.22 | consumed tokens: 680755200.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:20:25 | step: 83200 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.851273479289375e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 4.31 | consumed tokens: 681574400.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:20:41 | step: 83300 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.850887125940062e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.75 | consumed tokens: 682393600.0 | grad norm avg: 0.7 | grad norm last: 0.76 | 
2026-01-01T21:20:56 | step: 83400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8505000449949875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.39 | consumed tokens: 683212800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:21:12 | step: 83500 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.850112600252032e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.98 | consumed tokens: 684032000.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:21:28 | step: 83600 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.8497244279133156e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.77 | consumed tokens: 684851200.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-01T21:21:44 | step: 83700 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.849335891776718e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.91 | consumed tokens: 685670400.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-01T21:21:59 | step: 83800 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.84894699184224e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.23 | consumed tokens: 686489600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:22:15 | step: 83900 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.848557728109881e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.17 | consumed tokens: 687308800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:22:30 | step: 84000 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.848167736781761e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.09 | consumed tokens: 688128000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:22:46 | step: 84100 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8477770178578794e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.95 | consumed tokens: 688947200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:23:01 | step: 84200 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.847386298933998e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 689766400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:23:17 | step: 84300 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.846994852414355e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.09 | consumed tokens: 690585600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:23:33 | step: 84400 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.846603042096831e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.09 | consumed tokens: 691404800.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:23:48 | step: 84500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.846210504183546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.89 | consumed tokens: 692224000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:24:04 | step: 84600 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.84581760247238e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.42 | consumed tokens: 693043200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:24:19 | step: 84700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.845424336963333e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.36 | consumed tokens: 693862400.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T21:24:35 | step: 84800 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.845030707656406e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.83 | consumed tokens: 694681600.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:24:51 | step: 84900 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.844636350753717e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.31 | consumed tokens: 695500800.0 | grad norm avg: 0.7 | grad norm last: 0.76 | 
2026-01-01T21:25:07 | step: 85000 | train samples/s: 102.3 | train mfu (16-bit): -1.0 | lr mean: 4.844241630053148e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.25 | consumed tokens: 696320000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:25:24 | step: 85100 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.843846181756817e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.48 | consumed tokens: 697139200.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T21:25:39 | step: 85200 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.843450369662605e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.45 | consumed tokens: 697958400.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:25:55 | step: 85300 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.843054193770513e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 698777600.0 | grad norm avg: 0.71 | grad norm last: 0.74 | 
2026-01-01T21:26:10 | step: 85400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.842657290282659e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.61 | consumed tokens: 699596800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:26:26 | step: 85500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.842260022996925e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.97 | consumed tokens: 700416000.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T21:26:42 | step: 85600 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.84186239191331e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.25 | consumed tokens: 701235200.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T21:26:57 | step: 85700 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.841464397031814e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.31 | consumed tokens: 702054400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:27:13 | step: 85800 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8410656745545566e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.73 | consumed tokens: 702873600.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:27:29 | step: 85900 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.8406665882794186e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.72 | consumed tokens: 703692800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:27:44 | step: 86000 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.840266774408519e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.12 | consumed tokens: 704512000.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-01T21:28:00 | step: 86100 | train samples/s: 102.2 | train mfu (16-bit): -1.0 | lr mean: 4.839866596739739e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.7 | consumed tokens: 705331200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:28:16 | step: 86200 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.8394660552730784e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.75 | consumed tokens: 706150400.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:28:32 | step: 86300 | train samples/s: 103.1 | train mfu (16-bit): -1.0 | lr mean: 4.839065150008537e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.33 | consumed tokens: 706969600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:28:47 | step: 86400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.838663517148234e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.58 | consumed tokens: 707788800.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:29:03 | step: 86500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.83826152049005e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.31 | consumed tokens: 708608000.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:29:18 | step: 86600 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.837858796236105e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.28 | consumed tokens: 709427200.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-01T21:29:34 | step: 86700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8374557081842795e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.56 | consumed tokens: 710246400.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T21:29:50 | step: 86800 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.837052256334573e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.23 | train loss last: 2.78 | consumed tokens: 711065600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:30:05 | step: 86900 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.836648440686986e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.31 | consumed tokens: 711884800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:30:21 | step: 87000 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.836243897443637e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.09 | consumed tokens: 712704000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:30:37 | step: 87100 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.835838990402408e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.72 | consumed tokens: 713523200.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:30:52 | step: 87200 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.835433355765417e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.78 | consumed tokens: 714342400.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:31:08 | step: 87300 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.835027357330546e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.89 | consumed tokens: 715161600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:31:24 | step: 87400 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8346209950977936e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.11 | consumed tokens: 715980800.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:31:39 | step: 87500 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.834214269067161e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.56 | consumed tokens: 716800000.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:31:55 | step: 87600 | train samples/s: 102.9 | train mfu (16-bit): -1.0 | lr mean: 4.8338068154407665e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.88 | consumed tokens: 717619200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:32:11 | step: 87700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.8333989980164915e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.97 | consumed tokens: 718438400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:32:26 | step: 87800 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.832990452996455e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.28 | consumed tokens: 719257600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:32:42 | step: 87900 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.832581907976419e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.14 | consumed tokens: 720076800.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-01T21:32:57 | step: 88000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.83217227156274e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.78 | consumed tokens: 720896000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:33:13 | step: 88100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.831762635149062e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.22 | consumed tokens: 721715200.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:33:29 | step: 88200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.831352271139622e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 722534400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:33:44 | step: 88300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.830941543332301e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.24 | train loss last: 3.84 | consumed tokens: 723353600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:34:00 | step: 88400 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.8305304517271e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 724172800.0 | grad norm avg: 0.72 | grad norm last: 0.65 | 
2026-01-01T21:34:15 | step: 88500 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.830118632526137e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 724992000.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:34:31 | step: 88600 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.8297064495272934e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.78 | consumed tokens: 725811200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:34:46 | step: 88700 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.829293902730569e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.14 | train loss last: 3.05 | consumed tokens: 726630400.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:35:02 | step: 88800 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8288806283380836e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 727449600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:35:18 | step: 88900 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.828466990147717e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.34 | consumed tokens: 728268800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:35:33 | step: 89000 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.82805298815947e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.66 | consumed tokens: 729088000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:35:49 | step: 89100 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.827638258575462e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.55 | consumed tokens: 729907200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:36:05 | step: 89200 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.8272231651935726e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.0 | consumed tokens: 730726400.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:36:20 | step: 89300 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.826807708013803e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 731545600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:36:36 | step: 89400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8263915232382715e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.34 | consumed tokens: 732364800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:36:51 | step: 89500 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8259749746648595e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.7 | consumed tokens: 733184000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:37:07 | step: 89600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.825558062293567e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.22 | consumed tokens: 734003200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:37:23 | step: 89700 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.8251404223265126e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.16 | consumed tokens: 734822400.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:37:38 | step: 89800 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.824722418561578e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.91 | consumed tokens: 735641600.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:37:54 | step: 89900 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.824304050998762e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 736460800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:38:10 | step: 90000 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.823884955840185e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.03 | consumed tokens: 737280000.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:38:26 | step: 90100 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.823465860681608e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.44 | consumed tokens: 738099200.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:38:42 | step: 90200 | train samples/s: 103.2 | train mfu (16-bit): -1.0 | lr mean: 4.823045674129389e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.34 | consumed tokens: 738918400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:38:58 | step: 90300 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.82262548757717e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.28 | consumed tokens: 739737600.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T21:39:14 | step: 90400 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.8222045734291896e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.16 | consumed tokens: 740556800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:39:29 | step: 90500 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.8217832954833284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.06 | consumed tokens: 741376000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:39:45 | step: 90600 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.821361289941706e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 2.94 | consumed tokens: 742195200.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T21:40:00 | step: 90700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.8209389206022024e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.78 | consumed tokens: 743014400.0 | grad norm avg: 0.69 | grad norm last: 0.68 | 
2026-01-01T21:40:16 | step: 90800 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8205161874648184e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.25 | consumed tokens: 743833600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:40:31 | step: 90900 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.8200930905295536e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 744652800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:40:47 | step: 91000 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8196692659985274e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.83 | consumed tokens: 745472000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:41:03 | step: 91100 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.8192450776696205e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.2 | consumed tokens: 746291200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:41:18 | step: 91200 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.818820525542833e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.31 | consumed tokens: 747110400.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:41:34 | step: 91300 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.818395245820284e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.19 | consumed tokens: 747929600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:41:50 | step: 91400 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.817969602299854e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.91 | consumed tokens: 748748800.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:42:06 | step: 91500 | train samples/s: 101.9 | train mfu (16-bit): -1.0 | lr mean: 4.817543594981544e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.66 | consumed tokens: 749568000.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:42:21 | step: 91600 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.817116860067472e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.67 | consumed tokens: 750387200.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:42:37 | step: 91700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.816689761355519e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.17 | consumed tokens: 751206400.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:42:52 | step: 91800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.816262298845686e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.12 | consumed tokens: 752025600.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:43:08 | step: 91900 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.815834108740091e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.41 | consumed tokens: 752844800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:43:24 | step: 92000 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.815405554836616e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.42 | consumed tokens: 753664000.0 | grad norm avg: 0.7 | grad norm last: 1.09 | 
2026-01-01T21:43:39 | step: 92100 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.81497663713526e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.0 | consumed tokens: 754483200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:43:55 | step: 92200 | train samples/s: 103.7 | train mfu (16-bit): -1.0 | lr mean: 4.814546991838142e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.53 | consumed tokens: 755302400.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T21:44:11 | step: 92300 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.814116982743144e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.92 | consumed tokens: 756121600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:44:26 | step: 92400 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.813686609850265e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.36 | consumed tokens: 756940800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:44:42 | step: 92500 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.8132558731595054e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 2.81 | consumed tokens: 757760000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:44:58 | step: 92600 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.8128244088729843e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.64 | consumed tokens: 758579200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:45:13 | step: 92700 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.8123925807885826e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.94 | consumed tokens: 759398400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:45:29 | step: 92800 | train samples/s: 102.3 | train mfu (16-bit): -1.0 | lr mean: 4.8119600251084194e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.39 | consumed tokens: 760217600.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T21:45:45 | step: 92900 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.8115271056303754e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 761036800.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T21:46:00 | step: 93000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.811093822354451e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.86 | consumed tokens: 761856000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:46:16 | step: 93100 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.8106601752806455e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 4.5 | consumed tokens: 762675200.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-01T21:46:32 | step: 93200 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.810225800611079e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 4.34 | consumed tokens: 763494400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:46:47 | step: 93300 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.809791062143631e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.16 | consumed tokens: 764313600.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T21:47:03 | step: 93400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.809355959878303e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.58 | consumed tokens: 765132800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:47:18 | step: 93500 | train samples/s: 105.5 | train mfu (16-bit): -1.0 | lr mean: 4.8089201300172135e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.0 | consumed tokens: 765952000.0 | grad norm avg: 0.69 | grad norm last: 0.67 | 
2026-01-01T21:47:34 | step: 93600 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.808483936358243e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 3.14 | consumed tokens: 766771200.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:47:49 | step: 93700 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.808047378901392e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.7 | consumed tokens: 767590400.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:48:05 | step: 93800 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.80761009384878e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 4.88 | consumed tokens: 768409600.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:48:21 | step: 93900 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.807172444998287e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.94 | consumed tokens: 769228800.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:48:37 | step: 94000 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.806734432349913e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.12 | consumed tokens: 770048000.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-01T21:48:52 | step: 94100 | train samples/s: 103.0 | train mfu (16-bit): -1.0 | lr mean: 4.8062956921057776e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.38 | consumed tokens: 770867200.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-01T21:49:08 | step: 94200 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.805856951861642e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.0 | consumed tokens: 771686400.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:49:24 | step: 94300 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.805417120223865e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.19 | consumed tokens: 772505600.0 | grad norm avg: 0.69 | grad norm last: 0.71 | 
2026-01-01T21:49:39 | step: 94400 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.8049772885860875e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.97 | consumed tokens: 773324800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:49:55 | step: 94500 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.804536729352549e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.72 | consumed tokens: 774144000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:50:11 | step: 94600 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.804095806321129e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.77 | consumed tokens: 774963200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:50:26 | step: 94700 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.803654519491829e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.84 | consumed tokens: 775782400.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T21:50:42 | step: 94800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.803212505066767e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.12 | consumed tokens: 776601600.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:50:57 | step: 94900 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.802770126843825e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.22 | train loss last: 3.31 | consumed tokens: 777420800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:51:13 | step: 95000 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.802327384823002e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.53 | consumed tokens: 778240000.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:51:30 | step: 95100 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.8018839152064174e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.23 | consumed tokens: 779059200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:51:46 | step: 95200 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.801440081791952e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.36 | consumed tokens: 779878400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:52:01 | step: 95300 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.8009958845796064e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.86 | consumed tokens: 780697600.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:52:17 | step: 95400 | train samples/s: 103.1 | train mfu (16-bit): -1.0 | lr mean: 4.800550959771499e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 2.72 | consumed tokens: 781516800.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:52:33 | step: 95500 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.800105671165511e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.52 | consumed tokens: 782336000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:52:49 | step: 95600 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.799660018761642e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.16 | consumed tokens: 783155200.0 | grad norm avg: 0.69 | grad norm last: 0.7 | 
2026-01-01T21:53:04 | step: 95700 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.799214002559893e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.14 | consumed tokens: 783974400.0 | grad norm avg: 0.69 | grad norm last: 0.67 | 
2026-01-01T21:53:20 | step: 95800 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.798767258762382e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 2.69 | consumed tokens: 784793600.0 | grad norm avg: 0.69 | grad norm last: 0.72 | 
2026-01-01T21:53:36 | step: 95900 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.7983201511669904e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.33 | consumed tokens: 785612800.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:53:51 | step: 96000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.797872679773718e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.41 | consumed tokens: 786432000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:54:07 | step: 96100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.7974244807846844e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.64 | consumed tokens: 787251200.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T21:54:22 | step: 96200 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.79697591799777e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.7 | consumed tokens: 788070400.0 | grad norm avg: 0.69 | grad norm last: 0.67 | 
2026-01-01T21:54:38 | step: 96300 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.796526627615094e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.17 | consumed tokens: 788889600.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T21:54:54 | step: 96400 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.7960773372324184e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.61 | consumed tokens: 789708800.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:55:09 | step: 96500 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.795627319253981e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.5 | consumed tokens: 790528000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:55:25 | step: 96600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.795176937477663e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.34 | consumed tokens: 791347200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:55:41 | step: 96700 | train samples/s: 102.6 | train mfu (16-bit): -1.0 | lr mean: 4.794725828105584e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 4.0 | consumed tokens: 792166400.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:55:57 | step: 96800 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.794274354935624e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.12 | train loss last: 3.33 | consumed tokens: 792985600.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-01T21:56:12 | step: 96900 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.793822517967783e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.53 | consumed tokens: 793804800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:56:28 | step: 97000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.7933703172020614e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.67 | consumed tokens: 794624000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:56:43 | step: 97100 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.7929173888405785e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.67 | consumed tokens: 795443200.0 | grad norm avg: 0.69 | grad norm last: 0.66 | 
2026-01-01T21:56:59 | step: 97200 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.792464096681215e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.94 | consumed tokens: 796262400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:57:15 | step: 97300 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.79201007692609e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.02 | consumed tokens: 797081600.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:57:30 | step: 97400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.791556057170965e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.95 | consumed tokens: 797900800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T21:57:46 | step: 97500 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.791101309820078e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 2.97 | consumed tokens: 798720000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:58:02 | step: 97600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.790646198671311e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.02 | consumed tokens: 799539200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:58:17 | step: 97700 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.7901903599267825e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.66 | consumed tokens: 800358400.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:58:33 | step: 97800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.789734157384373e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.3 | consumed tokens: 801177600.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T21:58:48 | step: 97900 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.789277591044083e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.33 | consumed tokens: 801996800.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T21:59:04 | step: 98000 | train samples/s: 101.8 | train mfu (16-bit): -1.0 | lr mean: 4.788820297108032e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.06 | consumed tokens: 802816000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T21:59:20 | step: 98100 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.7883630031719804e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.8 | consumed tokens: 803635200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T21:59:36 | step: 98200 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.7879049816401675e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.3 | consumed tokens: 804454400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T21:59:51 | step: 98300 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.787446232512593e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 805273600.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T22:00:07 | step: 98400 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.786987483385019e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 3.06 | consumed tokens: 806092800.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T22:00:23 | step: 98500 | train samples/s: 103.6 | train mfu (16-bit): -1.0 | lr mean: 4.786528006661683e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.09 | consumed tokens: 806912000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T22:00:39 | step: 98600 | train samples/s: 104.1 | train mfu (16-bit): -1.0 | lr mean: 4.786067802342586e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 3.47 | consumed tokens: 807731200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T22:00:54 | step: 98700 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.785607598023489e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.11 | consumed tokens: 808550400.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T22:01:10 | step: 98800 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.7851466661086306e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.86 | consumed tokens: 809369600.0 | grad norm avg: 0.69 | grad norm last: 0.66 | 
2026-01-01T22:01:26 | step: 98900 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.7846853703958914e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.13 | train loss last: 2.92 | consumed tokens: 810188800.0 | grad norm avg: 0.69 | grad norm last: 0.66 | 
2026-01-01T22:01:41 | step: 99000 | train samples/s: 103.5 | train mfu (16-bit): -1.0 | lr mean: 4.784223347087391e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.12 | consumed tokens: 811008000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T22:01:57 | step: 99100 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.7837609599810094e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.78 | consumed tokens: 811827200.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T22:02:13 | step: 99200 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.783298209076747e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.56 | consumed tokens: 812646400.0 | grad norm avg: 0.69 | grad norm last: 0.74 | 
2026-01-01T22:02:29 | step: 99300 | train samples/s: 103.2 | train mfu (16-bit): -1.0 | lr mean: 4.7828350943746045e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 2.88 | consumed tokens: 813465600.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T22:02:44 | step: 99400 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.7823712520767e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 814284800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:03:00 | step: 99500 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.7819070459809154e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.7 | consumed tokens: 815104000.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:03:16 | step: 99600 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.78144247608725e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.14 | consumed tokens: 815923200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T22:03:31 | step: 99700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.780977178597823e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.8 | consumed tokens: 816742400.0 | grad norm avg: 0.69 | grad norm last: 0.76 | 
2026-01-01T22:03:47 | step: 99800 | train samples/s: 105.4 | train mfu (16-bit): -1.0 | lr mean: 4.780511881108396e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.03 | consumed tokens: 817561600.0 | grad norm avg: 0.69 | grad norm last: 0.71 | 
2026-01-01T22:04:02 | step: 99900 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.7800454922253266e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.25 | consumed tokens: 818380800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T22:04:18 | step: 100000 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.7795791033422574e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.97 | consumed tokens: 819200000.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T22:04:35 | step: 100100 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.779111986863427e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.23 | consumed tokens: 820019200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T22:04:51 | step: 100200 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.7786445065867156e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.14 | train loss last: 2.62 | consumed tokens: 820838400.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T22:05:06 | step: 100300 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.7781766625121236e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.39 | consumed tokens: 821657600.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:05:22 | step: 100400 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.77770809084177e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 2.48 | consumed tokens: 822476800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:05:37 | step: 100500 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.777239155373536e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 4.06 | consumed tokens: 823296000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T22:05:54 | step: 100600 | train samples/s: 101.9 | train mfu (16-bit): -1.0 | lr mean: 4.776769856107421e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 3.42 | consumed tokens: 824115200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T22:06:09 | step: 100700 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.776300193043426e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.39 | consumed tokens: 824934400.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T22:06:25 | step: 100800 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.775829802383669e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.84 | consumed tokens: 825753600.0 | grad norm avg: 0.71 | grad norm last: 0.69 | 
2026-01-01T22:06:40 | step: 100900 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.775359047926031e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.91 | consumed tokens: 826572800.0 | grad norm avg: 0.7 | grad norm last: 0.74 | 
2026-01-01T22:06:56 | step: 101000 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.774887565872632e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.44 | consumed tokens: 827392000.0 | grad norm avg: 0.7 | grad norm last: 0.75 | 
2026-01-01T22:07:12 | step: 101100 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.774416083819233e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 2.91 | consumed tokens: 828211200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T22:07:27 | step: 101200 | train samples/s: 103.8 | train mfu (16-bit): -1.0 | lr mean: 4.7739438741700724e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.36 | consumed tokens: 829030400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T22:07:43 | step: 101300 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.7734709369251505e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.91 | consumed tokens: 829849600.0 | grad norm avg: 0.69 | grad norm last: 0.71 | 
2026-01-01T22:07:58 | step: 101400 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.7729979996802285e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 4.12 | consumed tokens: 830668800.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T22:08:14 | step: 101500 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.772524334839545e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 2.56 | consumed tokens: 831488000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T22:08:30 | step: 101600 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.772050306200981e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 2.84 | consumed tokens: 832307200.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T22:08:45 | step: 101700 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.771575549966656e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.21 | train loss last: 3.06 | consumed tokens: 833126400.0 | grad norm avg: 0.69 | grad norm last: 0.67 | 
2026-01-01T22:09:01 | step: 101800 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.7711004299344495e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.14 | train loss last: 3.31 | consumed tokens: 833945600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T22:09:17 | step: 101900 | train samples/s: 103.1 | train mfu (16-bit): -1.0 | lr mean: 4.7706249461043626e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 3.31 | consumed tokens: 834764800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T22:09:32 | step: 102000 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.770149098476395e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.14 | consumed tokens: 835584000.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T22:09:48 | step: 102100 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.769672887050547e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.16 | consumed tokens: 836403200.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T22:10:04 | step: 102200 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.769195948028937e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.22 | consumed tokens: 837222400.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T22:10:19 | step: 102300 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.768718281411566e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.42 | consumed tokens: 838041600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T22:10:35 | step: 102400 | train samples/s: 104.8 | train mfu (16-bit): -1.0 | lr mean: 4.768240614794195e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 838860800.0 | grad norm avg: 0.7 | grad norm last: 0.67 | 
2026-01-01T22:10:50 | step: 102500 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.767762220581062e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.89 | consumed tokens: 839680000.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:11:06 | step: 102600 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.767283462570049e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.2 | consumed tokens: 840499200.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-01T22:11:22 | step: 102700 | train samples/s: 104.2 | train mfu (16-bit): -1.0 | lr mean: 4.766804340761155e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.94 | consumed tokens: 841318400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T22:11:38 | step: 102800 | train samples/s: 103.9 | train mfu (16-bit): -1.0 | lr mean: 4.7663244913564995e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.64 | consumed tokens: 842137600.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T22:11:53 | step: 102900 | train samples/s: 104.3 | train mfu (16-bit): -1.0 | lr mean: 4.7658442781539634e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.13 | train loss last: 2.36 | consumed tokens: 842956800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:12:09 | step: 103000 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.7653637011535466e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.7 | consumed tokens: 843776000.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T22:12:24 | step: 103100 | train samples/s: 105.3 | train mfu (16-bit): -1.0 | lr mean: 4.764882760355249e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.56 | consumed tokens: 844595200.0 | grad norm avg: 0.69 | grad norm last: 0.7 | 
2026-01-01T22:12:40 | step: 103200 | train samples/s: 104.4 | train mfu (16-bit): -1.0 | lr mean: 4.76440109196119e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.11 | consumed tokens: 845414400.0 | grad norm avg: 0.69 | grad norm last: 0.75 | 
2026-01-01T22:12:56 | step: 103300 | train samples/s: 101.9 | train mfu (16-bit): -1.0 | lr mean: 4.7639190597692505e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.23 | consumed tokens: 846233600.0 | grad norm avg: 0.69 | grad norm last: 0.67 | 
2026-01-01T22:13:12 | step: 103400 | train samples/s: 104.0 | train mfu (16-bit): -1.0 | lr mean: 4.7634362999815494e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.14 | train loss last: 2.73 | consumed tokens: 847052800.0 | grad norm avg: 0.7 | grad norm last: 0.76 | 
2026-01-01T22:13:27 | step: 103500 | train samples/s: 104.7 | train mfu (16-bit): -1.0 | lr mean: 4.762953540193848e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.06 | consumed tokens: 847872000.0 | grad norm avg: 0.69 | grad norm last: 0.7 | 
2026-01-01T22:13:43 | step: 103600 | train samples/s: 105.1 | train mfu (16-bit): -1.0 | lr mean: 4.762470052810386e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 2.86 | consumed tokens: 848691200.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T22:13:58 | step: 103700 | train samples/s: 105.2 | train mfu (16-bit): -1.0 | lr mean: 4.761985837831162e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 849510400.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T22:14:14 | step: 103800 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.761501622851938e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.19 | train loss last: 3.73 | consumed tokens: 850329600.0 | grad norm avg: 0.69 | grad norm last: 0.71 | 
2026-01-01T22:14:30 | step: 103900 | train samples/s: 104.5 | train mfu (16-bit): -1.0 | lr mean: 4.761016680276953e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.16 | train loss last: 3.33 | consumed tokens: 851148800.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T22:14:45 | step: 104000 | train samples/s: 104.6 | train mfu (16-bit): -1.0 | lr mean: 4.7605313739040866e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.2 | train loss last: 3.61 | consumed tokens: 851968000.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-01T22:15:01 | step: 104100 | train samples/s: 104.9 | train mfu (16-bit): -1.0 | lr mean: 4.76004570373334e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.15 | train loss last: 2.83 | consumed tokens: 852787200.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T22:15:16 | step: 104200 | train samples/s: 105.0 | train mfu (16-bit): -1.0 | lr mean: 4.759559305966832e-05 | peak memory rank 0 (MB): 3941.08 | train loss avg: 3.17 | train loss last: 3.39 | consumed tokens: 853606400.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
