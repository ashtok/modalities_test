==========================================
Experiment 4: Fine-tuning GPT-2 on 5 languages with 4 GPUs
Job ID: 2149946
Node: jnfat02
Start time: Thu Jan  1 07:25:51 PM CET 2026
==========================================
Thu Jan  1 19:25:52 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:05:00.0 Off |                    0 |
| N/A   29C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:06:00.0 Off |                    0 |
| N/A   30C    P8             32W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA L40S                    On  |   00000000:45:00.0 Off |                    0 |
| N/A   29C    P8             34W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA L40S                    On  |   00000000:46:00.0 Off |                    0 |
| N/A   30C    P8             33W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 1 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 2 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 3 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 31,109,952 parameters): weight_decay = 0.01
=> all (148 modules with 31,109,952 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2026-01-01 19:26:12.213544.



======================== Training Report ========================
Training target: 
	num_target_tokens: 5713166336
	num_target_steps: 174352 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 4
CUDA environment settings: 
	local_rank: 0
	world_size: 4
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (5713177600) does not match the number of target tokens (5713166336). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (174352) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (174352) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (174352) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2026-01-01 19:26:12.213815.
2026-01-01T19:26:42 | step: 100 | train samples/s: 244.7 | train mfu (16-bit): -1.0 | lr mean: 5.36468678546953e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.29 | train loss last: 4.28 | consumed tokens: 3276800.0 | grad norm avg: 1.8 | grad norm last: 1.68 | 
2026-01-01T19:27:11 | step: 200 | train samples/s: 254.1 | train mfu (16-bit): -1.0 | lr mean: 6.446925453928998e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.18 | train loss last: 4.29 | consumed tokens: 6553600.0 | grad norm avg: 1.39 | grad norm last: 1.3 | 
2026-01-01T19:27:39 | step: 300 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 8.21163303044159e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.14 | train loss last: 4.04 | consumed tokens: 9830400.0 | grad norm avg: 1.32 | grad norm last: 1.36 | 
2026-01-01T19:28:07 | step: 400 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 1.0601604117255192e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.1 | train loss last: 3.94 | consumed tokens: 13107200.0 | grad norm avg: 1.25 | grad norm last: 1.27 | 
2026-01-01T19:28:36 | step: 500 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 1.3539362953451928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.08 | train loss last: 4.22 | consumed tokens: 16384000.0 | grad norm avg: 1.2 | grad norm last: 1.11 | 
2026-01-01T19:29:04 | step: 600 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 1.692967998678796e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.06 | train loss last: 3.68 | consumed tokens: 19660800.0 | grad norm avg: 1.14 | grad norm last: 1.09 | 
2026-01-01T19:29:32 | step: 700 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 2.0662650058511645e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.02 | train loss last: 3.83 | consumed tokens: 22937600.0 | grad norm avg: 1.06 | grad norm last: 1.02 | 
2026-01-01T19:29:59 | step: 800 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 2.461726217006799e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.98 | train loss last: 3.96 | consumed tokens: 26214400.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2026-01-01T19:30:27 | step: 900 | train samples/s: 278.1 | train mfu (16-bit): -1.0 | lr mean: 2.86653248622315e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.95 | train loss last: 3.73 | consumed tokens: 29491200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2026-01-01T19:30:55 | step: 1000 | train samples/s: 280.9 | train mfu (16-bit): -1.0 | lr mean: 3.2675612601451576e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.91 | train loss last: 3.91 | consumed tokens: 32768000.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2026-01-01T19:31:23 | step: 1100 | train samples/s: 278.0 | train mfu (16-bit): -1.0 | lr mean: 3.651812221505679e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.9 | train loss last: 3.89 | consumed tokens: 36044800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T19:31:51 | step: 1200 | train samples/s: 274.7 | train mfu (16-bit): -1.0 | lr mean: 4.006829476566054e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.87 | train loss last: 3.95 | consumed tokens: 39321600.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-01T19:32:19 | step: 1300 | train samples/s: 280.8 | train mfu (16-bit): -1.0 | lr mean: 4.321104643167928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.85 | train loss last: 4.15 | consumed tokens: 42598400.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:32:47 | step: 1400 | train samples/s: 281.0 | train mfu (16-bit): -1.0 | lr mean: 4.5844499254599214e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.83 | train loss last: 4.06 | consumed tokens: 45875200.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:33:14 | step: 1500 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.788328806171194e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.82 | train loss last: 3.84 | consumed tokens: 49152000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T19:33:42 | step: 1600 | train samples/s: 277.4 | train mfu (16-bit): -1.0 | lr mean: 4.926131805405021e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.79 | train loss last: 3.86 | consumed tokens: 52428800.0 | grad norm avg: 0.67 | grad norm last: 0.66 | 
2026-01-01T19:34:10 | step: 1700 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.9933918489841744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.77 | train loss last: 3.99 | consumed tokens: 55705600.0 | grad norm avg: 0.66 | grad norm last: 0.66 | 
2026-01-01T19:34:38 | step: 1800 | train samples/s: 275.1 | train mfu (16-bit): -1.0 | lr mean: 4.999998782295734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.76 | train loss last: 3.85 | consumed tokens: 58982400.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T19:35:06 | step: 1900 | train samples/s: 277.8 | train mfu (16-bit): -1.0 | lr mean: 4.999989687348716e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.75 | train loss last: 3.92 | consumed tokens: 62259200.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T19:35:34 | step: 2000 | train samples/s: 280.9 | train mfu (16-bit): -1.0 | lr mean: 4.9999725888483226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.74 | train loss last: 3.65 | consumed tokens: 65536000.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T19:36:02 | step: 2100 | train samples/s: 280.8 | train mfu (16-bit): -1.0 | lr mean: 4.999947122996673e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.71 | train loss last: 3.55 | consumed tokens: 68812800.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T19:36:29 | step: 2200 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.999913289793767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.72 | train loss last: 3.6 | consumed tokens: 72089600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T19:36:57 | step: 2300 | train samples/s: 281.1 | train mfu (16-bit): -1.0 | lr mean: 4.9998714530374855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.7 | train loss last: 3.63 | consumed tokens: 75366400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:37:25 | step: 2400 | train samples/s: 275.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998212489299476e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.69 | train loss last: 3.86 | consumed tokens: 78643200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:37:53 | step: 2500 | train samples/s: 275.0 | train mfu (16-bit): -1.0 | lr mean: 4.9997626774711534e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.67 | train loss last: 3.54 | consumed tokens: 81920000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:38:21 | step: 2600 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.999695738661103e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.67 | train loss last: 3.5 | consumed tokens: 85196800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T19:38:49 | step: 2700 | train samples/s: 280.2 | train mfu (16-bit): -1.0 | lr mean: 4.999620796297677e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.64 | consumed tokens: 88473600.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T19:39:17 | step: 2800 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.9995374865829945e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.66 | train loss last: 3.95 | consumed tokens: 91750400.0 | grad norm avg: 0.59 | grad norm last: 0.63 | 
2026-01-01T19:39:44 | step: 2900 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.999445809517056e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.64 | train loss last: 3.51 | consumed tokens: 95027200.0 | grad norm avg: 0.61 | grad norm last: 0.69 | 
2026-01-01T19:40:12 | step: 3000 | train samples/s: 280.2 | train mfu (16-bit): -1.0 | lr mean: 4.999345765099861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.56 | consumed tokens: 98304000.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T19:40:40 | step: 3100 | train samples/s: 275.2 | train mfu (16-bit): -1.0 | lr mean: 4.99923771712929e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.93 | consumed tokens: 101580800.0 | grad norm avg: 0.6 | grad norm last: 0.59 | 
2026-01-01T19:41:09 | step: 3200 | train samples/s: 274.7 | train mfu (16-bit): -1.0 | lr mean: 4.999121301807463e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.62 | train loss last: 3.7 | consumed tokens: 104857600.0 | grad norm avg: 0.59 | grad norm last: 0.58 | 
2026-01-01T19:41:36 | step: 3300 | train samples/s: 280.1 | train mfu (16-bit): -1.0 | lr mean: 4.99899651913438e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.61 | train loss last: 3.54 | consumed tokens: 108134400.0 | grad norm avg: 0.59 | grad norm last: 0.6 | 
2026-01-01T19:42:04 | step: 3400 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.998863732907921e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.85 | consumed tokens: 111411200.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:42:32 | step: 3500 | train samples/s: 280.4 | train mfu (16-bit): -1.0 | lr mean: 4.998722579330206e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.68 | consumed tokens: 114688000.0 | grad norm avg: 0.59 | grad norm last: 0.54 | 
2026-01-01T19:43:00 | step: 3600 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.9985730584012344e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.45 | consumed tokens: 117964800.0 | grad norm avg: 0.59 | grad norm last: 0.59 | 
2026-01-01T19:43:27 | step: 3700 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.998415170121007e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.59 | train loss last: 3.61 | consumed tokens: 121241600.0 | grad norm avg: 0.58 | grad norm last: 0.55 | 
2026-01-01T19:43:55 | step: 3800 | train samples/s: 277.7 | train mfu (16-bit): -1.0 | lr mean: 4.9982489144895226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.68 | consumed tokens: 124518400.0 | grad norm avg: 0.59 | grad norm last: 0.62 | 
2026-01-01T19:44:24 | step: 3900 | train samples/s: 272.7 | train mfu (16-bit): -1.0 | lr mean: 4.998074655304663e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.46 | consumed tokens: 127795200.0 | grad norm avg: 0.6 | grad norm last: 0.71 | 
2026-01-01T19:44:51 | step: 4000 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.997892028768547e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.56 | consumed tokens: 131072000.0 | grad norm avg: 0.59 | grad norm last: 0.59 | 
2026-01-01T19:45:19 | step: 4100 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.997701398679055e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.58 | train loss last: 3.52 | consumed tokens: 134348800.0 | grad norm avg: 0.59 | grad norm last: 0.58 | 
2026-01-01T19:45:47 | step: 4200 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.9975020374404266e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.57 | consumed tokens: 137625600.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:46:15 | step: 4300 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.9972946726484224e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.38 | consumed tokens: 140902400.0 | grad norm avg: 0.59 | grad norm last: 0.65 | 
2026-01-01T19:46:42 | step: 4400 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.9970793043030426e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.41 | consumed tokens: 144179200.0 | grad norm avg: 0.59 | grad norm last: 0.62 | 
2026-01-01T19:47:10 | step: 4500 | train samples/s: 278.0 | train mfu (16-bit): -1.0 | lr mean: 4.996855204808526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.51 | consumed tokens: 147456000.0 | grad norm avg: 0.59 | grad norm last: 0.6 | 
2026-01-01T19:47:39 | step: 4600 | train samples/s: 272.6 | train mfu (16-bit): -1.0 | lr mean: 4.996623101760633e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.54 | train loss last: 3.45 | consumed tokens: 150732800.0 | grad norm avg: 0.59 | grad norm last: 0.67 | 
2026-01-01T19:48:07 | step: 4700 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9963826313614845e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.48 | consumed tokens: 154009600.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:48:34 | step: 4800 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.9961337936110795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.54 | train loss last: 3.48 | consumed tokens: 157286400.0 | grad norm avg: 0.6 | grad norm last: 0.6 | 
2026-01-01T19:49:02 | step: 4900 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.995876952307299e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.55 | train loss last: 3.44 | consumed tokens: 160563200.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:49:30 | step: 5000 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.995611743652262e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.58 | consumed tokens: 163840000.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T19:50:00 | step: 5100 | train samples/s: 276.7 | train mfu (16-bit): -1.0 | lr mean: 4.9953381676459685e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.28 | consumed tokens: 167116800.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:50:28 | step: 5200 | train samples/s: 276.4 | train mfu (16-bit): -1.0 | lr mean: 4.9950565880862996e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.61 | consumed tokens: 170393600.0 | grad norm avg: 0.6 | grad norm last: 0.64 | 
2026-01-01T19:50:56 | step: 5300 | train samples/s: 276.5 | train mfu (16-bit): -1.0 | lr mean: 4.9947666411753744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.5 | consumed tokens: 173670400.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T19:51:24 | step: 5400 | train samples/s: 277.2 | train mfu (16-bit): -1.0 | lr mean: 4.994468326913193e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.55 | consumed tokens: 176947200.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:51:51 | step: 5500 | train samples/s: 279.2 | train mfu (16-bit): -1.0 | lr mean: 4.994162009097636e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.46 | consumed tokens: 180224000.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:52:19 | step: 5600 | train samples/s: 279.4 | train mfu (16-bit): -1.0 | lr mean: 4.9938469601329416e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.54 | consumed tokens: 183500800.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:52:47 | step: 5700 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.9935242714127526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.46 | consumed tokens: 186777600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T19:53:15 | step: 5800 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9931928515434265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.49 | consumed tokens: 190054400.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:53:43 | step: 5900 | train samples/s: 276.3 | train mfu (16-bit): -1.0 | lr mean: 4.992853428120725e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.36 | consumed tokens: 193331200.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:54:11 | step: 6000 | train samples/s: 277.0 | train mfu (16-bit): -1.0 | lr mean: 4.992505637346767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.48 | consumed tokens: 196608000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:54:39 | step: 6100 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 4.9921494792215526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.27 | consumed tokens: 199884800.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T19:55:07 | step: 6200 | train samples/s: 277.5 | train mfu (16-bit): -1.0 | lr mean: 4.991785317542963e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.54 | consumed tokens: 203161600.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T19:55:35 | step: 6300 | train samples/s: 279.6 | train mfu (16-bit): -1.0 | lr mean: 4.9914127885131165e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.5 | consumed tokens: 206438400.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:56:03 | step: 6400 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.991031892132014e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.6 | consumed tokens: 209715200.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:56:31 | step: 6500 | train samples/s: 279.7 | train mfu (16-bit): -1.0 | lr mean: 4.990642992197536e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.73 | consumed tokens: 212992000.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T19:56:59 | step: 6600 | train samples/s: 273.9 | train mfu (16-bit): -1.0 | lr mean: 4.9902457249118015e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.48 | train loss last: 3.71 | consumed tokens: 216268800.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T19:57:27 | step: 6700 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9898404540726915e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.34 | consumed tokens: 219545600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T19:57:55 | step: 6800 | train samples/s: 279.7 | train mfu (16-bit): -1.0 | lr mean: 4.989426815882325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.48 | train loss last: 3.39 | consumed tokens: 222822400.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:58:23 | step: 6900 | train samples/s: 277.4 | train mfu (16-bit): -1.0 | lr mean: 4.9890048103407025e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.38 | consumed tokens: 226099200.0 | grad norm avg: 0.6 | grad norm last: 0.64 | 
2026-01-01T19:58:50 | step: 7000 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 4.9885744374478236e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.46 | train loss last: 3.48 | consumed tokens: 229376000.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:59:18 | step: 7100 | train samples/s: 273.8 | train mfu (16-bit): -1.0 | lr mean: 4.988136061001569e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.68 | consumed tokens: 232652800.0 | grad norm avg: 0.62 | grad norm last: 0.55 | 
2026-01-01T19:59:46 | step: 7200 | train samples/s: 273.3 | train mfu (16-bit): -1.0 | lr mean: 4.987689317204058e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.38 | consumed tokens: 235929600.0 | grad norm avg: 0.61 | grad norm last: 0.54 | 
2026-01-01T20:00:15 | step: 7300 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.987234569853172e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.41 | consumed tokens: 239206400.0 | grad norm avg: 0.6 | grad norm last: 0.53 | 
2026-01-01T20:00:43 | step: 7400 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.986771455151029e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.26 | consumed tokens: 242483200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:01:11 | step: 7500 | train samples/s: 273.1 | train mfu (16-bit): -1.0 | lr mean: 4.98629997309763e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.46 | train loss last: 3.29 | consumed tokens: 245760000.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T20:01:39 | step: 7600 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.985820487490855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.37 | consumed tokens: 249036800.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:02:08 | step: 7700 | train samples/s: 261.8 | train mfu (16-bit): -1.0 | lr mean: 4.985332634532824e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.54 | consumed tokens: 252313600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:02:36 | step: 7800 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.9848367780214176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.26 | consumed tokens: 255590400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:03:05 | step: 7900 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.9843325541587546e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.45 | consumed tokens: 258867200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:03:34 | step: 8000 | train samples/s: 260.3 | train mfu (16-bit): -1.0 | lr mean: 4.9838199629448354e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.37 | consumed tokens: 262144000.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:04:02 | step: 8100 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.9832993681775406e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.51 | consumed tokens: 265420800.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:04:30 | step: 8200 | train samples/s: 271.5 | train mfu (16-bit): -1.0 | lr mean: 4.9827704060589895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.32 | consumed tokens: 268697600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:04:57 | step: 8300 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.982233076589182e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 271974400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:05:26 | step: 8400 | train samples/s: 261.7 | train mfu (16-bit): -1.0 | lr mean: 4.981687743565999e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 275251200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:05:55 | step: 8500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.98113440698944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 278528000.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:06:23 | step: 8600 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.9805727030616254e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.58 | consumed tokens: 281804800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:06:52 | step: 8700 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.980002631782554e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.53 | consumed tokens: 285081600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:07:21 | step: 8800 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.9794241931522265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.42 | consumed tokens: 288358400.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:07:49 | step: 8900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.978838114766404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.57 | consumed tokens: 291635200.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:08:18 | step: 9000 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.9782433052314445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.45 | consumed tokens: 294912000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:08:47 | step: 9100 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.9776404921431094e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.36 | consumed tokens: 298188800.0 | grad norm avg: 0.6 | grad norm last: 0.59 | 
2026-01-01T20:09:15 | step: 9200 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.977029675501399e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.32 | consumed tokens: 301465600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:09:44 | step: 9300 | train samples/s: 261.3 | train mfu (16-bit): -1.0 | lr mean: 4.976410491508432e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.5 | consumed tokens: 304742400.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:10:13 | step: 9400 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.9757829401642084e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 308019200.0 | grad norm avg: 0.6 | grad norm last: 0.62 | 
2026-01-01T20:10:41 | step: 9500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9751473852666095e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.71 | consumed tokens: 311296000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:11:10 | step: 9600 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.974503826815635e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.26 | consumed tokens: 314572800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:11:39 | step: 9700 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.973851901013404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.5 | consumed tokens: 317849600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:12:07 | step: 9800 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.973191607859917e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.43 | consumed tokens: 321126400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:12:36 | step: 9900 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.972523311153054e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 324403200.0 | grad norm avg: 0.61 | grad norm last: 0.84 | 
2026-01-01T20:13:05 | step: 10000 | train samples/s: 259.9 | train mfu (16-bit): -1.0 | lr mean: 4.971846647094935e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.47 | consumed tokens: 327680000.0 | grad norm avg: 0.62 | grad norm last: 0.53 | 
2026-01-01T20:13:35 | step: 10100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.9711619794834405e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.2 | consumed tokens: 330956800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:14:04 | step: 10200 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.97046930831857e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.4 | consumed tokens: 334233600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:14:32 | step: 10300 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.969768269802444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.51 | consumed tokens: 337510400.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:15:01 | step: 10400 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.969058863935061e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.23 | consumed tokens: 340787200.0 | grad norm avg: 0.62 | grad norm last: 0.71 | 
2026-01-01T20:15:29 | step: 10500 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.968341454514302e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.16 | consumed tokens: 344064000.0 | grad norm avg: 0.6 | grad norm last: 0.56 | 
2026-01-01T20:15:58 | step: 10600 | train samples/s: 261.1 | train mfu (16-bit): -1.0 | lr mean: 4.967616041540168e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.6 | consumed tokens: 347340800.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:16:27 | step: 10700 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 4.966882261214778e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.63 | consumed tokens: 350617600.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:16:56 | step: 10800 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.966140477336012e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.46 | consumed tokens: 353894400.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T20:17:24 | step: 10900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9653903261059895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 357171200.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:17:53 | step: 11000 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9646321713225916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.54 | consumed tokens: 360448000.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:18:22 | step: 11100 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 4.9638656491879374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 363724800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:18:50 | step: 11200 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 4.9630911234999076e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.15 | consumed tokens: 367001600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:19:19 | step: 11300 | train samples/s: 259.3 | train mfu (16-bit): -1.0 | lr mean: 4.962308594258502e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.38 | consumed tokens: 370278400.0 | grad norm avg: 0.62 | grad norm last: 0.7 | 
2026-01-01T20:19:48 | step: 11400 | train samples/s: 261.4 | train mfu (16-bit): -1.0 | lr mean: 4.9615176976658404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.25 | consumed tokens: 373555200.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T20:20:17 | step: 11500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.960718797519803e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 376832000.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:20:45 | step: 11600 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.9599115300225094e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 380108800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:21:14 | step: 11700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.959096622769721e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.43 | consumed tokens: 383385600.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:21:42 | step: 11800 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.958272984367795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.21 | consumed tokens: 386662400.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:22:11 | step: 11900 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.957441706210375e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.1 | consumed tokens: 389939200.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:22:40 | step: 12000 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.956602060701698e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 393216000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T20:23:09 | step: 12100 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.955754047841765e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 396492800.0 | grad norm avg: 0.61 | grad norm last: 0.71 | 
2026-01-01T20:23:37 | step: 12200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.954898395226337e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.18 | consumed tokens: 399769600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:24:06 | step: 12300 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.954034375259653e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 403046400.0 | grad norm avg: 0.62 | grad norm last: 0.54 | 
2026-01-01T20:24:35 | step: 12400 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 4.953161987941712e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.57 | consumed tokens: 406323200.0 | grad norm avg: 0.62 | grad norm last: 0.54 | 
2026-01-01T20:25:03 | step: 12500 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.952281597070396e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.41 | consumed tokens: 409600000.0 | grad norm avg: 0.61 | grad norm last: 0.54 | 
2026-01-01T20:25:32 | step: 12600 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.951393566443585e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.26 | consumed tokens: 412876800.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T20:26:00 | step: 12700 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.950496804667637e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 416153600.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:26:29 | step: 12800 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.949592403136194e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 419430400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:26:57 | step: 12900 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.9486796342534944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.43 | consumed tokens: 422707200.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:27:26 | step: 13000 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 4.9477588618174195e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 425984000.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:27:54 | step: 13100 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.946829722030088e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.17 | consumed tokens: 429260800.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:28:23 | step: 13200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.945892942487262e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.21 | consumed tokens: 432537600.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:28:52 | step: 13300 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 4.94494779559318e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 2.98 | consumed tokens: 435814400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:29:20 | step: 13400 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.943994645145722e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 439091200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:29:49 | step: 13500 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.9430331273470074e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.32 | consumed tokens: 442368000.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:30:17 | step: 13600 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.942063969792798e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.55 | consumed tokens: 445644800.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:30:46 | step: 13700 | train samples/s: 261.9 | train mfu (16-bit): -1.0 | lr mean: 4.9410864448873326e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.38 | consumed tokens: 448921600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:31:15 | step: 13800 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.9401009164284915e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 452198400.0 | grad norm avg: 0.6 | grad norm last: 0.66 | 
2026-01-01T20:31:43 | step: 13900 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.939107384416275e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.15 | consumed tokens: 455475200.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:32:12 | step: 14000 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.938105485052802e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.16 | consumed tokens: 458752000.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:32:41 | step: 14100 | train samples/s: 264.1 | train mfu (16-bit): -1.0 | lr mean: 4.937095945933834e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 462028800.0 | grad norm avg: 0.6 | grad norm last: 0.56 | 
2026-01-01T20:33:09 | step: 14200 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.9360780394636095e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.29 | consumed tokens: 465305600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:33:38 | step: 14300 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 4.9350521294400096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 468582400.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:34:07 | step: 14400 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.934018215863034e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 471859200.0 | grad norm avg: 0.6 | grad norm last: 0.52 | 
2026-01-01T20:34:35 | step: 14500 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.932976298732683e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 475136000.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:35:04 | step: 14600 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9319263780489564e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.48 | consumed tokens: 478412800.0 | grad norm avg: 0.61 | grad norm last: 0.7 | 
2026-01-01T20:35:33 | step: 14700 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.9308680900139734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 481689600.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T20:36:01 | step: 14800 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.9298021622234955e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.14 | consumed tokens: 484966400.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T20:36:30 | step: 14900 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.9287278670817614e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.26 | consumed tokens: 488243200.0 | grad norm avg: 0.61 | grad norm last: 0.71 | 
2026-01-01T20:36:58 | step: 15000 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.9276455683866516e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 491520000.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:37:28 | step: 15100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.926555266138166e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.31 | consumed tokens: 494796800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:37:57 | step: 15200 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 4.925456960336305e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.23 | consumed tokens: 498073600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:38:26 | step: 15300 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.9243506509810686e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 501350400.0 | grad norm avg: 0.61 | grad norm last: 0.68 | 
2026-01-01T20:38:55 | step: 15400 | train samples/s: 262.8 | train mfu (16-bit): -1.0 | lr mean: 4.9232363380724564e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.3 | consumed tokens: 504627200.0 | grad norm avg: 0.6 | grad norm last: 0.54 | 
2026-01-01T20:39:24 | step: 15500 | train samples/s: 262.7 | train mfu (16-bit): -1.0 | lr mean: 4.9221140216104686e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.45 | consumed tokens: 507904000.0 | grad norm avg: 0.61 | grad norm last: 0.68 | 
2026-01-01T20:39:52 | step: 15600 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.920983701595105e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.42 | consumed tokens: 511180800.0 | grad norm avg: 0.62 | grad norm last: 0.53 | 
2026-01-01T20:40:21 | step: 15700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.919845378026366e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.36 | consumed tokens: 514457600.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:40:49 | step: 15800 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.9186990509042516e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.32 | consumed tokens: 517734400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:41:18 | step: 15900 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.9175447202287614e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.37 | consumed tokens: 521011200.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:41:46 | step: 16000 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.9163823859998956e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.22 | consumed tokens: 524288000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:42:15 | step: 16100 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.915212048217654e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.32 | consumed tokens: 527564800.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T20:42:44 | step: 16200 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.914033706882037e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.24 | consumed tokens: 530841600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:43:12 | step: 16300 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.9128473619930446e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.48 | consumed tokens: 534118400.0 | grad norm avg: 0.62 | grad norm last: 0.67 | 
2026-01-01T20:43:41 | step: 16400 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.9116530135506764e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 537395200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T20:44:10 | step: 16500 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.9104506615549326e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.45 | consumed tokens: 540672000.0 | grad norm avg: 0.62 | grad norm last: 0.66 | 
2026-01-01T20:44:39 | step: 16600 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.909240306005813e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.59 | consumed tokens: 543948800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:45:08 | step: 16700 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 4.908021946903318e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.21 | consumed tokens: 547225600.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:45:37 | step: 16800 | train samples/s: 262.0 | train mfu (16-bit): -1.0 | lr mean: 4.9067955842474476e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.08 | consumed tokens: 550502400.0 | grad norm avg: 0.6 | grad norm last: 0.62 | 
2026-01-01T20:46:05 | step: 16900 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.905561581836082e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.52 | consumed tokens: 553779200.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:46:34 | step: 17000 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.904319575871341e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 2.96 | consumed tokens: 557056000.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:47:03 | step: 17100 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.9030692025553435e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.14 | consumed tokens: 560332800.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:47:31 | step: 17200 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.901811189483851e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.06 | consumed tokens: 563609600.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T20:48:00 | step: 17300 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.900545172858983e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.22 | consumed tokens: 566886400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:48:29 | step: 17400 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 4.89927115268074e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.22 | consumed tokens: 570163200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:48:57 | step: 17500 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.8979894927470013e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.54 | consumed tokens: 573440000.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:49:25 | step: 17600 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.8966994654620066e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 2.92 | consumed tokens: 576716800.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:49:54 | step: 17700 | train samples/s: 262.8 | train mfu (16-bit): -1.0 | lr mean: 4.895401798421517e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.29 | consumed tokens: 579993600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:50:23 | step: 17800 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.894096127827652e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.33 | consumed tokens: 583270400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T20:50:52 | step: 17900 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.892782453680411e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.23 | consumed tokens: 586547200.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:51:20 | step: 18000 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.8914607759797946e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.22 | consumed tokens: 589824000.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:51:49 | step: 18100 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.890131458523683e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.29 | consumed tokens: 593100800.0 | grad norm avg: 0.6 | grad norm last: 0.68 | 
2026-01-01T20:52:18 | step: 18200 | train samples/s: 262.3 | train mfu (16-bit): -1.0 | lr mean: 4.888794137514196e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.0 | consumed tokens: 596377600.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:52:46 | step: 18300 | train samples/s: 272.7 | train mfu (16-bit): -1.0 | lr mean: 4.887448812951334e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.61 | consumed tokens: 599654400.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T20:53:14 | step: 18400 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.886095484835096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.27 | consumed tokens: 602931200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:53:43 | step: 18500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.884734516963363e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.17 | consumed tokens: 606208000.0 | grad norm avg: 0.63 | grad norm last: 0.68 | 
2026-01-01T20:54:11 | step: 18600 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.883365545538254e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.67 | consumed tokens: 609484800.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:54:40 | step: 18700 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.8819889343576506e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.17 | consumed tokens: 612761600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:55:08 | step: 18800 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.880603955825791e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.16 | consumed tokens: 616038400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:55:37 | step: 18900 | train samples/s: 260.4 | train mfu (16-bit): -1.0 | lr mean: 4.879211337538436e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.32 | consumed tokens: 619315200.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:56:06 | step: 19000 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 4.8778110794955865e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.42 | consumed tokens: 622592000.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:56:34 | step: 19100 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 4.8764024541014805e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.15 | consumed tokens: 625868800.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T20:57:02 | step: 19200 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.8749865527497604e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.33 | consumed tokens: 629145600.0 | grad norm avg: 0.62 | grad norm last: 0.7 | 
2026-01-01T20:57:31 | step: 19300 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.873562284046784e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.43 | consumed tokens: 632422400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T20:57:59 | step: 19400 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.872130375588313e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.4 | consumed tokens: 635699200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:58:27 | step: 19500 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.8706908273743466e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.37 | consumed tokens: 638976000.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:58:56 | step: 19600 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.869242911809124e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.46 | consumed tokens: 642252800.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T20:59:25 | step: 19700 | train samples/s: 262.1 | train mfu (16-bit): -1.0 | lr mean: 4.8677877202862874e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.53 | consumed tokens: 645529600.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:59:54 | step: 19800 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.8663241614121944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.21 | consumed tokens: 648806400.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T21:00:22 | step: 19900 | train samples/s: 267.6 | train mfu (16-bit): -1.0 | lr mean: 4.864853326580487e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.21 | consumed tokens: 652083200.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T21:00:50 | step: 20000 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.863374124397524e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.14 | consumed tokens: 655360000.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:01:21 | step: 20100 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.861887282459065e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.12 | consumed tokens: 658636800.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:01:49 | step: 20200 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.860392800765112e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.27 | consumed tokens: 661913600.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:02:18 | step: 20300 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.858890315517783e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.77 | consumed tokens: 665190400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:02:46 | step: 20400 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 4.8573801905149594e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.33 | consumed tokens: 668467200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:03:15 | step: 20500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.855862425756641e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.46 | consumed tokens: 671744000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:03:43 | step: 20600 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.8543366574449465e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.06 | consumed tokens: 675020800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:04:11 | step: 20700 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.8528028855798766e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.04 | consumed tokens: 678297600.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:04:40 | step: 20800 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.851261473959312e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.62 | consumed tokens: 681574400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:05:09 | step: 20900 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.849712422583252e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.18 | consumed tokens: 684851200.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T21:05:37 | step: 21000 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.848155367653817e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.41 | consumed tokens: 688128000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T21:06:06 | step: 21100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.846590672968887e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.26 | consumed tokens: 691404800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:06:34 | step: 21200 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.845018338528462e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.21 | consumed tokens: 694681600.0 | grad norm avg: 0.62 | grad norm last: 0.55 | 
2026-01-01T21:07:03 | step: 21300 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.843438000534661e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.52 | consumed tokens: 697958400.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T21:07:31 | step: 21400 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.8418500227853656e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.4 | consumed tokens: 701235200.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:08:00 | step: 21500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.840254405280575e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.15 | consumed tokens: 704512000.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T21:08:28 | step: 21600 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.838650784222409e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.17 | consumed tokens: 707788800.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T21:08:57 | step: 21700 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.837039523408748e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.45 | consumed tokens: 711065600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:09:25 | step: 21800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.8354206228395924e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 2.93 | consumed tokens: 714342400.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T21:09:54 | step: 21900 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.833794082514942e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.33 | consumed tokens: 717619200.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T21:10:22 | step: 22000 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.8321595386369154e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.98 | consumed tokens: 720896000.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:10:51 | step: 22100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.830517355003394e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.12 | consumed tokens: 724172800.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:11:20 | step: 22200 | train samples/s: 257.8 | train mfu (16-bit): -1.0 | lr mean: 4.828867531614378e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.43 | consumed tokens: 727449600.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:11:49 | step: 22300 | train samples/s: 259.2 | train mfu (16-bit): -1.0 | lr mean: 4.827210068469867e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.74 | consumed tokens: 730726400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:12:17 | step: 22400 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.825544965569861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 734003200.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:12:45 | step: 22500 | train samples/s: 267.7 | train mfu (16-bit): -1.0 | lr mean: 4.82387185911648e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.23 | consumed tokens: 737280000.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T21:13:14 | step: 22600 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.822191476705484e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 740556800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:13:42 | step: 22700 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 4.820503090741113e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.22 | consumed tokens: 743833600.0 | grad norm avg: 0.61 | grad norm last: 0.69 | 
2026-01-01T21:14:11 | step: 22800 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.818807065021247e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.48 | consumed tokens: 747110400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:14:39 | step: 22900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.8171033995458856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.24 | consumed tokens: 750387200.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:15:09 | step: 23000 | train samples/s: 252.8 | train mfu (16-bit): -1.0 | lr mean: 4.81539209431503e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.02 | consumed tokens: 753664000.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:15:37 | step: 23100 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 4.813673149328679e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.89 | consumed tokens: 756940800.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T21:16:06 | step: 23200 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.811946564586833e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.57 | consumed tokens: 760217600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:16:34 | step: 23300 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.8102123400894925e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.65 | consumed tokens: 763494400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:17:02 | step: 23400 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 4.808470112038776e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.14 | consumed tokens: 766771200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:17:31 | step: 23500 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 4.806720608030446e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.05 | consumed tokens: 770048000.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T21:17:59 | step: 23600 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.8049634642666206e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.04 | consumed tokens: 773324800.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:18:28 | step: 23700 | train samples/s: 262.0 | train mfu (16-bit): -1.0 | lr mean: 4.8031986807473004e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.67 | consumed tokens: 776601600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T21:18:57 | step: 23800 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.801426257472485e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.49 | consumed tokens: 779878400.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T21:19:25 | step: 23900 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 4.799646194442175e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.36 | consumed tokens: 783155200.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:19:54 | step: 24000 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 4.7978584916563705e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.3 | consumed tokens: 786432000.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:20:22 | step: 24100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.796063149115071e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.33 | consumed tokens: 789708800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T21:20:51 | step: 24200 | train samples/s: 259.8 | train mfu (16-bit): -1.0 | lr mean: 4.794260166818276e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.26 | consumed tokens: 792985600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T21:21:20 | step: 24300 | train samples/s: 262.6 | train mfu (16-bit): -1.0 | lr mean: 4.792449908563867e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.22 | consumed tokens: 796262400.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:21:48 | step: 24400 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.790631646756083e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.09 | consumed tokens: 799539200.0 | grad norm avg: 0.63 | grad norm last: 0.56 | 
2026-01-01T21:22:17 | step: 24500 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.788806108990684e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.2 | consumed tokens: 802816000.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:22:45 | step: 24600 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.7869729314697906e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.19 | consumed tokens: 806092800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:23:14 | step: 24700 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.785132114193402e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.15 | consumed tokens: 809369600.0 | grad norm avg: 0.62 | grad norm last: 0.66 | 
2026-01-01T21:23:42 | step: 24800 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.783283657161519e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.44 | consumed tokens: 812646400.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:24:11 | step: 24900 | train samples/s: 264.1 | train mfu (16-bit): -1.0 | lr mean: 4.7814279241720214e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.33 | consumed tokens: 815923200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:24:40 | step: 25000 | train samples/s: 261.5 | train mfu (16-bit): -1.0 | lr mean: 4.779564551427029e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.19 | consumed tokens: 819200000.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:25:10 | step: 25100 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.777693538926542e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.14 | consumed tokens: 822476800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T21:25:38 | step: 25200 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.7758148866705596e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.2 | consumed tokens: 825753600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T21:26:06 | step: 25300 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.773928958456963e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.37 | consumed tokens: 829030400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:26:35 | step: 25400 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.772035390487872e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.98 | consumed tokens: 832307200.0 | grad norm avg: 0.62 | grad norm last: 0.67 | 
2026-01-01T21:27:03 | step: 25500 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.770134182763286e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.52 | consumed tokens: 835584000.0 | grad norm avg: 0.64 | grad norm last: 0.75 | 
2026-01-01T21:27:31 | step: 25600 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.7682256990810856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.58 | consumed tokens: 838860800.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:28:00 | step: 25700 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.7663095756433904e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.31 | consumed tokens: 842137600.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T21:28:29 | step: 25800 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.7643858124502e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.25 | consumed tokens: 845414400.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T21:28:57 | step: 25900 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.762454773299396e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.3 | consumed tokens: 848691200.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:29:26 | step: 26000 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.760516094393097e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.55 | consumed tokens: 851968000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:29:54 | step: 26100 | train samples/s: 263.9 | train mfu (16-bit): -1.0 | lr mean: 4.7585701395291835e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 855244800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:30:23 | step: 26200 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.756616544909775e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.43 | consumed tokens: 858521600.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:30:51 | step: 26300 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.754655310534872e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.05 | consumed tokens: 861798400.0 | grad norm avg: 0.63 | grad norm last: 0.56 | 
2026-01-01T21:31:20 | step: 26400 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.7526871640002355e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.2 | consumed tokens: 865075200.0 | grad norm avg: 0.64 | grad norm last: 0.66 | 
2026-01-01T21:31:49 | step: 26500 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 4.750711013912223e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.21 | consumed tokens: 868352000.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:32:17 | step: 26600 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.748727587866597e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.37 | consumed tokens: 871628800.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:32:46 | step: 26700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.746736885863356e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.3 | consumed tokens: 874905600.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T21:33:14 | step: 26800 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.744738544104621e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.17 | consumed tokens: 878182400.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T21:33:43 | step: 26900 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 4.742732926388271e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.09 | consumed tokens: 881459200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:34:12 | step: 27000 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.7407196689164266e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 2.93 | consumed tokens: 884736000.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T21:34:40 | step: 27100 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 4.738699135486968e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.34 | consumed tokens: 888012800.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:35:09 | step: 27200 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.736671326099895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.39 | consumed tokens: 891289600.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:35:37 | step: 27300 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.734635876957327e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.36 | consumed tokens: 894566400.0 | grad norm avg: 0.63 | grad norm last: 0.56 | 
2026-01-01T21:36:06 | step: 27400 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.732593151857145e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.21 | consumed tokens: 897843200.0 | grad norm avg: 0.63 | grad norm last: 0.71 | 
2026-01-01T21:36:34 | step: 27500 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.730543150799349e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.28 | consumed tokens: 901120000.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:37:03 | step: 27600 | train samples/s: 260.4 | train mfu (16-bit): -1.0 | lr mean: 4.7284858737839386e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.39 | consumed tokens: 904396800.0 | grad norm avg: 0.62 | grad norm last: 0.66 | 
2026-01-01T21:37:31 | step: 27700 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.726420957013033e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.35 | consumed tokens: 907673600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T21:38:00 | step: 27800 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.724348764284514e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.18 | consumed tokens: 910950400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:38:29 | step: 27900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.7222689318004996e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.04 | consumed tokens: 914227200.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T21:38:57 | step: 28000 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 4.720182187156752e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 2.81 | consumed tokens: 917504000.0 | grad norm avg: 0.64 | grad norm last: 0.59 | 
2026-01-01T21:39:26 | step: 28100 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.718087802757509e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.13 | consumed tokens: 920780800.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T21:39:54 | step: 28200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.715986142400652e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 924057600.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:40:23 | step: 28300 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.713877206086181e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.04 | consumed tokens: 927334400.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T21:40:52 | step: 28400 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.711760993814096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.08 | consumed tokens: 930611200.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:41:20 | step: 28500 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.7096375055843964e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.45 | consumed tokens: 933888000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:41:49 | step: 28600 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.707506377599202e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.32 | consumed tokens: 937164800.0 | grad norm avg: 0.64 | grad norm last: 0.74 | 
2026-01-01T21:42:17 | step: 28700 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.705368337454274e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.0 | consumed tokens: 940441600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T21:42:45 | step: 28800 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.7032226575538516e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.16 | consumed tokens: 943718400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:43:13 | step: 28900 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.701069701695815e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.08 | consumed tokens: 946995200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:43:42 | step: 29000 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.6989098336780444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.21 | consumed tokens: 950272000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:44:11 | step: 29100 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.696742325904779e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 2.97 | consumed tokens: 953548800.0 | grad norm avg: 0.63 | grad norm last: 0.77 | 
2026-01-01T21:44:40 | step: 29200 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.6945679059717804e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.09 | consumed tokens: 956825600.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T21:45:08 | step: 29300 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.692385846283287e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.05 | consumed tokens: 960102400.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T21:45:36 | step: 29400 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 4.69019687443506e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.76 | consumed tokens: 963379200.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:46:05 | step: 29500 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.688000262831338e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.1 | consumed tokens: 966656000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:46:34 | step: 29600 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.685796739067882e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.24 | consumed tokens: 969932800.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:47:02 | step: 29700 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.683585939346813e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.52 | consumed tokens: 973209600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:47:31 | step: 29800 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.681367863668129e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.54 | consumed tokens: 976486400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:48:00 | step: 29900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.679142512031831e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.49 | consumed tokens: 979763200.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:48:28 | step: 30000 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.676909884437919e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 983040000.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:48:58 | step: 30100 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.674669980886392e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.38 | consumed tokens: 986316800.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T21:49:27 | step: 30200 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 4.6724231651751325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.18 | consumed tokens: 989593600.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:49:56 | step: 30300 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 4.6701690735062584e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 992870400.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:50:24 | step: 30400 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.66790770587977e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.99 | consumed tokens: 996147200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:50:53 | step: 30500 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.6656394260935485e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.29 | consumed tokens: 999424000.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T21:51:21 | step: 30600 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.663363870349713e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.44 | consumed tokens: 1002700800.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:51:50 | step: 30700 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.6610810386482626e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.29 | consumed tokens: 1005977600.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:52:18 | step: 30800 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 4.6587909309891984e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.13 | consumed tokens: 1009254400.0 | grad norm avg: 0.62 | grad norm last: 0.57 | 
2026-01-01T21:52:46 | step: 30900 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.656493911170401e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.17 | consumed tokens: 1012531200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:53:15 | step: 31000 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.654189615393989e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.51 | consumed tokens: 1015808000.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T21:53:44 | step: 31100 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.6518784074578434e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.03 | consumed tokens: 1019084800.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:54:12 | step: 31200 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.649559923564084e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.3 | consumed tokens: 1022361600.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:54:41 | step: 31300 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.64723416371271e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.01 | consumed tokens: 1025638400.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:55:09 | step: 31400 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.644901491701603e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.34 | consumed tokens: 1028915200.0 | grad norm avg: 0.62 | grad norm last: 0.56 | 
2026-01-01T21:55:37 | step: 31500 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.642561907530762e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.22 | consumed tokens: 1032192000.0 | grad norm avg: 0.62 | grad norm last: 0.74 | 
2026-01-01T21:56:06 | step: 31600 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.6402150474023074e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.1 | consumed tokens: 1035468800.0 | grad norm avg: 0.62 | grad norm last: 0.57 | 
2026-01-01T21:56:35 | step: 31700 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.6378609113162383e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 1038745600.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:57:03 | step: 31800 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.635499863070436e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.38 | consumed tokens: 1042022400.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:57:32 | step: 31900 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.6331319026649e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.13 | consumed tokens: 1045299200.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T21:58:00 | step: 32000 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.6307566663017496e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.35 | consumed tokens: 1048576000.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:58:28 | step: 32100 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.628374517778866e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.27 | consumed tokens: 1051852800.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:58:57 | step: 32200 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.625985093298368e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.52 | consumed tokens: 1055129600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T21:59:26 | step: 32300 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.6235891204560176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 2.7 | consumed tokens: 1058406400.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:59:54 | step: 32400 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.621185507858172e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 1061683200.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T22:00:23 | step: 32500 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.618775346898474e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.13 | consumed tokens: 1064960000.0 | grad norm avg: 0.63 | grad norm last: 0.65 | 
2026-01-01T22:00:51 | step: 32600 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.6163579099811614e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.27 | consumed tokens: 1068236800.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T22:01:20 | step: 32700 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.6139335609041154e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.2 | consumed tokens: 1071513600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:01:48 | step: 32800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.611502299667336e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.92 | consumed tokens: 1074790400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:02:17 | step: 32900 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.609064126270823e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.21 | consumed tokens: 1078067200.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:02:45 | step: 33000 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.606618676916696e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.38 | consumed tokens: 1081344000.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T22:03:14 | step: 33100 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 4.6041663154028356e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.05 | consumed tokens: 1084620800.0 | grad norm avg: 0.63 | grad norm last: 0.69 | 
2026-01-01T22:03:42 | step: 33200 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.601707405527122e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.17 | consumed tokens: 1087897600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:04:11 | step: 33300 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.599241219693795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.44 | consumed tokens: 1091174400.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:04:39 | step: 33400 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.596768121700734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.07 | consumed tokens: 1094451200.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:05:08 | step: 33500 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 4.594287747750059e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.29 | consumed tokens: 1097728000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T22:05:37 | step: 33600 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.591800825437531e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.96 | consumed tokens: 1101004800.0 | grad norm avg: 0.63 | grad norm last: 0.76 | 
2026-01-01T22:06:05 | step: 33700 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.5893069909652695e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.0 | consumed tokens: 1104281600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T22:06:34 | step: 33800 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.586806244333275e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.13 | consumed tokens: 1107558400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T22:07:03 | step: 33900 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.5842985855415463e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.35 | consumed tokens: 1110835200.0 | grad norm avg: 0.63 | grad norm last: 0.71 | 
2026-01-01T22:07:31 | step: 34000 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.581783650792204e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.17 | consumed tokens: 1114112000.0 | grad norm avg: 0.63 | grad norm last: 0.73 | 
2026-01-01T22:08:00 | step: 34100 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.5792621676810086e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.05 | consumed tokens: 1117388800.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:08:28 | step: 34200 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.57673377241008e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.5 | consumed tokens: 1120665600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T22:08:57 | step: 34300 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.5741984649794176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.11 | consumed tokens: 1123942400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T22:09:25 | step: 34400 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.5716566091869026e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.05 | consumed tokens: 1127219200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:09:54 | step: 34500 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.5691074774367735e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 1130496000.0 | grad norm avg: 0.62 | grad norm last: 0.57 | 
2026-01-01T22:10:22 | step: 34600 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.5665517973247916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.38 | consumed tokens: 1133772800.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T22:10:51 | step: 34700 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.5639888412551954e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.5 | consumed tokens: 1137049600.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T22:11:19 | step: 34800 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.5614193368237466e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.09 | consumed tokens: 1140326400.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T22:11:48 | step: 34900 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.558843284030445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.07 | consumed tokens: 1143603200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T22:12:16 | step: 35000 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.556259955279529e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 2.97 | consumed tokens: 1146880000.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:12:47 | step: 35100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.5536700781667605e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.27 | consumed tokens: 1150156800.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T22:13:15 | step: 35200 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 4.5510732888942584e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.28 | consumed tokens: 1153433600.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:13:44 | step: 35300 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.5484699512599036e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.08 | consumed tokens: 1156710400.0 | grad norm avg: 0.63 | grad norm last: 0.65 | 
2026-01-01T22:14:12 | step: 35400 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.545859701465815e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 2.95 | consumed tokens: 1159987200.0 | grad norm avg: 0.63 | grad norm last: 0.68 | 
2026-01-01T22:14:41 | step: 35500 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.5432425395119935e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.12 | consumed tokens: 1163264000.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:15:09 | step: 35600 | train samples/s: 262.9 | train mfu (16-bit): -1.0 | lr mean: 4.540618829196319e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.24 | consumed tokens: 1166540800.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
