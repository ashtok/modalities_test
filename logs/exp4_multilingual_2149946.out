==========================================
Experiment 4: Fine-tuning GPT-2 on 5 languages with 4 GPUs
Job ID: 2149946
Node: jnfat02
Start time: Thu Jan  1 07:25:51 PM CET 2026
==========================================
Thu Jan  1 19:25:52 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:05:00.0 Off |                    0 |
| N/A   29C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:06:00.0 Off |                    0 |
| N/A   30C    P8             32W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA L40S                    On  |   00000000:45:00.0 Off |                    0 |
| N/A   29C    P8             34W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA L40S                    On  |   00000000:46:00.0 Off |                    0 |
| N/A   30C    P8             33W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 1 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 2 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 3 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 31,109,952 parameters): weight_decay = 0.01
=> all (148 modules with 31,109,952 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2026-01-01 19:26:12.213544.



======================== Training Report ========================
Training target: 
	num_target_tokens: 5713166336
	num_target_steps: 174352 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 4
CUDA environment settings: 
	local_rank: 0
	world_size: 4
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (5713177600) does not match the number of target tokens (5713166336). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (174352) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (174352) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (174352) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2026-01-01 19:26:12.213815.
2026-01-01T19:26:42 | step: 100 | train samples/s: 244.7 | train mfu (16-bit): -1.0 | lr mean: 5.36468678546953e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.29 | train loss last: 4.28 | consumed tokens: 3276800.0 | grad norm avg: 1.8 | grad norm last: 1.68 | 
2026-01-01T19:27:11 | step: 200 | train samples/s: 254.1 | train mfu (16-bit): -1.0 | lr mean: 6.446925453928998e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.18 | train loss last: 4.29 | consumed tokens: 6553600.0 | grad norm avg: 1.39 | grad norm last: 1.3 | 
2026-01-01T19:27:39 | step: 300 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 8.21163303044159e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.14 | train loss last: 4.04 | consumed tokens: 9830400.0 | grad norm avg: 1.32 | grad norm last: 1.36 | 
2026-01-01T19:28:07 | step: 400 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 1.0601604117255192e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.1 | train loss last: 3.94 | consumed tokens: 13107200.0 | grad norm avg: 1.25 | grad norm last: 1.27 | 
2026-01-01T19:28:36 | step: 500 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 1.3539362953451928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.08 | train loss last: 4.22 | consumed tokens: 16384000.0 | grad norm avg: 1.2 | grad norm last: 1.11 | 
2026-01-01T19:29:04 | step: 600 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 1.692967998678796e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.06 | train loss last: 3.68 | consumed tokens: 19660800.0 | grad norm avg: 1.14 | grad norm last: 1.09 | 
2026-01-01T19:29:32 | step: 700 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 2.0662650058511645e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.02 | train loss last: 3.83 | consumed tokens: 22937600.0 | grad norm avg: 1.06 | grad norm last: 1.02 | 
2026-01-01T19:29:59 | step: 800 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 2.461726217006799e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.98 | train loss last: 3.96 | consumed tokens: 26214400.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2026-01-01T19:30:27 | step: 900 | train samples/s: 278.1 | train mfu (16-bit): -1.0 | lr mean: 2.86653248622315e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.95 | train loss last: 3.73 | consumed tokens: 29491200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2026-01-01T19:30:55 | step: 1000 | train samples/s: 280.9 | train mfu (16-bit): -1.0 | lr mean: 3.2675612601451576e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.91 | train loss last: 3.91 | consumed tokens: 32768000.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2026-01-01T19:31:23 | step: 1100 | train samples/s: 278.0 | train mfu (16-bit): -1.0 | lr mean: 3.651812221505679e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.9 | train loss last: 3.89 | consumed tokens: 36044800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T19:31:51 | step: 1200 | train samples/s: 274.7 | train mfu (16-bit): -1.0 | lr mean: 4.006829476566054e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.87 | train loss last: 3.95 | consumed tokens: 39321600.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-01T19:32:19 | step: 1300 | train samples/s: 280.8 | train mfu (16-bit): -1.0 | lr mean: 4.321104643167928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.85 | train loss last: 4.15 | consumed tokens: 42598400.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:32:47 | step: 1400 | train samples/s: 281.0 | train mfu (16-bit): -1.0 | lr mean: 4.5844499254599214e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.83 | train loss last: 4.06 | consumed tokens: 45875200.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:33:14 | step: 1500 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.788328806171194e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.82 | train loss last: 3.84 | consumed tokens: 49152000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T19:33:42 | step: 1600 | train samples/s: 277.4 | train mfu (16-bit): -1.0 | lr mean: 4.926131805405021e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.79 | train loss last: 3.86 | consumed tokens: 52428800.0 | grad norm avg: 0.67 | grad norm last: 0.66 | 
2026-01-01T19:34:10 | step: 1700 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.9933918489841744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.77 | train loss last: 3.99 | consumed tokens: 55705600.0 | grad norm avg: 0.66 | grad norm last: 0.66 | 
2026-01-01T19:34:38 | step: 1800 | train samples/s: 275.1 | train mfu (16-bit): -1.0 | lr mean: 4.999998782295734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.76 | train loss last: 3.85 | consumed tokens: 58982400.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T19:35:06 | step: 1900 | train samples/s: 277.8 | train mfu (16-bit): -1.0 | lr mean: 4.999989687348716e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.75 | train loss last: 3.92 | consumed tokens: 62259200.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T19:35:34 | step: 2000 | train samples/s: 280.9 | train mfu (16-bit): -1.0 | lr mean: 4.9999725888483226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.74 | train loss last: 3.65 | consumed tokens: 65536000.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T19:36:02 | step: 2100 | train samples/s: 280.8 | train mfu (16-bit): -1.0 | lr mean: 4.999947122996673e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.71 | train loss last: 3.55 | consumed tokens: 68812800.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T19:36:29 | step: 2200 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.999913289793767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.72 | train loss last: 3.6 | consumed tokens: 72089600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T19:36:57 | step: 2300 | train samples/s: 281.1 | train mfu (16-bit): -1.0 | lr mean: 4.9998714530374855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.7 | train loss last: 3.63 | consumed tokens: 75366400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:37:25 | step: 2400 | train samples/s: 275.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998212489299476e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.69 | train loss last: 3.86 | consumed tokens: 78643200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:37:53 | step: 2500 | train samples/s: 275.0 | train mfu (16-bit): -1.0 | lr mean: 4.9997626774711534e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.67 | train loss last: 3.54 | consumed tokens: 81920000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:38:21 | step: 2600 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.999695738661103e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.67 | train loss last: 3.5 | consumed tokens: 85196800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T19:38:49 | step: 2700 | train samples/s: 280.2 | train mfu (16-bit): -1.0 | lr mean: 4.999620796297677e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.64 | consumed tokens: 88473600.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T19:39:17 | step: 2800 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.9995374865829945e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.66 | train loss last: 3.95 | consumed tokens: 91750400.0 | grad norm avg: 0.59 | grad norm last: 0.63 | 
2026-01-01T19:39:44 | step: 2900 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.999445809517056e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.64 | train loss last: 3.51 | consumed tokens: 95027200.0 | grad norm avg: 0.61 | grad norm last: 0.69 | 
2026-01-01T19:40:12 | step: 3000 | train samples/s: 280.2 | train mfu (16-bit): -1.0 | lr mean: 4.999345765099861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.56 | consumed tokens: 98304000.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T19:40:40 | step: 3100 | train samples/s: 275.2 | train mfu (16-bit): -1.0 | lr mean: 4.99923771712929e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.93 | consumed tokens: 101580800.0 | grad norm avg: 0.6 | grad norm last: 0.59 | 
2026-01-01T19:41:09 | step: 3200 | train samples/s: 274.7 | train mfu (16-bit): -1.0 | lr mean: 4.999121301807463e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.62 | train loss last: 3.7 | consumed tokens: 104857600.0 | grad norm avg: 0.59 | grad norm last: 0.58 | 
2026-01-01T19:41:36 | step: 3300 | train samples/s: 280.1 | train mfu (16-bit): -1.0 | lr mean: 4.99899651913438e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.61 | train loss last: 3.54 | consumed tokens: 108134400.0 | grad norm avg: 0.59 | grad norm last: 0.6 | 
2026-01-01T19:42:04 | step: 3400 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.998863732907921e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.85 | consumed tokens: 111411200.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:42:32 | step: 3500 | train samples/s: 280.4 | train mfu (16-bit): -1.0 | lr mean: 4.998722579330206e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.68 | consumed tokens: 114688000.0 | grad norm avg: 0.59 | grad norm last: 0.54 | 
2026-01-01T19:43:00 | step: 3600 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.9985730584012344e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.45 | consumed tokens: 117964800.0 | grad norm avg: 0.59 | grad norm last: 0.59 | 
2026-01-01T19:43:27 | step: 3700 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.998415170121007e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.59 | train loss last: 3.61 | consumed tokens: 121241600.0 | grad norm avg: 0.58 | grad norm last: 0.55 | 
2026-01-01T19:43:55 | step: 3800 | train samples/s: 277.7 | train mfu (16-bit): -1.0 | lr mean: 4.9982489144895226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.68 | consumed tokens: 124518400.0 | grad norm avg: 0.59 | grad norm last: 0.62 | 
2026-01-01T19:44:24 | step: 3900 | train samples/s: 272.7 | train mfu (16-bit): -1.0 | lr mean: 4.998074655304663e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.46 | consumed tokens: 127795200.0 | grad norm avg: 0.6 | grad norm last: 0.71 | 
2026-01-01T19:44:51 | step: 4000 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.997892028768547e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.56 | consumed tokens: 131072000.0 | grad norm avg: 0.59 | grad norm last: 0.59 | 
2026-01-01T19:45:19 | step: 4100 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.997701398679055e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.58 | train loss last: 3.52 | consumed tokens: 134348800.0 | grad norm avg: 0.59 | grad norm last: 0.58 | 
2026-01-01T19:45:47 | step: 4200 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.9975020374404266e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.57 | consumed tokens: 137625600.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:46:15 | step: 4300 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.9972946726484224e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.38 | consumed tokens: 140902400.0 | grad norm avg: 0.59 | grad norm last: 0.65 | 
2026-01-01T19:46:42 | step: 4400 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.9970793043030426e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.41 | consumed tokens: 144179200.0 | grad norm avg: 0.59 | grad norm last: 0.62 | 
2026-01-01T19:47:10 | step: 4500 | train samples/s: 278.0 | train mfu (16-bit): -1.0 | lr mean: 4.996855204808526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.51 | consumed tokens: 147456000.0 | grad norm avg: 0.59 | grad norm last: 0.6 | 
2026-01-01T19:47:39 | step: 4600 | train samples/s: 272.6 | train mfu (16-bit): -1.0 | lr mean: 4.996623101760633e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.54 | train loss last: 3.45 | consumed tokens: 150732800.0 | grad norm avg: 0.59 | grad norm last: 0.67 | 
2026-01-01T19:48:07 | step: 4700 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9963826313614845e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.48 | consumed tokens: 154009600.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:48:34 | step: 4800 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.9961337936110795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.54 | train loss last: 3.48 | consumed tokens: 157286400.0 | grad norm avg: 0.6 | grad norm last: 0.6 | 
2026-01-01T19:49:02 | step: 4900 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.995876952307299e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.55 | train loss last: 3.44 | consumed tokens: 160563200.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:49:30 | step: 5000 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.995611743652262e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.58 | consumed tokens: 163840000.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T19:50:00 | step: 5100 | train samples/s: 276.7 | train mfu (16-bit): -1.0 | lr mean: 4.9953381676459685e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.28 | consumed tokens: 167116800.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:50:28 | step: 5200 | train samples/s: 276.4 | train mfu (16-bit): -1.0 | lr mean: 4.9950565880862996e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.61 | consumed tokens: 170393600.0 | grad norm avg: 0.6 | grad norm last: 0.64 | 
2026-01-01T19:50:56 | step: 5300 | train samples/s: 276.5 | train mfu (16-bit): -1.0 | lr mean: 4.9947666411753744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.5 | consumed tokens: 173670400.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T19:51:24 | step: 5400 | train samples/s: 277.2 | train mfu (16-bit): -1.0 | lr mean: 4.994468326913193e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.55 | consumed tokens: 176947200.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:51:51 | step: 5500 | train samples/s: 279.2 | train mfu (16-bit): -1.0 | lr mean: 4.994162009097636e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.46 | consumed tokens: 180224000.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:52:19 | step: 5600 | train samples/s: 279.4 | train mfu (16-bit): -1.0 | lr mean: 4.9938469601329416e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.54 | consumed tokens: 183500800.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:52:47 | step: 5700 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.9935242714127526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.46 | consumed tokens: 186777600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T19:53:15 | step: 5800 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9931928515434265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.49 | consumed tokens: 190054400.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:53:43 | step: 5900 | train samples/s: 276.3 | train mfu (16-bit): -1.0 | lr mean: 4.992853428120725e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.36 | consumed tokens: 193331200.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:54:11 | step: 6000 | train samples/s: 277.0 | train mfu (16-bit): -1.0 | lr mean: 4.992505637346767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.48 | consumed tokens: 196608000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:54:39 | step: 6100 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 4.9921494792215526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.27 | consumed tokens: 199884800.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T19:55:07 | step: 6200 | train samples/s: 277.5 | train mfu (16-bit): -1.0 | lr mean: 4.991785317542963e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.54 | consumed tokens: 203161600.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T19:55:35 | step: 6300 | train samples/s: 279.6 | train mfu (16-bit): -1.0 | lr mean: 4.9914127885131165e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.5 | consumed tokens: 206438400.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:56:03 | step: 6400 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.991031892132014e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.6 | consumed tokens: 209715200.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:56:31 | step: 6500 | train samples/s: 279.7 | train mfu (16-bit): -1.0 | lr mean: 4.990642992197536e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.73 | consumed tokens: 212992000.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T19:56:59 | step: 6600 | train samples/s: 273.9 | train mfu (16-bit): -1.0 | lr mean: 4.9902457249118015e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.48 | train loss last: 3.71 | consumed tokens: 216268800.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T19:57:27 | step: 6700 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9898404540726915e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.34 | consumed tokens: 219545600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T19:57:55 | step: 6800 | train samples/s: 279.7 | train mfu (16-bit): -1.0 | lr mean: 4.989426815882325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.48 | train loss last: 3.39 | consumed tokens: 222822400.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:58:23 | step: 6900 | train samples/s: 277.4 | train mfu (16-bit): -1.0 | lr mean: 4.9890048103407025e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.38 | consumed tokens: 226099200.0 | grad norm avg: 0.6 | grad norm last: 0.64 | 
2026-01-01T19:58:50 | step: 7000 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 4.9885744374478236e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.46 | train loss last: 3.48 | consumed tokens: 229376000.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:59:18 | step: 7100 | train samples/s: 273.8 | train mfu (16-bit): -1.0 | lr mean: 4.988136061001569e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.68 | consumed tokens: 232652800.0 | grad norm avg: 0.62 | grad norm last: 0.55 | 
2026-01-01T19:59:46 | step: 7200 | train samples/s: 273.3 | train mfu (16-bit): -1.0 | lr mean: 4.987689317204058e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.38 | consumed tokens: 235929600.0 | grad norm avg: 0.61 | grad norm last: 0.54 | 
2026-01-01T20:00:15 | step: 7300 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.987234569853172e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.41 | consumed tokens: 239206400.0 | grad norm avg: 0.6 | grad norm last: 0.53 | 
2026-01-01T20:00:43 | step: 7400 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.986771455151029e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.26 | consumed tokens: 242483200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:01:11 | step: 7500 | train samples/s: 273.1 | train mfu (16-bit): -1.0 | lr mean: 4.98629997309763e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.46 | train loss last: 3.29 | consumed tokens: 245760000.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T20:01:39 | step: 7600 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.985820487490855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.37 | consumed tokens: 249036800.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:02:08 | step: 7700 | train samples/s: 261.8 | train mfu (16-bit): -1.0 | lr mean: 4.985332634532824e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.54 | consumed tokens: 252313600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:02:36 | step: 7800 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.9848367780214176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.26 | consumed tokens: 255590400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:03:05 | step: 7900 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.9843325541587546e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.45 | consumed tokens: 258867200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:03:34 | step: 8000 | train samples/s: 260.3 | train mfu (16-bit): -1.0 | lr mean: 4.9838199629448354e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.37 | consumed tokens: 262144000.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:04:02 | step: 8100 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.9832993681775406e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.51 | consumed tokens: 265420800.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:04:30 | step: 8200 | train samples/s: 271.5 | train mfu (16-bit): -1.0 | lr mean: 4.9827704060589895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.32 | consumed tokens: 268697600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:04:57 | step: 8300 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.982233076589182e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 271974400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:05:26 | step: 8400 | train samples/s: 261.7 | train mfu (16-bit): -1.0 | lr mean: 4.981687743565999e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 275251200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:05:55 | step: 8500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.98113440698944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 278528000.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:06:23 | step: 8600 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.9805727030616254e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.58 | consumed tokens: 281804800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:06:52 | step: 8700 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.980002631782554e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.53 | consumed tokens: 285081600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:07:21 | step: 8800 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.9794241931522265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.42 | consumed tokens: 288358400.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:07:49 | step: 8900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.978838114766404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.57 | consumed tokens: 291635200.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:08:18 | step: 9000 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.9782433052314445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.45 | consumed tokens: 294912000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:08:47 | step: 9100 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.9776404921431094e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.36 | consumed tokens: 298188800.0 | grad norm avg: 0.6 | grad norm last: 0.59 | 
2026-01-01T20:09:15 | step: 9200 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.977029675501399e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.32 | consumed tokens: 301465600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:09:44 | step: 9300 | train samples/s: 261.3 | train mfu (16-bit): -1.0 | lr mean: 4.976410491508432e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.5 | consumed tokens: 304742400.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:10:13 | step: 9400 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.9757829401642084e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 308019200.0 | grad norm avg: 0.6 | grad norm last: 0.62 | 
2026-01-01T20:10:41 | step: 9500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9751473852666095e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.71 | consumed tokens: 311296000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:11:10 | step: 9600 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.974503826815635e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.26 | consumed tokens: 314572800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:11:39 | step: 9700 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.973851901013404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.5 | consumed tokens: 317849600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:12:07 | step: 9800 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.973191607859917e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.43 | consumed tokens: 321126400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:12:36 | step: 9900 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.972523311153054e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 324403200.0 | grad norm avg: 0.61 | grad norm last: 0.84 | 
2026-01-01T20:13:05 | step: 10000 | train samples/s: 259.9 | train mfu (16-bit): -1.0 | lr mean: 4.971846647094935e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.47 | consumed tokens: 327680000.0 | grad norm avg: 0.62 | grad norm last: 0.53 | 
2026-01-01T20:13:35 | step: 10100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.9711619794834405e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.2 | consumed tokens: 330956800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:14:04 | step: 10200 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.97046930831857e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.4 | consumed tokens: 334233600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:14:32 | step: 10300 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.969768269802444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.51 | consumed tokens: 337510400.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:15:01 | step: 10400 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.969058863935061e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.23 | consumed tokens: 340787200.0 | grad norm avg: 0.62 | grad norm last: 0.71 | 
2026-01-01T20:15:29 | step: 10500 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.968341454514302e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.16 | consumed tokens: 344064000.0 | grad norm avg: 0.6 | grad norm last: 0.56 | 
2026-01-01T20:15:58 | step: 10600 | train samples/s: 261.1 | train mfu (16-bit): -1.0 | lr mean: 4.967616041540168e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.6 | consumed tokens: 347340800.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:16:27 | step: 10700 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 4.966882261214778e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.63 | consumed tokens: 350617600.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:16:56 | step: 10800 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.966140477336012e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.46 | consumed tokens: 353894400.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T20:17:24 | step: 10900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9653903261059895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 357171200.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:17:53 | step: 11000 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9646321713225916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.54 | consumed tokens: 360448000.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:18:22 | step: 11100 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 4.9638656491879374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 363724800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:18:50 | step: 11200 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 4.9630911234999076e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.15 | consumed tokens: 367001600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:19:19 | step: 11300 | train samples/s: 259.3 | train mfu (16-bit): -1.0 | lr mean: 4.962308594258502e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.38 | consumed tokens: 370278400.0 | grad norm avg: 0.62 | grad norm last: 0.7 | 
2026-01-01T20:19:48 | step: 11400 | train samples/s: 261.4 | train mfu (16-bit): -1.0 | lr mean: 4.9615176976658404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.25 | consumed tokens: 373555200.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T20:20:17 | step: 11500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.960718797519803e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 376832000.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:20:45 | step: 11600 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.9599115300225094e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 380108800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:21:14 | step: 11700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.959096622769721e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.43 | consumed tokens: 383385600.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:21:42 | step: 11800 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.958272984367795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.21 | consumed tokens: 386662400.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:22:11 | step: 11900 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.957441706210375e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.1 | consumed tokens: 389939200.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:22:40 | step: 12000 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.956602060701698e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 393216000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T20:23:09 | step: 12100 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.955754047841765e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 396492800.0 | grad norm avg: 0.61 | grad norm last: 0.71 | 
2026-01-01T20:23:37 | step: 12200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.954898395226337e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.18 | consumed tokens: 399769600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:24:06 | step: 12300 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.954034375259653e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 403046400.0 | grad norm avg: 0.62 | grad norm last: 0.54 | 
2026-01-01T20:24:35 | step: 12400 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 4.953161987941712e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.57 | consumed tokens: 406323200.0 | grad norm avg: 0.62 | grad norm last: 0.54 | 
2026-01-01T20:25:03 | step: 12500 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.952281597070396e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.41 | consumed tokens: 409600000.0 | grad norm avg: 0.61 | grad norm last: 0.54 | 
2026-01-01T20:25:32 | step: 12600 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.951393566443585e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.26 | consumed tokens: 412876800.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T20:26:00 | step: 12700 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.950496804667637e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 416153600.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:26:29 | step: 12800 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.949592403136194e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.36 | consumed tokens: 419430400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:26:57 | step: 12900 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.9486796342534944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.43 | consumed tokens: 422707200.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:27:26 | step: 13000 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 4.9477588618174195e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.45 | consumed tokens: 425984000.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:27:54 | step: 13100 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.946829722030088e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.17 | consumed tokens: 429260800.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:28:23 | step: 13200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.945892942487262e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.21 | consumed tokens: 432537600.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:28:52 | step: 13300 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 4.94494779559318e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 2.98 | consumed tokens: 435814400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:29:20 | step: 13400 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.943994645145722e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.33 | consumed tokens: 439091200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:29:49 | step: 13500 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.9430331273470074e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.32 | consumed tokens: 442368000.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:30:17 | step: 13600 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.942063969792798e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.55 | consumed tokens: 445644800.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:30:46 | step: 13700 | train samples/s: 261.9 | train mfu (16-bit): -1.0 | lr mean: 4.9410864448873326e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.38 | consumed tokens: 448921600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:31:15 | step: 13800 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.9401009164284915e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 452198400.0 | grad norm avg: 0.6 | grad norm last: 0.66 | 
2026-01-01T20:31:43 | step: 13900 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.939107384416275e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.15 | consumed tokens: 455475200.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:32:12 | step: 14000 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.938105485052802e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.16 | consumed tokens: 458752000.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:32:41 | step: 14100 | train samples/s: 264.1 | train mfu (16-bit): -1.0 | lr mean: 4.937095945933834e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.28 | consumed tokens: 462028800.0 | grad norm avg: 0.6 | grad norm last: 0.56 | 
2026-01-01T20:33:09 | step: 14200 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.9360780394636095e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.29 | consumed tokens: 465305600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:33:38 | step: 14300 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 4.9350521294400096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.28 | consumed tokens: 468582400.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:34:07 | step: 14400 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.934018215863034e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.27 | consumed tokens: 471859200.0 | grad norm avg: 0.6 | grad norm last: 0.52 | 
2026-01-01T20:34:35 | step: 14500 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.932976298732683e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.27 | consumed tokens: 475136000.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:35:04 | step: 14600 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9319263780489564e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.48 | consumed tokens: 478412800.0 | grad norm avg: 0.61 | grad norm last: 0.7 | 
2026-01-01T20:35:33 | step: 14700 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.9308680900139734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.02 | consumed tokens: 481689600.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T20:36:01 | step: 14800 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.9298021622234955e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.14 | consumed tokens: 484966400.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T20:36:30 | step: 14900 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.9287278670817614e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.26 | consumed tokens: 488243200.0 | grad norm avg: 0.61 | grad norm last: 0.71 | 
2026-01-01T20:36:58 | step: 15000 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.9276455683866516e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.16 | consumed tokens: 491520000.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:37:28 | step: 15100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.926555266138166e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.35 | train loss last: 3.31 | consumed tokens: 494796800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:37:57 | step: 15200 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 4.925456960336305e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.23 | consumed tokens: 498073600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:38:26 | step: 15300 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.9243506509810686e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.34 | consumed tokens: 501350400.0 | grad norm avg: 0.61 | grad norm last: 0.68 | 
2026-01-01T20:38:55 | step: 15400 | train samples/s: 262.8 | train mfu (16-bit): -1.0 | lr mean: 4.9232363380724564e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.3 | consumed tokens: 504627200.0 | grad norm avg: 0.6 | grad norm last: 0.54 | 
2026-01-01T20:39:24 | step: 15500 | train samples/s: 262.7 | train mfu (16-bit): -1.0 | lr mean: 4.9221140216104686e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.45 | consumed tokens: 507904000.0 | grad norm avg: 0.61 | grad norm last: 0.68 | 
2026-01-01T20:39:52 | step: 15600 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.920983701595105e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.42 | consumed tokens: 511180800.0 | grad norm avg: 0.62 | grad norm last: 0.53 | 
2026-01-01T20:40:21 | step: 15700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.919845378026366e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.36 | consumed tokens: 514457600.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:40:49 | step: 15800 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.9186990509042516e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.32 | consumed tokens: 517734400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:41:18 | step: 15900 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.9175447202287614e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.37 | consumed tokens: 521011200.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:41:46 | step: 16000 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.9163823859998956e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.22 | consumed tokens: 524288000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:42:15 | step: 16100 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.915212048217654e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.34 | train loss last: 3.32 | consumed tokens: 527564800.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T20:42:44 | step: 16200 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.914033706882037e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.24 | consumed tokens: 530841600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:43:12 | step: 16300 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.9128473619930446e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.48 | consumed tokens: 534118400.0 | grad norm avg: 0.62 | grad norm last: 0.67 | 
2026-01-01T20:43:41 | step: 16400 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.9116530135506764e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.61 | consumed tokens: 537395200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T20:44:10 | step: 16500 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.9104506615549326e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.45 | consumed tokens: 540672000.0 | grad norm avg: 0.62 | grad norm last: 0.66 | 
2026-01-01T20:44:39 | step: 16600 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.909240306005813e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.59 | consumed tokens: 543948800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:45:08 | step: 16700 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 4.908021946903318e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.21 | consumed tokens: 547225600.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:45:37 | step: 16800 | train samples/s: 262.0 | train mfu (16-bit): -1.0 | lr mean: 4.9067955842474476e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.08 | consumed tokens: 550502400.0 | grad norm avg: 0.6 | grad norm last: 0.62 | 
2026-01-01T20:46:05 | step: 16900 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.905561581836082e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.52 | consumed tokens: 553779200.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:46:34 | step: 17000 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.904319575871341e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 2.96 | consumed tokens: 557056000.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:47:03 | step: 17100 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.9030692025553435e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.14 | consumed tokens: 560332800.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:47:31 | step: 17200 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.901811189483851e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.06 | consumed tokens: 563609600.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T20:48:00 | step: 17300 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.900545172858983e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.22 | consumed tokens: 566886400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:48:29 | step: 17400 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 4.89927115268074e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.33 | train loss last: 3.22 | consumed tokens: 570163200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:48:57 | step: 17500 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.8979894927470013e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.54 | consumed tokens: 573440000.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:49:25 | step: 17600 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.8966994654620066e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 2.92 | consumed tokens: 576716800.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:49:54 | step: 17700 | train samples/s: 262.8 | train mfu (16-bit): -1.0 | lr mean: 4.895401798421517e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.29 | consumed tokens: 579993600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:50:23 | step: 17800 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.894096127827652e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.33 | consumed tokens: 583270400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T20:50:52 | step: 17900 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.892782453680411e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.23 | consumed tokens: 586547200.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:51:20 | step: 18000 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.8914607759797946e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.22 | consumed tokens: 589824000.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:51:49 | step: 18100 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.890131458523683e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.29 | consumed tokens: 593100800.0 | grad norm avg: 0.6 | grad norm last: 0.68 | 
2026-01-01T20:52:18 | step: 18200 | train samples/s: 262.3 | train mfu (16-bit): -1.0 | lr mean: 4.888794137514196e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.0 | consumed tokens: 596377600.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:52:46 | step: 18300 | train samples/s: 272.7 | train mfu (16-bit): -1.0 | lr mean: 4.887448812951334e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.61 | consumed tokens: 599654400.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T20:53:14 | step: 18400 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.886095484835096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.27 | consumed tokens: 602931200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:53:43 | step: 18500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.884734516963363e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.17 | consumed tokens: 606208000.0 | grad norm avg: 0.63 | grad norm last: 0.68 | 
2026-01-01T20:54:11 | step: 18600 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.883365545538254e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.67 | consumed tokens: 609484800.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:54:40 | step: 18700 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.8819889343576506e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.17 | consumed tokens: 612761600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:55:08 | step: 18800 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.880603955825791e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.16 | consumed tokens: 616038400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:55:37 | step: 18900 | train samples/s: 260.4 | train mfu (16-bit): -1.0 | lr mean: 4.879211337538436e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.32 | train loss last: 3.32 | consumed tokens: 619315200.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:56:06 | step: 19000 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 4.8778110794955865e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.42 | consumed tokens: 622592000.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:56:34 | step: 19100 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 4.8764024541014805e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.15 | consumed tokens: 625868800.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T20:57:02 | step: 19200 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.8749865527497604e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.33 | consumed tokens: 629145600.0 | grad norm avg: 0.62 | grad norm last: 0.7 | 
2026-01-01T20:57:31 | step: 19300 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.873562284046784e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.43 | consumed tokens: 632422400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T20:57:59 | step: 19400 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.872130375588313e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.4 | consumed tokens: 635699200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:58:27 | step: 19500 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.8706908273743466e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.37 | consumed tokens: 638976000.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:58:56 | step: 19600 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.869242911809124e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.46 | consumed tokens: 642252800.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T20:59:25 | step: 19700 | train samples/s: 262.1 | train mfu (16-bit): -1.0 | lr mean: 4.8677877202862874e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.53 | consumed tokens: 645529600.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:59:54 | step: 19800 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.8663241614121944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.21 | consumed tokens: 648806400.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T21:00:22 | step: 19900 | train samples/s: 267.6 | train mfu (16-bit): -1.0 | lr mean: 4.864853326580487e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.31 | train loss last: 3.21 | consumed tokens: 652083200.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T21:00:50 | step: 20000 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.863374124397524e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.14 | consumed tokens: 655360000.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:01:21 | step: 20100 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.861887282459065e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.12 | consumed tokens: 658636800.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:01:49 | step: 20200 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.860392800765112e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.27 | consumed tokens: 661913600.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:02:18 | step: 20300 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.858890315517783e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.77 | consumed tokens: 665190400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:02:46 | step: 20400 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 4.8573801905149594e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.33 | consumed tokens: 668467200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:03:15 | step: 20500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.855862425756641e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.3 | train loss last: 3.46 | consumed tokens: 671744000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:03:43 | step: 20600 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.8543366574449465e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.06 | consumed tokens: 675020800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:04:11 | step: 20700 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.8528028855798766e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.04 | consumed tokens: 678297600.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:04:40 | step: 20800 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.851261473959312e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.62 | consumed tokens: 681574400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:05:09 | step: 20900 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.849712422583252e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.18 | consumed tokens: 684851200.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T21:05:37 | step: 21000 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.848155367653817e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.41 | consumed tokens: 688128000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T21:06:06 | step: 21100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.846590672968887e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.26 | consumed tokens: 691404800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:06:34 | step: 21200 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.845018338528462e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.21 | consumed tokens: 694681600.0 | grad norm avg: 0.62 | grad norm last: 0.55 | 
2026-01-01T21:07:03 | step: 21300 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.843438000534661e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.52 | consumed tokens: 697958400.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T21:07:31 | step: 21400 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.8418500227853656e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.4 | consumed tokens: 701235200.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:08:00 | step: 21500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.840254405280575e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.15 | consumed tokens: 704512000.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T21:08:28 | step: 21600 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.838650784222409e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.17 | consumed tokens: 707788800.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T21:08:57 | step: 21700 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.837039523408748e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.45 | consumed tokens: 711065600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:09:25 | step: 21800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.8354206228395924e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 2.93 | consumed tokens: 714342400.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T21:09:54 | step: 21900 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.833794082514942e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.33 | consumed tokens: 717619200.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T21:10:22 | step: 22000 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.8321595386369154e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.98 | consumed tokens: 720896000.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:10:51 | step: 22100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.830517355003394e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.29 | train loss last: 3.12 | consumed tokens: 724172800.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:11:20 | step: 22200 | train samples/s: 257.8 | train mfu (16-bit): -1.0 | lr mean: 4.828867531614378e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.43 | consumed tokens: 727449600.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:11:49 | step: 22300 | train samples/s: 259.2 | train mfu (16-bit): -1.0 | lr mean: 4.827210068469867e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.74 | consumed tokens: 730726400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:12:17 | step: 22400 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.825544965569861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 734003200.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:12:45 | step: 22500 | train samples/s: 267.7 | train mfu (16-bit): -1.0 | lr mean: 4.82387185911648e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.23 | consumed tokens: 737280000.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T21:13:14 | step: 22600 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.822191476705484e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.23 | consumed tokens: 740556800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:13:42 | step: 22700 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 4.820503090741113e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.22 | consumed tokens: 743833600.0 | grad norm avg: 0.61 | grad norm last: 0.69 | 
2026-01-01T21:14:11 | step: 22800 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.818807065021247e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.48 | consumed tokens: 747110400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:14:39 | step: 22900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.8171033995458856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.24 | consumed tokens: 750387200.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:15:09 | step: 23000 | train samples/s: 252.8 | train mfu (16-bit): -1.0 | lr mean: 4.81539209431503e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.02 | consumed tokens: 753664000.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:15:37 | step: 23100 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 4.813673149328679e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.89 | consumed tokens: 756940800.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T21:16:06 | step: 23200 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.811946564586833e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.57 | consumed tokens: 760217600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:16:34 | step: 23300 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.8102123400894925e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.65 | consumed tokens: 763494400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:17:02 | step: 23400 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 4.808470112038776e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.14 | consumed tokens: 766771200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:17:31 | step: 23500 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 4.806720608030446e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.05 | consumed tokens: 770048000.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T21:17:59 | step: 23600 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.8049634642666206e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.04 | consumed tokens: 773324800.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:18:28 | step: 23700 | train samples/s: 262.0 | train mfu (16-bit): -1.0 | lr mean: 4.8031986807473004e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.67 | consumed tokens: 776601600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T21:18:57 | step: 23800 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.801426257472485e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.49 | consumed tokens: 779878400.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T21:19:25 | step: 23900 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 4.799646194442175e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.36 | consumed tokens: 783155200.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:19:54 | step: 24000 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 4.7978584916563705e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 3.3 | consumed tokens: 786432000.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:20:22 | step: 24100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.796063149115071e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.33 | consumed tokens: 789708800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T21:20:51 | step: 24200 | train samples/s: 259.8 | train mfu (16-bit): -1.0 | lr mean: 4.794260166818276e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.26 | consumed tokens: 792985600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T21:21:20 | step: 24300 | train samples/s: 262.6 | train mfu (16-bit): -1.0 | lr mean: 4.792449908563867e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.22 | consumed tokens: 796262400.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:21:48 | step: 24400 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.790631646756083e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.09 | consumed tokens: 799539200.0 | grad norm avg: 0.63 | grad norm last: 0.56 | 
2026-01-01T21:22:17 | step: 24500 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.788806108990684e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.2 | consumed tokens: 802816000.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:22:45 | step: 24600 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.7869729314697906e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.19 | consumed tokens: 806092800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:23:14 | step: 24700 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.785132114193402e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.15 | consumed tokens: 809369600.0 | grad norm avg: 0.62 | grad norm last: 0.66 | 
2026-01-01T21:23:42 | step: 24800 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.783283657161519e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.44 | consumed tokens: 812646400.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:24:11 | step: 24900 | train samples/s: 264.1 | train mfu (16-bit): -1.0 | lr mean: 4.7814279241720214e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.33 | consumed tokens: 815923200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:24:40 | step: 25000 | train samples/s: 261.5 | train mfu (16-bit): -1.0 | lr mean: 4.779564551427029e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.19 | consumed tokens: 819200000.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:25:10 | step: 25100 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.777693538926542e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.14 | consumed tokens: 822476800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T21:25:38 | step: 25200 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.7758148866705596e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.2 | consumed tokens: 825753600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T21:26:06 | step: 25300 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.773928958456963e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.37 | consumed tokens: 829030400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:26:35 | step: 25400 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.772035390487872e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.28 | train loss last: 2.98 | consumed tokens: 832307200.0 | grad norm avg: 0.62 | grad norm last: 0.67 | 
2026-01-01T21:27:03 | step: 25500 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.770134182763286e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.52 | consumed tokens: 835584000.0 | grad norm avg: 0.64 | grad norm last: 0.75 | 
2026-01-01T21:27:31 | step: 25600 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.7682256990810856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.58 | consumed tokens: 838860800.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:28:00 | step: 25700 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 4.7663095756433904e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.31 | consumed tokens: 842137600.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T21:28:29 | step: 25800 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.7643858124502e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.25 | consumed tokens: 845414400.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T21:28:57 | step: 25900 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.762454773299396e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.3 | consumed tokens: 848691200.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:29:26 | step: 26000 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.760516094393097e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.55 | consumed tokens: 851968000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:29:54 | step: 26100 | train samples/s: 263.9 | train mfu (16-bit): -1.0 | lr mean: 4.7585701395291835e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.31 | consumed tokens: 855244800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:30:23 | step: 26200 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.756616544909775e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.27 | train loss last: 3.43 | consumed tokens: 858521600.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:30:51 | step: 26300 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.754655310534872e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.05 | consumed tokens: 861798400.0 | grad norm avg: 0.63 | grad norm last: 0.56 | 
2026-01-01T21:31:20 | step: 26400 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.7526871640002355e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.2 | consumed tokens: 865075200.0 | grad norm avg: 0.64 | grad norm last: 0.66 | 
2026-01-01T21:31:49 | step: 26500 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 4.750711013912223e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.21 | consumed tokens: 868352000.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:32:17 | step: 26600 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.748727587866597e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.37 | consumed tokens: 871628800.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:32:46 | step: 26700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.746736885863356e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.3 | consumed tokens: 874905600.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T21:33:14 | step: 26800 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.744738544104621e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.17 | consumed tokens: 878182400.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T21:33:43 | step: 26900 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 4.742732926388271e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.09 | consumed tokens: 881459200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:34:12 | step: 27000 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.7407196689164266e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 2.93 | consumed tokens: 884736000.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T21:34:40 | step: 27100 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 4.738699135486968e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.34 | consumed tokens: 888012800.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:35:09 | step: 27200 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.736671326099895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.39 | consumed tokens: 891289600.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:35:37 | step: 27300 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.734635876957327e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.36 | consumed tokens: 894566400.0 | grad norm avg: 0.63 | grad norm last: 0.56 | 
2026-01-01T21:36:06 | step: 27400 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.732593151857145e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.21 | consumed tokens: 897843200.0 | grad norm avg: 0.63 | grad norm last: 0.71 | 
2026-01-01T21:36:34 | step: 27500 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.730543150799349e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.28 | consumed tokens: 901120000.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:37:03 | step: 27600 | train samples/s: 260.4 | train mfu (16-bit): -1.0 | lr mean: 4.7284858737839386e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.39 | consumed tokens: 904396800.0 | grad norm avg: 0.62 | grad norm last: 0.66 | 
2026-01-01T21:37:31 | step: 27700 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.726420957013033e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.35 | consumed tokens: 907673600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T21:38:00 | step: 27800 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.724348764284514e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.18 | consumed tokens: 910950400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:38:29 | step: 27900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.7222689318004996e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.04 | consumed tokens: 914227200.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T21:38:57 | step: 28000 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 4.720182187156752e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 2.81 | consumed tokens: 917504000.0 | grad norm avg: 0.64 | grad norm last: 0.59 | 
2026-01-01T21:39:26 | step: 28100 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.718087802757509e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.13 | consumed tokens: 920780800.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T21:39:54 | step: 28200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.715986142400652e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 924057600.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:40:23 | step: 28300 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.713877206086181e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.04 | consumed tokens: 927334400.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T21:40:52 | step: 28400 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.711760993814096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.08 | consumed tokens: 930611200.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:41:20 | step: 28500 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.7096375055843964e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.45 | consumed tokens: 933888000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:41:49 | step: 28600 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.707506377599202e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.26 | train loss last: 3.32 | consumed tokens: 937164800.0 | grad norm avg: 0.64 | grad norm last: 0.74 | 
2026-01-01T21:42:17 | step: 28700 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.705368337454274e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.0 | consumed tokens: 940441600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T21:42:45 | step: 28800 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.7032226575538516e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.16 | consumed tokens: 943718400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:43:13 | step: 28900 | train samples/s: 267.8 | train mfu (16-bit): -1.0 | lr mean: 4.701069701695815e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.08 | consumed tokens: 946995200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:43:42 | step: 29000 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.6989098336780444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.21 | consumed tokens: 950272000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:44:11 | step: 29100 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.696742325904779e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 2.97 | consumed tokens: 953548800.0 | grad norm avg: 0.63 | grad norm last: 0.77 | 
2026-01-01T21:44:40 | step: 29200 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.6945679059717804e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.09 | consumed tokens: 956825600.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T21:45:08 | step: 29300 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.692385846283287e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.05 | consumed tokens: 960102400.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T21:45:36 | step: 29400 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 4.69019687443506e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.76 | consumed tokens: 963379200.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:46:05 | step: 29500 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.688000262831338e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.1 | consumed tokens: 966656000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:46:34 | step: 29600 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.685796739067882e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.24 | consumed tokens: 969932800.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:47:02 | step: 29700 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.683585939346813e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.52 | consumed tokens: 973209600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:47:31 | step: 29800 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.681367863668129e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.54 | consumed tokens: 976486400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:48:00 | step: 29900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.679142512031831e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.49 | consumed tokens: 979763200.0 | grad norm avg: 0.62 | grad norm last: 0.6 | 
2026-01-01T21:48:28 | step: 30000 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.676909884437919e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.27 | consumed tokens: 983040000.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T21:48:58 | step: 30100 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.674669980886392e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.38 | consumed tokens: 986316800.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T21:49:27 | step: 30200 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 4.6724231651751325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.18 | consumed tokens: 989593600.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:49:56 | step: 30300 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 4.6701690735062584e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.25 | train loss last: 3.23 | consumed tokens: 992870400.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:50:24 | step: 30400 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.66790770587977e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.99 | consumed tokens: 996147200.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:50:53 | step: 30500 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.6656394260935485e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.29 | consumed tokens: 999424000.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T21:51:21 | step: 30600 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.663363870349713e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.44 | consumed tokens: 1002700800.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T21:51:50 | step: 30700 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.6610810386482626e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.29 | consumed tokens: 1005977600.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T21:52:18 | step: 30800 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 4.6587909309891984e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.13 | consumed tokens: 1009254400.0 | grad norm avg: 0.62 | grad norm last: 0.57 | 
2026-01-01T21:52:46 | step: 30900 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.656493911170401e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.17 | consumed tokens: 1012531200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:53:15 | step: 31000 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.654189615393989e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.51 | consumed tokens: 1015808000.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T21:53:44 | step: 31100 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.6518784074578434e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.03 | consumed tokens: 1019084800.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T21:54:12 | step: 31200 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.649559923564084e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.3 | consumed tokens: 1022361600.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T21:54:41 | step: 31300 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.64723416371271e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.01 | consumed tokens: 1025638400.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T21:55:09 | step: 31400 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.644901491701603e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.34 | consumed tokens: 1028915200.0 | grad norm avg: 0.62 | grad norm last: 0.56 | 
2026-01-01T21:55:37 | step: 31500 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.642561907530762e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.22 | consumed tokens: 1032192000.0 | grad norm avg: 0.62 | grad norm last: 0.74 | 
2026-01-01T21:56:06 | step: 31600 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.6402150474023074e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.1 | consumed tokens: 1035468800.0 | grad norm avg: 0.62 | grad norm last: 0.57 | 
2026-01-01T21:56:35 | step: 31700 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 4.6378609113162383e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 1038745600.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T21:57:03 | step: 31800 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.635499863070436e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.38 | consumed tokens: 1042022400.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T21:57:32 | step: 31900 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.6331319026649e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.13 | consumed tokens: 1045299200.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T21:58:00 | step: 32000 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.6307566663017496e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.35 | consumed tokens: 1048576000.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T21:58:28 | step: 32100 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.628374517778866e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.24 | train loss last: 3.27 | consumed tokens: 1051852800.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T21:58:57 | step: 32200 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.625985093298368e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.52 | consumed tokens: 1055129600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T21:59:26 | step: 32300 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.6235891204560176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 2.7 | consumed tokens: 1058406400.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T21:59:54 | step: 32400 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.621185507858172e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.19 | consumed tokens: 1061683200.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T22:00:23 | step: 32500 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.618775346898474e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.13 | consumed tokens: 1064960000.0 | grad norm avg: 0.63 | grad norm last: 0.65 | 
2026-01-01T22:00:51 | step: 32600 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.6163579099811614e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.27 | consumed tokens: 1068236800.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T22:01:20 | step: 32700 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.6139335609041154e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.2 | consumed tokens: 1071513600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:01:48 | step: 32800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.611502299667336e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.92 | consumed tokens: 1074790400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:02:17 | step: 32900 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.609064126270823e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.21 | consumed tokens: 1078067200.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:02:45 | step: 33000 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.606618676916696e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.38 | consumed tokens: 1081344000.0 | grad norm avg: 0.63 | grad norm last: 0.6 | 
2026-01-01T22:03:14 | step: 33100 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 4.6041663154028356e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.05 | consumed tokens: 1084620800.0 | grad norm avg: 0.63 | grad norm last: 0.69 | 
2026-01-01T22:03:42 | step: 33200 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.601707405527122e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.17 | consumed tokens: 1087897600.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:04:11 | step: 33300 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.599241219693795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 3.44 | consumed tokens: 1091174400.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:04:39 | step: 33400 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.596768121700734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.07 | consumed tokens: 1094451200.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:05:08 | step: 33500 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 4.594287747750059e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.29 | consumed tokens: 1097728000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T22:05:37 | step: 33600 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.591800825437531e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.96 | consumed tokens: 1101004800.0 | grad norm avg: 0.63 | grad norm last: 0.76 | 
2026-01-01T22:06:05 | step: 33700 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.5893069909652695e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.0 | consumed tokens: 1104281600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T22:06:34 | step: 33800 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.586806244333275e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.13 | consumed tokens: 1107558400.0 | grad norm avg: 0.62 | grad norm last: 0.62 | 
2026-01-01T22:07:03 | step: 33900 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.5842985855415463e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.35 | consumed tokens: 1110835200.0 | grad norm avg: 0.63 | grad norm last: 0.71 | 
2026-01-01T22:07:31 | step: 34000 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.581783650792204e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.17 | consumed tokens: 1114112000.0 | grad norm avg: 0.63 | grad norm last: 0.73 | 
2026-01-01T22:08:00 | step: 34100 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.5792621676810086e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.05 | consumed tokens: 1117388800.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:08:28 | step: 34200 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.57673377241008e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.5 | consumed tokens: 1120665600.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T22:08:57 | step: 34300 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.5741984649794176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.11 | consumed tokens: 1123942400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T22:09:25 | step: 34400 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.5716566091869026e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.05 | consumed tokens: 1127219200.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:09:54 | step: 34500 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.5691074774367735e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.3 | consumed tokens: 1130496000.0 | grad norm avg: 0.62 | grad norm last: 0.57 | 
2026-01-01T22:10:22 | step: 34600 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.5665517973247916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.38 | consumed tokens: 1133772800.0 | grad norm avg: 0.63 | grad norm last: 0.57 | 
2026-01-01T22:10:51 | step: 34700 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.5639888412551954e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.5 | consumed tokens: 1137049600.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T22:11:19 | step: 34800 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.5614193368237466e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.09 | consumed tokens: 1140326400.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T22:11:48 | step: 34900 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.558843284030445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.07 | consumed tokens: 1143603200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T22:12:16 | step: 35000 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.556259955279529e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 2.97 | consumed tokens: 1146880000.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:12:47 | step: 35100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.5536700781667605e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.27 | consumed tokens: 1150156800.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T22:13:15 | step: 35200 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 4.5510732888942584e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.28 | consumed tokens: 1153433600.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T22:13:44 | step: 35300 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.5484699512599036e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.08 | consumed tokens: 1156710400.0 | grad norm avg: 0.63 | grad norm last: 0.65 | 
2026-01-01T22:14:12 | step: 35400 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.545859701465815e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.23 | train loss last: 2.95 | consumed tokens: 1159987200.0 | grad norm avg: 0.63 | grad norm last: 0.68 | 
2026-01-01T22:14:41 | step: 35500 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.5432425395119935e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.12 | consumed tokens: 1163264000.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:15:09 | step: 35600 | train samples/s: 262.9 | train mfu (16-bit): -1.0 | lr mean: 4.540618829196319e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.24 | consumed tokens: 1166540800.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T22:15:38 | step: 35700 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.537988206720911e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.04 | consumed tokens: 1169817600.0 | grad norm avg: 0.63 | grad norm last: 0.61 | 
2026-01-01T22:16:07 | step: 35800 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.5353506720857695e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1173094400.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:16:35 | step: 35900 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.532706952886656e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.32 | consumed tokens: 1176371200.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T22:17:04 | step: 36000 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.530055957729928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 2.91 | consumed tokens: 1179648000.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T22:17:32 | step: 36100 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.527398414211348e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.49 | consumed tokens: 1182924800.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T22:18:01 | step: 36200 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.5247343223309144e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.5 | consumed tokens: 1186201600.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T22:18:30 | step: 36300 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.522063318290748e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.25 | consumed tokens: 1189478400.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:18:58 | step: 36400 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.519385765888728e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 3.16 | consumed tokens: 1192755200.0 | grad norm avg: 0.63 | grad norm last: 0.59 | 
2026-01-01T22:19:27 | step: 36500 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 4.516701665124856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.02 | consumed tokens: 1196032000.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T22:19:55 | step: 36600 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 4.51401065220125e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.18 | consumed tokens: 1199308800.0 | grad norm avg: 0.64 | grad norm last: 0.67 | 
2026-01-01T22:20:24 | step: 36700 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.511313090915792e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.23 | consumed tokens: 1202585600.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:20:52 | step: 36800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.5086086174706e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.22 | train loss last: 2.88 | consumed tokens: 1205862400.0 | grad norm avg: 0.62 | grad norm last: 0.7 | 
2026-01-01T22:21:21 | step: 36900 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 4.505897959461436e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 2.81 | consumed tokens: 1209139200.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:21:49 | step: 37000 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 4.503180389292538e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.3 | consumed tokens: 1212416000.0 | grad norm avg: 0.64 | grad norm last: 0.62 | 
2026-01-01T22:22:18 | step: 37100 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.500456270761788e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.61 | consumed tokens: 1215692800.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:22:46 | step: 37200 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.497725240071304e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.05 | consumed tokens: 1218969600.0 | grad norm avg: 0.63 | grad norm last: 0.58 | 
2026-01-01T22:23:14 | step: 37300 | train samples/s: 267.6 | train mfu (16-bit): -1.0 | lr mean: 4.494988024816848e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.02 | consumed tokens: 1222246400.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:23:42 | step: 37400 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.492243897402659e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.14 | consumed tokens: 1225523200.0 | grad norm avg: 0.63 | grad norm last: 0.68 | 
2026-01-01T22:24:11 | step: 37500 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.489493221626617e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.05 | consumed tokens: 1228800000.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T22:24:40 | step: 37600 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.486736361286603e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.08 | consumed tokens: 1232076800.0 | grad norm avg: 0.64 | grad norm last: 0.71 | 
2026-01-01T22:25:08 | step: 37700 | train samples/s: 272.1 | train mfu (16-bit): -1.0 | lr mean: 4.4839725887868553e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.48 | consumed tokens: 1235353600.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T22:25:36 | step: 37800 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.481202267925255e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.06 | consumed tokens: 1238630400.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:26:04 | step: 37900 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.478425398701802e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.3 | consumed tokens: 1241907200.0 | grad norm avg: 0.63 | grad norm last: 0.62 | 
2026-01-01T22:26:33 | step: 38000 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.475641981116496e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.2 | consumed tokens: 1245184000.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:27:01 | step: 38100 | train samples/s: 270.4 | train mfu (16-bit): -1.0 | lr mean: 4.4728520151693374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.38 | consumed tokens: 1248460800.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T22:27:29 | step: 38200 | train samples/s: 272.0 | train mfu (16-bit): -1.0 | lr mean: 4.470055500860326e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.1 | consumed tokens: 1251737600.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T22:27:57 | step: 38300 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.467252438189462e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.07 | consumed tokens: 1255014400.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T22:28:26 | step: 38400 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 4.464442827156745e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.36 | consumed tokens: 1258291200.0 | grad norm avg: 0.64 | grad norm last: 0.61 | 
2026-01-01T22:28:54 | step: 38500 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 4.461627031560056e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.26 | consumed tokens: 1261568000.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T22:29:23 | step: 38600 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.4588043238036335e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 2.98 | consumed tokens: 1264844800.0 | grad norm avg: 0.64 | grad norm last: 0.69 | 
2026-01-01T22:29:51 | step: 38700 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.455975431483239e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.34 | consumed tokens: 1268121600.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T22:30:19 | step: 38800 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.4531399908009917e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 2.87 | consumed tokens: 1271398400.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T22:30:47 | step: 38900 | train samples/s: 270.6 | train mfu (16-bit): -1.0 | lr mean: 4.450298365554772e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.48 | consumed tokens: 1274675200.0 | grad norm avg: 0.64 | grad norm last: 0.68 | 
2026-01-01T22:31:16 | step: 39000 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.4474498281488195e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.26 | consumed tokens: 1277952000.0 | grad norm avg: 0.63 | grad norm last: 0.71 | 
2026-01-01T22:31:44 | step: 39100 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.4445951061788946e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.27 | consumed tokens: 1281228800.0 | grad norm avg: 0.64 | grad norm last: 0.59 | 
2026-01-01T22:32:12 | step: 39200 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.441733835847117e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.27 | consumed tokens: 1284505600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T22:32:41 | step: 39300 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.4388660171534866e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.23 | consumed tokens: 1287782400.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T22:33:09 | step: 39400 | train samples/s: 263.9 | train mfu (16-bit): -1.0 | lr mean: 4.435992013895884e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.15 | consumed tokens: 1291059200.0 | grad norm avg: 0.64 | grad norm last: 0.68 | 
2026-01-01T22:33:38 | step: 39500 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 4.433111462276429e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.03 | consumed tokens: 1294336000.0 | grad norm avg: 0.65 | grad norm last: 0.64 | 
2026-01-01T22:34:06 | step: 39600 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.430224726093002e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.04 | consumed tokens: 1297612800.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T22:34:34 | step: 39700 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.4273314415477216e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.19 | consumed tokens: 1300889600.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:35:03 | step: 39800 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.4244319724384695e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.21 | train loss last: 3.24 | consumed tokens: 1304166400.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:35:31 | step: 39900 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.421525954967365e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.38 | consumed tokens: 1307443200.0 | grad norm avg: 0.64 | grad norm last: 0.61 | 
2026-01-01T22:36:00 | step: 40000 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.418613389134407e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.28 | consumed tokens: 1310720000.0 | grad norm avg: 0.65 | grad norm last: 0.66 | 
2026-01-01T22:36:30 | step: 40100 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.415695002535358e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.5 | consumed tokens: 1313996800.0 | grad norm avg: 0.64 | grad norm last: 0.6 | 
2026-01-01T22:36:59 | step: 40200 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.4127697037765756e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.17 | consumed tokens: 1317273600.0 | grad norm avg: 0.64 | grad norm last: 0.6 | 
2026-01-01T22:37:27 | step: 40300 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.409838584251702e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.02 | consumed tokens: 1320550400.0 | grad norm avg: 0.64 | grad norm last: 0.6 | 
2026-01-01T22:37:55 | step: 40400 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 4.406900916364975e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 1323827200.0 | grad norm avg: 0.64 | grad norm last: 0.68 | 
2026-01-01T22:38:24 | step: 40500 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.403956700116396e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.18 | consumed tokens: 1327104000.0 | grad norm avg: 0.65 | grad norm last: 0.73 | 
2026-01-01T22:38:52 | step: 40600 | train samples/s: 270.2 | train mfu (16-bit): -1.0 | lr mean: 4.4010062993038446e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.07 | consumed tokens: 1330380800.0 | grad norm avg: 0.63 | grad norm last: 0.63 | 
2026-01-01T22:39:20 | step: 40700 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.398049713927321e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.17 | consumed tokens: 1333657600.0 | grad norm avg: 0.64 | grad norm last: 0.69 | 
2026-01-01T22:39:49 | step: 40800 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.3950869439868256e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.0 | consumed tokens: 1336934400.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:40:17 | step: 40900 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.392117989482358e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.27 | consumed tokens: 1340211200.0 | grad norm avg: 0.63 | grad norm last: 0.64 | 
2026-01-01T22:40:46 | step: 41000 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.389142486616038e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.45 | consumed tokens: 1343488000.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:41:14 | step: 41100 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.3861607991857454e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.48 | consumed tokens: 1346764800.0 | grad norm avg: 0.63 | grad norm last: 0.67 | 
2026-01-01T22:41:42 | step: 41200 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.383172927191481e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.3 | consumed tokens: 1350041600.0 | grad norm avg: 0.64 | grad norm last: 0.67 | 
2026-01-01T22:42:11 | step: 41300 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 4.3801788706332445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 2.71 | consumed tokens: 1353318400.0 | grad norm avg: 0.64 | grad norm last: 0.66 | 
2026-01-01T22:42:39 | step: 41400 | train samples/s: 267.7 | train mfu (16-bit): -1.0 | lr mean: 4.377178629511036e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.02 | consumed tokens: 1356595200.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:43:07 | step: 41500 | train samples/s: 269.9 | train mfu (16-bit): -1.0 | lr mean: 4.374171840026975e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1359872000.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:43:36 | step: 41600 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 4.371159229776822e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.07 | consumed tokens: 1363148800.0 | grad norm avg: 0.65 | grad norm last: 0.58 | 
2026-01-01T22:44:04 | step: 41700 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.368140434962697e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.16 | consumed tokens: 1366425600.0 | grad norm avg: 0.65 | grad norm last: 0.59 | 
2026-01-01T22:44:33 | step: 41800 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.36511509178672e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.34 | consumed tokens: 1369702400.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T22:45:01 | step: 41900 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 4.362083927844651e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.31 | consumed tokens: 1372979200.0 | grad norm avg: 0.65 | grad norm last: 0.71 | 
2026-01-01T22:45:29 | step: 42000 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.35904657933861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.07 | consumed tokens: 1376256000.0 | grad norm avg: 0.64 | grad norm last: 0.66 | 
2026-01-01T22:45:58 | step: 42100 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.356003046268597e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.48 | consumed tokens: 1379532800.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T22:46:27 | step: 42200 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.352953328634612e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.27 | consumed tokens: 1382809600.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T22:46:55 | step: 42300 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.349897426436655e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.01 | consumed tokens: 1386086400.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T22:47:23 | step: 42400 | train samples/s: 270.0 | train mfu (16-bit): -1.0 | lr mean: 4.346835339674726e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.22 | consumed tokens: 1389363200.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T22:47:52 | step: 42500 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.343767068348825e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.14 | consumed tokens: 1392640000.0 | grad norm avg: 0.66 | grad norm last: 0.66 | 
2026-01-01T22:48:20 | step: 42600 | train samples/s: 269.9 | train mfu (16-bit): -1.0 | lr mean: 4.3406929762568325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.08 | consumed tokens: 1395916800.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:48:48 | step: 42700 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 4.337612699600868e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.2 | train loss last: 3.23 | consumed tokens: 1399193600.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:49:17 | step: 42800 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.334526238380931e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1402470400.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T22:49:45 | step: 42900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.3314339563949034e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 2.93 | consumed tokens: 1405747200.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:50:13 | step: 43000 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.3283354898449033e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.22 | consumed tokens: 1409024000.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T22:50:42 | step: 43100 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 4.325230838730931e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.19 | consumed tokens: 1412300800.0 | grad norm avg: 0.65 | grad norm last: 0.71 | 
2026-01-01T22:51:10 | step: 43200 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.322120366850868e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.67 | consumed tokens: 1415577600.0 | grad norm avg: 0.66 | grad norm last: 0.67 | 
2026-01-01T22:51:38 | step: 43300 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.3190037104068324e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.11 | consumed tokens: 1418854400.0 | grad norm avg: 0.65 | grad norm last: 0.68 | 
2026-01-01T22:52:07 | step: 43400 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.3158812331967056e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.43 | consumed tokens: 1422131200.0 | grad norm avg: 0.64 | grad norm last: 0.6 | 
2026-01-01T22:52:35 | step: 43500 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.312752571422607e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.07 | consumed tokens: 1425408000.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T22:53:04 | step: 43600 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.3096180888824165e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.68 | consumed tokens: 1428684800.0 | grad norm avg: 0.65 | grad norm last: 0.82 | 
2026-01-01T22:53:32 | step: 43700 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.306477421778254e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.12 | consumed tokens: 1431961600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T22:54:01 | step: 43800 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.30333057011012e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.15 | consumed tokens: 1435238400.0 | grad norm avg: 0.65 | grad norm last: 0.58 | 
2026-01-01T22:54:29 | step: 43900 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 4.300178261473775e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 2.98 | consumed tokens: 1438515200.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T22:54:57 | step: 44000 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.297019768273458e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.52 | consumed tokens: 1441792000.0 | grad norm avg: 0.65 | grad norm last: 0.74 | 
2026-01-01T22:55:26 | step: 44100 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.293855090509169e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.01 | consumed tokens: 1445068800.0 | grad norm avg: 0.64 | grad norm last: 0.73 | 
2026-01-01T22:55:54 | step: 44200 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.290684955776669e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.07 | consumed tokens: 1448345600.0 | grad norm avg: 0.65 | grad norm last: 0.67 | 
2026-01-01T22:56:22 | step: 44300 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.287508636480197e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.16 | consumed tokens: 1451622400.0 | grad norm avg: 0.65 | grad norm last: 0.6 | 
2026-01-01T22:56:51 | step: 44400 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.284326496417634e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 1454899200.0 | grad norm avg: 0.65 | grad norm last: 0.68 | 
2026-01-01T22:57:20 | step: 44500 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 4.281138171791099e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.12 | consumed tokens: 1458176000.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T22:57:48 | step: 44600 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.2779440263984725e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.2 | consumed tokens: 1461452800.0 | grad norm avg: 0.65 | grad norm last: 0.68 | 
2026-01-01T22:58:17 | step: 44700 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.274744424037635e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.2 | consumed tokens: 1464729600.0 | grad norm avg: 0.65 | grad norm last: 0.61 | 
2026-01-01T22:58:45 | step: 44800 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.271538637112826e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.47 | consumed tokens: 1468006400.0 | grad norm avg: 0.65 | grad norm last: 0.57 | 
2026-01-01T22:59:14 | step: 44900 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.2683270294219255e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.5 | consumed tokens: 1471283200.0 | grad norm avg: 0.64 | grad norm last: 0.68 | 
2026-01-01T22:59:42 | step: 45000 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.265109237167053e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.07 | consumed tokens: 1474560000.0 | grad norm avg: 0.64 | grad norm last: 0.69 | 
2026-01-01T23:00:12 | step: 45100 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 4.26188598794397e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.27 | consumed tokens: 1477836800.0 | grad norm avg: 0.65 | grad norm last: 0.68 | 
2026-01-01T23:00:40 | step: 45200 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.258656917954795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 2.86 | consumed tokens: 1481113600.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T23:01:08 | step: 45300 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.255422027199529e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.07 | consumed tokens: 1484390400.0 | grad norm avg: 0.66 | grad norm last: 0.72 | 
2026-01-01T23:01:37 | step: 45400 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.252181315678172e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.08 | consumed tokens: 1487667200.0 | grad norm avg: 0.66 | grad norm last: 0.61 | 
2026-01-01T23:02:06 | step: 45500 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 4.248934783390723e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.22 | consumed tokens: 1490944000.0 | grad norm avg: 0.66 | grad norm last: 0.69 | 
2026-01-01T23:02:34 | step: 45600 | train samples/s: 270.0 | train mfu (16-bit): -1.0 | lr mean: 4.245682430337183e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 2.7 | consumed tokens: 1494220800.0 | grad norm avg: 0.66 | grad norm last: 0.71 | 
2026-01-01T23:03:02 | step: 45700 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 4.242424256517552e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1497497600.0 | grad norm avg: 0.66 | grad norm last: 0.65 | 
2026-01-01T23:03:30 | step: 45800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.239160261931829e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.15 | consumed tokens: 1500774400.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T23:03:59 | step: 45900 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.235890810377896e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.03 | consumed tokens: 1504051200.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T23:04:27 | step: 46000 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.2326151742599905e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.31 | consumed tokens: 1507328000.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T23:04:56 | step: 46100 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.2293340811738744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.29 | consumed tokens: 1510604800.0 | grad norm avg: 0.65 | grad norm last: 0.6 | 
2026-01-01T23:05:24 | step: 46200 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 4.226047167321667e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.39 | consumed tokens: 1513881600.0 | grad norm avg: 0.64 | grad norm last: 0.64 | 
2026-01-01T23:05:53 | step: 46300 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 4.222754796501249e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.21 | consumed tokens: 1517158400.0 | grad norm avg: 0.65 | grad norm last: 0.66 | 
2026-01-01T23:06:21 | step: 46400 | train samples/s: 271.1 | train mfu (16-bit): -1.0 | lr mean: 4.21945660491474e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.33 | consumed tokens: 1520435200.0 | grad norm avg: 0.66 | grad norm last: 0.63 | 
2026-01-01T23:06:49 | step: 46500 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 4.216152592562139e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.26 | consumed tokens: 1523712000.0 | grad norm avg: 0.65 | grad norm last: 0.73 | 
2026-01-01T23:07:17 | step: 46600 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.212843123241328e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.31 | consumed tokens: 1526988800.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T23:07:46 | step: 46700 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.209527833154425e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.23 | consumed tokens: 1530265600.0 | grad norm avg: 0.64 | grad norm last: 0.65 | 
2026-01-01T23:08:14 | step: 46800 | train samples/s: 263.9 | train mfu (16-bit): -1.0 | lr mean: 4.206206722301431e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1533542400.0 | grad norm avg: 0.66 | grad norm last: 0.67 | 
2026-01-01T23:08:43 | step: 46900 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.202880154480226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.19 | train loss last: 3.29 | consumed tokens: 1536819200.0 | grad norm avg: 0.66 | grad norm last: 0.67 | 
2026-01-01T23:09:11 | step: 47000 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.199548129690811e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 2.95 | consumed tokens: 1540096000.0 | grad norm avg: 0.65 | grad norm last: 0.67 | 
2026-01-01T23:09:40 | step: 47100 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.1962102841353044e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.34 | consumed tokens: 1543372800.0 | grad norm avg: 0.67 | grad norm last: 0.63 | 
2026-01-01T23:10:08 | step: 47200 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 4.1928666178137064e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.53 | consumed tokens: 1546649600.0 | grad norm avg: 0.65 | grad norm last: 0.68 | 
2026-01-01T23:10:36 | step: 47300 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.1895178583217785e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.2 | consumed tokens: 1549926400.0 | grad norm avg: 0.65 | grad norm last: 0.64 | 
2026-01-01T23:11:05 | step: 47400 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.186163278063759e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.07 | consumed tokens: 1553203200.0 | grad norm avg: 0.66 | grad norm last: 0.63 | 
2026-01-01T23:11:33 | step: 47500 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 4.1828028770396486e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 2.98 | consumed tokens: 1556480000.0 | grad norm avg: 0.65 | grad norm last: 0.66 | 
2026-01-01T23:12:02 | step: 47600 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.1794370190473273e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.05 | consumed tokens: 1559756800.0 | grad norm avg: 0.65 | grad norm last: 0.61 | 
2026-01-01T23:12:30 | step: 47700 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.1760657040867954e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.36 | consumed tokens: 1563033600.0 | grad norm avg: 0.66 | grad norm last: 0.65 | 
2026-01-01T23:12:58 | step: 47800 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 4.172688932158053e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.19 | consumed tokens: 1566310400.0 | grad norm avg: 0.66 | grad norm last: 0.68 | 
2026-01-01T23:13:27 | step: 47900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.1693067032611e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1569587200.0 | grad norm avg: 0.66 | grad norm last: 0.64 | 
2026-01-01T23:13:55 | step: 48000 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.165918653598055e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.14 | consumed tokens: 1572864000.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T23:14:23 | step: 48100 | train samples/s: 270.5 | train mfu (16-bit): -1.0 | lr mean: 4.1625251469668e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 2.81 | consumed tokens: 1576140800.0 | grad norm avg: 0.66 | grad norm last: 0.69 | 
2026-01-01T23:14:52 | step: 48200 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 4.159126183367334e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.38 | consumed tokens: 1579417600.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T23:15:20 | step: 48300 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 4.155721762799658e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.05 | consumed tokens: 1582694400.0 | grad norm avg: 0.66 | grad norm last: 0.63 | 
2026-01-01T23:15:49 | step: 48400 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 4.152311885263771e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.21 | consumed tokens: 1585971200.0 | grad norm avg: 0.67 | grad norm last: 0.71 | 
2026-01-01T23:16:17 | step: 48500 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 4.148896550759673e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.25 | consumed tokens: 1589248000.0 | grad norm avg: 0.65 | grad norm last: 0.66 | 
2026-01-01T23:16:45 | step: 48600 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.145475759287365e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.13 | consumed tokens: 1592524800.0 | grad norm avg: 0.66 | grad norm last: 0.77 | 
2026-01-01T23:17:14 | step: 48700 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.142049510846846e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.18 | train loss last: 3.07 | consumed tokens: 1595801600.0 | grad norm avg: 0.67 | grad norm last: 0.66 | 
2026-01-01T23:17:42 | step: 48800 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 4.138617805438116e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.24 | consumed tokens: 1599078400.0 | grad norm avg: 0.66 | grad norm last: 0.6 | 
2026-01-01T23:18:11 | step: 48900 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 4.135180643061176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.27 | consumed tokens: 1602355200.0 | grad norm avg: 0.66 | grad norm last: 0.71 | 
2026-01-01T23:18:39 | step: 49000 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 4.131738387513906e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1605632000.0 | grad norm avg: 0.66 | grad norm last: 0.59 | 
2026-01-01T23:19:08 | step: 49100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.128290311200544e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.18 | consumed tokens: 1608908800.0 | grad norm avg: 0.65 | grad norm last: 0.6 | 
2026-01-01T23:19:36 | step: 49200 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 4.124837141716853e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.24 | consumed tokens: 1612185600.0 | grad norm avg: 0.66 | grad norm last: 0.59 | 
2026-01-01T23:20:04 | step: 49300 | train samples/s: 270.4 | train mfu (16-bit): -1.0 | lr mean: 4.121378515264951e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.07 | consumed tokens: 1615462400.0 | grad norm avg: 0.64 | grad norm last: 0.59 | 
2026-01-01T23:20:32 | step: 49400 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 4.117914431844838e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.45 | consumed tokens: 1618739200.0 | grad norm avg: 0.64 | grad norm last: 0.62 | 
2026-01-01T23:21:01 | step: 49500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.1144448914565146e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.21 | consumed tokens: 1622016000.0 | grad norm avg: 0.66 | grad norm last: 0.6 | 
2026-01-01T23:21:29 | step: 49600 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.110970257897861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.04 | consumed tokens: 1625292800.0 | grad norm avg: 0.67 | grad norm last: 0.63 | 
2026-01-01T23:21:58 | step: 49700 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 4.1074901673709974e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.32 | consumed tokens: 1628569600.0 | grad norm avg: 0.65 | grad norm last: 0.59 | 
2026-01-01T23:22:26 | step: 49800 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 4.104004619875923e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.27 | consumed tokens: 1631846400.0 | grad norm avg: 0.65 | grad norm last: 0.6 | 
2026-01-01T23:22:54 | step: 49900 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 4.100513979210518e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.21 | consumed tokens: 1635123200.0 | grad norm avg: 0.66 | grad norm last: 0.64 | 
2026-01-01T23:23:23 | step: 50000 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.097018245374784e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.02 | consumed tokens: 1638400000.0 | grad norm avg: 0.66 | grad norm last: 0.64 | 
2026-01-01T23:23:53 | step: 50100 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.093516690772958e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.32 | consumed tokens: 1641676800.0 | grad norm avg: 0.65 | grad norm last: 0.62 | 
2026-01-01T23:24:21 | step: 50200 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 4.090010406798683e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.01 | consumed tokens: 1644953600.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T23:24:50 | step: 50300 | train samples/s: 270.2 | train mfu (16-bit): -1.0 | lr mean: 4.0864986658561975e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.24 | consumed tokens: 1648230400.0 | grad norm avg: 0.66 | grad norm last: 0.67 | 
2026-01-01T23:25:18 | step: 50400 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.082981467945501e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.02 | consumed tokens: 1651507200.0 | grad norm avg: 0.66 | grad norm last: 0.63 | 
2026-01-01T23:25:46 | step: 50500 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.079459176864475e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.02 | consumed tokens: 1654784000.0 | grad norm avg: 0.66 | grad norm last: 0.67 | 
2026-01-01T23:26:15 | step: 50600 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.075931792613119e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 2.94 | consumed tokens: 1658060800.0 | grad norm avg: 0.66 | grad norm last: 0.75 | 
2026-01-01T23:26:43 | step: 50700 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 4.072398951393552e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 2.85 | consumed tokens: 1661337600.0 | grad norm avg: 0.67 | grad norm last: 0.6 | 
2026-01-01T23:27:11 | step: 50800 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.0688610170036554e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.96 | consumed tokens: 1664614400.0 | grad norm avg: 0.66 | grad norm last: 0.68 | 
2026-01-01T23:27:40 | step: 50900 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.065317989443429e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.76 | consumed tokens: 1667891200.0 | grad norm avg: 0.65 | grad norm last: 0.63 | 
2026-01-01T23:28:08 | step: 51000 | train samples/s: 270.1 | train mfu (16-bit): -1.0 | lr mean: 4.061769868712872e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 1671168000.0 | grad norm avg: 0.66 | grad norm last: 0.66 | 
2026-01-01T23:28:37 | step: 51100 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.058216291014105e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.48 | consumed tokens: 1674444800.0 | grad norm avg: 0.67 | grad norm last: 0.62 | 
2026-01-01T23:29:05 | step: 51200 | train samples/s: 270.4 | train mfu (16-bit): -1.0 | lr mean: 4.054657620145008e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.25 | consumed tokens: 1677721600.0 | grad norm avg: 0.67 | grad norm last: 0.67 | 
2026-01-01T23:29:33 | step: 51300 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.0510942199034616e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.07 | consumed tokens: 1680998400.0 | grad norm avg: 0.65 | grad norm last: 0.74 | 
2026-01-01T23:30:02 | step: 51400 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 4.047525362693705e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.26 | consumed tokens: 1684275200.0 | grad norm avg: 0.67 | grad norm last: 0.64 | 
2026-01-01T23:30:30 | step: 51500 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 4.043951048515737e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.2 | consumed tokens: 1687552000.0 | grad norm avg: 0.66 | grad norm last: 0.7 | 
2026-01-01T23:30:59 | step: 51600 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.04037200496532e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.33 | consumed tokens: 1690828800.0 | grad norm avg: 0.66 | grad norm last: 0.71 | 
2026-01-01T23:31:27 | step: 51700 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 4.0367878682445735e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.3 | consumed tokens: 1694105600.0 | grad norm avg: 0.67 | grad norm last: 0.64 | 
2026-01-01T23:31:55 | step: 51800 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.033198638353497e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 1697382400.0 | grad norm avg: 0.66 | grad norm last: 0.75 | 
2026-01-01T23:32:24 | step: 51900 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.02960431529209e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.42 | consumed tokens: 1700659200.0 | grad norm avg: 0.66 | grad norm last: 0.62 | 
2026-01-01T23:32:52 | step: 52000 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.0260048990603536e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.26 | consumed tokens: 1703936000.0 | grad norm avg: 0.67 | grad norm last: 0.69 | 
2026-01-01T23:33:20 | step: 52100 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 4.022400389658287e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.11 | consumed tokens: 1707212800.0 | grad norm avg: 0.66 | grad norm last: 0.65 | 
2026-01-01T23:33:49 | step: 52200 | train samples/s: 251.8 | train mfu (16-bit): -1.0 | lr mean: 4.0187911508837715e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.29 | consumed tokens: 1710489600.0 | grad norm avg: 0.66 | grad norm last: 0.6 | 
2026-01-01T23:34:18 | step: 52300 | train samples/s: 256.2 | train mfu (16-bit): -1.0 | lr mean: 4.015176455141045e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.41 | consumed tokens: 1713766400.0 | grad norm avg: 0.67 | grad norm last: 0.69 | 
2026-01-01T23:34:46 | step: 52400 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 4.0115570300258696e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.03 | consumed tokens: 1717043200.0 | grad norm avg: 0.66 | grad norm last: 0.65 | 
2026-01-01T23:35:14 | step: 52500 | train samples/s: 270.0 | train mfu (16-bit): -1.0 | lr mean: 4.007932511740364e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.85 | consumed tokens: 1720320000.0 | grad norm avg: 0.66 | grad norm last: 0.63 | 
2026-01-01T23:35:42 | step: 52600 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 4.004302900284529e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.17 | consumed tokens: 1723596800.0 | grad norm avg: 0.67 | grad norm last: 0.69 | 
2026-01-01T23:36:11 | step: 52700 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 4.000668559456244e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 3.26 | consumed tokens: 1726873600.0 | grad norm avg: 0.67 | grad norm last: 0.72 | 
2026-01-01T23:36:39 | step: 52800 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 3.9970291254576296e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.33 | consumed tokens: 1730150400.0 | grad norm avg: 0.66 | grad norm last: 0.69 | 
2026-01-01T23:37:07 | step: 52900 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 3.993384962086566e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.98 | consumed tokens: 1733427200.0 | grad norm avg: 0.67 | grad norm last: 0.61 | 
2026-01-01T23:37:36 | step: 53000 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 3.9897353417472914e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.47 | consumed tokens: 1736704000.0 | grad norm avg: 0.67 | grad norm last: 0.75 | 
2026-01-01T23:38:04 | step: 53100 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 3.9860813558334485e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.93 | consumed tokens: 1739980800.0 | grad norm avg: 0.67 | grad norm last: 0.72 | 
2026-01-01T23:38:32 | step: 53200 | train samples/s: 270.4 | train mfu (16-bit): -1.0 | lr mean: 3.982421912951395e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 2.85 | consumed tokens: 1743257600.0 | grad norm avg: 0.66 | grad norm last: 0.67 | 
2026-01-01T23:39:01 | step: 53300 | train samples/s: 270.3 | train mfu (16-bit): -1.0 | lr mean: 3.978758104494773e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.85 | consumed tokens: 1746534400.0 | grad norm avg: 0.68 | grad norm last: 0.67 | 
2026-01-01T23:39:29 | step: 53400 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 3.975089202867821e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.28 | consumed tokens: 1749811200.0 | grad norm avg: 0.67 | grad norm last: 0.64 | 
2026-01-01T23:39:57 | step: 53500 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 3.971415208070539e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.28 | consumed tokens: 1753088000.0 | grad norm avg: 0.67 | grad norm last: 0.61 | 
2026-01-01T23:40:26 | step: 53600 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 3.967736483900808e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.26 | consumed tokens: 1756364800.0 | grad norm avg: 0.67 | grad norm last: 0.64 | 
2026-01-01T23:40:54 | step: 53700 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 3.9640530303586274e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.18 | consumed tokens: 1759641600.0 | grad norm avg: 0.67 | grad norm last: 0.69 | 
2026-01-01T23:41:22 | step: 53800 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 3.960364483646117e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.98 | consumed tokens: 1762918400.0 | grad norm avg: 0.67 | grad norm last: 0.68 | 
2026-01-01T23:41:51 | step: 53900 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 3.9566712075611576e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.02 | consumed tokens: 1766195200.0 | grad norm avg: 0.67 | grad norm last: 0.68 | 
2026-01-01T23:42:19 | step: 54000 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 3.952973202103749e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.2 | consumed tokens: 1769472000.0 | grad norm avg: 0.67 | grad norm last: 0.69 | 
2026-01-01T23:42:47 | step: 54100 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 3.949270467273891e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.98 | consumed tokens: 1772748800.0 | grad norm avg: 0.67 | grad norm last: 0.72 | 
2026-01-01T23:43:16 | step: 54200 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 3.945563003071584e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.04 | consumed tokens: 1776025600.0 | grad norm avg: 0.68 | grad norm last: 0.7 | 
2026-01-01T23:43:44 | step: 54300 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 3.941850445698947e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.04 | consumed tokens: 1779302400.0 | grad norm avg: 0.67 | grad norm last: 0.64 | 
2026-01-01T23:44:12 | step: 54400 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 3.9381331589538604e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.87 | consumed tokens: 1782579200.0 | grad norm avg: 0.68 | grad norm last: 0.65 | 
2026-01-01T23:44:41 | step: 54500 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 3.934411142836325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.06 | consumed tokens: 1785856000.0 | grad norm avg: 0.68 | grad norm last: 0.72 | 
2026-01-01T23:45:09 | step: 54600 | train samples/s: 270.2 | train mfu (16-bit): -1.0 | lr mean: 3.930684761144221e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.3 | consumed tokens: 1789132800.0 | grad norm avg: 0.68 | grad norm last: 0.72 | 
2026-01-01T23:45:37 | step: 54700 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 3.926953286281787e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.17 | consumed tokens: 1792409600.0 | grad norm avg: 0.68 | grad norm last: 0.69 | 
2026-01-01T23:46:06 | step: 54800 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 3.923217082046904e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.27 | consumed tokens: 1795686400.0 | grad norm avg: 0.68 | grad norm last: 0.66 | 
2026-01-01T23:46:34 | step: 54900 | train samples/s: 270.0 | train mfu (16-bit): -1.0 | lr mean: 3.919476148439571e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.01 | consumed tokens: 1798963200.0 | grad norm avg: 0.67 | grad norm last: 0.72 | 
2026-01-01T23:47:02 | step: 55000 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 3.9157308492576703e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.08 | consumed tokens: 1802240000.0 | grad norm avg: 0.68 | grad norm last: 0.66 | 
2026-01-01T23:47:32 | step: 55100 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 3.9119804569054395e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.14 | consumed tokens: 1805516800.0 | grad norm avg: 0.68 | grad norm last: 0.65 | 
2026-01-01T23:48:01 | step: 55200 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 3.90822569897864e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.17 | train loss last: 2.68 | consumed tokens: 1808793600.0 | grad norm avg: 0.67 | grad norm last: 0.72 | 
2026-01-01T23:48:29 | step: 55300 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 3.9044662116793916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 1812070400.0 | grad norm avg: 0.67 | grad norm last: 0.67 | 
2026-01-01T23:48:58 | step: 55400 | train samples/s: 266.9 | train mfu (16-bit): -1.0 | lr mean: 3.900701995007694e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 1815347200.0 | grad norm avg: 0.68 | grad norm last: 0.64 | 
2026-01-01T23:49:26 | step: 55500 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 3.896933048963547e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.37 | consumed tokens: 1818624000.0 | grad norm avg: 0.68 | grad norm last: 0.61 | 
2026-01-01T23:49:55 | step: 55600 | train samples/s: 267.6 | train mfu (16-bit): -1.0 | lr mean: 3.893159737344831e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.14 | consumed tokens: 1821900800.0 | grad norm avg: 0.68 | grad norm last: 0.64 | 
2026-01-01T23:50:23 | step: 55700 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 3.8893816963536665e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.27 | consumed tokens: 1825177600.0 | grad norm avg: 0.69 | grad norm last: 0.69 | 
2026-01-01T23:50:51 | step: 55800 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 3.8855989259900525e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 2.98 | consumed tokens: 1828454400.0 | grad norm avg: 0.68 | grad norm last: 0.69 | 
2026-01-01T23:51:20 | step: 55900 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 3.88181179005187e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.31 | consumed tokens: 1831731200.0 | grad norm avg: 0.69 | grad norm last: 0.76 | 
2026-01-01T23:51:48 | step: 56000 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 3.8780199247412384e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1835008000.0 | grad norm avg: 0.68 | grad norm last: 0.68 | 
2026-01-01T23:52:17 | step: 56100 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 3.874223693856038e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.53 | consumed tokens: 1838284800.0 | grad norm avg: 0.69 | grad norm last: 0.67 | 
2026-01-01T23:52:45 | step: 56200 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 3.870422733598389e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 2.93 | consumed tokens: 1841561600.0 | grad norm avg: 0.67 | grad norm last: 0.63 | 
2026-01-01T23:53:13 | step: 56300 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 3.866617407766171e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.0 | consumed tokens: 1844838400.0 | grad norm avg: 0.69 | grad norm last: 0.71 | 
2026-01-01T23:53:42 | step: 56400 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 3.8628073525615036e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.11 | consumed tokens: 1848115200.0 | grad norm avg: 0.7 | grad norm last: 0.73 | 
2026-01-01T23:54:10 | step: 56500 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 3.858992931782268e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.05 | consumed tokens: 1851392000.0 | grad norm avg: 0.68 | grad norm last: 0.67 | 
2026-01-01T23:54:39 | step: 56600 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 3.855174145428464e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 1854668800.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-01T23:55:07 | step: 56700 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 3.8513506297022104e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.08 | consumed tokens: 1857945600.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-01T23:55:36 | step: 56800 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 3.8475227484013885e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 1861222400.0 | grad norm avg: 0.68 | grad norm last: 0.66 | 
2026-01-01T23:56:04 | step: 56900 | train samples/s: 269.6 | train mfu (16-bit): -1.0 | lr mean: 3.8436901377281174e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.21 | consumed tokens: 1864499200.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T23:56:32 | step: 57000 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 3.8398535252781585e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.66 | consumed tokens: 1867776000.0 | grad norm avg: 0.69 | grad norm last: 0.71 | 
2026-01-01T23:57:00 | step: 57100 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 3.8360121834557503e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.29 | consumed tokens: 1871052800.0 | grad norm avg: 0.69 | grad norm last: 0.73 | 
2026-01-01T23:57:29 | step: 57200 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 3.8321668398566544e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.19 | consumed tokens: 1874329600.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-01T23:57:57 | step: 57300 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 3.828316766885109e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.39 | consumed tokens: 1877606400.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-01T23:58:26 | step: 57400 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 3.8244623283389956e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.03 | consumed tokens: 1880883200.0 | grad norm avg: 0.7 | grad norm last: 0.79 | 
2026-01-01T23:58:54 | step: 57500 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 3.8206035242183134e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.16 | train loss last: 3.12 | consumed tokens: 1884160000.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-01T23:59:23 | step: 57600 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 3.816740354523063e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.35 | consumed tokens: 1887436800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-01T23:59:51 | step: 57700 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 3.8128728192532435e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.12 | consumed tokens: 1890713600.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-02T00:00:19 | step: 57800 | train samples/s: 269.8 | train mfu (16-bit): -1.0 | lr mean: 3.809000918408856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 1893990400.0 | grad norm avg: 0.71 | grad norm last: 0.7 | 
2026-01-02T00:00:47 | step: 57900 | train samples/s: 269.1 | train mfu (16-bit): -1.0 | lr mean: 3.8051246519898996e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.21 | consumed tokens: 1897267200.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-02T00:01:16 | step: 58000 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 3.801244019996375e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.93 | consumed tokens: 1900544000.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-02T00:01:44 | step: 58100 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 3.797359386226162e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.09 | consumed tokens: 1903820800.0 | grad norm avg: 0.71 | grad norm last: 0.83 | 
2026-01-02T00:02:13 | step: 58200 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 3.7934700230835006e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.1 | consumed tokens: 1907097600.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-02T00:02:41 | step: 58300 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 3.789576658164151e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.3 | consumed tokens: 1910374400.0 | grad norm avg: 0.7 | grad norm last: 0.68 | 
2026-01-02T00:03:10 | step: 58400 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 3.7856792914681137e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.8 | consumed tokens: 1913651200.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-02T00:03:38 | step: 58500 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 3.781777195399627e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.17 | consumed tokens: 1916928000.0 | grad norm avg: 0.71 | grad norm last: 0.75 | 
2026-01-02T00:04:06 | step: 58600 | train samples/s: 271.4 | train mfu (16-bit): -1.0 | lr mean: 3.777871097554453e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.27 | consumed tokens: 1920204800.0 | grad norm avg: 0.7 | grad norm last: 0.65 | 
2026-01-02T00:04:34 | step: 58700 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 3.77396063413471e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.06 | consumed tokens: 1923481600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-02T00:05:03 | step: 58800 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 3.770046168938279e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.23 | consumed tokens: 1926758400.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-02T00:05:31 | step: 58900 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 3.76612733816728e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 1930035200.0 | grad norm avg: 0.68 | grad norm last: 0.64 | 
2026-01-02T00:05:59 | step: 59000 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 3.762204505619593e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.86 | consumed tokens: 1933312000.0 | grad norm avg: 0.69 | grad norm last: 0.7 | 
2026-01-02T00:06:28 | step: 59100 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 3.7582773074973375e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.26 | consumed tokens: 1936588800.0 | grad norm avg: 0.7 | grad norm last: 0.69 | 
2026-01-02T00:06:56 | step: 59200 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 3.754346107598394e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.19 | consumed tokens: 1939865600.0 | grad norm avg: 0.71 | grad norm last: 0.79 | 
2026-01-02T00:07:24 | step: 59300 | train samples/s: 270.2 | train mfu (16-bit): -1.0 | lr mean: 3.7504105421248823e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.0 | consumed tokens: 1943142400.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-02T00:07:52 | step: 59400 | train samples/s: 270.4 | train mfu (16-bit): -1.0 | lr mean: 3.746470974874683e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.42 | consumed tokens: 1946419200.0 | grad norm avg: 0.7 | grad norm last: 0.72 | 
2026-01-02T00:08:21 | step: 59500 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 3.742527405847795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.94 | consumed tokens: 1949696000.0 | grad norm avg: 0.7 | grad norm last: 0.64 | 
2026-01-02T00:08:49 | step: 59600 | train samples/s: 269.7 | train mfu (16-bit): -1.0 | lr mean: 3.7385794712463394e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.91 | consumed tokens: 1952972800.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-02T00:09:18 | step: 59700 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 3.7346275348681957e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.24 | consumed tokens: 1956249600.0 | grad norm avg: 0.72 | grad norm last: 0.65 | 
2026-01-02T00:09:46 | step: 59800 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 3.730671596713364e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.25 | consumed tokens: 1959526400.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-02T00:10:15 | step: 59900 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 3.726711656781845e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.3 | consumed tokens: 1962803200.0 | grad norm avg: 0.71 | grad norm last: 0.67 | 
2026-01-02T00:10:43 | step: 60000 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 3.722747715073638e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.18 | consumed tokens: 1966080000.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-02T00:11:14 | step: 60100 | train samples/s: 263.9 | train mfu (16-bit): -1.0 | lr mean: 3.718779407790862e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 1969356800.0 | grad norm avg: 0.7 | grad norm last: 0.66 | 
2026-01-02T00:11:42 | step: 60200 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 3.7148070987313986e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.48 | consumed tokens: 1972633600.0 | grad norm avg: 0.7 | grad norm last: 0.71 | 
2026-01-02T00:12:10 | step: 60300 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 3.710831151693128e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.34 | consumed tokens: 1975910400.0 | grad norm avg: 0.7 | grad norm last: 0.78 | 
2026-01-02T00:12:38 | step: 60400 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 3.706850839080289e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.18 | consumed tokens: 1979187200.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-02T00:13:07 | step: 60500 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 3.702866524690762e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.33 | consumed tokens: 1982464000.0 | grad norm avg: 0.72 | grad norm last: 0.68 | 
2026-01-02T00:13:35 | step: 60600 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 3.698878572322428e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.25 | consumed tokens: 1985740800.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-02T00:14:03 | step: 60700 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 3.694886254379526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.1 | consumed tokens: 1989017600.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-02T00:14:32 | step: 60800 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 3.690890298457816e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 1992294400.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-02T00:15:00 | step: 60900 | train samples/s: 269.5 | train mfu (16-bit): -1.0 | lr mean: 3.686890340759419e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.35 | consumed tokens: 1995571200.0 | grad norm avg: 0.73 | grad norm last: 0.67 | 
2026-01-02T00:15:28 | step: 61000 | train samples/s: 272.0 | train mfu (16-bit): -1.0 | lr mean: 3.682886381284334e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.86 | consumed tokens: 1998848000.0 | grad norm avg: 0.72 | grad norm last: 0.67 | 
2026-01-02T00:15:56 | step: 61100 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 3.678878420032561e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2002124800.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-02T00:16:25 | step: 61200 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 3.674866820801981e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.07 | consumed tokens: 2005401600.0 | grad norm avg: 0.72 | grad norm last: 0.71 | 
2026-01-02T00:16:53 | step: 61300 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 3.670851219794713e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.22 | consumed tokens: 2008678400.0 | grad norm avg: 0.72 | grad norm last: 0.73 | 
2026-01-02T00:17:22 | step: 61400 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 3.666831617010757e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.2 | consumed tokens: 2011955200.0 | grad norm avg: 0.72 | grad norm last: 0.78 | 
2026-01-02T00:17:50 | step: 61500 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 3.6628083762479946e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.46 | consumed tokens: 2015232000.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-02T00:18:19 | step: 61600 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 3.658781133708544e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.04 | consumed tokens: 2018508800.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-02T00:18:47 | step: 61700 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 3.6547502531902865e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2021785600.0 | grad norm avg: 0.72 | grad norm last: 0.7 | 
2026-01-02T00:19:15 | step: 61800 | train samples/s: 269.4 | train mfu (16-bit): -1.0 | lr mean: 3.650715370895341e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.16 | consumed tokens: 2025062400.0 | grad norm avg: 0.71 | grad norm last: 0.71 | 
2026-01-02T00:19:43 | step: 61900 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 3.6466768506215885e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.01 | consumed tokens: 2028339200.0 | grad norm avg: 0.71 | grad norm last: 0.68 | 
2026-01-02T00:20:12 | step: 62000 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 3.642634328571148e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 2031616000.0 | grad norm avg: 0.71 | grad norm last: 0.84 | 
2026-01-02T00:20:40 | step: 62100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 3.638588168541901e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.16 | consumed tokens: 2034892800.0 | grad norm avg: 0.71 | grad norm last: 0.72 | 
2026-01-02T00:21:09 | step: 62200 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 3.6345380067359656e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 2.94 | consumed tokens: 2038169600.0 | grad norm avg: 0.71 | grad norm last: 0.73 | 
2026-01-02T00:21:37 | step: 62300 | train samples/s: 267.9 | train mfu (16-bit): -1.0 | lr mean: 3.630484206951223e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.99 | consumed tokens: 2041446400.0 | grad norm avg: 0.71 | grad norm last: 0.76 | 
2026-01-02T00:22:06 | step: 62400 | train samples/s: 268.4 | train mfu (16-bit): -1.0 | lr mean: 3.626426769187674e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.04 | consumed tokens: 2044723200.0 | grad norm avg: 0.74 | grad norm last: 0.81 | 
2026-01-02T00:22:34 | step: 62500 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 3.6223656934453174e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.11 | consumed tokens: 2048000000.0 | grad norm avg: 0.71 | grad norm last: 0.66 | 
2026-01-02T00:23:03 | step: 62600 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 3.618300979724154e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.69 | consumed tokens: 2051276800.0 | grad norm avg: 0.73 | grad norm last: 0.82 | 
2026-01-02T00:23:31 | step: 62700 | train samples/s: 267.5 | train mfu (16-bit): -1.0 | lr mean: 3.6142322642263025e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 2054553600.0 | grad norm avg: 0.73 | grad norm last: 0.79 | 
2026-01-02T00:24:00 | step: 62800 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 3.610159910749644e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.01 | consumed tokens: 2057830400.0 | grad norm avg: 0.74 | grad norm last: 0.8 | 
2026-01-02T00:24:28 | step: 62900 | train samples/s: 259.7 | train mfu (16-bit): -1.0 | lr mean: 3.6060839192941785e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.28 | consumed tokens: 2061107200.0 | grad norm avg: 0.72 | grad norm last: 0.76 | 
2026-01-02T00:24:56 | step: 63000 | train samples/s: 262.6 | train mfu (16-bit): -1.0 | lr mean: 3.602004289859906e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.81 | consumed tokens: 2064384000.0 | grad norm avg: 0.74 | grad norm last: 0.7 | 
2026-01-02T00:25:24 | step: 63100 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 3.597921386244707e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.34 | consumed tokens: 2067660800.0 | grad norm avg: 0.73 | grad norm last: 0.72 | 
2026-01-02T00:25:53 | step: 63200 | train samples/s: 254.9 | train mfu (16-bit): -1.0 | lr mean: 3.59383448085282e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.35 | consumed tokens: 2070937600.0 | grad norm avg: 0.72 | grad norm last: 0.79 | 
2026-01-02T00:26:21 | step: 63300 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 3.589743937482126e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.43 | consumed tokens: 2074214400.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-02T00:26:49 | step: 63400 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 3.585649756132625e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.04 | consumed tokens: 2077491200.0 | grad norm avg: 0.73 | grad norm last: 0.8 | 
2026-01-02T00:27:18 | step: 63500 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 3.581551936804317e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 2080768000.0 | grad norm avg: 0.73 | grad norm last: 0.69 | 
2026-01-02T00:27:46 | step: 63600 | train samples/s: 269.2 | train mfu (16-bit): -1.0 | lr mean: 3.5774508432950824e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.42 | consumed tokens: 2084044800.0 | grad norm avg: 0.72 | grad norm last: 0.79 | 
2026-01-02T00:28:14 | step: 63700 | train samples/s: 268.1 | train mfu (16-bit): -1.0 | lr mean: 3.573346111807041e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.0 | consumed tokens: 2087321600.0 | grad norm avg: 0.72 | grad norm last: 0.75 | 
2026-01-02T00:28:43 | step: 63800 | train samples/s: 261.6 | train mfu (16-bit): -1.0 | lr mean: 3.569237742340192e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.0 | consumed tokens: 2090598400.0 | grad norm avg: 0.74 | grad norm last: 0.67 | 
2026-01-02T00:29:11 | step: 63900 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 3.5651257348945364e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.26 | consumed tokens: 2093875200.0 | grad norm avg: 0.74 | grad norm last: 0.77 | 
2026-01-02T00:29:40 | step: 64000 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 3.561010453267954e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 2097152000.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-02T00:30:09 | step: 64100 | train samples/s: 254.6 | train mfu (16-bit): -1.0 | lr mean: 3.556891533662565e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.3 | consumed tokens: 2100428800.0 | grad norm avg: 0.74 | grad norm last: 0.84 | 
2026-01-02T00:30:38 | step: 64200 | train samples/s: 261.6 | train mfu (16-bit): -1.0 | lr mean: 3.552768976078369e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 2103705600.0 | grad norm avg: 0.75 | grad norm last: 0.77 | 
2026-01-02T00:31:07 | step: 64300 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 3.548643144313246e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.79 | consumed tokens: 2106982400.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-02T00:31:35 | step: 64400 | train samples/s: 257.0 | train mfu (16-bit): -1.0 | lr mean: 3.544513674569316e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.96 | consumed tokens: 2110259200.0 | grad norm avg: 0.75 | grad norm last: 0.83 | 
2026-01-02T00:32:04 | step: 64500 | train samples/s: 262.7 | train mfu (16-bit): -1.0 | lr mean: 3.54038093064446e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.34 | consumed tokens: 2113536000.0 | grad norm avg: 0.76 | grad norm last: 0.79 | 
2026-01-02T00:32:33 | step: 64600 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 3.5362449125386775e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.6 | consumed tokens: 2116812800.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-02T00:33:01 | step: 64700 | train samples/s: 253.4 | train mfu (16-bit): -1.0 | lr mean: 3.532105256454088e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 2.77 | consumed tokens: 2120089600.0 | grad norm avg: 0.73 | grad norm last: 0.75 | 
2026-01-02T00:33:30 | step: 64800 | train samples/s: 256.1 | train mfu (16-bit): -1.0 | lr mean: 3.527961962390691e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.31 | consumed tokens: 2123366400.0 | grad norm avg: 0.73 | grad norm last: 0.82 | 
2026-01-02T00:33:59 | step: 64900 | train samples/s: 255.8 | train mfu (16-bit): -1.0 | lr mean: 3.5238157579442486e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.91 | consumed tokens: 2126643200.0 | grad norm avg: 0.75 | grad norm last: 0.75 | 
2026-01-02T00:34:28 | step: 65000 | train samples/s: 260.5 | train mfu (16-bit): -1.0 | lr mean: 3.5196655517211184e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.91 | consumed tokens: 2129920000.0 | grad norm avg: 0.75 | grad norm last: 0.73 | 
2026-01-02T00:34:58 | step: 65100 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 3.5155124351149425e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.36 | consumed tokens: 2133196800.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-02T00:35:26 | step: 65200 | train samples/s: 268.3 | train mfu (16-bit): -1.0 | lr mean: 3.51135604432784e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2136473600.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-02T00:35:54 | step: 65300 | train samples/s: 260.5 | train mfu (16-bit): -1.0 | lr mean: 3.507196015561931e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.03 | consumed tokens: 2139750400.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-02T00:36:23 | step: 65400 | train samples/s: 255.4 | train mfu (16-bit): -1.0 | lr mean: 3.503032712615095e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.57 | consumed tokens: 2143027200.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-02T00:36:52 | step: 65500 | train samples/s: 252.7 | train mfu (16-bit): -1.0 | lr mean: 3.498866135487333e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.07 | consumed tokens: 2146304000.0 | grad norm avg: 0.76 | grad norm last: 0.73 | 
2026-01-02T00:37:20 | step: 65600 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 3.4946962841786444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.05 | consumed tokens: 2149580800.0 | grad norm avg: 0.74 | grad norm last: 0.71 | 
2026-01-02T00:37:49 | step: 65700 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 3.4905231586890295e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.77 | consumed tokens: 2152857600.0 | grad norm avg: 0.74 | grad norm last: 0.69 | 
2026-01-02T00:38:17 | step: 65800 | train samples/s: 260.0 | train mfu (16-bit): -1.0 | lr mean: 3.486346759018488e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.06 | consumed tokens: 2156134400.0 | grad norm avg: 0.74 | grad norm last: 0.74 | 
2026-01-02T00:38:46 | step: 65900 | train samples/s: 261.3 | train mfu (16-bit): -1.0 | lr mean: 3.4821670851670206e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2159411200.0 | grad norm avg: 0.76 | grad norm last: 0.7 | 
2026-01-02T00:39:15 | step: 66000 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 3.4779841371346265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.39 | consumed tokens: 2162688000.0 | grad norm avg: 0.76 | grad norm last: 0.77 | 
2026-01-02T00:39:43 | step: 66100 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 3.473797914921306e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.38 | consumed tokens: 2165964800.0 | grad norm avg: 0.76 | grad norm last: 0.72 | 
2026-01-02T00:40:12 | step: 66200 | train samples/s: 260.8 | train mfu (16-bit): -1.0 | lr mean: 3.469608418527059e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.37 | consumed tokens: 2169241600.0 | grad norm avg: 0.74 | grad norm last: 0.65 | 
2026-01-02T00:40:41 | step: 66300 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 3.465416011749767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.32 | consumed tokens: 2172518400.0 | grad norm avg: 0.77 | grad norm last: 0.91 | 
2026-01-02T00:41:09 | step: 66400 | train samples/s: 263.9 | train mfu (16-bit): -1.0 | lr mean: 3.461220330791548e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.15 | consumed tokens: 2175795200.0 | grad norm avg: 0.76 | grad norm last: 0.79 | 
2026-01-02T00:41:38 | step: 66500 | train samples/s: 251.5 | train mfu (16-bit): -1.0 | lr mean: 3.4570213756524026e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.97 | consumed tokens: 2179072000.0 | grad norm avg: 0.75 | grad norm last: 0.74 | 
2026-01-02T00:42:07 | step: 66600 | train samples/s: 252.2 | train mfu (16-bit): -1.0 | lr mean: 3.452819146332331e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.15 | train loss last: 3.41 | consumed tokens: 2182348800.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-02T00:42:36 | step: 66700 | train samples/s: 251.9 | train mfu (16-bit): -1.0 | lr mean: 3.448614006629214e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.98 | consumed tokens: 2185625600.0 | grad norm avg: 0.74 | grad norm last: 0.72 | 
2026-01-02T00:43:05 | step: 66800 | train samples/s: 260.7 | train mfu (16-bit): -1.0 | lr mean: 3.44440559274517e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.13 | consumed tokens: 2188902400.0 | grad norm avg: 0.75 | grad norm last: 0.7 | 
2026-01-02T00:43:34 | step: 66900 | train samples/s: 260.3 | train mfu (16-bit): -1.0 | lr mean: 3.4401939046802e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.5 | consumed tokens: 2192179200.0 | grad norm avg: 0.75 | grad norm last: 0.69 | 
2026-01-02T00:44:02 | step: 67000 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 3.435979306232184e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.03 | consumed tokens: 2195456000.0 | grad norm avg: 0.75 | grad norm last: 0.85 | 
2026-01-02T00:44:31 | step: 67100 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 3.431761797401123e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.95 | consumed tokens: 2198732800.0 | grad norm avg: 0.76 | grad norm last: 0.8 | 
2026-01-02T00:44:59 | step: 67200 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 3.427541014389135e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.95 | consumed tokens: 2202009600.0 | grad norm avg: 0.75 | grad norm last: 0.82 | 
2026-01-02T00:45:28 | step: 67300 | train samples/s: 250.9 | train mfu (16-bit): -1.0 | lr mean: 3.423316957196221e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.35 | consumed tokens: 2205286400.0 | grad norm avg: 0.75 | grad norm last: 0.65 | 
2026-01-02T00:45:57 | step: 67400 | train samples/s: 254.2 | train mfu (16-bit): -1.0 | lr mean: 3.419089989620261e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.11 | consumed tokens: 2208563200.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-02T00:46:26 | step: 67500 | train samples/s: 255.4 | train mfu (16-bit): -1.0 | lr mean: 3.4148601116612554e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 2211840000.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-02T00:46:54 | step: 67600 | train samples/s: 256.7 | train mfu (16-bit): -1.0 | lr mean: 3.410627323319204e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.29 | consumed tokens: 2215116800.0 | grad norm avg: 0.76 | grad norm last: 0.72 | 
2026-01-02T00:47:23 | step: 67700 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 3.4063912607962266e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.18 | consumed tokens: 2218393600.0 | grad norm avg: 0.76 | grad norm last: 0.79 | 
2026-01-02T00:47:52 | step: 67800 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 3.402152287890203e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.1 | consumed tokens: 2221670400.0 | grad norm avg: 0.77 | grad norm last: 0.87 | 
2026-01-02T00:48:20 | step: 67900 | train samples/s: 264.1 | train mfu (16-bit): -1.0 | lr mean: 3.3979104046011344e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.15 | consumed tokens: 2224947200.0 | grad norm avg: 0.78 | grad norm last: 0.8 | 
2026-01-02T00:48:49 | step: 68000 | train samples/s: 259.3 | train mfu (16-bit): -1.0 | lr mean: 3.393665247131139e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.78 | consumed tokens: 2228224000.0 | grad norm avg: 0.77 | grad norm last: 0.73 | 
2026-01-02T00:49:18 | step: 68100 | train samples/s: 261.0 | train mfu (16-bit): -1.0 | lr mean: 3.389417543075979e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.33 | consumed tokens: 2231500800.0 | grad norm avg: 0.78 | grad norm last: 0.83 | 
2026-01-02T00:49:47 | step: 68200 | train samples/s: 262.9 | train mfu (16-bit): -1.0 | lr mean: 3.385166564839892e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.94 | consumed tokens: 2234777600.0 | grad norm avg: 0.77 | grad norm last: 0.74 | 
2026-01-02T00:50:16 | step: 68300 | train samples/s: 261.7 | train mfu (16-bit): -1.0 | lr mean: 3.3809130400186405e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.14 | train loss last: 3.33 | consumed tokens: 2238054400.0 | grad norm avg: 0.77 | grad norm last: 0.77 | 
2026-01-02T00:50:45 | step: 68400 | train samples/s: 262.0 | train mfu (16-bit): -1.0 | lr mean: 3.3766562410164624e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.2 | consumed tokens: 2241331200.0 | grad norm avg: 0.77 | grad norm last: 0.72 | 
2026-01-02T00:51:13 | step: 68500 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 3.3723968954291195e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.01 | consumed tokens: 2244608000.0 | grad norm avg: 0.77 | grad norm last: 0.73 | 
2026-01-02T00:51:42 | step: 68600 | train samples/s: 261.5 | train mfu (16-bit): -1.0 | lr mean: 3.36813427566085e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.71 | consumed tokens: 2247884800.0 | grad norm avg: 0.77 | grad norm last: 0.81 | 
2026-01-02T00:52:11 | step: 68700 | train samples/s: 264.1 | train mfu (16-bit): -1.0 | lr mean: 3.363869109307416e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.0 | consumed tokens: 2251161600.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-02T00:52:40 | step: 68800 | train samples/s: 260.3 | train mfu (16-bit): -1.0 | lr mean: 3.359601032570936e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.12 | consumed tokens: 2254438400.0 | grad norm avg: 0.77 | grad norm last: 0.75 | 
2026-01-02T00:53:08 | step: 68900 | train samples/s: 253.6 | train mfu (16-bit): -1.0 | lr mean: 3.35533004545141e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2257715200.0 | grad norm avg: 0.78 | grad norm last: 0.85 | 
2026-01-02T00:53:37 | step: 69000 | train samples/s: 260.2 | train mfu (16-bit): -1.0 | lr mean: 3.351056147948839e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.36 | consumed tokens: 2260992000.0 | grad norm avg: 0.78 | grad norm last: 0.74 | 
2026-01-02T00:54:06 | step: 69100 | train samples/s: 267.6 | train mfu (16-bit): -1.0 | lr mean: 3.3467797038611025e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.11 | consumed tokens: 2264268800.0 | grad norm avg: 0.79 | grad norm last: 0.74 | 
2026-01-02T00:54:34 | step: 69200 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 3.3425003493903205e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.93 | consumed tokens: 2267545600.0 | grad norm avg: 0.77 | grad norm last: 0.99 | 
2026-01-02T00:55:03 | step: 69300 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 3.338218084536493e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2270822400.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-02T00:55:31 | step: 69400 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 3.3339329092996195e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.52 | consumed tokens: 2274099200.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-02T00:56:00 | step: 69500 | train samples/s: 263.2 | train mfu (16-bit): -1.0 | lr mean: 3.329645187477581e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.91 | consumed tokens: 2277376000.0 | grad norm avg: 0.77 | grad norm last: 0.81 | 
2026-01-02T00:56:29 | step: 69600 | train samples/s: 252.5 | train mfu (16-bit): -1.0 | lr mean: 3.325354919070378e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.3 | consumed tokens: 2280652800.0 | grad norm avg: 0.78 | grad norm last: 0.78 | 
2026-01-02T00:56:57 | step: 69700 | train samples/s: 257.7 | train mfu (16-bit): -1.0 | lr mean: 3.321061740280129e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.95 | consumed tokens: 2283929600.0 | grad norm avg: 0.79 | grad norm last: 0.8 | 
2026-01-02T00:57:26 | step: 69800 | train samples/s: 256.3 | train mfu (16-bit): -1.0 | lr mean: 3.3167656511068344e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.46 | consumed tokens: 2287206400.0 | grad norm avg: 0.79 | grad norm last: 0.78 | 
2026-01-02T00:57:54 | step: 69900 | train samples/s: 257.9 | train mfu (16-bit): -1.0 | lr mean: 3.3124673791462556e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.35 | consumed tokens: 2290483200.0 | grad norm avg: 0.8 | grad norm last: 0.79 | 
2026-01-02T00:58:23 | step: 70000 | train samples/s: 250.9 | train mfu (16-bit): -1.0 | lr mean: 3.30816583300475e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 2293760000.0 | grad norm avg: 0.78 | grad norm last: 0.83 | 
2026-01-02T00:58:53 | step: 70100 | train samples/s: 254.4 | train mfu (16-bit): -1.0 | lr mean: 3.303862104075961e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2297036800.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2026-01-02T00:59:22 | step: 70200 | train samples/s: 258.4 | train mfu (16-bit): -1.0 | lr mean: 3.2995554647641256e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.98 | consumed tokens: 2300313600.0 | grad norm avg: 0.78 | grad norm last: 0.77 | 
2026-01-02T00:59:50 | step: 70300 | train samples/s: 252.3 | train mfu (16-bit): -1.0 | lr mean: 3.2952462788671255e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.95 | consumed tokens: 2303590400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-02T01:00:19 | step: 70400 | train samples/s: 259.7 | train mfu (16-bit): -1.0 | lr mean: 3.29093418258708e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.18 | consumed tokens: 2306867200.0 | grad norm avg: 0.78 | grad norm last: 0.73 | 
2026-01-02T01:00:47 | step: 70500 | train samples/s: 258.6 | train mfu (16-bit): -1.0 | lr mean: 3.2866199035197496e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2310144000.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-02T01:01:15 | step: 70600 | train samples/s: 258.1 | train mfu (16-bit): -1.0 | lr mean: 3.282302714069374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.04 | consumed tokens: 2313420800.0 | grad norm avg: 0.79 | grad norm last: 0.77 | 
2026-01-02T01:01:44 | step: 70700 | train samples/s: 255.9 | train mfu (16-bit): -1.0 | lr mean: 3.277982978033833e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2316697600.0 | grad norm avg: 0.79 | grad norm last: 0.72 | 
2026-01-02T01:02:12 | step: 70800 | train samples/s: 257.7 | train mfu (16-bit): -1.0 | lr mean: 3.2736606954131275e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 2319974400.0 | grad norm avg: 0.79 | grad norm last: 0.81 | 
2026-01-02T01:02:41 | step: 70900 | train samples/s: 257.7 | train mfu (16-bit): -1.0 | lr mean: 3.2693362300051376e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.93 | consumed tokens: 2323251200.0 | grad norm avg: 0.79 | grad norm last: 0.94 | 
2026-01-02T01:03:09 | step: 71000 | train samples/s: 256.4 | train mfu (16-bit): -1.0 | lr mean: 3.265008854214102e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.07 | consumed tokens: 2326528000.0 | grad norm avg: 0.79 | grad norm last: 0.82 | 
2026-01-02T01:03:37 | step: 71100 | train samples/s: 257.5 | train mfu (16-bit): -1.0 | lr mean: 3.2606789318379015e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.09 | consumed tokens: 2329804800.0 | grad norm avg: 0.8 | grad norm last: 0.74 | 
2026-01-02T01:04:06 | step: 71200 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 3.256346462876536e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.72 | consumed tokens: 2333081600.0 | grad norm avg: 0.81 | grad norm last: 0.77 | 
2026-01-02T01:04:34 | step: 71300 | train samples/s: 262.9 | train mfu (16-bit): -1.0 | lr mean: 3.252011811127886e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.07 | consumed tokens: 2336358400.0 | grad norm avg: 0.8 | grad norm last: 0.72 | 
2026-01-02T01:05:02 | step: 71400 | train samples/s: 254.3 | train mfu (16-bit): -1.0 | lr mean: 3.247674248996191e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.13 | consumed tokens: 2339635200.0 | grad norm avg: 0.79 | grad norm last: 0.74 | 
2026-01-02T01:05:31 | step: 71500 | train samples/s: 260.9 | train mfu (16-bit): -1.0 | lr mean: 3.243334504077211e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.09 | consumed tokens: 2342912000.0 | grad norm avg: 0.8 | grad norm last: 0.85 | 
2026-01-02T01:05:59 | step: 71600 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 3.2389922125730664e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.02 | consumed tokens: 2346188800.0 | grad norm avg: 0.81 | grad norm last: 0.78 | 
2026-01-02T01:06:27 | step: 71700 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 3.2346477382816374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.01 | consumed tokens: 2349465600.0 | grad norm avg: 0.79 | grad norm last: 0.76 | 
2026-01-02T01:06:56 | step: 71800 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 3.230300353607163e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.0 | consumed tokens: 2352742400.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-02T01:07:24 | step: 71900 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 3.225951149943285e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.15 | consumed tokens: 2356019200.0 | grad norm avg: 0.79 | grad norm last: 0.79 | 
2026-01-02T01:07:53 | step: 72000 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 3.221599035896361e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.23 | consumed tokens: 2359296000.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-02T01:08:22 | step: 72100 | train samples/s: 255.6 | train mfu (16-bit): -1.0 | lr mean: 3.217244739062153e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.28 | consumed tokens: 2362572800.0 | grad norm avg: 0.81 | grad norm last: 0.75 | 
2026-01-02T01:08:50 | step: 72200 | train samples/s: 265.4 | train mfu (16-bit): -1.0 | lr mean: 3.21288789564278e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.82 | consumed tokens: 2365849600.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2026-01-02T01:09:19 | step: 72300 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 3.2085288694361225e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.89 | consumed tokens: 2369126400.0 | grad norm avg: 0.82 | grad norm last: 0.87 | 
2026-01-02T01:09:47 | step: 72400 | train samples/s: 256.2 | train mfu (16-bit): -1.0 | lr mean: 3.204167660442181e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.96 | consumed tokens: 2372403200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-02T01:10:16 | step: 72500 | train samples/s: 260.2 | train mfu (16-bit): -1.0 | lr mean: 3.1998039048630744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.24 | consumed tokens: 2375680000.0 | grad norm avg: 0.82 | grad norm last: 0.81 | 
2026-01-02T01:10:44 | step: 72600 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 3.1954379664966837e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 2378956800.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2026-01-02T01:11:14 | step: 72700 | train samples/s: 255.3 | train mfu (16-bit): -1.0 | lr mean: 3.191069481545128e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.4 | consumed tokens: 2382233600.0 | grad norm avg: 0.8 | grad norm last: 0.92 | 
2026-01-02T01:11:42 | step: 72800 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 3.1866991776041687e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.01 | consumed tokens: 2385510400.0 | grad norm avg: 0.8 | grad norm last: 0.78 | 
2026-01-02T01:12:11 | step: 72900 | train samples/s: 261.2 | train mfu (16-bit): -1.0 | lr mean: 3.1823263270780444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.02 | consumed tokens: 2388787200.0 | grad norm avg: 0.81 | grad norm last: 0.82 | 
2026-01-02T01:12:40 | step: 73000 | train samples/s: 251.5 | train mfu (16-bit): -1.0 | lr mean: 3.177950929966755e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.35 | consumed tokens: 2392064000.0 | grad norm avg: 0.83 | grad norm last: 0.8 | 
2026-01-02T01:13:08 | step: 73100 | train samples/s: 253.5 | train mfu (16-bit): -1.0 | lr mean: 3.1735737138660625e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.02 | consumed tokens: 2395340800.0 | grad norm avg: 0.82 | grad norm last: 0.78 | 
2026-01-02T01:13:37 | step: 73200 | train samples/s: 253.7 | train mfu (16-bit): -1.0 | lr mean: 3.1691943149780855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.81 | consumed tokens: 2398617600.0 | grad norm avg: 0.8 | grad norm last: 0.83 | 
2026-01-02T01:14:06 | step: 73300 | train samples/s: 252.8 | train mfu (16-bit): -1.0 | lr mean: 3.1648123695049435e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.12 | consumed tokens: 2401894400.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-02T01:14:35 | step: 73400 | train samples/s: 250.6 | train mfu (16-bit): -1.0 | lr mean: 3.160428605042398e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 2405171200.0 | grad norm avg: 0.81 | grad norm last: 0.79 | 
2026-01-02T01:15:04 | step: 73500 | train samples/s: 253.8 | train mfu (16-bit): -1.0 | lr mean: 3.1560422939946875e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.14 | consumed tokens: 2408448000.0 | grad norm avg: 0.82 | grad norm last: 0.82 | 
2026-01-02T01:15:32 | step: 73600 | train samples/s: 254.6 | train mfu (16-bit): -1.0 | lr mean: 3.1516541639575735e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.1 | consumed tokens: 2411724800.0 | grad norm avg: 0.83 | grad norm last: 0.92 | 
2026-01-02T01:16:01 | step: 73700 | train samples/s: 252.6 | train mfu (16-bit): -1.0 | lr mean: 3.1472634873352945e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2415001600.0 | grad norm avg: 0.82 | grad norm last: 0.74 | 
2026-01-02T01:16:30 | step: 73800 | train samples/s: 261.2 | train mfu (16-bit): -1.0 | lr mean: 3.142870991723612e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.46 | consumed tokens: 2418278400.0 | grad norm avg: 0.82 | grad norm last: 0.76 | 
2026-01-02T01:16:58 | step: 73900 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 3.138476313324645e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 2421555200.0 | grad norm avg: 0.81 | grad norm last: 0.81 | 
2026-01-02T01:17:27 | step: 74000 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 3.134079452138394e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.34 | consumed tokens: 2424832000.0 | grad norm avg: 0.82 | grad norm last: 0.79 | 
2026-01-02T01:17:56 | step: 74100 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 3.129680408164859e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.25 | consumed tokens: 2428108800.0 | grad norm avg: 0.8 | grad norm last: 0.82 | 
2026-01-02T01:18:24 | step: 74200 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 3.12527954520192e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.87 | consumed tokens: 2431385600.0 | grad norm avg: 0.82 | grad norm last: 0.86 | 
2026-01-02T01:18:52 | step: 74300 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 3.120876499451697e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.17 | consumed tokens: 2434662400.0 | grad norm avg: 0.84 | grad norm last: 0.83 | 
2026-01-02T01:19:21 | step: 74400 | train samples/s: 262.7 | train mfu (16-bit): -1.0 | lr mean: 3.1164712709141895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.87 | consumed tokens: 2437939200.0 | grad norm avg: 0.83 | grad norm last: 0.88 | 
2026-01-02T01:19:49 | step: 74500 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 3.1120642233872786e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.06 | consumed tokens: 2441216000.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2026-01-02T01:20:18 | step: 74600 | train samples/s: 266.0 | train mfu (16-bit): -1.0 | lr mean: 3.1076549930730835e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 2444492800.0 | grad norm avg: 0.83 | grad norm last: 0.79 | 
2026-01-02T01:20:47 | step: 74700 | train samples/s: 258.9 | train mfu (16-bit): -1.0 | lr mean: 3.103243943769485e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.08 | consumed tokens: 2447769600.0 | grad norm avg: 0.82 | grad norm last: 0.83 | 
2026-01-02T01:21:15 | step: 74800 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 3.098830711678602e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.77 | consumed tokens: 2451046400.0 | grad norm avg: 0.82 | grad norm last: 0.88 | 
2026-01-02T01:21:44 | step: 74900 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 3.094415660598315e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.06 | consumed tokens: 2454323200.0 | grad norm avg: 0.83 | grad norm last: 0.94 | 
2026-01-02T01:22:12 | step: 75000 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 3.0899984267307445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.97 | consumed tokens: 2457600000.0 | grad norm avg: 0.84 | grad norm last: 0.76 | 
2026-01-02T01:22:42 | step: 75100 | train samples/s: 261.2 | train mfu (16-bit): -1.0 | lr mean: 3.08557937387377e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.35 | consumed tokens: 2460876800.0 | grad norm avg: 0.84 | grad norm last: 0.82 | 
2026-01-02T01:23:11 | step: 75200 | train samples/s: 261.8 | train mfu (16-bit): -1.0 | lr mean: 3.0811585020273924e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.21 | consumed tokens: 2464153600.0 | grad norm avg: 0.83 | grad norm last: 0.89 | 
2026-01-02T01:23:39 | step: 75300 | train samples/s: 259.2 | train mfu (16-bit): -1.0 | lr mean: 3.07673544739373e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.49 | consumed tokens: 2467430400.0 | grad norm avg: 0.83 | grad norm last: 0.85 | 
2026-01-02T01:24:08 | step: 75400 | train samples/s: 253.7 | train mfu (16-bit): -1.0 | lr mean: 3.0723109375685453e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.97 | consumed tokens: 2470707200.0 | grad norm avg: 0.85 | grad norm last: 0.81 | 
2026-01-02T01:24:36 | step: 75500 | train samples/s: 259.2 | train mfu (16-bit): -1.0 | lr mean: 3.067884244956076e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 2473984000.0 | grad norm avg: 0.84 | grad norm last: 0.88 | 
2026-01-02T01:25:04 | step: 75600 | train samples/s: 257.3 | train mfu (16-bit): -1.0 | lr mean: 3.0634557333542034e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.56 | consumed tokens: 2477260800.0 | grad norm avg: 0.85 | grad norm last: 0.84 | 
2026-01-02T01:25:33 | step: 75700 | train samples/s: 260.4 | train mfu (16-bit): -1.0 | lr mean: 3.0590250389650464e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.41 | consumed tokens: 2480537600.0 | grad norm avg: 0.85 | grad norm last: 0.87 | 
2026-01-02T01:26:01 | step: 75800 | train samples/s: 260.1 | train mfu (16-bit): -1.0 | lr mean: 3.0545928893843666e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.8 | consumed tokens: 2483814400.0 | grad norm avg: 0.85 | grad norm last: 0.86 | 
2026-01-02T01:26:29 | step: 75900 | train samples/s: 261.7 | train mfu (16-bit): -1.0 | lr mean: 3.0501589208142832e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.76 | consumed tokens: 2487091200.0 | grad norm avg: 0.84 | grad norm last: 0.84 | 
2026-01-02T01:26:58 | step: 76000 | train samples/s: 255.7 | train mfu (16-bit): -1.0 | lr mean: 3.045722951355856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.9 | consumed tokens: 2490368000.0 | grad norm avg: 0.85 | grad norm last: 0.94 | 
2026-01-02T01:27:27 | step: 76100 | train samples/s: 258.3 | train mfu (16-bit): -1.0 | lr mean: 3.0412853448069654e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2493644800.0 | grad norm avg: 0.83 | grad norm last: 0.83 | 
2026-01-02T01:27:55 | step: 76200 | train samples/s: 260.1 | train mfu (16-bit): -1.0 | lr mean: 3.0368459192686714e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.95 | consumed tokens: 2496921600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2026-01-02T01:28:24 | step: 76300 | train samples/s: 266.2 | train mfu (16-bit): -1.0 | lr mean: 3.032404674740974e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.07 | consumed tokens: 2500198400.0 | grad norm avg: 0.85 | grad norm last: 0.75 | 
2026-01-02T01:28:52 | step: 76400 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 3.0279616112238728e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.3 | consumed tokens: 2503475200.0 | grad norm avg: 0.87 | grad norm last: 0.93 | 
2026-01-02T01:29:21 | step: 76500 | train samples/s: 268.0 | train mfu (16-bit): -1.0 | lr mean: 3.0235169106163085e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.14 | consumed tokens: 2506752000.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2026-01-02T01:29:49 | step: 76600 | train samples/s: 268.8 | train mfu (16-bit): -1.0 | lr mean: 3.0190703910193406e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2510028800.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2026-01-02T01:30:18 | step: 76700 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 3.0146222343319096e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.19 | consumed tokens: 2513305600.0 | grad norm avg: 0.84 | grad norm last: 0.9 | 
2026-01-02T01:30:47 | step: 76800 | train samples/s: 262.0 | train mfu (16-bit): -1.0 | lr mean: 3.010172258655075e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.06 | consumed tokens: 2516582400.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2026-01-02T01:31:15 | step: 76900 | train samples/s: 260.2 | train mfu (16-bit): -1.0 | lr mean: 3.0057208277867176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.22 | consumed tokens: 2519859200.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2026-01-02T01:31:43 | step: 77000 | train samples/s: 258.5 | train mfu (16-bit): -1.0 | lr mean: 3.0012675779289566e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 2523136000.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2026-01-02T01:32:12 | step: 77100 | train samples/s: 256.3 | train mfu (16-bit): -1.0 | lr mean: 2.9968126909807324e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.19 | consumed tokens: 2526412800.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2026-01-02T01:32:40 | step: 77200 | train samples/s: 260.4 | train mfu (16-bit): -1.0 | lr mean: 2.992356166942045e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.14 | consumed tokens: 2529689600.0 | grad norm avg: 0.86 | grad norm last: 0.84 | 
2026-01-02T01:33:08 | step: 77300 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 2.9878980058128946e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.9 | consumed tokens: 2532966400.0 | grad norm avg: 0.85 | grad norm last: 0.92 | 
2026-01-02T01:33:37 | step: 77400 | train samples/s: 252.6 | train mfu (16-bit): -1.0 | lr mean: 2.9834382075932808e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.93 | consumed tokens: 2536243200.0 | grad norm avg: 0.9 | grad norm last: 0.74 | 
2026-01-02T01:34:06 | step: 77500 | train samples/s: 255.8 | train mfu (16-bit): -1.0 | lr mean: 2.9789769541821443e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.37 | consumed tokens: 2539520000.0 | grad norm avg: 0.86 | grad norm last: 0.92 | 
2026-01-02T01:34:35 | step: 77600 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 2.9745140636805445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2542796800.0 | grad norm avg: 0.87 | grad norm last: 0.89 | 
2026-01-02T01:35:03 | step: 77700 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 2.9700495360884815e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.96 | consumed tokens: 2546073600.0 | grad norm avg: 0.85 | grad norm last: 0.83 | 
2026-01-02T01:35:32 | step: 77800 | train samples/s: 263.2 | train mfu (16-bit): -1.0 | lr mean: 2.9655835533048958e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.89 | consumed tokens: 2549350400.0 | grad norm avg: 0.88 | grad norm last: 0.99 | 
2026-01-02T01:36:00 | step: 77900 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 2.9611159334308468e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.88 | consumed tokens: 2552627200.0 | grad norm avg: 0.88 | grad norm last: 0.86 | 
2026-01-02T01:36:29 | step: 78000 | train samples/s: 263.2 | train mfu (16-bit): -1.0 | lr mean: 2.956646858365275e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.01 | consumed tokens: 2555904000.0 | grad norm avg: 0.86 | grad norm last: 1.02 | 
2026-01-02T01:36:58 | step: 78100 | train samples/s: 262.3 | train mfu (16-bit): -1.0 | lr mean: 2.9521763281081803e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.25 | consumed tokens: 2559180800.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2026-01-02T01:37:27 | step: 78200 | train samples/s: 249.5 | train mfu (16-bit): -1.0 | lr mean: 2.9477041607606225e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.02 | consumed tokens: 2562457600.0 | grad norm avg: 0.87 | grad norm last: 0.92 | 
2026-01-02T01:37:55 | step: 78300 | train samples/s: 257.9 | train mfu (16-bit): -1.0 | lr mean: 2.943230720120482e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.17 | consumed tokens: 2565734400.0 | grad norm avg: 0.87 | grad norm last: 0.9 | 
2026-01-02T01:38:23 | step: 78400 | train samples/s: 259.8 | train mfu (16-bit): -1.0 | lr mean: 2.9387556423898786e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.25 | consumed tokens: 2569011200.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2026-01-02T01:38:52 | step: 78500 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 2.9342791094677523e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.97 | consumed tokens: 2572288000.0 | grad norm avg: 0.87 | grad norm last: 0.88 | 
2026-01-02T01:39:21 | step: 78600 | train samples/s: 262.1 | train mfu (16-bit): -1.0 | lr mean: 2.9298013032530434e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.24 | consumed tokens: 2575564800.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2026-01-02T01:39:49 | step: 78700 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 2.9253220418468118e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.44 | consumed tokens: 2578841600.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-02T01:40:18 | step: 78800 | train samples/s: 259.6 | train mfu (16-bit): -1.0 | lr mean: 2.9208413252490573e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.43 | consumed tokens: 2582118400.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2026-01-02T01:40:46 | step: 78900 | train samples/s: 261.3 | train mfu (16-bit): -1.0 | lr mean: 2.91635915345978e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.22 | consumed tokens: 2585395200.0 | grad norm avg: 0.87 | grad norm last: 0.86 | 
2026-01-02T01:41:15 | step: 79000 | train samples/s: 258.5 | train mfu (16-bit): -1.0 | lr mean: 2.91187570837792e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.88 | consumed tokens: 2588672000.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2026-01-02T01:41:43 | step: 79100 | train samples/s: 257.7 | train mfu (16-bit): -1.0 | lr mean: 2.9073909900034778e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 3.04 | consumed tokens: 2591948800.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2026-01-02T01:42:12 | step: 79200 | train samples/s: 259.9 | train mfu (16-bit): -1.0 | lr mean: 2.9029048164375126e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.91 | consumed tokens: 2595225600.0 | grad norm avg: 0.86 | grad norm last: 0.91 | 
2026-01-02T01:42:40 | step: 79300 | train samples/s: 259.6 | train mfu (16-bit): -1.0 | lr mean: 2.898417369578965e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 2598502400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-02T01:43:09 | step: 79400 | train samples/s: 252.5 | train mfu (16-bit): -1.0 | lr mean: 2.8939284675288945e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.38 | consumed tokens: 2601779200.0 | grad norm avg: 0.88 | grad norm last: 0.91 | 
2026-01-02T01:43:38 | step: 79500 | train samples/s: 253.8 | train mfu (16-bit): -1.0 | lr mean: 2.889438474085182e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.11 | consumed tokens: 2605056000.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-02T01:44:06 | step: 79600 | train samples/s: 258.7 | train mfu (16-bit): -1.0 | lr mean: 2.8849470254499465e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.04 | consumed tokens: 2608332800.0 | grad norm avg: 0.9 | grad norm last: 0.9 | 
2026-01-02T01:44:34 | step: 79700 | train samples/s: 261.4 | train mfu (16-bit): -1.0 | lr mean: 2.8804543035221286e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.31 | consumed tokens: 2611609600.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2026-01-02T01:45:03 | step: 79800 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 2.8759604902006686e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2614886400.0 | grad norm avg: 0.89 | grad norm last: 0.85 | 
2026-01-02T01:45:31 | step: 79900 | train samples/s: 256.2 | train mfu (16-bit): -1.0 | lr mean: 2.871465403586626e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.96 | consumed tokens: 2618163200.0 | grad norm avg: 0.86 | grad norm last: 0.89 | 
2026-01-02T01:46:00 | step: 80000 | train samples/s: 259.9 | train mfu (16-bit): -1.0 | lr mean: 2.866969043680001e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.26 | consumed tokens: 2621440000.0 | grad norm avg: 0.88 | grad norm last: 0.83 | 
2026-01-02T01:46:30 | step: 80100 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 2.8624714104807936e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.89 | consumed tokens: 2624716800.0 | grad norm avg: 0.9 | grad norm last: 0.78 | 
2026-01-02T01:47:00 | step: 80200 | train samples/s: 257.4 | train mfu (16-bit): -1.0 | lr mean: 2.857972685887944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.94 | consumed tokens: 2627993600.0 | grad norm avg: 0.88 | grad norm last: 0.9 | 
2026-01-02T01:47:28 | step: 80300 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 2.853472688002512e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.29 | consumed tokens: 2631270400.0 | grad norm avg: 0.91 | grad norm last: 0.97 | 
2026-01-02T01:47:57 | step: 80400 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 2.8489715987234376e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.22 | consumed tokens: 2634547200.0 | grad norm avg: 0.88 | grad norm last: 0.89 | 
2026-01-02T01:48:25 | step: 80500 | train samples/s: 257.5 | train mfu (16-bit): -1.0 | lr mean: 2.8444694180507213e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.99 | consumed tokens: 2637824000.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2026-01-02T01:48:54 | step: 80600 | train samples/s: 254.7 | train mfu (16-bit): -1.0 | lr mean: 2.8399661459843628e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.09 | consumed tokens: 2641100800.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2026-01-02T01:49:23 | step: 80700 | train samples/s: 252.5 | train mfu (16-bit): -1.0 | lr mean: 2.835461600625422e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.94 | consumed tokens: 2644377600.0 | grad norm avg: 0.91 | grad norm last: 0.94 | 
2026-01-02T01:49:52 | step: 80800 | train samples/s: 252.1 | train mfu (16-bit): -1.0 | lr mean: 2.8309559638728388e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.08 | consumed tokens: 2647654400.0 | grad norm avg: 0.89 | grad norm last: 0.84 | 
2026-01-02T01:50:21 | step: 80900 | train samples/s: 255.0 | train mfu (16-bit): -1.0 | lr mean: 2.8264492357266136e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.16 | consumed tokens: 2650931200.0 | grad norm avg: 0.9 | grad norm last: 0.96 | 
2026-01-02T01:50:49 | step: 81000 | train samples/s: 253.9 | train mfu (16-bit): -1.0 | lr mean: 2.8219415980856866e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 2654208000.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2026-01-02T01:51:18 | step: 81100 | train samples/s: 254.7 | train mfu (16-bit): -1.0 | lr mean: 2.817432687152177e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.43 | consumed tokens: 2657484800.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2026-01-02T01:51:47 | step: 81200 | train samples/s: 250.9 | train mfu (16-bit): -1.0 | lr mean: 2.812922866723966e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.21 | consumed tokens: 2660761600.0 | grad norm avg: 0.88 | grad norm last: 0.95 | 
2026-01-02T01:52:16 | step: 81300 | train samples/s: 256.2 | train mfu (16-bit): -1.0 | lr mean: 2.8084119549021125e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.32 | consumed tokens: 2664038400.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2026-01-02T01:52:45 | step: 81400 | train samples/s: 255.0 | train mfu (16-bit): -1.0 | lr mean: 2.8039001335855573e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.28 | consumed tokens: 2667315200.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2026-01-02T01:53:13 | step: 81500 | train samples/s: 258.5 | train mfu (16-bit): -1.0 | lr mean: 2.79938722087536e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.18 | consumed tokens: 2670592000.0 | grad norm avg: 0.91 | grad norm last: 0.86 | 
2026-01-02T01:53:42 | step: 81600 | train samples/s: 265.8 | train mfu (16-bit): -1.0 | lr mean: 2.794873398670461e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.2 | consumed tokens: 2673868800.0 | grad norm avg: 0.89 | grad norm last: 1.0 | 
2026-01-02T01:54:10 | step: 81700 | train samples/s: 269.3 | train mfu (16-bit): -1.0 | lr mean: 2.79035848507192e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.78 | consumed tokens: 2677145600.0 | grad norm avg: 0.88 | grad norm last: 0.98 | 
2026-01-02T01:54:38 | step: 81800 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 2.785842661978677e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.54 | consumed tokens: 2680422400.0 | grad norm avg: 0.89 | grad norm last: 0.86 | 
2026-01-02T01:55:07 | step: 81900 | train samples/s: 263.1 | train mfu (16-bit): -1.0 | lr mean: 2.7813259293907322e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.22 | consumed tokens: 2683699200.0 | grad norm avg: 0.91 | grad norm last: 1.08 | 
2026-01-02T01:55:35 | step: 82000 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 2.7768082873080857e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.03 | consumed tokens: 2686976000.0 | grad norm avg: 0.9 | grad norm last: 1.04 | 
2026-01-02T01:56:04 | step: 82100 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 2.7722897357307374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.95 | consumed tokens: 2690252800.0 | grad norm avg: 0.9 | grad norm last: 0.82 | 
2026-01-02T01:56:32 | step: 82200 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 2.7677702746586874e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.07 | consumed tokens: 2693529600.0 | grad norm avg: 0.89 | grad norm last: 0.97 | 
2026-01-02T01:57:01 | step: 82300 | train samples/s: 258.3 | train mfu (16-bit): -1.0 | lr mean: 2.7632499040919356e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.24 | consumed tokens: 2696806400.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2026-01-02T01:57:29 | step: 82400 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 2.758728624030482e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.38 | consumed tokens: 2700083200.0 | grad norm avg: 0.9 | grad norm last: 0.86 | 
2026-01-02T01:57:58 | step: 82500 | train samples/s: 264.4 | train mfu (16-bit): -1.0 | lr mean: 2.754206616373267e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.18 | consumed tokens: 2703360000.0 | grad norm avg: 0.9 | grad norm last: 0.89 | 
2026-01-02T01:58:26 | step: 82600 | train samples/s: 262.3 | train mfu (16-bit): -1.0 | lr mean: 2.7496836992213503e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.0 | consumed tokens: 2706636800.0 | grad norm avg: 0.91 | grad norm last: 1.07 | 
2026-01-02T01:58:55 | step: 82700 | train samples/s: 253.8 | train mfu (16-bit): -1.0 | lr mean: 2.745160054473672e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 2.97 | consumed tokens: 2709913600.0 | grad norm avg: 0.92 | grad norm last: 0.99 | 
2026-01-02T01:59:24 | step: 82800 | train samples/s: 254.3 | train mfu (16-bit): -1.0 | lr mean: 2.740635500231292e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.1 | consumed tokens: 2713190400.0 | grad norm avg: 0.91 | grad norm last: 0.8 | 
2026-01-02T01:59:53 | step: 82900 | train samples/s: 255.2 | train mfu (16-bit): -1.0 | lr mean: 2.7361102183931507e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 2.93 | consumed tokens: 2716467200.0 | grad norm avg: 0.9 | grad norm last: 0.99 | 
2026-01-02T02:00:21 | step: 83000 | train samples/s: 264.2 | train mfu (16-bit): -1.0 | lr mean: 2.731584208959248e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.49 | consumed tokens: 2719744000.0 | grad norm avg: 0.91 | grad norm last: 1.11 | 
2026-01-02T02:00:50 | step: 83100 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 2.7270572900306433e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 2723020800.0 | grad norm avg: 0.91 | grad norm last: 0.95 | 
2026-01-02T02:01:18 | step: 83200 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 2.7225296435062774e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 2726297600.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2026-01-02T02:01:47 | step: 83300 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 2.7180014512850903e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.05 | consumed tokens: 2729574400.0 | grad norm avg: 0.89 | grad norm last: 0.89 | 
2026-01-02T02:02:15 | step: 83400 | train samples/s: 268.2 | train mfu (16-bit): -1.0 | lr mean: 2.7134723495692015e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.1 | consumed tokens: 2732851200.0 | grad norm avg: 0.92 | grad norm last: 0.91 | 
2026-01-02T02:02:44 | step: 83500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 2.7089427021564916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.86 | consumed tokens: 2736128000.0 | grad norm avg: 0.93 | grad norm last: 0.98 | 
2026-01-02T02:03:13 | step: 83600 | train samples/s: 261.8 | train mfu (16-bit): -1.0 | lr mean: 2.7044123271480203e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.44 | consumed tokens: 2739404800.0 | grad norm avg: 0.91 | grad norm last: 0.9 | 
2026-01-02T02:03:41 | step: 83700 | train samples/s: 267.1 | train mfu (16-bit): -1.0 | lr mean: 2.6998812245437875e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.2 | consumed tokens: 2742681600.0 | grad norm avg: 0.91 | grad norm last: 0.84 | 
2026-01-02T02:04:09 | step: 83800 | train samples/s: 262.7 | train mfu (16-bit): -1.0 | lr mean: 2.6953493943437934e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.04 | consumed tokens: 2745958400.0 | grad norm avg: 0.91 | grad norm last: 0.93 | 
2026-01-02T02:04:38 | step: 83900 | train samples/s: 254.8 | train mfu (16-bit): -1.0 | lr mean: 2.6908170184469782e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.2 | consumed tokens: 2749235200.0 | grad norm avg: 0.91 | grad norm last: 0.83 | 
2026-01-02T02:05:07 | step: 84000 | train samples/s: 263.4 | train mfu (16-bit): -1.0 | lr mean: 2.686284096853342e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2752512000.0 | grad norm avg: 0.91 | grad norm last: 0.88 | 
2026-01-02T02:05:35 | step: 84100 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 2.6817504476639442e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.11 | consumed tokens: 2755788800.0 | grad norm avg: 0.92 | grad norm last: 0.98 | 
2026-01-02T02:06:04 | step: 84200 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 2.6772162527777255e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.98 | consumed tokens: 2759065600.0 | grad norm avg: 0.92 | grad norm last: 0.95 | 
2026-01-02T02:06:32 | step: 84300 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 2.6726815121946856e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.42 | consumed tokens: 2762342400.0 | grad norm avg: 0.92 | grad norm last: 0.89 | 
2026-01-02T02:07:01 | step: 84400 | train samples/s: 265.3 | train mfu (16-bit): -1.0 | lr mean: 2.6681462259148248e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.2 | consumed tokens: 2765619200.0 | grad norm avg: 0.92 | grad norm last: 1.07 | 
2026-01-02T02:07:29 | step: 84500 | train samples/s: 266.6 | train mfu (16-bit): -1.0 | lr mean: 2.6636103939381428e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.26 | consumed tokens: 2768896000.0 | grad norm avg: 0.92 | grad norm last: 1.0 | 
2026-01-02T02:07:58 | step: 84600 | train samples/s: 259.5 | train mfu (16-bit): -1.0 | lr mean: 2.6590738343656994e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.17 | consumed tokens: 2772172800.0 | grad norm avg: 0.91 | grad norm last: 0.91 | 
2026-01-02T02:08:26 | step: 84700 | train samples/s: 268.9 | train mfu (16-bit): -1.0 | lr mean: 2.6545369109953754e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.51 | consumed tokens: 2775449600.0 | grad norm avg: 0.91 | grad norm last: 0.85 | 
2026-01-02T02:08:55 | step: 84800 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 2.6499994419282302e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.49 | consumed tokens: 2778726400.0 | grad norm avg: 0.92 | grad norm last: 0.9 | 
2026-01-02T02:09:23 | step: 84900 | train samples/s: 264.6 | train mfu (16-bit): -1.0 | lr mean: 2.6454616090632044e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.21 | consumed tokens: 2782003200.0 | grad norm avg: 0.91 | grad norm last: 0.99 | 
2026-01-02T02:09:52 | step: 85000 | train samples/s: 254.6 | train mfu (16-bit): -1.0 | lr mean: 2.6409232305013575e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.11 | consumed tokens: 2785280000.0 | grad norm avg: 0.92 | grad norm last: 0.93 | 
2026-01-02T02:10:23 | step: 85100 | train samples/s: 253.0 | train mfu (16-bit): -1.0 | lr mean: 2.6363843062426895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.5 | consumed tokens: 2788556800.0 | grad norm avg: 0.92 | grad norm last: 0.86 | 
2026-01-02T02:10:52 | step: 85200 | train samples/s: 253.5 | train mfu (16-bit): -1.0 | lr mean: 2.6318450181861408e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.01 | consumed tokens: 2791833600.0 | grad norm avg: 0.92 | grad norm last: 0.85 | 
2026-01-02T02:11:21 | step: 85300 | train samples/s: 249.9 | train mfu (16-bit): -1.0 | lr mean: 2.6273053663317114e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.27 | consumed tokens: 2795110400.0 | grad norm avg: 0.94 | grad norm last: 0.91 | 
2026-01-02T02:11:50 | step: 85400 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 2.622765168780461e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.13 | train loss last: 2.76 | consumed tokens: 2798387200.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2026-01-02T02:12:18 | step: 85500 | train samples/s: 268.7 | train mfu (16-bit): -1.0 | lr mean: 2.6182246074313298e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.01 | consumed tokens: 2801664000.0 | grad norm avg: 0.93 | grad norm last: 0.9 | 
2026-01-02T02:12:47 | step: 85600 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 2.613683682284318e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.17 | consumed tokens: 2804940800.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2026-01-02T02:13:15 | step: 85700 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 2.6091423933394253e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.05 | consumed tokens: 2808217600.0 | grad norm avg: 0.93 | grad norm last: 0.92 | 
2026-01-02T02:13:44 | step: 85800 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 2.604600740596652e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.95 | consumed tokens: 2811494400.0 | grad norm avg: 0.93 | grad norm last: 0.85 | 
2026-01-02T02:14:12 | step: 85900 | train samples/s: 257.0 | train mfu (16-bit): -1.0 | lr mean: 2.600058724055998e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.08 | train loss last: 2.83 | consumed tokens: 2814771200.0 | grad norm avg: 0.94 | grad norm last: 0.82 | 
2026-01-02T02:14:41 | step: 86000 | train samples/s: 256.1 | train mfu (16-bit): -1.0 | lr mean: 2.5955165256164037e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.26 | consumed tokens: 2818048000.0 | grad norm avg: 0.94 | grad norm last: 0.95 | 
2026-01-02T02:15:09 | step: 86100 | train samples/s: 255.6 | train mfu (16-bit): -1.0 | lr mean: 2.5909737814799882e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.23 | consumed tokens: 2821324800.0 | grad norm avg: 0.92 | grad norm last: 0.92 | 
2026-01-02T02:15:38 | step: 86200 | train samples/s: 258.4 | train mfu (16-bit): -1.0 | lr mean: 2.5864308554446325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.98 | consumed tokens: 2824601600.0 | grad norm avg: 0.93 | grad norm last: 1.02 | 
2026-01-02T02:16:06 | step: 86300 | train samples/s: 256.7 | train mfu (16-bit): -1.0 | lr mean: 2.5818877475103363e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.55 | consumed tokens: 2827878400.0 | grad norm avg: 0.92 | grad norm last: 0.97 | 
2026-01-02T02:16:34 | step: 86400 | train samples/s: 259.7 | train mfu (16-bit): -1.0 | lr mean: 2.5773442757781595e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.04 | consumed tokens: 2831155200.0 | grad norm avg: 0.93 | grad norm last: 1.06 | 
2026-01-02T02:17:02 | step: 86500 | train samples/s: 259.8 | train mfu (16-bit): -1.0 | lr mean: 2.5728006221470423e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.47 | consumed tokens: 2834432000.0 | grad norm avg: 0.94 | grad norm last: 0.83 | 
2026-01-02T02:17:31 | step: 86600 | train samples/s: 254.9 | train mfu (16-bit): -1.0 | lr mean: 2.5682566047180444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.21 | consumed tokens: 2837708800.0 | grad norm avg: 0.94 | grad norm last: 0.9 | 
2026-01-02T02:17:59 | step: 86700 | train samples/s: 261.4 | train mfu (16-bit): -1.0 | lr mean: 2.563712405390106e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 2.99 | consumed tokens: 2840985600.0 | grad norm avg: 0.93 | grad norm last: 0.86 | 
2026-01-02T02:18:28 | step: 86800 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 2.559168206062168e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 3.46 | consumed tokens: 2844262400.0 | grad norm avg: 0.95 | grad norm last: 0.82 | 
2026-01-02T02:18:56 | step: 86900 | train samples/s: 257.5 | train mfu (16-bit): -1.0 | lr mean: 2.554623642936349e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.1 | consumed tokens: 2847539200.0 | grad norm avg: 0.93 | grad norm last: 0.94 | 
2026-01-02T02:19:25 | step: 87000 | train samples/s: 253.3 | train mfu (16-bit): -1.0 | lr mean: 2.5500788979115896e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.0 | consumed tokens: 2850816000.0 | grad norm avg: 0.93 | grad norm last: 0.99 | 
2026-01-02T02:19:53 | step: 87100 | train samples/s: 258.8 | train mfu (16-bit): -1.0 | lr mean: 2.54553397098789e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.08 | train loss last: 2.91 | consumed tokens: 2854092800.0 | grad norm avg: 0.93 | grad norm last: 1.03 | 
2026-01-02T02:20:22 | step: 87200 | train samples/s: 256.7 | train mfu (16-bit): -1.0 | lr mean: 2.5409890440641902e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 2.98 | consumed tokens: 2857369600.0 | grad norm avg: 0.95 | grad norm last: 0.96 | 
2026-01-02T02:20:50 | step: 87300 | train samples/s: 257.6 | train mfu (16-bit): -1.0 | lr mean: 2.53644375334261e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 3.27 | consumed tokens: 2860646400.0 | grad norm avg: 0.95 | grad norm last: 0.85 | 
2026-01-02T02:21:18 | step: 87400 | train samples/s: 263.6 | train mfu (16-bit): -1.0 | lr mean: 2.5318986445199698e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.02 | consumed tokens: 2863923200.0 | grad norm avg: 0.95 | grad norm last: 1.01 | 
2026-01-02T02:21:46 | step: 87500 | train samples/s: 263.3 | train mfu (16-bit): -1.0 | lr mean: 2.527353171899449e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.09 | train loss last: 2.86 | consumed tokens: 2867200000.0 | grad norm avg: 0.95 | grad norm last: 0.92 | 
2026-01-02T02:22:15 | step: 87600 | train samples/s: 260.6 | train mfu (16-bit): -1.0 | lr mean: 2.5228078811778687e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.11 | train loss last: 2.8 | consumed tokens: 2870476800.0 | grad norm avg: 0.96 | grad norm last: 0.86 | 
2026-01-02T02:22:44 | step: 87700 | train samples/s: 253.8 | train mfu (16-bit): -1.0 | lr mean: 2.5182622266584076e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.1 | train loss last: 3.23 | consumed tokens: 2873753600.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2026-01-02T02:23:12 | step: 87800 | train samples/s: 259.1 | train mfu (16-bit): -1.0 | lr mean: 2.513716754037887e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.12 | train loss last: 3.35 | consumed tokens: 2877030400.0 | grad norm avg: 0.96 | grad norm last: 0.92 | 
