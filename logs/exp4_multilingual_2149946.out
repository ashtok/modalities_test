==========================================
Experiment 4: Fine-tuning GPT-2 on 5 languages with 4 GPUs
Job ID: 2149946
Node: jnfat02
Start time: Thu Jan  1 07:25:51 PM CET 2026
==========================================
Thu Jan  1 19:25:52 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:05:00.0 Off |                    0 |
| N/A   29C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:06:00.0 Off |                    0 |
| N/A   30C    P8             32W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA L40S                    On  |   00000000:45:00.0 Off |                    0 |
| N/A   29C    P8             34W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA L40S                    On  |   00000000:46:00.0 Off |                    0 |
| N/A   30C    P8             33W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Rank 0 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 1 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 2 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Rank 3 received experiment_id: 2026-01-01__19-26-08_a91e58afaade00f6
Instantiated <class 'int'>: settings -> training_target -> num_target_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: settings -> training_target -> num_target_steps
Instantiated <class 'modalities.models.huggingface.huggingface_model.HuggingFacePretrainedModel'>: model_raw

Wrapped layer classes: [<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>]

Instantiated <class 'torch.distributed.fsdp.fully_sharded_data_parallel.FullyShardedDataParallel'>: wrapped_model
=> optimizer groups:
all (148 modules with 31,109,952 parameters): weight_decay = 0.01
=> all (148 modules with 31,109,952 parameters)
Instantiated <class 'torch.optim.adamw.AdamW'>: optimizer
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps -> config -> global_num_tokens
Instantiated <class 'int'>: lr_scheduler -> config -> total_steps
Instantiated <class 'torch.optim.lr_scheduler.OneCycleLR'>: lr_scheduler
Instantiated <class 'modalities.checkpointing.stateful.app_state.AppState'>: app_state
Instantiated <class 'modalities.loss_functions.CLMCrossEntropyLoss'>: loss_fn
Instantiated <class 'modalities.dataloader.dataset.PackedMemMapDatasetContinuous'>: train_dataset
Instantiated <class 'modalities.dataloader.samplers.ResumableDistributedSampler'>: train_dataloader -> config -> batch_sampler -> config -> sampler
Instantiated <class 'torch.utils.data.sampler.BatchSampler'>: train_dataloader -> config -> batch_sampler
Instantiated <class 'modalities.models.gpt2.collator.GPT2LLMCollateFn'>: collate_fn
Instantiated <class 'modalities.dataloader.dataloader.LLMDataLoader'>: train_dataloader
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps -> config -> global_num_tokens
Instantiated <class 'int'>: progress_subscriber -> config -> num_target_steps
Instantiated <class 'modalities.logging_broker.subscriber_impl.progress_subscriber.RichProgressSubscriber'>: progress_subscriber
Instantiated <class 'modalities.logging_broker.subscriber_impl.results_subscriber.WandBEvaluationResultSubscriber'>: evaluation_subscriber
Instantiated <class 'modalities.checkpointing.checkpoint_saving_strategies.SaveKMostRecentCheckpointsStrategy'>: checkpoint_saving -> config -> checkpoint_saving_strategy
Instantiated <class 'modalities.checkpointing.fsdp.fsdp_checkpoint_saving.FSDP1CheckpointSaving'>: checkpoint_saving -> config -> checkpoint_saving_execution
Instantiated <class 'modalities.checkpointing.checkpoint_saving.CheckpointSaving'>: checkpoint_saving
Instantiated <class 'modalities.training.gradient_clipping.fsdp_gradient_clipper.FSDP1GradientClipper'>: gradient_clipper
Model initialized at 2026-01-01 19:26:12.213544.



======================== Training Report ========================
Training target: 
	num_target_tokens: 5713166336
	num_target_steps: 174352 
Intervals: 
	training_log_interval_in_steps: 100
	checkpointing_interval_in_steps: 5000
	evaluation_interval_in_steps: 1000
Step profile: 
	gradient_accumulation_steps: 4
	local_train_micro_batch_size: 4
	sequence_length: 512
	dp_degree: 4
CUDA environment settings: 
	local_rank: 0
	world_size: 4
	global_rank: 0
Consistency enforcement: 
	enforce_tokens_per_step_consistency: True
	enforce_last_step_logged: False
	enforce_last_step_evaluated: False
	enforce_last_step_checkpointed: False
Training progress: 
	global_num_seen_tokens: 0
	num_seen_steps: 0
	num_seen_samples: 0
	last_step: -1
Warnings: 
	[38;5;214mNumber of tokens in the dataset (5713177600) does not match the number of target tokens (5713166336). Missing 0.00% of tokens in the dataset.
	Last step will not be logged. Since remaining_steps (174352) is not a multiple of training_log_interval_in_steps (100).
	Last step will not be evaluated. Since remaining_steps (174352) is not a multiple of evaluation_interval_in_steps (1000).
	Last step will not be checkpointed. Since remaining_steps (174352) is not a multiple of checkpointing_interval_in_steps (5000). [0m 
====================================================================



Start model training at 2026-01-01 19:26:12.213815.
2026-01-01T19:26:42 | step: 100 | train samples/s: 244.7 | train mfu (16-bit): -1.0 | lr mean: 5.36468678546953e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.29 | train loss last: 4.28 | consumed tokens: 3276800.0 | grad norm avg: 1.8 | grad norm last: 1.68 | 
2026-01-01T19:27:11 | step: 200 | train samples/s: 254.1 | train mfu (16-bit): -1.0 | lr mean: 6.446925453928998e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.18 | train loss last: 4.29 | consumed tokens: 6553600.0 | grad norm avg: 1.39 | grad norm last: 1.3 | 
2026-01-01T19:27:39 | step: 300 | train samples/s: 265.0 | train mfu (16-bit): -1.0 | lr mean: 8.21163303044159e-06 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.14 | train loss last: 4.04 | consumed tokens: 9830400.0 | grad norm avg: 1.32 | grad norm last: 1.36 | 
2026-01-01T19:28:07 | step: 400 | train samples/s: 268.5 | train mfu (16-bit): -1.0 | lr mean: 1.0601604117255192e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.1 | train loss last: 3.94 | consumed tokens: 13107200.0 | grad norm avg: 1.25 | grad norm last: 1.27 | 
2026-01-01T19:28:36 | step: 500 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 1.3539362953451928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.08 | train loss last: 4.22 | consumed tokens: 16384000.0 | grad norm avg: 1.2 | grad norm last: 1.11 | 
2026-01-01T19:29:04 | step: 600 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 1.692967998678796e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.06 | train loss last: 3.68 | consumed tokens: 19660800.0 | grad norm avg: 1.14 | grad norm last: 1.09 | 
2026-01-01T19:29:32 | step: 700 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 2.0662650058511645e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 4.02 | train loss last: 3.83 | consumed tokens: 22937600.0 | grad norm avg: 1.06 | grad norm last: 1.02 | 
2026-01-01T19:29:59 | step: 800 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 2.461726217006799e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.98 | train loss last: 3.96 | consumed tokens: 26214400.0 | grad norm avg: 0.98 | grad norm last: 0.94 | 
2026-01-01T19:30:27 | step: 900 | train samples/s: 278.1 | train mfu (16-bit): -1.0 | lr mean: 2.86653248622315e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.95 | train loss last: 3.73 | consumed tokens: 29491200.0 | grad norm avg: 0.9 | grad norm last: 0.91 | 
2026-01-01T19:30:55 | step: 1000 | train samples/s: 280.9 | train mfu (16-bit): -1.0 | lr mean: 3.2675612601451576e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.91 | train loss last: 3.91 | consumed tokens: 32768000.0 | grad norm avg: 0.86 | grad norm last: 0.78 | 
2026-01-01T19:31:23 | step: 1100 | train samples/s: 278.0 | train mfu (16-bit): -1.0 | lr mean: 3.651812221505679e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.9 | train loss last: 3.89 | consumed tokens: 36044800.0 | grad norm avg: 0.81 | grad norm last: 0.8 | 
2026-01-01T19:31:51 | step: 1200 | train samples/s: 274.7 | train mfu (16-bit): -1.0 | lr mean: 4.006829476566054e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.87 | train loss last: 3.95 | consumed tokens: 39321600.0 | grad norm avg: 0.78 | grad norm last: 0.72 | 
2026-01-01T19:32:19 | step: 1300 | train samples/s: 280.8 | train mfu (16-bit): -1.0 | lr mean: 4.321104643167928e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.85 | train loss last: 4.15 | consumed tokens: 42598400.0 | grad norm avg: 0.74 | grad norm last: 0.76 | 
2026-01-01T19:32:47 | step: 1400 | train samples/s: 281.0 | train mfu (16-bit): -1.0 | lr mean: 4.5844499254599214e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.83 | train loss last: 4.06 | consumed tokens: 45875200.0 | grad norm avg: 0.72 | grad norm last: 0.69 | 
2026-01-01T19:33:14 | step: 1500 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.788328806171194e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.82 | train loss last: 3.84 | consumed tokens: 49152000.0 | grad norm avg: 0.7 | grad norm last: 0.7 | 
2026-01-01T19:33:42 | step: 1600 | train samples/s: 277.4 | train mfu (16-bit): -1.0 | lr mean: 4.926131805405021e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.79 | train loss last: 3.86 | consumed tokens: 52428800.0 | grad norm avg: 0.67 | grad norm last: 0.66 | 
2026-01-01T19:34:10 | step: 1700 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.9933918489841744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.77 | train loss last: 3.99 | consumed tokens: 55705600.0 | grad norm avg: 0.66 | grad norm last: 0.66 | 
2026-01-01T19:34:38 | step: 1800 | train samples/s: 275.1 | train mfu (16-bit): -1.0 | lr mean: 4.999998782295734e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.76 | train loss last: 3.85 | consumed tokens: 58982400.0 | grad norm avg: 0.65 | grad norm last: 0.65 | 
2026-01-01T19:35:06 | step: 1900 | train samples/s: 277.8 | train mfu (16-bit): -1.0 | lr mean: 4.999989687348716e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.75 | train loss last: 3.92 | consumed tokens: 62259200.0 | grad norm avg: 0.64 | grad norm last: 0.63 | 
2026-01-01T19:35:34 | step: 2000 | train samples/s: 280.9 | train mfu (16-bit): -1.0 | lr mean: 4.9999725888483226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.74 | train loss last: 3.65 | consumed tokens: 65536000.0 | grad norm avg: 0.63 | grad norm last: 0.66 | 
2026-01-01T19:36:02 | step: 2100 | train samples/s: 280.8 | train mfu (16-bit): -1.0 | lr mean: 4.999947122996673e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.71 | train loss last: 3.55 | consumed tokens: 68812800.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T19:36:29 | step: 2200 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.999913289793767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.72 | train loss last: 3.6 | consumed tokens: 72089600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T19:36:57 | step: 2300 | train samples/s: 281.1 | train mfu (16-bit): -1.0 | lr mean: 4.9998714530374855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.7 | train loss last: 3.63 | consumed tokens: 75366400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:37:25 | step: 2400 | train samples/s: 275.2 | train mfu (16-bit): -1.0 | lr mean: 4.9998212489299476e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.69 | train loss last: 3.86 | consumed tokens: 78643200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:37:53 | step: 2500 | train samples/s: 275.0 | train mfu (16-bit): -1.0 | lr mean: 4.9997626774711534e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.67 | train loss last: 3.54 | consumed tokens: 81920000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:38:21 | step: 2600 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.999695738661103e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.67 | train loss last: 3.5 | consumed tokens: 85196800.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T19:38:49 | step: 2700 | train samples/s: 280.2 | train mfu (16-bit): -1.0 | lr mean: 4.999620796297677e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.64 | consumed tokens: 88473600.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T19:39:17 | step: 2800 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.9995374865829945e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.66 | train loss last: 3.95 | consumed tokens: 91750400.0 | grad norm avg: 0.59 | grad norm last: 0.63 | 
2026-01-01T19:39:44 | step: 2900 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.999445809517056e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.64 | train loss last: 3.51 | consumed tokens: 95027200.0 | grad norm avg: 0.61 | grad norm last: 0.69 | 
2026-01-01T19:40:12 | step: 3000 | train samples/s: 280.2 | train mfu (16-bit): -1.0 | lr mean: 4.999345765099861e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.56 | consumed tokens: 98304000.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T19:40:40 | step: 3100 | train samples/s: 275.2 | train mfu (16-bit): -1.0 | lr mean: 4.99923771712929e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.65 | train loss last: 3.93 | consumed tokens: 101580800.0 | grad norm avg: 0.6 | grad norm last: 0.59 | 
2026-01-01T19:41:09 | step: 3200 | train samples/s: 274.7 | train mfu (16-bit): -1.0 | lr mean: 4.999121301807463e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.62 | train loss last: 3.7 | consumed tokens: 104857600.0 | grad norm avg: 0.59 | grad norm last: 0.58 | 
2026-01-01T19:41:36 | step: 3300 | train samples/s: 280.1 | train mfu (16-bit): -1.0 | lr mean: 4.99899651913438e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.61 | train loss last: 3.54 | consumed tokens: 108134400.0 | grad norm avg: 0.59 | grad norm last: 0.6 | 
2026-01-01T19:42:04 | step: 3400 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.998863732907921e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.85 | consumed tokens: 111411200.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:42:32 | step: 3500 | train samples/s: 280.4 | train mfu (16-bit): -1.0 | lr mean: 4.998722579330206e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.68 | consumed tokens: 114688000.0 | grad norm avg: 0.59 | grad norm last: 0.54 | 
2026-01-01T19:43:00 | step: 3600 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.9985730584012344e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.45 | consumed tokens: 117964800.0 | grad norm avg: 0.59 | grad norm last: 0.59 | 
2026-01-01T19:43:27 | step: 3700 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.998415170121007e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.59 | train loss last: 3.61 | consumed tokens: 121241600.0 | grad norm avg: 0.58 | grad norm last: 0.55 | 
2026-01-01T19:43:55 | step: 3800 | train samples/s: 277.7 | train mfu (16-bit): -1.0 | lr mean: 4.9982489144895226e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.68 | consumed tokens: 124518400.0 | grad norm avg: 0.59 | grad norm last: 0.62 | 
2026-01-01T19:44:24 | step: 3900 | train samples/s: 272.7 | train mfu (16-bit): -1.0 | lr mean: 4.998074655304663e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.6 | train loss last: 3.46 | consumed tokens: 127795200.0 | grad norm avg: 0.6 | grad norm last: 0.71 | 
2026-01-01T19:44:51 | step: 4000 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.997892028768547e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.56 | consumed tokens: 131072000.0 | grad norm avg: 0.59 | grad norm last: 0.59 | 
2026-01-01T19:45:19 | step: 4100 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.997701398679055e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.58 | train loss last: 3.52 | consumed tokens: 134348800.0 | grad norm avg: 0.59 | grad norm last: 0.58 | 
2026-01-01T19:45:47 | step: 4200 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.9975020374404266e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.57 | consumed tokens: 137625600.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:46:15 | step: 4300 | train samples/s: 280.5 | train mfu (16-bit): -1.0 | lr mean: 4.9972946726484224e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.38 | consumed tokens: 140902400.0 | grad norm avg: 0.59 | grad norm last: 0.65 | 
2026-01-01T19:46:42 | step: 4400 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.9970793043030426e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.41 | consumed tokens: 144179200.0 | grad norm avg: 0.59 | grad norm last: 0.62 | 
2026-01-01T19:47:10 | step: 4500 | train samples/s: 278.0 | train mfu (16-bit): -1.0 | lr mean: 4.996855204808526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.57 | train loss last: 3.51 | consumed tokens: 147456000.0 | grad norm avg: 0.59 | grad norm last: 0.6 | 
2026-01-01T19:47:39 | step: 4600 | train samples/s: 272.6 | train mfu (16-bit): -1.0 | lr mean: 4.996623101760633e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.54 | train loss last: 3.45 | consumed tokens: 150732800.0 | grad norm avg: 0.59 | grad norm last: 0.67 | 
2026-01-01T19:48:07 | step: 4700 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9963826313614845e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.56 | train loss last: 3.48 | consumed tokens: 154009600.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:48:34 | step: 4800 | train samples/s: 280.3 | train mfu (16-bit): -1.0 | lr mean: 4.9961337936110795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.54 | train loss last: 3.48 | consumed tokens: 157286400.0 | grad norm avg: 0.6 | grad norm last: 0.6 | 
2026-01-01T19:49:02 | step: 4900 | train samples/s: 280.7 | train mfu (16-bit): -1.0 | lr mean: 4.995876952307299e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.55 | train loss last: 3.44 | consumed tokens: 160563200.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:49:30 | step: 5000 | train samples/s: 280.6 | train mfu (16-bit): -1.0 | lr mean: 4.995611743652262e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.58 | consumed tokens: 163840000.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T19:50:00 | step: 5100 | train samples/s: 276.7 | train mfu (16-bit): -1.0 | lr mean: 4.9953381676459685e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.28 | consumed tokens: 167116800.0 | grad norm avg: 0.59 | grad norm last: 0.56 | 
2026-01-01T19:50:28 | step: 5200 | train samples/s: 276.4 | train mfu (16-bit): -1.0 | lr mean: 4.9950565880862996e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.61 | consumed tokens: 170393600.0 | grad norm avg: 0.6 | grad norm last: 0.64 | 
2026-01-01T19:50:56 | step: 5300 | train samples/s: 276.5 | train mfu (16-bit): -1.0 | lr mean: 4.9947666411753744e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.53 | train loss last: 3.5 | consumed tokens: 173670400.0 | grad norm avg: 0.6 | grad norm last: 0.63 | 
2026-01-01T19:51:24 | step: 5400 | train samples/s: 277.2 | train mfu (16-bit): -1.0 | lr mean: 4.994468326913193e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.55 | consumed tokens: 176947200.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:51:51 | step: 5500 | train samples/s: 279.2 | train mfu (16-bit): -1.0 | lr mean: 4.994162009097636e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.52 | train loss last: 3.46 | consumed tokens: 180224000.0 | grad norm avg: 0.59 | grad norm last: 0.57 | 
2026-01-01T19:52:19 | step: 5600 | train samples/s: 279.4 | train mfu (16-bit): -1.0 | lr mean: 4.9938469601329416e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.54 | consumed tokens: 183500800.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:52:47 | step: 5700 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.9935242714127526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.46 | consumed tokens: 186777600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T19:53:15 | step: 5800 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9931928515434265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.49 | consumed tokens: 190054400.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:53:43 | step: 5900 | train samples/s: 276.3 | train mfu (16-bit): -1.0 | lr mean: 4.992853428120725e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.36 | consumed tokens: 193331200.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:54:11 | step: 6000 | train samples/s: 277.0 | train mfu (16-bit): -1.0 | lr mean: 4.992505637346767e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.51 | train loss last: 3.48 | consumed tokens: 196608000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T19:54:39 | step: 6100 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 4.9921494792215526e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.27 | consumed tokens: 199884800.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T19:55:07 | step: 6200 | train samples/s: 277.5 | train mfu (16-bit): -1.0 | lr mean: 4.991785317542963e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.54 | consumed tokens: 203161600.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T19:55:35 | step: 6300 | train samples/s: 279.6 | train mfu (16-bit): -1.0 | lr mean: 4.9914127885131165e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.5 | consumed tokens: 206438400.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T19:56:03 | step: 6400 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.991031892132014e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.6 | consumed tokens: 209715200.0 | grad norm avg: 0.6 | grad norm last: 0.55 | 
2026-01-01T19:56:31 | step: 6500 | train samples/s: 279.7 | train mfu (16-bit): -1.0 | lr mean: 4.990642992197536e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.5 | train loss last: 3.73 | consumed tokens: 212992000.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T19:56:59 | step: 6600 | train samples/s: 273.9 | train mfu (16-bit): -1.0 | lr mean: 4.9902457249118015e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.48 | train loss last: 3.71 | consumed tokens: 216268800.0 | grad norm avg: 0.6 | grad norm last: 0.58 | 
2026-01-01T19:57:27 | step: 6700 | train samples/s: 279.5 | train mfu (16-bit): -1.0 | lr mean: 4.9898404540726915e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.49 | train loss last: 3.34 | consumed tokens: 219545600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T19:57:55 | step: 6800 | train samples/s: 279.7 | train mfu (16-bit): -1.0 | lr mean: 4.989426815882325e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.48 | train loss last: 3.39 | consumed tokens: 222822400.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:58:23 | step: 6900 | train samples/s: 277.4 | train mfu (16-bit): -1.0 | lr mean: 4.9890048103407025e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.38 | consumed tokens: 226099200.0 | grad norm avg: 0.6 | grad norm last: 0.64 | 
2026-01-01T19:58:50 | step: 7000 | train samples/s: 279.9 | train mfu (16-bit): -1.0 | lr mean: 4.9885744374478236e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.46 | train loss last: 3.48 | consumed tokens: 229376000.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T19:59:18 | step: 7100 | train samples/s: 273.8 | train mfu (16-bit): -1.0 | lr mean: 4.988136061001569e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.68 | consumed tokens: 232652800.0 | grad norm avg: 0.62 | grad norm last: 0.55 | 
2026-01-01T19:59:46 | step: 7200 | train samples/s: 273.3 | train mfu (16-bit): -1.0 | lr mean: 4.987689317204058e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.38 | consumed tokens: 235929600.0 | grad norm avg: 0.61 | grad norm last: 0.54 | 
2026-01-01T20:00:15 | step: 7300 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.987234569853172e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.47 | train loss last: 3.41 | consumed tokens: 239206400.0 | grad norm avg: 0.6 | grad norm last: 0.53 | 
2026-01-01T20:00:43 | step: 7400 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.986771455151029e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.26 | consumed tokens: 242483200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:01:11 | step: 7500 | train samples/s: 273.1 | train mfu (16-bit): -1.0 | lr mean: 4.98629997309763e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.46 | train loss last: 3.29 | consumed tokens: 245760000.0 | grad norm avg: 0.61 | grad norm last: 0.55 | 
2026-01-01T20:01:39 | step: 7600 | train samples/s: 268.6 | train mfu (16-bit): -1.0 | lr mean: 4.985820487490855e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.37 | consumed tokens: 249036800.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:02:08 | step: 7700 | train samples/s: 261.8 | train mfu (16-bit): -1.0 | lr mean: 4.985332634532824e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.54 | consumed tokens: 252313600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:02:36 | step: 7800 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.9848367780214176e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.26 | consumed tokens: 255590400.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:03:05 | step: 7900 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.9843325541587546e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.45 | consumed tokens: 258867200.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:03:34 | step: 8000 | train samples/s: 260.3 | train mfu (16-bit): -1.0 | lr mean: 4.9838199629448354e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.37 | consumed tokens: 262144000.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:04:02 | step: 8100 | train samples/s: 269.0 | train mfu (16-bit): -1.0 | lr mean: 4.9832993681775406e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.51 | consumed tokens: 265420800.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:04:30 | step: 8200 | train samples/s: 271.5 | train mfu (16-bit): -1.0 | lr mean: 4.9827704060589895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.32 | consumed tokens: 268697600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:04:57 | step: 8300 | train samples/s: 276.9 | train mfu (16-bit): -1.0 | lr mean: 4.982233076589182e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 271974400.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:05:26 | step: 8400 | train samples/s: 261.7 | train mfu (16-bit): -1.0 | lr mean: 4.981687743565999e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 275251200.0 | grad norm avg: 0.61 | grad norm last: 0.58 | 
2026-01-01T20:05:55 | step: 8500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.98113440698944e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.62 | consumed tokens: 278528000.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:06:23 | step: 8600 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.9805727030616254e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.44 | train loss last: 3.58 | consumed tokens: 281804800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:06:52 | step: 8700 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.980002631782554e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.53 | consumed tokens: 285081600.0 | grad norm avg: 0.61 | grad norm last: 0.56 | 
2026-01-01T20:07:21 | step: 8800 | train samples/s: 267.0 | train mfu (16-bit): -1.0 | lr mean: 4.9794241931522265e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.42 | consumed tokens: 288358400.0 | grad norm avg: 0.61 | grad norm last: 0.67 | 
2026-01-01T20:07:49 | step: 8900 | train samples/s: 266.1 | train mfu (16-bit): -1.0 | lr mean: 4.978838114766404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.45 | train loss last: 3.57 | consumed tokens: 291635200.0 | grad norm avg: 0.61 | grad norm last: 0.59 | 
2026-01-01T20:08:18 | step: 9000 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.9782433052314445e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.45 | consumed tokens: 294912000.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:08:47 | step: 9100 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.9776404921431094e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.36 | consumed tokens: 298188800.0 | grad norm avg: 0.6 | grad norm last: 0.59 | 
2026-01-01T20:09:15 | step: 9200 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.977029675501399e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.32 | consumed tokens: 301465600.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:09:44 | step: 9300 | train samples/s: 261.3 | train mfu (16-bit): -1.0 | lr mean: 4.976410491508432e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.5 | consumed tokens: 304742400.0 | grad norm avg: 0.61 | grad norm last: 0.65 | 
2026-01-01T20:10:13 | step: 9400 | train samples/s: 267.3 | train mfu (16-bit): -1.0 | lr mean: 4.9757829401642084e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.55 | consumed tokens: 308019200.0 | grad norm avg: 0.6 | grad norm last: 0.62 | 
2026-01-01T20:10:41 | step: 9500 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9751473852666095e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.71 | consumed tokens: 311296000.0 | grad norm avg: 0.61 | grad norm last: 0.6 | 
2026-01-01T20:11:10 | step: 9600 | train samples/s: 266.8 | train mfu (16-bit): -1.0 | lr mean: 4.974503826815635e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.26 | consumed tokens: 314572800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:11:39 | step: 9700 | train samples/s: 263.5 | train mfu (16-bit): -1.0 | lr mean: 4.973851901013404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.5 | consumed tokens: 317849600.0 | grad norm avg: 0.61 | grad norm last: 0.64 | 
2026-01-01T20:12:07 | step: 9800 | train samples/s: 265.6 | train mfu (16-bit): -1.0 | lr mean: 4.973191607859917e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.43 | consumed tokens: 321126400.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
2026-01-01T20:12:36 | step: 9900 | train samples/s: 264.0 | train mfu (16-bit): -1.0 | lr mean: 4.972523311153054e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.42 | train loss last: 3.14 | consumed tokens: 324403200.0 | grad norm avg: 0.61 | grad norm last: 0.84 | 
2026-01-01T20:13:05 | step: 10000 | train samples/s: 259.9 | train mfu (16-bit): -1.0 | lr mean: 4.971846647094935e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.43 | train loss last: 3.47 | consumed tokens: 327680000.0 | grad norm avg: 0.62 | grad norm last: 0.53 | 
2026-01-01T20:13:35 | step: 10100 | train samples/s: 267.2 | train mfu (16-bit): -1.0 | lr mean: 4.9711619794834405e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.2 | consumed tokens: 330956800.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:14:04 | step: 10200 | train samples/s: 266.7 | train mfu (16-bit): -1.0 | lr mean: 4.97046930831857e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.4 | consumed tokens: 334233600.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:14:32 | step: 10300 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.969768269802444e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.51 | consumed tokens: 337510400.0 | grad norm avg: 0.61 | grad norm last: 0.63 | 
2026-01-01T20:15:01 | step: 10400 | train samples/s: 264.8 | train mfu (16-bit): -1.0 | lr mean: 4.969058863935061e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.23 | consumed tokens: 340787200.0 | grad norm avg: 0.62 | grad norm last: 0.71 | 
2026-01-01T20:15:29 | step: 10500 | train samples/s: 264.3 | train mfu (16-bit): -1.0 | lr mean: 4.968341454514302e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.16 | consumed tokens: 344064000.0 | grad norm avg: 0.6 | grad norm last: 0.56 | 
2026-01-01T20:15:58 | step: 10600 | train samples/s: 261.1 | train mfu (16-bit): -1.0 | lr mean: 4.967616041540168e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.6 | consumed tokens: 347340800.0 | grad norm avg: 0.6 | grad norm last: 0.61 | 
2026-01-01T20:16:27 | step: 10700 | train samples/s: 262.5 | train mfu (16-bit): -1.0 | lr mean: 4.966882261214778e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.63 | consumed tokens: 350617600.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:16:56 | step: 10800 | train samples/s: 264.7 | train mfu (16-bit): -1.0 | lr mean: 4.966140477336012e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.46 | consumed tokens: 353894400.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T20:17:24 | step: 10900 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9653903261059895e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.41 | consumed tokens: 357171200.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:17:53 | step: 11000 | train samples/s: 266.5 | train mfu (16-bit): -1.0 | lr mean: 4.9646321713225916e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.41 | train loss last: 3.54 | consumed tokens: 360448000.0 | grad norm avg: 0.62 | grad norm last: 0.58 | 
2026-01-01T20:18:22 | step: 11100 | train samples/s: 263.8 | train mfu (16-bit): -1.0 | lr mean: 4.9638656491879374e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.72 | consumed tokens: 363724800.0 | grad norm avg: 0.62 | grad norm last: 0.63 | 
2026-01-01T20:18:50 | step: 11200 | train samples/s: 265.1 | train mfu (16-bit): -1.0 | lr mean: 4.9630911234999076e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.4 | train loss last: 3.15 | consumed tokens: 367001600.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:19:19 | step: 11300 | train samples/s: 259.3 | train mfu (16-bit): -1.0 | lr mean: 4.962308594258502e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.39 | train loss last: 3.38 | consumed tokens: 370278400.0 | grad norm avg: 0.62 | grad norm last: 0.7 | 
2026-01-01T20:19:48 | step: 11400 | train samples/s: 261.4 | train mfu (16-bit): -1.0 | lr mean: 4.9615176976658404e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.25 | consumed tokens: 373555200.0 | grad norm avg: 0.62 | grad norm last: 0.68 | 
2026-01-01T20:20:17 | step: 11500 | train samples/s: 266.3 | train mfu (16-bit): -1.0 | lr mean: 4.960718797519803e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.52 | consumed tokens: 376832000.0 | grad norm avg: 0.61 | grad norm last: 0.62 | 
2026-01-01T20:20:45 | step: 11600 | train samples/s: 263.7 | train mfu (16-bit): -1.0 | lr mean: 4.9599115300225094e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.12 | consumed tokens: 380108800.0 | grad norm avg: 0.62 | grad norm last: 0.64 | 
2026-01-01T20:21:14 | step: 11700 | train samples/s: 264.5 | train mfu (16-bit): -1.0 | lr mean: 4.959096622769721e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.43 | consumed tokens: 383385600.0 | grad norm avg: 0.61 | grad norm last: 0.57 | 
2026-01-01T20:21:42 | step: 11800 | train samples/s: 264.9 | train mfu (16-bit): -1.0 | lr mean: 4.958272984367795e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.21 | consumed tokens: 386662400.0 | grad norm avg: 0.61 | grad norm last: 0.66 | 
2026-01-01T20:22:11 | step: 11900 | train samples/s: 265.2 | train mfu (16-bit): -1.0 | lr mean: 4.957441706210375e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.1 | consumed tokens: 389939200.0 | grad norm avg: 0.61 | grad norm last: 0.61 | 
2026-01-01T20:22:40 | step: 12000 | train samples/s: 262.4 | train mfu (16-bit): -1.0 | lr mean: 4.956602060701698e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.27 | consumed tokens: 393216000.0 | grad norm avg: 0.62 | grad norm last: 0.65 | 
2026-01-01T20:23:09 | step: 12100 | train samples/s: 262.2 | train mfu (16-bit): -1.0 | lr mean: 4.955754047841765e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 396492800.0 | grad norm avg: 0.61 | grad norm last: 0.71 | 
2026-01-01T20:23:37 | step: 12200 | train samples/s: 266.4 | train mfu (16-bit): -1.0 | lr mean: 4.954898395226337e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.18 | consumed tokens: 399769600.0 | grad norm avg: 0.62 | grad norm last: 0.59 | 
2026-01-01T20:24:06 | step: 12300 | train samples/s: 265.9 | train mfu (16-bit): -1.0 | lr mean: 4.954034375259653e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.37 | train loss last: 3.5 | consumed tokens: 403046400.0 | grad norm avg: 0.62 | grad norm last: 0.54 | 
2026-01-01T20:24:35 | step: 12400 | train samples/s: 263.0 | train mfu (16-bit): -1.0 | lr mean: 4.953161987941712e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.57 | consumed tokens: 406323200.0 | grad norm avg: 0.62 | grad norm last: 0.54 | 
2026-01-01T20:25:03 | step: 12500 | train samples/s: 265.7 | train mfu (16-bit): -1.0 | lr mean: 4.952281597070396e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.41 | consumed tokens: 409600000.0 | grad norm avg: 0.61 | grad norm last: 0.54 | 
2026-01-01T20:25:32 | step: 12600 | train samples/s: 267.4 | train mfu (16-bit): -1.0 | lr mean: 4.951393566443585e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.36 | train loss last: 3.26 | consumed tokens: 412876800.0 | grad norm avg: 0.6 | grad norm last: 0.57 | 
2026-01-01T20:26:00 | step: 12700 | train samples/s: 265.5 | train mfu (16-bit): -1.0 | lr mean: 4.950496804667637e-05 | peak memory rank 0 (MB): 2511.46 | train loss avg: 3.38 | train loss last: 3.39 | consumed tokens: 416153600.0 | grad norm avg: 0.62 | grad norm last: 0.61 | 
                                                                                                                                                                                                                                                                                       